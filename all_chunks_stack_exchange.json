[
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/44948/402101 \n Viewed through the lens of probability inequalities and connections to the multiple-observation case, this result might not seem so impossible, or, at least, it might seem more plausible.\n\nLet $\\renewcommand{\\Pr}{\\mathbb P}\\newcommand{\\Ind}[1]{\\mathbf 1_{(#1)}}X \\sim \\mathcal N(\\mu,\\sigma^2)$ with $\\mu$ and $\\sigma^2$ unknown. We can write $X = \\sigma  Z + \\mu$ for $Z \\sim \\mathcal N(0,1)$.\n\nMain Claim\n: $[0,X^2/q_\\alpha)$ is a $(1-\\alpha)$ confidence interval for $\\sigma^2$ where $q_\\alpha$ is the $\\alpha$-level\n  quantile of a chi-squared distribution with one degree of freedom. Furthermore, since this interval has \nexactly\n $(1-\\alpha)$ coverage when $\\mu = 0$, it is the narrowest possible interval of the form $[0,b X^2)$ for some $b \\in \\mathbb R$.\n\nA reason for optimism\n\nRecall that in the $n \\geq 2$ case, with $T = \\sum_{i=1}^n (X_i - \\bar X)^2$, the \ntypical\n $(1-\\alpha)$ confidence interval for $\\sigma^2$ is\n$$\n\\Big(\\frac{T}{q_{n-1,(1-\\alpha)/2}}, \\frac{T}{q_{n-1,\\alpha/2}} \\Big) \\>,\n$$\nwhere $q_{k,a}$ is the $a$-level quantile of a chi-squared with $k$ degrees of freedom. This, of course, holds for any $\\mu$. While this is the \nmost popular\n interval (called the \nequal-tailed interval\n for obvious reasons), it is neither the only one nor even the one of smallest width! As should be apparent, another valid selection is\n$$\n\\Big(0,\\frac{T}{q_{n-1,\\alpha}}\\Big) \\>.\n$$\n\nSince, $T \\leq \\sum_{i=1}^n X_i^2$, then\n$$\n\\Big(0,\\frac{\\sum_{i=1}^n X_i^2}{q_{n-1,\\alpha}}\\Big) \\>,\n$$\nalso has coverage of at least $(1-\\alpha)$.\n\nViewed in this light, we might then be optimistic that the interval in the main claim is true for $n = 1$. The main difference is that there is no zero-degree-of-freedom chi-squared distribution for the case of a single observation, so we must hope that using a one-degree-of-freedom quantile will work.\n\nA half step toward our destination\n (\nExploiting the right tail\n)\n\nBefore diving into a proof of the main claim, let's first look at a preliminary claim that is not nearly as strong or satisfying statistically, but perhaps gives some additional insight into what is going on. You can skip down to the proof of the main claim below, without much (if any) loss. In this section and the next, the proofs\u2014while slightly subtle\u2014are based on only elementary facts: monotonicity of probabilities, and symmetry and unimodality of the normal distribution.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/44948/402101 \n Before diving into a proof of the main claim, let's first look at a preliminary claim that is not nearly as strong or satisfying statistically, but perhaps gives some additional insight into what is going on. You can skip down to the proof of the main claim below, without much (if any) loss. In this section and the next, the proofs\u2014while slightly subtle\u2014are based on only elementary facts: monotonicity of probabilities, and symmetry and unimodality of the normal distribution.\n\nAuxiliary claim\n: $[0,X^2/z^2_\\alpha)$ is a $(1-\\alpha)$ confidence interval for $\\sigma^2$ as long as $\\alpha > 1/2$. Here $z_\\alpha$ is the $\\alpha$-level quantile of a standard normal.\n\nProof\n. $|X| = |-X|$ and $|\\sigma Z + \\mu| \\stackrel{d}{=} |-\\sigma Z+\\mu|$ by symmetry, so in what follows we can take $\\mu \\geq 0$ without loss of generality. Now, for $\\theta \\geq 0$ and $\\mu \\geq 0$,\n$$\n\\Pr(|X| > \\theta) \\geq \\Pr( X > \\theta) = \\Pr( \\sigma Z + \\mu > \\theta) \\geq \\Pr( Z > \\theta/\\sigma) \\>,\n$$\nand so with $\\theta = z_{\\alpha} \\sigma$, we see that \n$$\n\\Pr(0 \\leq \\sigma^2 < X^2 / z^2_\\alpha) \\geq 1 - \\alpha \\>.\n$$\nThis works only for $\\alpha > 1/2$, since that is what is needed for $z_\\alpha > 0$.\n\nThis proves the auxiliary claim. While illustrative, it is unsatifying from a statistical perspective since it requires an absurdly large $\\alpha$ to work.\n\nProving the main claim\n\nA refinement of the above argument leads to a result that will work for an arbitrary confidence level. First, note that\n$$\n\\Pr(|X| > \\theta) = \\Pr(|Z + \\mu/\\sigma| > \\theta / \\sigma ) \\>.\n$$\nSet $a = \\mu/\\sigma \\geq 0$ and $b = \\theta / \\sigma \\geq 0$. Then,\n$$\n\\Pr(|Z + a| > b) = \\Phi(a-b) + \\Phi(-a-b) \\>.\n$$\nIf we can show that the right-hand side increases in $a$ for every fixed $b$, then we can employ a similar argument as in the previous argument. This is at least plausible, since we'd like to believe that if the mean increases, then it becomes more probable that we see a value with a modulus that exceeds $b$. (However, we have to watch out for how quickly the mass is decreasing in the left tail!)\n\nSet $f_b(a) = \\Phi(a-b) + \\Phi(-a-b)$. Then\n$$\nf'_b(a) = \\varphi(a-b) - \\varphi(-a-b) = \\varphi(a-b) - \\varphi(a+b) \\>.\n$$\nNote that $f'_b(0) = 0$ and for positive $u$, $\\varphi(u)$ is decreasing in $u$. Now, for $a \\in (0,2b)$, it is easy to see that $\\varphi(a-b) \\geq \\varphi(-b) = \\varphi(b)$. These facts taken together easily imply that\n$$\nf'_b(a) \\geq 0\n$$\nfor all $a \\geq 0$ and any fixed $b \\geq 0$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/44948/402101 \n Set $f_b(a) = \\Phi(a-b) + \\Phi(-a-b)$. Then\n$$\nf'_b(a) = \\varphi(a-b) - \\varphi(-a-b) = \\varphi(a-b) - \\varphi(a+b) \\>.\n$$\nNote that $f'_b(0) = 0$ and for positive $u$, $\\varphi(u)$ is decreasing in $u$. Now, for $a \\in (0,2b)$, it is easy to see that $\\varphi(a-b) \\geq \\varphi(-b) = \\varphi(b)$. These facts taken together easily imply that\n$$\nf'_b(a) \\geq 0\n$$\nfor all $a \\geq 0$ and any fixed $b \\geq 0$.\n\nHence, we have shown that for $a \\geq 0$ and $b \\geq 0$,\n$$\n\\Pr(|Z + a| > b) \\geq \\Pr(|Z| > b) = 2\\Phi(-b) \\>.\n$$\n\nUnraveling all of this, if we take $\\theta = \\sqrt{q_\\alpha} \\sigma$, we get\n$$\n\\Pr(X^2 > q_\\alpha \\sigma^2) \\geq \\Pr(Z^2  > q_\\alpha) = 1 - \\alpha \\>,\n$$\nwhich establishes the main claim.\n\nClosing remark\n: A careful reading of the above argument shows that it uses only the symmetric and unimodal properties of the normal distribution. Hence, the approach works analogously for obtaining confidence intervals from a single observation from any symmetric unimodal location-scale family, e.g., Cauchy or Laplace distributions.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/80579/402101 \n Although collinearity (of predictor variables) is a possible explanation, I would like to suggest it is not an illuminating explanation because we know collinearity is related to \"common information\" among the predictors, so there is nothing mysterious or counter-intuitive about the side effect of introducing a second correlated predictor into the model.\n\nLet us then consider the case of \ntwo predictors that are truly orthogonal\n: there is absolutely no collinearity among them.  A remarkable change in significance can still happen.\n\nDesignate the predictor variables $X_1$ and $X_2$ and let $Y$ name the predictand.  The regression of $Y$ against $X_1$ will fail to be significant when the variation in $Y$ around its mean is not appreciably reduced when $X_1$ is used as the independent variable.  \nWhen that variation is strongly associated with a second variable $X_2$,\n however, the situation changes.  Recall that multiple regression of $Y$ against $X_1$ and $X_2$ is equivalent to\n\nSeparately regress $Y$ and $X_1$ against $X_2$.\n\n\nRegress the $Y$ residuals against the $X_1$ residuals.\n\nSeparately regress $Y$ and $X_1$ against $X_2$.\n\nRegress the $Y$ residuals against the $X_1$ residuals.\n\nThe residuals from the first step have removed the effect of $X_2$.  When $X_2$ is closely correlated with $Y$, this can expose a relatively small amount of variation that had previously been masked.  If \nthis\n variation is associated with $X_1$, we obtain a significant result.\n\nAll this might perhaps be clarified with a concrete example.\n  To begin, let's use \nR\n to generate two orthogonal independent variables along with some independent random error $\\varepsilon$:\n\nR\n\nn <- 32\nset.seed(182)\nu <-matrix(rnorm(2*n), ncol=2)\nu0 <- cbind(u[,1] - mean(u[,1]), u[,2] - mean(u[,2]))\nx <- svd(u0)$u\neps <- rnorm(n)\n\nn <- 32\nset.seed(182)\nu <-matrix(rnorm(2*n), ncol=2)\nu0 <- cbind(u[,1] - mean(u[,1]), u[,2] - mean(u[,2]))\nx <- svd(u0)$u\neps <- rnorm(n)\n\n(The \nsvd\n step assures the two columns of matrix \nx\n (representing $X_1$ and $X_2$) are orthogonal, ruling out collinearity as a possible explanation of any subsequent results.)\n\nsvd\n\nx\n\nNext, create $Y$ as a linear combination of the $X$'s and the error.  I have adjusted the coefficients to produce the counter-intuitive behavior:\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\nThis is a realization of the model $Y \\sim_{iid} N(0.05 X_1 + 1.00 X_2, 0.01^2)$ with $n=32$ cases.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/80579/402101 \n svd\n\nx\n\nNext, create $Y$ as a linear combination of the $X$'s and the error.  I have adjusted the coefficients to produce the counter-intuitive behavior:\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\nThis is a realization of the model $Y \\sim_{iid} N(0.05 X_1 + 1.00 X_2, 0.01^2)$ with $n=32$ cases.\n\nLook at the two regressions in question.  \nFirst\n, regress $Y$ against $X_1$ only:\n\n> summary(lm(y ~ x[,1]))\n...\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.002576   0.032423  -0.079    0.937\nx[, 1]       0.068950   0.183410   0.376    0.710\n\n> summary(lm(y ~ x[,1]))\n...\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.002576   0.032423  -0.079    0.937\nx[, 1]       0.068950   0.183410   0.376    0.710\n\nThe high p-value of 0.710 shows that $X_1$ is completely non-significant.\n\nNext\n, regress $Y$ against $X_1$ and $X_2$:\n\n> summary(lm(y ~ x))\n...\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.002576   0.001678  -1.535    0.136    \nx1           0.068950   0.009490   7.265 5.32e-08 ***\nx2           1.003276   0.009490 105.718  < 2e-16 ***\n\n> summary(lm(y ~ x))\n...\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.002576   0.001678  -1.535    0.136    \nx1           0.068950   0.009490   7.265 5.32e-08 ***\nx2           1.003276   0.009490 105.718  < 2e-16 ***\n\nSuddenly, in the presence of $X_2$, $X_1$ is \nstrongly\n significant, as indicated by the near-zero p-values for both variables.\n\nWe can visualize this behavior\n by means of a scatterplot matrix of the variables $X_1$, $X_2$, and $Y$ along with the \nresiduals\n used in the two-step characterization of multiple regression above.  Because $X_1$ and $X_2$ are orthogonal, the $X_1$ residuals will be the same as $X_1$ and therefore need not be redrawn.  We will include the residuals of $Y$ against $X_2$ in the scatterplot matrix, giving this figure:\n\nlmy <- lm(y ~ x[,2])\nd <- data.frame(X1=x[,1], X2=x[,2], Y=y, RY=residuals(lmy))\nplot(d)\n\nlmy <- lm(y ~ x[,2])\nd <- data.frame(X1=x[,1], X2=x[,2], Y=y, RY=residuals(lmy))\nplot(d)\n\nHere is a rendering of it (with a little prettification):\n\n\n\nThis matrix of graphics has four rows and four columns, which I will count down from the top and from left to right.\n\nNotice:\n\nThe $(X_1, X_2)$ scatterplot in the second row and first column confirms the orthogonality of these predictors: the least squares line is horizontal and correlation is zero.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/80579/402101 \n lmy <- lm(y ~ x[,2])\nd <- data.frame(X1=x[,1], X2=x[,2], Y=y, RY=residuals(lmy))\nplot(d)\n\nHere is a rendering of it (with a little prettification):\n\n\n\nThis matrix of graphics has four rows and four columns, which I will count down from the top and from left to right.\n\nNotice:\n\nThe $(X_1, X_2)$ scatterplot in the second row and first column confirms the orthogonality of these predictors: the least squares line is horizontal and correlation is zero.\n\n\nThe $(X_1, Y)$ scatterplot in the third row and first column exhibits the slight but completely insignificant relationship reported by the first regression of $Y$ against $X_1$.  (The correlation coefficient, $\\rho$, is only $0.07$).\n\n\nThe $(X_2, Y)$ scatterplot in the third row and second column shows the strong relationship between $Y$ and the second independent variable.  (The correlation coefficient is $0.996$).\n\n\nThe fourth row examines the relationships between the \nresiduals\n of $Y$ (regressed against $X_2$) and other variables:\n\n\n\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.\n\n\nThe residuals \nare\n strongly correlated with $X_1$ ($\\rho = 0.80$).  The regression against $X_2$ has unmasked this previously hidden behavior.\n\n\nBy construction, there is no remaining correlation between the residuals and $X_2$.\n\n\nThere is little correlation between $Y$ and these residuals ($\\rho = 0.09$).  This shows how the residuals can behave entirely differently than $Y$ itself.  \nThat's\n how $X_1$ can suddenly be revealed as a significant contributor to the regression.\n\nThe $(X_1, X_2)$ scatterplot in the second row and first column confirms the orthogonality of these predictors: the least squares line is horizontal and correlation is zero.\n\nThe $(X_1, Y)$ scatterplot in the third row and first column exhibits the slight but completely insignificant relationship reported by the first regression of $Y$ against $X_1$.  (The correlation coefficient, $\\rho$, is only $0.07$).\n\nThe $(X_2, Y)$ scatterplot in the third row and second column shows the strong relationship between $Y$ and the second independent variable.  (The correlation coefficient is $0.996$).\n\nThe fourth row examines the relationships between the \nresiduals\n of $Y$ (regressed against $X_2$) and other variables:\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/80579/402101 \n The $(X_2, Y)$ scatterplot in the third row and second column shows the strong relationship between $Y$ and the second independent variable.  (The correlation coefficient is $0.996$).\n\nThe fourth row examines the relationships between the \nresiduals\n of $Y$ (regressed against $X_2$) and other variables:\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.\n\n\nThe residuals \nare\n strongly correlated with $X_1$ ($\\rho = 0.80$).  The regression against $X_2$ has unmasked this previously hidden behavior.\n\n\nBy construction, there is no remaining correlation between the residuals and $X_2$.\n\n\nThere is little correlation between $Y$ and these residuals ($\\rho = 0.09$).  This shows how the residuals can behave entirely differently than $Y$ itself.  \nThat's\n how $X_1$ can suddenly be revealed as a significant contributor to the regression.\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.\n\nThe residuals \nare\n strongly correlated with $X_1$ ($\\rho = 0.80$).  The regression against $X_2$ has unmasked this previously hidden behavior.\n\nBy construction, there is no remaining correlation between the residuals and $X_2$.\n\nThere is little correlation between $Y$ and these residuals ($\\rho = 0.09$).  This shows how the residuals can behave entirely differently than $Y$ itself.  \nThat's\n how $X_1$ can suddenly be revealed as a significant contributor to the regression.\n\nFinally, it is worth remarking that the two estimates of the $X_1$ coefficient (both equal to $0.06895$, not far from the intended value of $0.05$) agree \nonly\n because $X_1$ and $X_2$ are orthogonal.  Except in designed experiments, it is rare for orthogonality to hold exactly.  A departure from orthogonality usually causes the coefficient estimates to change.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/100050/402101 \n The correction is called \nBessel's correction\n and it has a mathematical proof. Personally, I was taught it the easy way: using $n-1$ is how you correct the bias of $E[\\frac{1}{n}\\sum_1^n(x_i - \\bar x)^2]$ (see \nhere\n).\n\nYou can also explain the correction based on the concept of degrees of freedom, simulation isn't strictly needed.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/97522/402101 \n The point is that sometimes, different models (for the same data) can lead to likelihood functions which differ by a multiplicative constant, but the information content must clearly be the same. An example:\n\nWe model \n$n$\n independent Bernoulli experiments, leading to data \n$X_1, \\dots, X_n$\n, each with a Bernoulli distribution with (probability) parameter \n$p$\n. This leads to the likelihood function\n\n$$\n   \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}\n$$\n\nOr we can summarize the data by the binomially distributed variable \n$Y=X_1+X_2+\\dotsm+X_n$\n, which has a binomial distribution, leading to the likelihood function\n\n$$\n   \\binom{n}{y} p^y (1-p)^{n-y}\n$$\n\nwhich, as a function of the unknown parameter \n$p$\n, is proportional to the former likelihood function.  The two likelihood functions clearly contains the same information, and should lead to the same inferences!\n\nAnd indeed, by definition, they are considered the same likelihood function.\n\nAnother viewpoint:  observe that when the likelihood functions are used in Bayes theorem, as needed for bayesian analysis, such multiplicative constants simply cancel!  so they are clearly irrelevant to bayesian inference.  Likewise, it will cancel when calculating likelihood ratios, as used in optimal hypothesis tests (Neyman-Pearson lemma.)  And it will have no influence on the value of maximum likelihood estimators. So we can see that in much of frequentist inference it cannot play a role.\n\nWe can argue from still another viewpoint.  The Bernoulli probability function (hereafter we use the term \"density\") above is really a density with respect to counting measure, that is, the measure on the non-negative integers with mass one for each non-negative integer.  But we could have defined a density with respect to some other dominating measure. In this example this will seem (and is) artificial, but in larger spaces (function spaces) it is really fundamental! Let us, for the purpose of illustration, use the specific geometric distribution, written \n$\\lambda$\n, with \n$\\lambda(0)=1/2$\n, \n$\\lambda(1)=1/4$\n, \n$\\lambda(2)=1/8$\n and so on. Then the density of the Bernoulli distribution \nwith respect to \n$\\lambda$\n is given by\n\n$$\n   f_{\\lambda}(x) = p^x (1-p)^{1-x}\\cdot 2^{x+1}\n$$\n\nmeaning that \n$$\n   P(X=x)= f_\\lambda(x) \\cdot \\lambda(x)\n$$\n\nWith this new, dominating, measure, the likelihood function becomes (with notation from above) \n\n$$\n   \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} 2^{x_i+1} = p^y (1-p)^{n-y} 2^{y+n}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/97522/402101 \n $$\n   f_{\\lambda}(x) = p^x (1-p)^{1-x}\\cdot 2^{x+1}\n$$\n\nmeaning that \n$$\n   P(X=x)= f_\\lambda(x) \\cdot \\lambda(x)\n$$\n\nWith this new, dominating, measure, the likelihood function becomes (with notation from above) \n\n$$\n   \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} 2^{x_i+1} = p^y (1-p)^{n-y} 2^{y+n}\n$$\n\nnote the extra factor \n$2^{y+n}$\n.  So when changing the dominating measure used in the definition of the likelihood function, there arises a new multiplicative constant, which does not depend on the unknown parameter \n$p$\n, and is clearly irrelevant. That is another way to see how multiplicative constants must be irrelevant.  This argument can be generalized using Radon-Nikodym derivatives (as the argument above is an example of.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/36038/402101 \n You can mechanically check that the expected value does not exist, but this should be physically intuitive, at least if you accept \nHuygens' principle\n and the \nLaw of Large Numbers\n. The conclusion of the Law of Large Numbers fails for a Cauchy distribution, so it can't have a mean. If you average $n$ independent Cauchy random variables, the result does not converge to $0$ as $n\\to \\infty$ with probability $1$. It stays a Cauchy distribution of the same size. This is important in optics.\n\nThe Cauchy distribution is the normalized intensity of light on a line from a point source. Huygens' principle says that you can determine the intensity by assuming that the light is re-emitted from any line between the source and the target. So, the intensity of light on a line $2$ meters away can be determined by assuming that the light first hits a line $1$ meter away, and is re-emitted at any forward angle. The intensity of light on a line $n$ meters away can be expressed as the $n$-fold convolution of the distribution of light on a line $1$ meter away. That is, the sum of $n$ independent Cauchy distributions is a Cauchy distribution scaled by a factor of $n$.\n\nIf the Cauchy distribution had a mean, then the $25$th percentile of the $n$-fold convolution divided by $n$ would have to converge to $0$ by the Law of Large Numbers. Instead it stays constant. If you mark the $25$th percentile on a (transparent) line $1$ meter away, $2$ meters away, etc. then these points form a straight line, at $45$ degrees. They don't bend toward $0$.\n\nThis tells you about the Cauchy distribution in particular, but you should know the integral test because there are other distributions with no mean which don't have a clear physical interpretation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/355462/402101 \n The other answers already here do a great job of explaining why Gaussian RVs don't converge to anything as the variance increases without bound, but I want to point out a seemingly-uniform property that such a collection of Gaussians \ndoes\n satisfy that I think might be enough for someone to guess that they are becoming uniform, but that turns out to not be strong enough to conclude that.\n$\\newcommand{\\len}{\\text{len}}$\n\nConsider a collection of random variables $\\{X_1,X_2,\\dots\\}$ where $X_n \\sim \\mathcal N(0, n^2)$. Let $A = [a_1,a_2]$ be a fixed interval of finite length, and for some $c \\in \\mathbb R$ define $B = A +c$, i.e. $B$ is $A$ but just shifted over by $c$. For an interval $I = [i_1,i_2]$ define $\\len (I) = i_2-i_1$ to be the length of $I$, and note that $\\len(A) = \\len(B)$.\n\nI'll now prove the following result:\n\nResult\n:  $\\vert P(X_n \\in A) - P(x_n\\in B)\\vert \\to 0$ as $n \\to \\infty$.\n\nI call this uniform-like because it says that the distribution of $X_n$ increasingly has two fixed intervals of equal length having equal probability, no matter how far apart they may be. That's definitely a very uniform feature, but as we'll see this doesn't say anything about the actual distribution of the $X_n$ converging to a uniform one.\n\nPf: note that $X_n = n X_1$ where $X_1 \\sim \\mathcal N(0, 1)$ so\n$$\nP(X_n \\in A) = P(a_1 \\leq n X_1 \\leq a_2) = P\\left(\\frac{a_1}{n} \\leq X_1 \\leq \\frac{a_2}n\\right)\n$$\n$$\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{a_1/n}^{a_2/n} e^{-x^2/2}\\,\\text dx.\n$$\nI can use the (very rough) bound that $e^{-x^2/2} \\leq 1$ to get\n$$\n\\frac{1}{\\sqrt{2\\pi}}\\int_{a_1/n}^{a_2/n} e^{-x^2/2}\\,\\text dx \\leq \\frac{1}{\\sqrt{2\\pi}}\\int_{a_1/n}^{a_2/n} 1\\,\\text dx\n$$\n$$\n= \\frac{\\text{len}(A)}{n\\sqrt{2\\pi}}.\n$$\n\nI can do the same thing for $B$ to get\n$$\nP(X_n \\in B) \\leq \\frac{\\text{len}(B)}{n\\sqrt{2\\pi}}.\n$$\n\nPutting these together I have\n$$\n\\left\\vert P(X_n \\in A) - P(X_n \\in B)\\right\\vert \\leq \\frac{\\sqrt 2 \\text{len}(A) }{n\\sqrt{\\pi}} \\to 0\n$$\nas $n\\to\\infty$ (I'm using the triangle inequality here).\n\n$\\square$\n\nHow is this different from $X_n$ converging on a uniform distribution? I just proved that the probabilities given to any two fixed intervals of the same finite length get closer and closer, and intuitively that makes sense that as the densities are \"flattening out\" from $A$ and $B$'s perspectives.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/355462/402101 \n $\\square$\n\nHow is this different from $X_n$ converging on a uniform distribution? I just proved that the probabilities given to any two fixed intervals of the same finite length get closer and closer, and intuitively that makes sense that as the densities are \"flattening out\" from $A$ and $B$'s perspectives.\n\nBut in order for $X_n$ to converge on a uniform distribution, I'd need $P(X_n \\in I)$ to head towards being proportional to $\\text{len}(I)$ for \nany\n interval $I$, and that is a very different thing because this needs to apply to any $I$, not just one fixed in advance (and as mentioned elsewhere, this is also not even possible for a distribution with unbounded support).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/137381/402101 \n When I see panel data, I think longitudinal data, so observations collected on the same individuals at multiple times, on the same topics. Repeated cross sections should be the same topics, but you get different samples of individuals at each observation.  I'd welcome other descriptions.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/26962/402101 \n There are several alternatives to \nStepwise Regression\n. The most used I have seen are:\n\nExpert opinion\n to decide which variables to include in the model.\n\n\nPartial Least Squares Regression\n. You essentially get latent variables and do a regression with them. You could also do \nPCA\n yourself and then use the principal variables.\n\n\nLeast Absolute Shrinkage and Selection Operator\n (LASSO).\n\nBoth \nPLS Regression\n and \nLASSO\n are implemented in R packages like\n\nPLS\n: \nhttp://cran.r-project.org/web/packages/pls/\n and\n\nLARS\n: \nhttp://cran.r-project.org/web/packages/lars/index.html\n\nIf you only want to \nexplore\n the relationship between your dependent variable and the independent variables (e.g. you do not need statistical significance tests), I would also recommend \nMachine Learning\n methods like \nRandom Forests\n or \nClassification/Regression Trees\n. \nRandom Forests\n can also approximate complex non-linear relationships between your dependent and independent variables, which might not have been revealed by linear techniques (like \nLinear Regression\n).\n\nA good starting point to \nMachine Learning\n might be the Machine Learning task view on CRAN:\n\nMachine Learning Task View\n: \nhttp://cran.r-project.org/web/views/MachineLearning.html",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/148713/402101 \n There are (at least) three senses in which a regression can be considered \"linear.\"\n  To distinguish them, let's start with an extremely general regression model\n\n$$Y = f(X,\\theta,\\varepsilon).$$\n\nTo keep the discussion simple, take the independent variables \n$X$\n to be fixed and accurately measured (rather than random variables).  They model \n$n$\n observations of \n$p$\n attributes each, giving rise to the \n$n$\n-vector of responses \n$Y$\n.  Conventionally, \n$X$\n is represented as an \n$n\\times p$\n matrix and \n$Y$\n as a column \n$n$\n-vector.  The (finite \n$q$\n-vector) \n$\\theta$\n comprises the \nparameters\n.  \n$\\varepsilon$\n is a vector-valued random variable.  It usually has \n$n$\n components, but sometimes has fewer.  The function \n$f$\n is vector-valued (with \n$n$\n components to match \n$Y$\n) and is usually assumed continuous in its last two arguments (\n$\\theta$\n and \n$\\varepsilon$\n).\n\nThe archetypal example\n, of fitting a line to \n$(x,y)$\n data, is the case where \n$X$\n is a vector of numbers \n$(x_i,\\,i=1,2,\\ldots,n)$\n--the x-values; \n$Y$\n is a parallel vector of \n$n$\n numbers \n$(y_i)$\n; \n$\\theta = (\\alpha,\\beta)$\n gives the intercept \n$\\alpha$\n and slope \n$\\beta$\n; and \n$\\varepsilon = (\\varepsilon_1,\\varepsilon_2,\\ldots,\\varepsilon_n)$\n is a vector of \"random errors\" whose components are independent (and usually assumed to have identical but unknown distributions of mean zero).  In the preceding notation,\n\n$$y_i = \\alpha + \\beta x_i +\\varepsilon_i = f(X,\\theta,\\varepsilon)_i$$\n\nwith \n$\\theta = (\\alpha,\\beta)$\n.\n\nThe regression function may be linear in any (or all) of its three arguments:\n\n\"Linear regression, or a \"linear model,\" ordinarily means that \n$f$\n is linear as a function of the \nparameters\n \n$\\theta$\n.  The \nSAS meaning of \"nonlinear regression\"\n is in this sense, with the added assumption that \n$f$\n is differentiable in its second argument (the parameters). This assumption makes it easier to find solutions.\n\n\n\n\nA \"linear relationship between \n$X$\n and \n$Y$\n\" means \n$f$\n is linear as a\nfunction of \n$X$\n.\n\n\n\n\nA model has \nadditive errors\n when \n$f$\n is linear in \n$\\varepsilon$\n.\nIn such cases it is \nalways\n assumed that \n$\\mathbb{E}(\\varepsilon) =\n   0$\n.  (Otherwise, it wouldn't be right to think of \n$\\varepsilon$\n as\n\"errors\" or \"deviations\" from \"correct\" values.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/148713/402101 \n A \"linear relationship between \n$X$\n and \n$Y$\n\" means \n$f$\n is linear as a\nfunction of \n$X$\n.\n\n\n\n\nA model has \nadditive errors\n when \n$f$\n is linear in \n$\\varepsilon$\n.\nIn such cases it is \nalways\n assumed that \n$\\mathbb{E}(\\varepsilon) =\n   0$\n.  (Otherwise, it wouldn't be right to think of \n$\\varepsilon$\n as\n\"errors\" or \"deviations\" from \"correct\" values.)\n\n\"Linear regression, or a \"linear model,\" ordinarily means that \n$f$\n is linear as a function of the \nparameters\n \n$\\theta$\n.  The \nSAS meaning of \"nonlinear regression\"\n is in this sense, with the added assumption that \n$f$\n is differentiable in its second argument (the parameters). This assumption makes it easier to find solutions.\n\nA \"linear relationship between \n$X$\n and \n$Y$\n\" means \n$f$\n is linear as a\nfunction of \n$X$\n.\n\nA model has \nadditive errors\n when \n$f$\n is linear in \n$\\varepsilon$\n.\nIn such cases it is \nalways\n assumed that \n$\\mathbb{E}(\\varepsilon) =\n   0$\n.  (Otherwise, it wouldn't be right to think of \n$\\varepsilon$\n as\n\"errors\" or \"deviations\" from \"correct\" values.)\n\nEvery possible combination of these characteristics can happen and is useful.\n  Let's survey the possibilities.\n\nA linear model of a linear relationship with additive errors.\n  This is ordinary (multiple) regression, already exhibited above and more generally written as\n\n\n$$Y = X\\theta + \\varepsilon.$$\n\n\n$X$\n has been augmented, if necessary, by adjoining a column of constants, and \n$\\theta$\n is a \n$p$\n-vector.\n\n\n\n\nA linear model of a nonlinear relationship with additive errors.\n  This can be couched as a multiple regression by augmenting the columns of \n$X$\n with nonlinear functions of \n$X$\n itself.  For instance,\n\n\n$$y_i = \\alpha + \\beta x_i^2 + \\varepsilon$$\n\n\nis of this form.  It is linear in \n$\\theta=(\\alpha,\\beta)$\n; it has additive errors; and it is linear in the values \n$(1,x_i^2)$\n even though \n$x_i^2$\n is a nonlinear function of \n$x_i$\n.\n\n\n\n\nA linear model of a linear relationship with nonadditive errors.\n  An example is multiplicative error,\n\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/148713/402101 \n $$y_i = \\alpha + \\beta x_i^2 + \\varepsilon$$\n\n\nis of this form.  It is linear in \n$\\theta=(\\alpha,\\beta)$\n; it has additive errors; and it is linear in the values \n$(1,x_i^2)$\n even though \n$x_i^2$\n is a nonlinear function of \n$x_i$\n.\n\n\n\n\nA linear model of a linear relationship with nonadditive errors.\n  An example is multiplicative error,\n\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i.$$\n\n\n(In such cases the \n$\\varepsilon_i$\n can be interpreted as \"multiplicative errors\" when the location of \n$\\varepsilon_i$\n is \n$1$\n.  However, the proper sense of location is not necessarily the expectation \n$\\mathbb{E}(\\varepsilon_i)$\n anymore: it might be the median or the geometric mean, for instance.  A similar comment about location assumptions applies, \nmutatis mutandis\n, in all other non-additive-error contexts too.)\n\n\n\n\nA linear model of a nonlinear relationship with nonadditive errors.\n \nE.g.\n,\n\n\n$$y_i = (\\alpha + \\beta x_i^2)\\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a linear relationship with additive errors.\n  A nonlinear model involves combinations of its parameters that not only are nonlinear, \nthey cannot even be linearized by re-expressing the parameters.\n\n\n\n\nAs a \nnon-example,\n consider\n\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\n\n\n\nAs an \nexample,\n consider\n\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\n\n\n\n\n\nIt is impossible to find a new parameter \n$\\alpha^\\prime$\n, depending on \n$\\alpha$\n, that will linearize this as a function of \n$\\alpha^\\prime$\n (while keeping it linear in \n$x_i$\n as well).\n\n\n\n\nA nonlinear model of a nonlinear relationship with additive errors.\n\n\n$$y_i = \\alpha + \\alpha^2 x_i^2 + \\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a linear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i)\\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a nonlinear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i^2)\\varepsilon_i.$$\n\nA linear model of a linear relationship with additive errors.\n  This is ordinary (multiple) regression, already exhibited above and more generally written as\n\n$$Y = X\\theta + \\varepsilon.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/148713/402101 \n $$y_i = \\alpha + \\alpha^2 x_i^2 + \\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a linear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i)\\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a nonlinear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i^2)\\varepsilon_i.$$\n\nA linear model of a linear relationship with additive errors.\n  This is ordinary (multiple) regression, already exhibited above and more generally written as\n\n$$Y = X\\theta + \\varepsilon.$$\n\n$X$\n has been augmented, if necessary, by adjoining a column of constants, and \n$\\theta$\n is a \n$p$\n-vector.\n\nA linear model of a nonlinear relationship with additive errors.\n  This can be couched as a multiple regression by augmenting the columns of \n$X$\n with nonlinear functions of \n$X$\n itself.  For instance,\n\n$$y_i = \\alpha + \\beta x_i^2 + \\varepsilon$$\n\nis of this form.  It is linear in \n$\\theta=(\\alpha,\\beta)$\n; it has additive errors; and it is linear in the values \n$(1,x_i^2)$\n even though \n$x_i^2$\n is a nonlinear function of \n$x_i$\n.\n\nA linear model of a linear relationship with nonadditive errors.\n  An example is multiplicative error,\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i.$$\n\n(In such cases the \n$\\varepsilon_i$\n can be interpreted as \"multiplicative errors\" when the location of \n$\\varepsilon_i$\n is \n$1$\n.  However, the proper sense of location is not necessarily the expectation \n$\\mathbb{E}(\\varepsilon_i)$\n anymore: it might be the median or the geometric mean, for instance.  A similar comment about location assumptions applies, \nmutatis mutandis\n, in all other non-additive-error contexts too.)\n\nA linear model of a nonlinear relationship with nonadditive errors.\n \nE.g.\n,\n\n$$y_i = (\\alpha + \\beta x_i^2)\\varepsilon_i.$$\n\nA nonlinear model of a linear relationship with additive errors.\n  A nonlinear model involves combinations of its parameters that not only are nonlinear, \nthey cannot even be linearized by re-expressing the parameters.\n\nAs a \nnon-example,\n consider\n\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\n\n\n\nAs an \nexample,\n consider\n\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\nAs a \nnon-example,\n consider",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/148713/402101 \n As a \nnon-example,\n consider\n\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\n\n\n\nAs an \nexample,\n consider\n\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\nAs a \nnon-example,\n consider\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\nAs an \nexample,\n consider\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\nIt is impossible to find a new parameter \n$\\alpha^\\prime$\n, depending on \n$\\alpha$\n, that will linearize this as a function of \n$\\alpha^\\prime$\n (while keeping it linear in \n$x_i$\n as well).\n\nA nonlinear model of a nonlinear relationship with additive errors.\n\n$$y_i = \\alpha + \\alpha^2 x_i^2 + \\varepsilon_i.$$\n\nA nonlinear model of a linear relationship with nonadditive errors.\n\n$$y_i = (\\alpha + \\alpha^2 x_i)\\varepsilon_i.$$\n\nA nonlinear model of a nonlinear relationship with nonadditive errors.\n\n$$y_i = (\\alpha + \\alpha^2 x_i^2)\\varepsilon_i.$$\n\nAlthough these exhibit eight distinct \nforms\n of regression, they do not constitute a \nclassification system\n because some forms can be converted into others.  A standard example is the conversion of a linear model with nonadditive errors (assumed to have positive support)\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i$$\n\ninto a linear model of a nonlinear relationship with additive errors via the logarithm,\n\n$$\\log(y_i) = \\mu_i + \\log(\\alpha + \\beta x_i) + (\\log(\\varepsilon_i) - \\mu_i)$$\n\nHere, the log geometric mean \n$\\mu_i = \\mathbb{E}\\left(\\log(\\varepsilon_i)\\right)$\n has been removed from the error terms (to ensure they have zero means, as required) and incorporated into the other terms (where its value will need to be estimated). Indeed, one major reason to re-express the dependent variable \n$Y$\n is to create a model with additive errors.  Re-expression can also linearize \n$Y$\n as a function of either (or both) of the parameters and explanatory variables.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/148713/402101 \n Here, the log geometric mean \n$\\mu_i = \\mathbb{E}\\left(\\log(\\varepsilon_i)\\right)$\n has been removed from the error terms (to ensure they have zero means, as required) and incorporated into the other terms (where its value will need to be estimated). Indeed, one major reason to re-express the dependent variable \n$Y$\n is to create a model with additive errors.  Re-expression can also linearize \n$Y$\n as a function of either (or both) of the parameters and explanatory variables.\n\nCollinearity\n (of the column vectors in \n$X$\n) can be an issue in \nany\n form of regression.  The key to understanding this is to recognize that collinearity leads to difficulties in estimating the parameters.  Abstractly and quite generally, compare two models \n$Y = f(X,\\theta,\\varepsilon)$\n and \n$Y=f(X^\\prime,\\theta,\\varepsilon^\\prime)$\n where \n$X^\\prime$\n is \n$X$\n with one column slightly changed.  If this induces enormous changes in the \nestimates\n \n$\\hat\\theta$\n and \n$\\hat\\theta^\\prime$\n, then obviously we have a problem.  One way in which this problem can arise is in a linear model, linear in \n$X$\n (that is, types (1) or (5) above), where the components of \n$\\theta$\n are in one-to-one correspondence with the columns of \n$X$\n.  When one column is a non-trivial linear combination of the others, the estimate of its corresponding parameter can be any real number at all. That is an extreme example of such sensitivity.\n\nFrom this point of view it should be clear that \ncollinearity is a potential problem for linear models of nonlinear relationships\n (regardless of the additivity of the errors) and that this generalized concept of collinearity is potentially a problem in any regression model.  When you have redundant variables, you will have problems identifying some parameters.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/250344/402101 \n The sample standard deviation \n$S=\\sqrt{\\frac{\\sum (X - \\bar{X})^2}{n-1}}$\n is complete and sufficient for \n$\\sigma$\n so the set of unbiased estimators of \n$\\sigma^k$\n given by\n\n$$\n\\frac{(n-1)^\\frac{k}{2}}{2^\\frac{k}{2}} \\cdot \\frac{\\Gamma\\left(\\frac{n-1}{2}\\right)}{\\Gamma\\left(\\frac{n+k-1}{2}\\right)} \\cdot S^k = \\frac{S^k}{c_k}\n$$\n\n(See \nWhy is sample standard deviation a biased estimator of \n$\\sigma$\n?\n) are, by the Lehmann\u2013Scheff\u00e9 theorem, UMVUE. Consistent, though biased, estimators of \n$\\sigma^k$\n can also be formed as\n\n$$\n\\tilde{\\sigma}^k_j= \\left(\\frac{S^j}{c_j}\\right)^\\frac{k}{j}\n$$\n\n(the unbiased estimators being specified when \n$j=k$\n). The bias of each is given by\n\n$$\\operatorname{E}\\tilde{\\sigma}^k_j - \\sigma^k =\\left( \\frac{c_k}{c_j^\\frac{k}{j}} -1 \\right) \\sigma^k$$\n\n& its variance by\n\n$$\\operatorname{Var}\\tilde{\\sigma}^{k}_j=\\operatorname{E}\\tilde{\\sigma}^{2k}_j - \\left(\\operatorname{E}\\tilde{\\sigma}^k_j\\right)^2=\\frac{c_{2k}-c_k^2}{c_j^\\frac{2k}{j}} \\sigma^{2k}$$\n\nFor the two estimators of \n$\\sigma$\n you've considered, \n$\\tilde{\\sigma}^1_1=\\frac{S}{c_1}$\n & \n$\\tilde{\\sigma}^1_2=S$\n, the lack of bias of \n$\\tilde{\\sigma}_1$\n is more than offset by its larger variance when compared to \n$\\tilde{\\sigma}_2$\n:\n\n$$\\begin{align}\n\\operatorname{E}\\tilde{\\sigma}_1 - \\sigma &= 0 \\\\\n\\operatorname{E}\\tilde{\\sigma}_2 - \\sigma &=(c_1 -1) \\sigma \\\\\n\\operatorname{Var}\\tilde{\\sigma}_1 =\\operatorname{E}\\tilde{\\sigma}^{2}_1 - \\left(\\operatorname{E}\\tilde{\\sigma}^1_1\\right)^2 &=\\frac{c_{2}-c_1^2}{c_1^2} \\sigma^{2} = \\left(\\frac{1}{c_1^2}-1\\right) \\sigma^2 \\\\\n\\operatorname{Var}\\tilde{\\sigma}_2 =\\operatorname{E}\\tilde{\\sigma}^{2}_1 - \\left(\\operatorname{E}\\tilde{\\sigma}_2\\right)^2 &=\\frac{c_{2}-c_1^2}{c_2} \\sigma^{2}=(1-c_1^2)\\sigma^2\n\\end{align}$$\n\n(Note that \n$c_2=1$\n, as \n$S^2$\n is already an unbiased estimator of \n$\\sigma^2$\n.)\n\n\n\nThe mean square error of \n$a_k S^k$\n as an estimator of \n$\\sigma^2$\n is given by\n\n$$\n\\begin{align}\n(\\operatorname{E} a_k S^k - \\sigma^k)^2 + \\operatorname{E} (a_k S^k)^2 - (\\operatorname{E} a_k S^k)^2\n&= [ (a_k c_k -1)^2 +  a_k^2 c_{2k}  - a_k^2 c_k^2 ] \\sigma^{2k}\\\\\n&= ( a_k^2 c_{2k} -2  a_k c_k + 1 ) \\sigma^{2k}\n\\end{align}\n$$\n\n& therefore minimized when\n\n$$a_k  = \\frac{c_k}{c_{2k}}$$\n\n, allowing the definition of another set of estimators of potential interest:\n\n$$\n\\hat{\\sigma}^k_j= \\left(\\frac{c_j S^j}{c_{2j}}\\right)^\\frac{k}{j}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/250344/402101 \n $$\n\\begin{align}\n(\\operatorname{E} a_k S^k - \\sigma^k)^2 + \\operatorname{E} (a_k S^k)^2 - (\\operatorname{E} a_k S^k)^2\n&= [ (a_k c_k -1)^2 +  a_k^2 c_{2k}  - a_k^2 c_k^2 ] \\sigma^{2k}\\\\\n&= ( a_k^2 c_{2k} -2  a_k c_k + 1 ) \\sigma^{2k}\n\\end{align}\n$$\n\n& therefore minimized when\n\n$$a_k  = \\frac{c_k}{c_{2k}}$$\n\n, allowing the definition of another set of estimators of potential interest:\n\n$$\n\\hat{\\sigma}^k_j= \\left(\\frac{c_j S^j}{c_{2j}}\\right)^\\frac{k}{j}\n$$\n\nCuriously, \n$\\hat{\\sigma}^1_1=c_1S$\n, so the same constant that divides \n$S$\n to remove bias multiplies \n$S$\n to reduce MSE. Anyway, when \n$j=k$\n these are the uniformly minimum-square-error location-invariant & scale-equivariant estimators of \n$\\sigma^k$\n (you don't want your estimate to change at all if you measure in kelvins rather than degrees Celsius, & you want it to change by a factor of \n$\\left(\\frac{9}{5}\\right)^k$\n if you measure in Fahrenheit).\n\nNone of the above has any bearing on the construction of hypothesis tests or confidence intervals (see e.g. \nWhy does this excerpt say that unbiased estimation of standard deviation usually isn't relevant?\n). And \n$\\tilde{\\sigma}^k_j$\n & \n$\\hat{\\sigma}^k_j$\n exhaust neither estimators nor parameter scales of potential interest\u2014consider the maximum-likelihood estimator\n\u2020\n \n$\\sqrt{\\frac{n-1}{n}}S$\n, or the median-unbiased estimator \n$\\sqrt{\\frac{n-1}{\\chi^2_{n-1}(0.5)}}S$\n; or the geometric standard deviation of a lognormal distribution \n$\\mathrm{e}^\\sigma$\n. It may be worth showing a few more-or-less popular estimates made from a small sample (\n$n=2$\n) together with the upper & lower bounds, \n$\\sqrt{\\frac{(n-1)s^2}{\\chi^2_{n-1}(\\alpha)}}$\n & \n$\\sqrt{\\frac{(n-1)s^2}{\\chi^2_{n-1}(1-\\alpha)}}$\n, of the equal-tailed confidence interval having coverage \n$1-\\alpha$\n:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/250344/402101 \n The span between the most divergent estimates is negligible in comparison with the width of any confidence interval having decent coverage. (The 95% C.I., for instance, is \n$(0.45s,31.9s)$\n.) There's no sense in being finicky about the properties of a point estimator unless you're prepared to be fairly explicit about what you want you want to use it for\u2014most explicitly you can define a custom loss function for a particular application. A reason you might prefer an exactly (or almost) unbiased estimator is that you're going to use it in subsequent calculations during which you don't want bias to accumulate: your illustration of averaging biased estimates of standard deviation is a simple example of such (a more complex example might be using them as a response in a linear regression). In principle an all-encompassing model should obviate the need for unbiased estimates as an intermediate step, but might be considerably more tricky to specify & fit.\n\n\u2020 The value of \n$\\sigma$\n that makes the observed data most probable has an appeal as an estimate independent of consideration of its sampling distribution.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/874/402101 \n Keep in mind that ridge regression can't zero out coefficients; thus, you either end up including all the coefficients in the model, or none of them. In contrast, the LASSO does both parameter shrinkage and variable selection automatically. If some of your covariates are highly correlated, you may want to look at the Elastic Net [3] instead of the LASSO.\n\nI'd personally recommend using the Non-Negative Garotte (NNG) [1] as it's consistent in terms of estimation and variable selection [2]. Unlike LASSO and ridge regression, NNG requires an initial estimate that is then shrunk towards the origin. In the original paper, Breiman recommends the least-squares solution for the initial estimate (you may however want to start the search from a ridge regression solution and use something like GCV to select the penalty parameter).\n\nIn terms of available software, I've implemented the original NNG in MATLAB (based on Breiman's original FORTRAN code). You can download it from:\n\nhttp://www.emakalic.org/blog/wp-content/uploads/2010/04/nngarotte.zip\n\nBTW, if you prefer a Bayesian solution, check out [4,5].\n\nReferences:\n\n[1] Breiman, L. Better Subset Regression Using the Nonnegative Garrote Technometrics, 1995, 37, 373-384\n\n[2] Yuan, M. & Lin, Y. On the non-negative garrotte estimator Journal of the Royal Statistical Society (Series B), 2007, 69, 143-161\n\n[3] Zou, H. & Hastie, T. Regularization and variable selection via the elastic net Journal of the Royal Statistical Society (Series B), 2005, 67, 301-320\n\n[4] Park, T. & Casella, G. The Bayesian Lasso Journal of the American Statistical Association, 2008, 103, 681-686\n\n[5] Kyung, M.; Gill, J.; Ghosh, M. & Casella, G. Penalized Regression, Standard Errors, and Bayesian Lassos Bayesian Analysis, 2010, 5, 369-412",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/308831/402101 \n I don't think the formula given in the question can be correct in all cases, it is developed using joint normality.  Without joint normality we can use copulas.  For \n$X,Y$\n random variables with joint distribution with cumulative distribution function \n$F(x,y)$\n and joint density \n$f(x,y)$\n define the transformed random variables (rv) \n$U=F_X(X), V=F_Y(Y)$\n where \n$F_X, F_Y$\n denotes the marginal cumulative distribution functions (cdf). Then the joint distribution of \n$U;V$\n\n\n$$ \\DeclareMathOperator{\\P}{\\mathbb{P}}\n  \\P(U \\le u, V \\le v)=C(u,v)\n$$\n\nis the \ncopula\n of \n$X$\n and \n$Y$\n, with copula density \n$c(u,v)$\n (when it exists).  So, in your setup let us assume that the copula density exists, and for simplicity I will take both \n$X$\n and \n$Y$\n as standard normals.  So what possibility exists for the conditional expectation of \n$X$\n given \n$Y=y$\n ? Using Sklar's theorem we can write the joint density as\n\n$$\n   f(x,y) = c(\\Phi(x),\\Phi(y)) \\phi(x) \\phi(y)\n$$\n\nwhere \n$\\Phi, \\phi$\n are the standard normal cdf, pdf, respectively.  Then the conditional density is given by\n\n$$\n   f(x \\mid y) = \\frac{f(x,y)}{f(y)}=\\frac{c(\\Phi(x),\\Phi(y))\\phi(x)\\phi(y)}{\\phi(y)}= c(\\Phi(x),\\Phi(y))\\phi(x)\n$$\n\nThen we can look at this with various copula functions, see \nhttps://en.wikipedia.org/wiki/Copula_(probability_theory)\n .\n\nThere is a general inequality for copulas\n\n$$\n  W(u,v)=\\max(u+v-1,0) \\le C(u,v) \\le M(u,w)=\\min(u,v)\n$$\n\nwhere both upper and lower limits are copulas (This is the Frechet-Hoeffding bounds). The upper limit isn't very interesting, since it gives \n$\\P(U=V)=1$\n so gives correlation equal 1. The lower limit similarly corresponds to \n$U=1-V$\n with probability one, so correlation is -1.  But for these two extremal copula the conditional expectation function certainly is linear!\n\nLets look at some intermediate cases.  I will use the R package \ncopula\n and some numerical integration to find the conditional expectation function, for the case of the gumbel copula. The code can be simply adapted for other copulas.\n\ncopula\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/308831/402101 \n copula\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )\n\nthe last command makes a function representing the conditional density given \n$Y=y$\n. Let us look at how this looks like for three different values of \n$y$\n:\n\ncond_dens <- make_cond(c(-1, 0, 1))\n    plot(cond_dens[[1]],  from=-3,  to=3, col=\"blue\", ylim=c(0, 0.7))\n    plot(cond_dens[[2]],  from=-3,  to=3,  col=\"orange\",  add=TRUE)\n    plot(cond_dens[[3]],  from=-3,  to=3,  col=\"red\",  add=TRUE)\n    title(\"conditional densities for y=-1, 0, 1\")\n\ncond_dens <- make_cond(c(-1, 0, 1))\n    plot(cond_dens[[1]],  from=-3,  to=3, col=\"blue\", ylim=c(0, 0.7))\n    plot(cond_dens[[2]],  from=-3,  to=3,  col=\"orange\",  add=TRUE)\n    plot(cond_dens[[3]],  from=-3,  to=3,  col=\"red\",  add=TRUE)\n    title(\"conditional densities for y=-1, 0, 1\")\n\n\n\nshowing clearly that the conditional distributions now are non-normal. We can also see clearly that the conditional variance is non-constant.\nFor more examples using the \ncopula\n package see \nhttps://www.r-bloggers.com/modelling-dependence-with-copulas-in-r/\n and \nGenerating values from copula using copula package in R\n . Then we can find the conditional expectation function using numerical integration:\n\ncopula\n\nplot(function(y) sapply(make_cond(y), FUN=function(fun)\n                   integrate(function(x) x*fun(x) ,\n                             lower=-Inf,  upper=Inf)$value), \n                              from=-3,  to=3,\n         ylab=\"conditional expectation given y\", xlab=\"y\")\n    title(\"conditional expectation of X given Y=y\")\n\nplot(function(y) sapply(make_cond(y), FUN=function(fun)\n                   integrate(function(x) x*fun(x) ,\n                             lower=-Inf,  upper=Inf)$value), \n                              from=-3,  to=3,\n         ylab=\"conditional expectation given y\", xlab=\"y\")\n    title(\"conditional expectation of X given Y=y\")\n\n\n\nand it is quite clear that the conditional expectation function is not linear!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/125266/402101 \n What is a difference in differences estimator\n\nDifference in differences (DiD) is a tool to estimate treatment effects comparing the pre- and post-treatment differences in the outcome of a treatment and a control group. In general, we are interested in estimating the effect of a treatment $D_i$ (e.g. union status, medication, etc.) on an outcome $Y_i$ (e.g. wages, health, etc.) as in\n$$Y_{it} = \\alpha_i + \\lambda_t + \\rho D_{it} + X'_{it}\\beta + \\epsilon_{it}$$\nwhere $\\alpha_i$ are individual fixed effects (characteristics of individuals that do not change over time), $\\lambda_t$ are time fixed effects, $X_{it}$ are time-varying covariates like individuals' age, and $\\epsilon_{it}$ is an error term. Individuals and time are indexed by $i$ and $t$, respectively. If there is a correlation between the fixed effects and $D_{it}$ then estimating this regression via OLS will be biased given that the fixed effects are not controlled for. This is the typical \nomitted variable bias\n.\n\nTo see the effect of a treatment we would like to know the difference between a person in a world in which she received the treatment and one in which she does not. Of course, only one of these is ever observable in practice. Therefore we look for people with the same pre-treatment trends in the outcome. Suppose we have two periods $t = 1, 2$ and two groups $s = A,B$. Then, under the assumption that the trends in the treatment and control groups would have continued the same way as before in the absence of treatment, we can estimate the treatment effect as\n$$\\rho = (E[Y_{ist}|s=A,t=2] - E[Y_{ist}|s=A,t=1]) - (E[Y_{ist}|s=B,t=2] - E[Y_{ist}|s=B,t=1])$$\n\nGraphically this would look something like this:\n\nYou can simply calculate these means by hand, i.e. obtain the mean outcome of group $A$ in both periods and take their difference. Then obtain the mean outcome of group $B$ in both periods and take their difference. Then take the difference in the differences and that's the treatment effect. However, it is more convenient to do this in a regression framework because this allows you\n\nto control for covariates\n\n\nto obtain standard errors for the treatment effect to see if it is significant",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/125266/402101 \n You can simply calculate these means by hand, i.e. obtain the mean outcome of group $A$ in both periods and take their difference. Then obtain the mean outcome of group $B$ in both periods and take their difference. Then take the difference in the differences and that's the treatment effect. However, it is more convenient to do this in a regression framework because this allows you\n\nto control for covariates\n\n\nto obtain standard errors for the treatment effect to see if it is significant\n\nTo do this, you can follow either of two equivalent strategies. Generate a control group dummy $\\text{treat}_i$ which is equal to 1 if a person is in group $A$ and 0 otherwise, generate a time dummy $\\text{time}_t$ which is equal to 1 if $t=2$ and 0 otherwise, and then regress\n$$Y_{it} = \\beta_1 + \\beta_2 (\\text{treat}_i) + \\beta_3 (\\text{time}_t) + \\rho (\\text{treat}_i \\cdot \\text{time}_t) + \\epsilon_{it}$$\n\nOr you simply generate a dummy $T_{it}$ which equals one if a person is in the treatment group AND the time period is the post-treatment period and is zero otherwise. Then you would regress\n$$Y_{it} = \\beta_1 \\gamma_s + \\beta_2 \\lambda_t + \\rho T_{it} + \\epsilon_{it}$$\n\nwhere $\\gamma_s$ is again a dummy for the control group and $\\lambda_t$ are time dummies. The two regressions give you the same results for two periods and two groups. The second equation is more general though as it easily extends to multiple groups and time periods. In either case, this is how you can estimate the difference in differences parameter in a way such that you can include control variables (I left those out from the above equations to not clutter them up but you can simply include them) and obtain standard errors for inference.\n\nWhy is the difference in differences estimator useful?\n\nAs stated before, DiD is a method to estimate treatment effects with non-experimental data. That's the most useful feature. DiD is also a version of fixed effects estimation. Whereas the fixed effects model assumes $E(Y_{0it}|i,t) = \\alpha_i + \\lambda_t$, DiD makes a similar assumption but at the group level, $E(Y_{0it}|s,t) = \\gamma_s + \\lambda_t$. So the expected value of the outcome here is the sum of a group and a time effect. So what's the difference? For DiD you don't necessarily need panel data as long as your repeated cross sections are drawn from the same aggregate unit $s$. This makes DiD applicable to a wider array of data than the standard fixed effects models that require panel data.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/125266/402101 \n Can we trust difference in differences?\n\nThe most important assumption in DiD is the parallel trends assumption (see the figure above). Never trust a study that does not graphically show these trends! Papers in the 1990s might have gotten away with this but nowadays our understanding of DiD is much better. If there is no convincing graph that shows the parallel trends in the pre-treatment outcomes for the treatment and control groups, be cautious. If the parallel trends assumption holds and we can credibly rule out any other time-variant changes that may confound the treatment, then DiD is a trustworthy method.\n\nAnother word of caution should be applied when it comes to the treatment of standard errors. With many years of data you need to adjust the standard errors for autocorrelation. In the past, this has been neglected but since \nBertrand et al. (2004) \"How Much Should We Trust Differences-In-Differences Estimates?\"\n we know that this is an issue. In the paper they provide several remedies for dealing with autocorrelation. The easiest is to cluster on the individual panel identifier which allows for arbitrary correlation of the residuals among individual time series. This corrects for both autocorrelation and heteroscedasticity.\n\nFor further references see these lecture notes by \nWaldinger\n and \nPischke\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/56910/402101 \n If all the assumptions hold and you have the correct form for $R^2$ then the usual F statistic can be computed as $F = \\frac{ R^2 }{ 1- R^2} \\times \\frac{ \\text{df}_2 }{ \\text{df}_1 }$.  This value can then be compared to the appropriate F distribution to do an F test.  This can be derived/confirmed with basic algebra.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/129954/402101 \n First a general comment: Note that the Anderson-Darling test is for completely specified distributions, while the Shapiro-Wilk is for normals with any mean and variance. However, as noted in D'Agostino & Stephens\n$^{[1]}$\n the Anderson-Darling adapts in a very convenient way to the estimation case, akin to (but converges faster and is modified in a way that's simpler to deal with than) the Lilliefors test for the Kolmogorov-Smirnov case. Specifically, at the normal, by \n$n=5$\n, tables of the asymptotic value of \n$A^*=A^2\\left(1+\\frac{4}{n}-\\frac{25}{n^2}\\right)$\n may be used (don't be testing goodness of fit for n<5).\n\nI have read somewhere in the literature that the Shapiro\u2013Wilk test is considered to be the best normality test because for a given significance level, \u03b1, the probability of rejecting the null hypothesis if it's false is higher than in the case of the other normality tests.\n\nAs a general statement this is false.\n\nWhich normality tests are \"better\" depends on which classes of alternatives you're interested in. One reason the Shapiro-Wilk is popular is that it tends to have very good power under a broad range of useful alternatives. It comes up in many studies of power, and usually performs very well, but it's not universally best.\n\nIt's quite easy to find alternatives under which it's less powerful.\n\nFor example, against light tailed alternatives it often has less power than the studentized range \n$u=\\frac{\\max(x)\u2212\\min(x)}{sd(x)}$\n (compare them on a test of normality on uniform data, for example - at \n$n=30$\n, a test based on \n$u$\n has power of about 63% compared to a bit over 38% for the Shapiro Wilk).\n\nThe Anderson-Darling (adjusted for parameter estimation) does better at the double exponential. Moment-skewness does better against some skew alternatives.\n\nCould you please explain to me, using mathematical arguments if possible, how exactly it works compared to some of the other normality tests (say the Anderson\u2013Darling test)?\n\nI will explain in general terms (if you want more specific details the original papers and some of the later papers that discuss them would be your best bet):",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/129954/402101 \n The Anderson-Darling (adjusted for parameter estimation) does better at the double exponential. Moment-skewness does better against some skew alternatives.\n\nCould you please explain to me, using mathematical arguments if possible, how exactly it works compared to some of the other normality tests (say the Anderson\u2013Darling test)?\n\nI will explain in general terms (if you want more specific details the original papers and some of the later papers that discuss them would be your best bet):\n\nConsider a simpler but closely related test, the Shapiro-Francia; it's effectively a function of the correlation between the order statistics and the expected order statistics under normality (and as such, a pretty direct measure of \"how straight the line is\" in the normal Q-Q plot). As I recall, the Shapiro-Wilk is more powerful because it also takes into account the covariances between the order statistics, producing a best linear estimator of \n$\\sigma$\n from the Q-Q plot, which is then scaled by \n$s$\n. When the distribution is far from normal, the ratio isn't close to 1.\n\nBy comparison the Anderson-Darling, like the Kolmogorov-Smirnov and the Cram\u00e9r-von Mises, is based on the empirical CDF. Specifically, it's based on weighted deviations between ECDF and theoretical ECDF (the weighting-for-variance makes it more sensitive to deviations in the tail).\n\nThe test by Shapiro and Chen\n$^{[2]}$\n (1995) (based on spacings between order statistics) often exhibits slightly more power than the Shapiro-Wilk (but not always); they often perform very similarly.\n\n--\n\nUse the Shapiro Wilk because it's often powerful, widely available and many people are familiar with it (removing the need to explain in detail what it is if you use it in a paper) -- just don't use it under the illusion that it's \"the best normality test\". There isn't one best normality test.\n\n[1]: D\u2019Agostino, R. B. and Stephens, M. A. (1986)\n\n\nGoodness of Fit Techniques\n,\n\nMarcel Dekker, New York.\n\n[2]: Chen, L. and Shapiro, S. (1995)\n\n\"An Alternative test for normality based on normalized spacings.\"\n\n\nJournal of Statistical Computation and Simulation\n \n53\n, 269-287.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73340/402101 \n There is nothing wrong with your current strategy.  If you have a multiple regression model with \nonly\n two explanatory variables then you could try to make a 3D-ish plot that displays the predicted regression plane, but most software don't make this easy to do.  Another possibility is to use a \ncoplot\n (see also: \ncoplot in R\n or \nthis pdf\n), which can represent three or even four variables, but many people don't know how to read them.  Essentially however, if you don't have any interactions, then the predicted \nmarginal\n relationship between \n$x_j$\n and \n$y$\n will be the same as predicted \nconditional\n relationship (plus or minus some vertical shift) at any specific level of your other \n$x$\n variables.  Thus, you can simply set all other \n$x$\n variables at their means and find the predicted line \n$\\hat y = \\hat\\beta_0 + \\cdots + \\hat\\beta_j x_j + \\cdots + \\hat\\beta_p \\bar x_p$\n and plot that line on a scatterplot of \n$(x_j, y)$\n pairs.  Moreover, you will end up with \n$p$\n such plots, although you might not include some of them if you think they are not important. (For example, it is common to have a multiple regression model with a single variable of interest and some control variables, and only present the first such plot).\n\nOn the other hand, if you \ndo\n have interactions, then you should figure out which of the interacting variables you are most interested in and plot the predicted relationship between that variable and the response variable, but with several lines on the same plot.  The other interacting variable is set to different levels for each of those lines.  Typical values would be the mean and \n$\\pm$\n 1 SD of the interacting variable.  To make this clearer, imagine you have only two variables, \n$x_1$\n and \n$x_2$\n, and you have an interaction between them, and that \n$x_1$\n is the focus of your study, then you might make a single plot with these three lines:\n\n\n\\begin{align}\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 (\\bar x_2 - s_{x_2})  + \\hat\\beta_3 x_1(\\bar x_2 - s_{x_2}) \\\\\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 \\bar x_2 \\quad\\quad\\quad\\  + \\hat\\beta_3 x_1\\bar x_2 \\\\\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 (\\bar x_2 + s_{x_2}) + \\hat\\beta_3 x_1(\\bar x_2 + s_{x_2}) \n\\end{align}\n\nAn example plot that's similar (albeit with a binary moderator) can be seen in my answer to \nPlot regression with interaction in R\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49456/402101 \n Methods of \ncensored regression\n can handle data like this.\n  They assume the \nresiduals\n behave as in ordinary linear regression but have been modified so that\n\n(Left censoring): all values smaller than a low threshold, which is independent of the data, (but can vary from one case to the other) have not been quantified; and/or\n\n\n(Right censoring): all values larger than than a high threshold, which is independent of the data (but can vary from one case to the other) have not been quantified.\n\n(Left censoring): all values smaller than a low threshold, which is independent of the data, (but can vary from one case to the other) have not been quantified; and/or\n\n(Right censoring): all values larger than than a high threshold, which is independent of the data (but can vary from one case to the other) have not been quantified.\n\n\"Not quantified\" means we know whether or not a value falls below (or above) its threshold, \nbut that's all.\n\nThe fitting methods typically use maximum likelihood.  When the model for the response $Y$ corresponding to a vector $X$ is in the form\n\n$$Y \\sim X \\beta + \\varepsilon$$\n\nwith iid $\\varepsilon$ having a common distribution $F_\\sigma$ with PDF $f_\\sigma$ (where $\\sigma$ are unknown \"nuisance parameters\"), then--in the absence of censoring--the log likelihood of observations $(x_i, y_i)$ is\n\n$$\\Lambda = \\sum_{i=1}^n \\log f_\\sigma(y_i - x_i\\beta).$$\n\nWith censoring present we may divide the cases into three (possibly empty) classes: for indexes $i=1$ to $n_1$, the $y_i$ contain the \nlower threshold\n values and represent \nleft censored\n data; for indexes $i=n_1+1$ to $n_2$, the $y_i$ are quantified; and for the remaining indexes, the $y_i$ contain the \nupper threshold\n values and represent \nright censored\n data.  The log likelihood is obtained in the same way as before: it is the log of the product of the probabilities.\n\n$$\\Lambda = \\sum_{i=1}^{n_1} \\log F_\\sigma(y_i - x_i\\beta) + \\sum_{i=n_1+1}^{n_2} \\log f_\\sigma(y_i - x_i\\beta) + \\sum_{i=n_2+1}^n \\log (1 - F_\\sigma(y_i - x_i\\beta)).$$\n\nThis is maximized numerically as a function of $(\\beta, \\sigma)$.\n\nIn my experience, such methods can work well when less than half the data are censored; otherwise, the results can be unstable.\n\nHere is a simple \nR\n example\n using the \ncensReg\n package\n to illustrate how OLS and censored results can differ (a lot) even with plenty of data.  It qualitatively reproduces the data in the question.\n\nR\n\ncensReg",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49456/402101 \n This is maximized numerically as a function of $(\\beta, \\sigma)$.\n\nIn my experience, such methods can work well when less than half the data are censored; otherwise, the results can be unstable.\n\nHere is a simple \nR\n example\n using the \ncensReg\n package\n to illustrate how OLS and censored results can differ (a lot) even with plenty of data.  It qualitatively reproduces the data in the question.\n\nR\n\ncensReg\n\nlibrary(\"censReg\")\nset.seed(17)\nn.data <- 2960\ncoeff  <- c(-0.001, 0.005)\nsigma  <- 0.005\nx      <- rnorm(n.data, 0.5)\ny      <- as.vector(coeff %*% rbind(rep(1, n.data), x) + rnorm(n.data, 0, sigma))\ny.cen           <- y\ny.cen[y < 0]    <- 0\ny.cen[y > 0.01] <- 0.01\ndata = data.frame(list(x, y.cen))\n\nlibrary(\"censReg\")\nset.seed(17)\nn.data <- 2960\ncoeff  <- c(-0.001, 0.005)\nsigma  <- 0.005\nx      <- rnorm(n.data, 0.5)\ny      <- as.vector(coeff %*% rbind(rep(1, n.data), x) + rnorm(n.data, 0, sigma))\ny.cen           <- y\ny.cen[y < 0]    <- 0\ny.cen[y > 0.01] <- 0.01\ndata = data.frame(list(x, y.cen))\n\nThe key things to notice are the parameters: the \ntrue\n slope is $0.005$, the \ntrue\n intercept is $-0.001$, and the \ntrue\n error SD is $0.005$.\n\nLet's use both \nlm\n and \ncensReg\n to fit a line:\n\nlm\n\ncensReg\n\nfit <- censReg(y.cen ~ x, data=data, left=0.0, right=0.01)\nsummary(fit)\n\nfit <- censReg(y.cen ~ x, data=data, left=0.0, right=0.01)\nsummary(fit)\n\nThe results of this censored regression, given by \nprint(fit)\n, are\n\nprint(fit)\n\n(Intercept)           x       sigma \n  -0.001028    0.004935    0.004856\n\n(Intercept)           x       sigma \n  -0.001028    0.004935    0.004856\n\nThose are remarkably close to the correct values of $-0.001$, $0.005$, and $0.005$, respectively.\n\nfit.OLS <- lm(y.cen ~ x, data=data)\nsummary(fit.OLS)\n\nfit.OLS <- lm(y.cen ~ x, data=data)\nsummary(fit.OLS)\n\nThe OLS fit, given by \nprint(fit.OLS)\n, is\n\nprint(fit.OLS)\n\n(Intercept)            x  \n   0.001996     0.002345\n\n(Intercept)            x  \n   0.001996     0.002345\n\nNot even remotely close!  The estimated standard error reported by \nsummary\n is $0.002864$, less than half the true value.  \nThese kinds of biases are typical of regressions with lots of censored data.\n\nsummary\n\nFor comparison, let's limit the regression to the quantified data:\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)\n\n(Intercept)  x[0 <= y & y <= 0.01]  \n   0.003240               0.001461\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49456/402101 \n summary\n\nFor comparison, let's limit the regression to the quantified data:\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)\n\n(Intercept)  x[0 <= y & y <= 0.01]  \n   0.003240               0.001461\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)\n\n(Intercept)  x[0 <= y & y <= 0.01]  \n   0.003240               0.001461\n\nEven worse!\n\nA few pictures summarize the situation.\n\nlineplot <- function() {\n  abline(coef(fit)[1:2], col=\"Red\", lwd=2)\n  abline(coef(fit.OLS), col=\"Blue\", lty=2, lwd=2)\n  abline(coef(fit.part), col=rgb(.2, .6, .2), lty=3, lwd=2)\n}\npar(mfrow=c(1,4))\nplot(x,y, pch=19, cex=0.5, col=\"Gray\", main=\"Hypothetical Data\")\nlineplot()\nplot(x,y.cen, pch=19, cex=0.5, col=\"Gray\", main=\"Censored Data\")\nlineplot()\nhist(y.cen, breaks=50, main=\"Censored Data\")\nhist(y[0 <= y & y <= 0.01], breaks=50, main=\"Quantified Data\")\n\nlineplot <- function() {\n  abline(coef(fit)[1:2], col=\"Red\", lwd=2)\n  abline(coef(fit.OLS), col=\"Blue\", lty=2, lwd=2)\n  abline(coef(fit.part), col=rgb(.2, .6, .2), lty=3, lwd=2)\n}\npar(mfrow=c(1,4))\nplot(x,y, pch=19, cex=0.5, col=\"Gray\", main=\"Hypothetical Data\")\nlineplot()\nplot(x,y.cen, pch=19, cex=0.5, col=\"Gray\", main=\"Censored Data\")\nlineplot()\nhist(y.cen, breaks=50, main=\"Censored Data\")\nhist(y[0 <= y & y <= 0.01], breaks=50, main=\"Quantified Data\")\n\n\n\nThe difference between the \"hypothetical data\" and \"censored data\" plots is that all y-values below $0$ or above $0.01$ in the former have been moved to their respective thresholds to produce the latter plot.  As a result, you can see the censored data all lined up along the bottom and top.\n\nSolid red lines are the censored fits, dashed blue lines the OLS fits, \nboth of them based on the censored data only\n. The dashed green lines are the fits to the quantified data only. It is clear which is best: the blue and green lines are noticeably poor and only the red (for the censored regression fit) looks about right.  The histograms at the right confirm that the $Y$ values of this synthetic dataset are indeed qualitatively like those of the question (mean = $0.0032$, SD = $0.0037$).  The rightmost histogram shows the center (quantified) part of the histogram in detail.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/119708/402101 \n In an unpenalized regression, you can often get a ridge* in parameter space, where many different values along the ridge all do as well or nearly as well on the least squares criterion.\n\n*  (at least, it's a ridge in the \nlikelihood function\n -- they're actually \nvalleys\n$ in the RSS criterion, but I'll continue to call it a ridge, as this seems to be conventional -- or even, as Alexis points out in comments, I could call that a \nthalweg\n, being the valley's counterpart of a ridge)\n\nIn the presence of a ridge in the least squares criterion in parameter space, the penalty you get with ridge regression gets rid of those ridges by pushing the criterion up as the parameters head away from the origin:\n\n[\nClearer image\n]\n\nIn the first plot, a large change in parameter values (along the ridge) produces a miniscule change in the RSS criterion. This can cause numerical instability; it's very sensitive to small changes (e.g. a tiny change in a data value, even truncation or rounding error). The parameter estimates are almost perfectly correlated. You may get parameter estimates that are very large in magnitude.\n\nBy contrast, by lifting up the thing that ridge regression minimizes (by adding the $L_2$ penalty) when the parameters are far from 0, small changes in conditions (such as a little rounding or truncation error) can't produce gigantic changes in the resulting estimates. The penalty term results in shrinkage toward 0 (resulting in some bias). A small amount of bias can buy a substantial improvement in the variance (by eliminating that ridge).\n\nThe uncertainty of the estimates are reduced (the standard errors are inversely related to the second derivative, which is made larger by the penalty).\n\nCorrelation in parameter estimates is reduced. You now won't get parameter estimates that are very large in magnitude if the RSS for small parameters would not be much worse.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/342792/402101 \n 'Y must be normally distributed'\n\nmust?\n\nIn the cases that you mention it is sloppy language (abbreviating \n'the error in Y must be normally distributed'\n), but they don't really (strongly) say that \nthe response must\n be normally distributed, or at least it does not seem to me that their words were intended like that.\n\nspeaks about \n\"a continuous variable \n$Y$\n\"\n, but also about \"\n$Y_i$\n\" as in \n$$E(Y_i) = \\beta_0 + \\beta_1 x_i$$\n where we could regard \n$Y_i$\n, which is as amoeba called in the comments 'conditional', normally distributed,\n\n$$Y_i \\sim N(\\beta_0 + \\beta_1x_i,\\sigma^2)$$\n\nThe article uses \n$Y$\n and \n$Y_i$\n interchangeably. Throughout the entire article one speaks about the 'distribution of Y', for instance:\n\nwhen explaining some variant of GLM (binary logistic regression), \n\n\n\n\nRandom component\n: The distribution of \n$Y$\n is assumed to be \n$Binomial(n,\\pi)$\n,...\n\n\n\n\nin some definition\n\n\n\n\nRandom Component\n \u2013 refers to the probability distribution of the response variable (\n$Y$\n); e.g. normal distribution for \n$Y$\n in the linear regression, or binomial distribution for \n$Y$\n in the binary logistic regression.\n\nwhen explaining some variant of GLM (binary logistic regression),\n\nRandom component\n: The distribution of \n$Y$\n is assumed to be \n$Binomial(n,\\pi)$\n,...\n\nin some definition\n\nRandom Component\n \u2013 refers to the probability distribution of the response variable (\n$Y$\n); e.g. normal distribution for \n$Y$\n in the linear regression, or binomial distribution for \n$Y$\n in the binary logistic regression.\n\nhowever at some other point they also refer to \n$Y_i$\n instead of \n$Y$\n:\n\nThe dependent variable \n$Y_i$\n does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...)\n\nThe dependent variable \n$Y_i$\n does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...)\n\nis an extremely brief, simplified, stylized description. I am not sure you should take this serious. For instance, it speaks about\n\n..requires \nall\n variables to be multivariate normal...\n\nso that is not just the response variable,\n\nand also the the 'multivariate' descriptor is vague. I am not sure \nhow to get that interpreted.\n\nhas an additional context explained in brackets:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/342792/402101 \n is an extremely brief, simplified, stylized description. I am not sure you should take this serious. For instance, it speaks about\n\n..requires \nall\n variables to be multivariate normal...\n\nso that is not just the response variable,\n\nand also the the 'multivariate' descriptor is vague. I am not sure \nhow to get that interpreted.\n\nhas an additional context explained in brackets:\n\nOrdinary linear regression predicts the expected value of a given\n  unknown quantity (the response variable, a random variable) as \na\n  linear combination of a set of observed values (predictors)\n. This\n  implies that a constant change in a predictor leads to a constant\n  change in the response variable (i.e. a linear-response model).\n  This is appropriate when the response variable has a normal\n  distribution \n(intuitively, when a response variable can vary\n  essentially indefinitely in either direction with no fixed \"zero\n  value\", or more generally for any quantity that only varies by a\n  relatively small amount, e.g. human heights).\n\nThis 'no fixed zero value' seems to point to the case that a \nlinear combination\n \n$y+\\epsilon$\n when \n$\\epsilon \\sim N(0,\\sigma)$\n has an infinite domain (from minus infinity to plus infinity) whereas often many variables have some finite cut-off value (such as counts not allowing negative values).\n\nThe particular line has been added on \nMarch 8 2012\n, but note that the first line of the Wikipedia article still reads \n\"a flexible generalization of ordinary linear regression that allows for response variables that have \nerror distribution models\n other than a normal distribution\"\n and is not so much (not everywhere) wrong.\n\nSo, based on these three examples (which indeed could \ngenerate\n misconceptions, or at least could be misunderstood) I would not say that \n\"this misconception has spread\"\n. Or at least it does not seem to me that the intention of those three examples is to argue that Y must be normally distributed (although I do remember this issue has arised before here on stackexchange, the swap between normally distributed errors and normally distributed response variable is easy to make).\n\nSo, the assumption that 'Y must be normally distributed' seems to me not like a \nwidespread\n believe/misconception (as in something that spreads like a red herring), but more like a common error (which is not \nspread\n but made independently each time).\n\nAn example of the mistake on this website is in the following question",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/342792/402101 \n So, the assumption that 'Y must be normally distributed' seems to me not like a \nwidespread\n believe/misconception (as in something that spreads like a red herring), but more like a common error (which is not \nspread\n but made independently each time).\n\nAn example of the mistake on this website is in the following question\n\nWhat if residuals are normally distributed, but y is not?\n\nI would consider this as a beginners question. It is not present in the materials like the Penn State course material, the Wikipedia website, and recently noted in the comments the book  'Extending the Linear Regression with R'.\n\nThe writers of those works do correctly understand the material. Indeed, they use phrases such as 'Y must be normally distributed', but based on the context and the used formulas you can see that they all mean 'Y, conditional on X, must be normally distributed' and not 'the marginal Y must be normally distributed'. They are not misconceiving the idea themselves, and at least the idea is not widespread among statisticians and people that write books and other course materials. But misreading their ambiguous words may indeed cause the misconception.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1492/402101 \n Consider the simplest case where $Y$ is regressed against $X$ and $Z$ and where $X$ and $Z$ are highly positively correlated. Then the effect of $X$ on $Y$ is hard to distinguish from the effect of $Z$ on $Y$ because any increase in $X$ tends to be associated with an increase in $Z$.\n\nAnother way to look at this is to consider the equation. If we write $Y = b_0 + b_1X + b_2Z + e$, then the coefficient $b_1$ is the increase in $Y$ for every unit increase in $X$ while holding $Z$ constant. But in practice, it is often impossible to hold $Z$ constant and the positive correlation between $X$ and $Z$ mean that a unit increase in $X$ is usually accompanied by some increase in $Z$ at the same time.\n\nA similar but more complicated explanation holds for other forms of multicollinearity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/350132/402101 \n Pearl's theory of causality is completely \nnon-parametric\n. Interactions are not made explicit because of that, neither in the graph nor in the structural equations it represents. However, causal effects can vary (wildly) by assumption.\n\nIf an effect is identified and you estimate it from data non-parametrically, you obtain a complete distribution of causal effects (instead of, say, a single parameter). Accordingly, you can evaluate the causal effect of tobacco exposure conditional on asbestos exposure non-parametrically to see whether it changes, without committing to any functional form.\n\nLet's have a look at the structural equations in your case, which correspond to your \"DAG\" stripped of the red arrow:\n\nMesothelioma = $f_{1}$(Tobacco, Asbestos, $\\epsilon_{m}$)\n\nTobacco = $f_{2}$($\\epsilon_{t}$)\n\nAsbestos = $f_{3}$($\\epsilon_{a}$)\n\nwhere the $\\epsilon$ are assumed to be independent because of missing dashed arrows between them.\n\nWe have left the respective functions f() and the distributions of the errors unspecified, except for saying that the latter are independent. Nonetheless, we can apply Pearl's theory and immediately state that the causal effects of both tobacco and asbestos exposure on mesothelioma are \nidentified\n. This means that if we had infinitely many observations from this process, we could exactly measure the effect of \nsetting\n the exposures to different levels by simply \nseeing\n the incidences of mesothelioma in individuals with different levels of exposure. So we could infer causality without doing an actual experiment. This is because there exist no back-door paths from the exposure variables to the outcome variable.\n\nSo you would get\n\nP(mesothelioma | do(Tobacco = t)) = P(mesothelioma | Tobacco = t)\n\nThe same logic holds for the causal effect of asbestos, which allows you to simply evaluate:\n\nP(mesothelioma | Tobacco = t, Asbestos = a) - P(mesothelioma | Tobacco = t', Asbestos = a)\n\nin comparison to\n\nP(mesothelioma | Tobacco = t, Asbestos = a') - P(mesothelioma | Tobacco = t', Asbestos = a')\n\nfor all relevant values of t and a in order to estimate the interaction effects.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/350132/402101 \n So you would get\n\nP(mesothelioma | do(Tobacco = t)) = P(mesothelioma | Tobacco = t)\n\nThe same logic holds for the causal effect of asbestos, which allows you to simply evaluate:\n\nP(mesothelioma | Tobacco = t, Asbestos = a) - P(mesothelioma | Tobacco = t', Asbestos = a)\n\nin comparison to\n\nP(mesothelioma | Tobacco = t, Asbestos = a') - P(mesothelioma | Tobacco = t', Asbestos = a')\n\nfor all relevant values of t and a in order to estimate the interaction effects.\n\nIn your concrete example, let's assume that the outcome variable is a Bernoulli variable - you can either have mesothelioma or not - and that a person has been exposed to a very high asbestos level a. Then, it is very likely that he will suffer from mesothelioma; accordingly, the effect of increasing tobacco exposure will be very low. On the other hand, if asbestos levels a' are very low, increasing tobacco exposure will have a greater effect. This would constitute an interaction between the effects of tobacco and asbestos.\n\nOf course, non-parametric estimation can be extremely demanding and noisy with finite data and lots of different t and a values, so you might think about assuming some structure in f(). But basically you can do it without that.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6086/402101 \n Balance in the Training Set\n\nFor logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward:   Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept.  Details are in \nKing and Zeng (2001)\n [\nPDF\n].\n\nThese 'rare event corrections' were designed for case control research designs, mostly used in epidemiology, that select cases by choosing a fixed, usually balanced number of 0 cases and 1 cases, and then need to correct for the resulting sample selection bias.  Indeed, you might train your classifier the same way.  Pick a nice balanced sample and then correct the intercept to take into account the fact that you've selected on the dependent variable to learn more about rarer classes than a random sample would be able to tell you.\n\nMaking Predictions\n\nOn a related but distinct topic: Don't forget that you should be thresholding intelligently to make predictions.  It is not always best to predict 1 when the model probability is greater 0.5.  Another threshold may be better.  To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24833/402101 \n To understand what that diagram could mean, we have to define some things.  Let's say that Venn diagram displays the overlapping (or shared) variance amongst 4 different variables, and that we want to predict the level of $Wiki$ by recourse to our knowledge of $Digg$, $Forum$, and $Blog$.  That is, we want to be able to reduce the uncertainty (i.e., variance) in $Wiki$ from the null variance down to the residual variance.  How well can that be done?  That is the question that a \nVenn diagram\n is answering for you.\n\nEach circle represents a set of points, and thereby, an amount of variance.  For the most part, we are interested in the variance in $Wiki$, but the figure also displays the variances in the predictors.  There are a few things to notice about our figure.  First, each variable has the same amount of variance--they are all the same size (although not everyone will use Venn diagrams quite so literally).  Also, there is the same amount of overlap, etc., etc.  A more important thing to notice is that there is a good deal of overlap amongst the predictor variables.  This means that they are correlated.  This situation is very common when dealing with secondary (i.e., archival) data, observational research, or real-world prediction scenarios.  On the other hand, if this were a designed experiment, it would probably imply poor design or execution.  To continue with this example for a little bit longer, we can see that our predictive ability will be moderate; most of the variability in $Wiki$ remains as residual variability after all the variables have been used (eyeballing the diagram, I would guess $R^2\\approx.35$).  Another thing to note is that, once $Digg$ and $Blog$ have been entered into the model, $Forum$ accounts for \nnone\n of the variability in $Wiki$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24833/402101 \n Now, after having fit a model with multiple predictors, people often want to \ntest\n those predictors to see if they are related to the response variable (although it's not clear this is as important as people seem to believe it is).  Our problem is that to test these predictors, we must \npartition the Sum of Squares\n, and since our predictors are correlated, there are SS that could be attributed to \nmore than one\n predictor.  In fact, in the asterisked region, the SS could be attributed to \nany\n of the three predictors.  This means that there is no \nunique partition\n of the SS, and thus no unique test.  How this issue is handled depends on the \ntype of SS\n that the researcher uses and \nother judgments made by the researcher\n.  Since many software applications return type III SS by default, many people \nthrow away\n the information contained in the overlapping regions \nwithout realizing they have made a judgment call\n.  I explain these issues, the different types of SS, and go into some detail \nhere\n.\n\nThe question, as stated, specifically asks about where all of this shows up in the \nbetas\n / regression equation.  The answer is that it does not.  Some information about that is contained in my answer \nhere\n (although you'll have to read between the lines a little bit).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/288509/402101 \n An oracle knows the truth: it knows the true subset and is willing to act on it. The oracle property is that the asymptotic distribution of the estimator is the same as the asymptotic distribution of the MLE on only the true support. That is, the estimator adapts to knowing the true support \nwithout\n paying a price (in terms of the asymptotic distribution.)\n\nBy the asymptotic optimality properties of the MLE discussed in, for instance, Keener's theoretical statistics in theorem 9.14, we know, under some technical conditions which hold when, for instance, the error is Gaussian, that $$\\sqrt{n} \\left( \\hat\\beta_S - \\beta^*_S \\right) \\to \\mathcal{N} (0, I^{-1}(\\beta^*_S)),$$ where we assume that $\\beta^*_S$ is the true coefficient on the true support $S$. Notice that the variance of the asymptotic distribution is the inverse of the Fisher information, showing that $\\hat\\beta_S$ is asymptotically efficient. Since the MLE knowing the true support achieves this, it is also required as part of the oracle property.\n\nHowever, we do pay a steep nonasymptotic price: see, for instance,\n\nHannes Leeb, Benedikt M. P\u00f6tscher, Sparse estimators and the oracle property, or the return of Hodges\u2019 estimator, Journal of Econometrics, Volume 142, Issue 1, 2008, Pages 201-211,\n\nwhich shows that the risk of any \"oracle estimator\" (in the sense of Fan and Li, 2001) has a supremum which diverges to infinity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473501/402101 \n This is quite a ubiquitous misunderstanding of the central limit theorem, which I have also encountered in my statistical teaching.  Over the years I have encountered this problem so often that I have developed a Socratic method to deal with it.  I identify a student that has accepted this idea and then engage the student to tease out what this would logically imply.  It is fairly simple to get to the \nreductio ad absurdum\n of the false version of the theorem, which is that \nevery sequence of IID random variables has a normal distribution\n.  A typical conversation would go something like this.\n\nTeacher:\n I noticed in this assignment question that you said that because \n$n$\n is large, the data are approximately normally distributed.  Can you take me through your reasoning for that bit?\n\nStudent:\n Is that wrong?\n\nTeacher:\n I don't know.  Let's have a look at it.\n\nStudent:\n Well, I used that theorem you talked about in class; that main one you mentioned a bunch of times.  I forget the name.\n\nTeacher:\n The central limit theorem?\n\nStudent:\n Yeah, the central limit theorem.\n\nTeacher:\n Great, and when does that theorem apply?\n\nStudent:\n I think if the variables are IID.\n\nTeacher:\n And have finite variance.\n\nStudent:\n Yeah, and finite variance.\n\nTeacher:\n Okay, so the random variables have some \nfixed\n distribution with finite variance, is that right?\n\nStudent:\n Yeah.\n\nTeacher:\n And the distribution isn't changing or anything?\n\nStudent:\n No, they're IID with a fixed distribution.\n\nTeacher:\n Okay great, so let me see if I can state the theorem.  The central limit theorem says that if you have an IID sequence of random variables with finite variance, and you take a sample of \n$n$\n of them, then as that sample size \n$n$\n gets large the distribution of the random variables converges to a normal distribution.  Is that right?\n\nStudent:\n Yeah, I think so.\n\nTeacher:\n Okay great, so let's think about what that would mean.  Suppose I have a sequence like that.  If I take say, a thousand sample values, what is the distribution of those random variables?\n\nStudent:\n It's approximately a normal distribution.\n\nTeacher:\n How close?\n\nStudent:\n Pretty close I think.\n\nTeacher:\n Okay, what if I take a billion sample values.  How close now?\n\nStudent:\n Really close I'd say.\n\nTeacher:\n And if we have a sequence of these things, then in theory we can take \n$n$\n as high as we want can't we?  So we can make the distribution as close to a normal distribution as we want.\n\nStudent:\n Yeah.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473501/402101 \n Student:\n It's approximately a normal distribution.\n\nTeacher:\n How close?\n\nStudent:\n Pretty close I think.\n\nTeacher:\n Okay, what if I take a billion sample values.  How close now?\n\nStudent:\n Really close I'd say.\n\nTeacher:\n And if we have a sequence of these things, then in theory we can take \n$n$\n as high as we want can't we?  So we can make the distribution as close to a normal distribution as we want.\n\nStudent:\n Yeah.\n\nTeacher:\n So let's say we take \n$n$\n big enough that we're happy to say that the random variables basically have a normal distribution.  And that's a fixed distribution right?\n\nStudent:\n Yeah.\n\nTeacher:\n And they're IID right?  These random variables are IID?\n\nStudent:\n Yeah, they're IID.\n\nTeacher:\n Okay, so they all have the same distribution.\n\nStudent:\n Yeah.\n\nTeacher:\n Okay, so that means the \nfirst\n value in the sequence, it also has a normal distribution.  Is that right?\n\nStudent:\n Yeah.  I mean, it's an approximation, but yeah, if \n$n$\n is really large then it effectively has a normal distribution.\n\nTeacher:\n Okay great.  And so does the second value in the sequence, and so on, right?\n\nStudent:\n Yeah.\n\nTeacher:\n Okay, so really, as soon as we started sampling, we were already getting values that are essentially normal distributed.  We didn't really need to wait until \n$n$\n gets large before that started happening.\n\nStudent:\n Hmmm.  I'm not sure.  That sounds wrong.  The theorem says you need a large \n$n$\n, so I guess I think you can't apply it if you only sampled a small number of values.\n\nTeacher:\n Okay, so let's say we are sampling a billion values.  Then we have large \n$n$\n.  And we've established that this means that the first few random variables in the sequence are normally distributed, to a very close approximation.  If that's true, can't we just stop sampling early?  Say we were going to sample a billion values, but then we stop sampling after the first value.  Was that random variable still normally distributed?\n\nStudent:\n I think maybe it isn't.\n\nTeacher:\n Okay, so at some point its distribution changes?\n\nStudent:\n I'm not sure.  I'm a bit confused about it now.\n\nTeacher:\n Hmmm, well it seems we have something strange going on here.  Why don't you have another read of the material on the central limit theorem and see if you can figure out how to resolve that contradiction.  Let's talk more about it then.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473501/402101 \n Student:\n I think maybe it isn't.\n\nTeacher:\n Okay, so at some point its distribution changes?\n\nStudent:\n I'm not sure.  I'm a bit confused about it now.\n\nTeacher:\n Hmmm, well it seems we have something strange going on here.  Why don't you have another read of the material on the central limit theorem and see if you can figure out how to resolve that contradiction.  Let's talk more about it then.\n\nThat is one possible approach, which seeks to reduce the false theorem down to the \nreductio\n which says that every IID sequence (with finite variance) must be composed of normal random variables.  (You should be prepared to encounter some variations in the student's views, which might include confusion between the probability distribution of a random variable and the sampling distribution of a sample.  Additional confusions or issues may take the conversation off on a tangent, but the above captures a common flow of argument.)  Assuming that the conversation goes as it should, either the student will get to the false (and absurd) conclusion and realise something is wrong, or they will defend against this conclusion by saying that the distribution changes as \n$n$\n gets large (or they may handwave a bit, and you might have to lawyer them to a conclusion).  Either way, this usually provokes some further thinking that can lead them to re-read the theorem.  Here is another approach:\n\nTeacher:\n Let's look at this another way.  Suppose we have an IID sequence of random variables from some other distribution; one that is \nnot\n a normal distribution.  Is that possible?  For example, could we have a sequence of random variables representing outcome of coin flip, from the Bernoulli distribution?\n\nStudent:\n Yeah, we can have that.\n\nTeacher:\n Okay, great.  And these are all IID values, so again, they all have the same distribution.  So every random variable in that sequence is going to have a distribution that is \nnot\n a normal distribution, right?\n\nStudent:\n Yeah.\n\nTeacher:\n In fact, in this case, every value in the sequence will be the outcome of a coin flip, which we set as zero or one.  Is that right?\n\nStudent:\n Yeah, as long as we label them that way.\n\nTeacher:\n Okay, great.  So if all the values in the sequence are zeroes or ones,\nno matter how many of them we sample, we are always going to get a histogram showing values at zero and one, right?\n\nStudent:\n Yeah.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473501/402101 \n Student:\n Yeah.\n\nTeacher:\n In fact, in this case, every value in the sequence will be the outcome of a coin flip, which we set as zero or one.  Is that right?\n\nStudent:\n Yeah, as long as we label them that way.\n\nTeacher:\n Okay, great.  So if all the values in the sequence are zeroes or ones,\nno matter how many of them we sample, we are always going to get a histogram showing values at zero and one, right?\n\nStudent:\n Yeah.\n\nTeacher:\n Okay.  And do you think if we sample more and more values, we will get closer and closer to the true distribution?  Like, if it is a fair coin, does the histogram eventually converge to where the relative frequency bars are the same height?\n\nStudent:\n I guess so.  I think it does.\n\nTeacher:\n I think you're right.  In fact, we call that result the \"law of large numbers\".  Anyway, it seems like we have a bit of a problem here doesn't it.  If we sample a large number of the values then the central limit theorem says we converge to a normal distribution, but it sounds like the \"law of large numbers\" says we actually converge to the true distribution, which isn't a normal distribution.  In fact, it's a distribution that is just probabilities on the zero value and the one value, which looks nothing like the normal distribution.  So which is it?\n\nStudent:\n I think when \n$n$\n is large it looks like a normal distribution.\n\nTeacher:\n So describe it to me.  Let's say we have flipped the coin a billion times.  Describe the distribution of the outcomes and explain why that looks like a normal distribution.\n\nStudent:\n I'm not really sure how to do that.\n\nTeacher:\n Okay.  Well, do you agree that if we have a billion coin flips, all those outcomes are zeroes and ones?\n\nStudent:\n Yeah.\n\nTeacher:\n Okay, so describe what its histogram looks like.\n\nStudent:\n It's just two bars on those values.\n\nTeacher:\n Okay, so not \"bell curve\" shaped?\n\nStudent:\n Yeah, I guess not.\n\nTeacher:\n Hmmm, so perhaps the central limit theorem doesn't say what we thought.  Why don't you read the material on the central limit theorem again and see if you can figure out what it says.  Let's talk more about it then.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/47782/402101 \n The short version is that the Beta distribution can be understood as representing a distribution \nof probabilities\n, that is, it represents all the possible values of a probability when we don't know what that probability is. Here is my favorite intuitive explanation of this:\n\nAnyone who follows baseball is familiar with \nbatting averages\n\u2014simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it's just a percentage between \n0\n and \n1\n). \n.266\n is in general considered an average batting average, while \n.300\n is considered an excellent one.\n\n0\n\n1\n\n.266\n\n.300\n\nImagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly \n1.000\n, while if he strikes out, his batting average is \n0.000\n. It doesn't get much better if you go up to bat five or six times- you could get a lucky streak and get an average of \n1.000\n, or an unlucky streak and get an average of \n0\n, neither of which are a remotely good predictor of how you will bat that season.\n\n1.000\n\n0.000\n\n1.000\n\n0\n\nWhy is your batting average in the first few hits not a good predictor of your eventual batting average? When a player's first at-bat is a strikeout, why does no one predict that he'll never get a hit all season? Because we're going in with \nprior expectations.\n We know that in history, most batting averages over a season have hovered between something like \n.215\n and \n.360\n, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he'll end up a bit worse than average, but we know he probably won't deviate from that range.\n\n.215\n\n.360\n\nGiven our batting average problem, which can be represented with a \nbinomial distribution\n (a series of successes and failures), the best way to represent these prior expectations (what we in statistics just call a \nprior\n) is with the Beta distribution- it's saying, before we've seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is \n(0, 1)\n, just like a probability, so we already know we're on the right track, but the appropriateness of the Beta for this task goes far beyond that.\n\n(0, 1)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/47782/402101 \n (0, 1)\n\nWe expect that the player's season-long batting average will be most likely around \n.27\n, but that it could reasonably range from \n.21\n to \n.35\n. This can be represented with a Beta distribution with parameters \n$\\alpha=81$\n and \n$\\beta=219$\n:\n\n.27\n\n.21\n\n.35\n\ncurve(dbeta(x, 81, 219))\n\ncurve(dbeta(x, 81, 219))\n\n\n\nI came up with these parameters for two reasons:\n\nThe mean is \n$\\frac{\\alpha}{\\alpha+\\beta}=\\frac{81}{81+219}=.270$\n\n\nAs you can see in the plot, this distribution lies almost entirely within \n(.2, .35)\n- the reasonable range for a batting average.\n\n(.2, .35)\n\nYou asked what the x axis represents in a beta distribution density plot\u2014here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution \nof probabilities\n.\n\nBut here's why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now \n1 hit; 1 at bat\n. We have to then \nupdate\n our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (\nit's shown here\n), the result is \nvery simple\n. The new Beta distribution will be:\n\n1 hit; 1 at bat\n\n$\\mbox{Beta}(\\alpha_0+\\mbox{hits}, \\beta_0+\\mbox{misses})$\n\nWhere \n$\\alpha_0$\n and \n$\\beta_0$\n are the parameters we started with- that is, 81 and 219. Thus, in this case, \n$\\alpha$\n  has increased by 1 (his one hit), while \n$\\beta$\n has not increased at all (no misses yet). That means our new distribution is \n$\\mbox{Beta}(81+1, 219)$\n, or:\n\ncurve(dbeta(x, 82, 219))\n\ncurve(dbeta(x, 82, 219))\n\n\n\nNotice that it has barely changed at all- the change is indeed invisible to the naked eye! (That's because one hit doesn't really mean anything).\n\nHowever, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be \n$\\mbox{Beta}(81+100, 219+200)$\n, or:\n\ncurve(dbeta(x, 81+100, 219+200))\n\ncurve(dbeta(x, 81+100, 219+200))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/47782/402101 \n However, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be \n$\\mbox{Beta}(81+100, 219+200)$\n, or:\n\ncurve(dbeta(x, 81+100, 219+200))\n\ncurve(dbeta(x, 81+100, 219+200))\n\n\n\nNotice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player's batting average is.\n\nOne of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is \n$\\frac{\\alpha}{\\alpha+\\beta}$\n. Thus, after 100 hits of 300 \nreal\n at-bats, the expected value of the new Beta distribution is \n$\\frac{81+100}{81+100+219+200}=.303$\n- notice that it is lower than the naive estimate of \n$\\frac{100}{100+200}=.333$\n, but higher than the estimate you started the season with (\n$\\frac{81}{81+219}=.270$\n). You might notice that this formula is equivalent to adding a \"head start\" to the number of hits and non-hits of a player- you're saying \"start him off in the season with 81 hits and 219 non hits on his record\").\n\nThus, the Beta distribution is best for representing a probabilistic distribution \nof probabilities\n: the case where we don't know what a probability is in advance, but we have some reasonable guesses.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/29326/402101 \n Linear regression uses the general linear equation $Y=b_0+\u2211(b_i X_i)+\\epsilon$ where $Y$ is a continuous dependent variable and independent variables $X_i$ are \nusually\n continuous (but can also be binary, e.g. when the linear model is used in a t-test) or other discrete domains. $\\epsilon$ is a term for the variance that is not explained by the model and is usually just called \"error\". Individual dependent values denoted by $Y_j$ can be solved by modifying the equation a little: $Y_j=b_0 + \\sum{(b_i X_{ij})+\\epsilon_j}$\n\nLogistic regression is another generalized linear model (GLM) procedure using the same basic formula, but instead of the continuous $Y$, it is regressing for the probability of a categorical outcome. In simplest form, this means that we're considering just one outcome variable and two states of that variable- either 0 or 1.\n\nThe equation for the probability of $Y=1$ looks like this:\n$$\nP(Y=1) = {1 \\over 1+e^{-(b_0+\\sum{(b_iX_i)})}}\n$$\n\nYour independent variables $X_i$ can be continuous or binary. The regression coefficients $b_i$ can be exponentiated to give you the change in odds of $Y$ per change in $X_i$, i.e., $Odds={P(Y=1) \\over P(Y=0)}={P(Y=1) \\over 1-P(Y=1)}$ and ${\\Delta Odds}=  e^{b_i}$.  $\\Delta Odds$ is called the odds ratio, $Odds(X_i+1)\\over Odds(X_i)$. In English, you can say that the odds of $Y=1$ increase by a factor of $e^{b_i}$ per unit change in $X_i$.\n\nExample: If you wanted to see how body mass index predicts blood cholesterol (a continuous measure), you'd use linear regression as described at the top of my answer. If you wanted to see how BMI predicts the odds of being a diabetic (a binary diagnosis), you'd use logistic regression.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/179/402101 \n For a univariate KDE, you are better off using something other than Silverman's rule which is based on a normal approximation. One excellent approach is the Sheather-Jones method, easily implemented in R; for example,\n\nplot(density(precip, bw=\"SJ\"))\n\nplot(density(precip, bw=\"SJ\"))\n\nThe situation for multivariate KDE is not so well studied, and the tools are not so mature. Rather than a bandwidth, you need a bandwidth matrix. To simplify the problem, most people assume a diagonal matrix, although this may not lead to the best results. The \nks package in R\n provides some very useful tools including allowing a full (not necessarily diagonal) bandwidth matrix.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3443/402101 \n At the beginning of an article aiming at promoting the use of PSs in epidemiology, Oakes and Church (1) cited Hern\u00e1n and Robins's claims about confounding effect in epidemiology (2):\n\nCan you guarantee that the results\n  from your observational study are\n  unaffected by unmeasured confounding?\n  The only answer an epidemiologist can\n  provide is \u2018no\u2019.\n\nThis is not just to say that we cannot ensure that results from observational studies are unbiased or useless (because, as @propofol said, their results can be useful for designing RCTs), but also that PSs do certainly not offer a complete solution to this problem, or at least do not necessarily yield better results than other matching or multivariate methods (see e.g. (10)).\n\nPropensity scores (PS) are, by construction, \nprobabilistic\n not \ncausal\n indicators. The choice of the covariates that enter the propensity score function is a key element for ensuring its reliability, and their weakness, as has been said, mainly stands from not controlling for unobserved confounders (which is quite likely in retrospective or \ncase-control\n studies). Others factors have to be considered: (a) model misspecification will impact direct effect estimates (not really more than in the OLS case, though), (b) there may be missing data at the level of the covariates, (c) PSs do not overcome synergistic effects which are know to affect causal interpretation (8,9).\n\nAs for references, I found Roger Newson's slides -- \nCausality, confounders, and propensity scores\n -- relatively well-balanced about the pros and cons of using propensity scores, with illustrations from real studies.\nThere were also several good papers discussing the use of propensity scores in observational studies or environmental epidemiology two years ago in \nStatistics in Medicine\n, and I enclose a couple of them at the end (3-6). But I like Pearl's review (7) because it offers a larger perspective on causality issues (PSs are discussed p. 117 and 130). Obviously, you will find many more illustrations by looking at applied research. I would like to add two recent articles from William R Shadish that came across Andrew Gelman's website (11,12). The use of propensity scores is discussed, but the two papers more largely focus on causal inference in observational studies (and how it compare to randomized settings).\n\nReferences",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3443/402101 \n References\n\nOakes, J.M. and Church, T.R. (2007). \nInvited Commentary: Advancing Propensity Score Methods in Epidemiology\n. \nAmerican Journal of Epidemiology\n, 165(10), 1119-1121.\n\n\nHernan M.A. and Robins J.M. (2006). \nInstruments for causal inference: an epidemiologist's dream?\n \nEpidemiology\n, 17, 360-72.\n\n\nRubin, D. (2007). \nThe design versus the analysis of observational studies for causal effects: Parallels with the design of randomized trials\n. \nStatistics in Medicine\n, 26, 20\u201336.\n\n\nShrier, I. (2008). \nLetter to the editor\n. \nStatistics in Medicine\n, 27, 2740\u20132741.\n\n\nPearl, J. (2009). \nRemarks on the method of propensity score\n. \nStatistics in Medicine\n, 28, 1415\u20131424.\n\n\nStuart, E.A. (2008). \nDeveloping practical recommendations for the use of propensity scores: Discussion of \u2018A critical appraisal of propensity score matching in the medical literature between 1996 and 2003\u2019 by Peter Austin\n. \nStatistics in Medicine\n, 27, 2062\u20132065. \n\n\nPearl, J. (2009). \nCausal inference in statistics: An overview\n. \nStatistics Surveys\n, 3, 96-146.\n\n\nOakes, J.M. and Johnson, P.J. (2006). \nPropensity score matching for social epidemiology\n. In \nMethods in Social Epidemiology\n, J.M. Oakes and S. Kaufman (Eds.), pp. 364-386. Jossez-Bass.\n\n\nH\u00f6fler, M (2005). \nCausal inference based on counterfactuals\n. \nBMC Medical Research Methodology\n, 5, 28.\n\n\nWinkelmayer, W.C. and Kurth, T. (2004). \nPropensity scores: help or hype?\n \nNephrology Dialysis Transplantation\n, 19(7), 1671-1673.\n\n\nShadish, W.R., Clark, M.H., and Steiner, P.M. (2008). \nCan Nonrandomized Experiments Yield Accurate Answers? A Randomized Experiment Comparing Random and Nonrandom Assignments\n. \nJASA\n, 103(484), 1334-1356.\n\n\nCook, T.D., Shadish, W.R., and Wong, V.C. (2008). \nThree Conditions under Which Experiments and Observational Studies Produce Comparable Causal Estimates: New Findings from Within-Study Comparisons\n. \nJournal of Policy Analysis and Management\n, 27(4), 724\u2013750.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/85914/402101 \n Traditionally, the \nnull hypothesis\n is a point value.  (It is typically $0$, but can in fact be any point value.)  The alternative hypothesis is that the true value is \nany value other than the null value\n.  Because a continuous variable (such as a mean difference) can take on a value which is indefinitely close to the null value but still not quite equal and thus make the null hypothesis false, a traditional point null hypothesis cannot be proven.\n\nImagine your null hypothesis is $0$, and the mean difference you observe is $0.01$.  Is it reasonable to assume the null hypothesis is true?  You don't know yet; it would be helpful to know what our \nconfidence interval\n looks like.  Let's say that your 95% confidence interval is $(-4.99,\\ 5.01)$.  Now, should we conclude that the true value is $0$?  I would not feel comfortable saying that, because the CI is very wide, and there are many, large non-zero values that we might reasonably suspect are consistent with our data.  So let's say we gather much, much more data, and now our observed mean difference is $0.01$, but the 95% CI is $(0.005,\\ 0.015)$.  The observed mean difference has stayed the same (which would be amazing if it really happened), but the confidence interval now excludes the null value.  Of course, this is just a thought experiment, but it should make the basic ideas clear.  We can never prove that the true value is any particular point value; we can only (possibly) disprove that it is some point value.  In statistical hypothesis testing, the fact that the p-value is > 0.05 (and that the 95% CI includes zero) means that \nwe are not sure if the null hypothesis is true\n.\n\nAs for your concrete case, you cannot construct a test where the alternative hypothesis is that the mean difference is $0$ and the null hypothesis is anything other than zero.  This violates the logic of hypothesis testing.  It is perfectly reasonable that it is your substantive, scientific hypothesis, but it cannot be your alternative hypothesis in a hypothesis testing situation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/85914/402101 \n As for your concrete case, you cannot construct a test where the alternative hypothesis is that the mean difference is $0$ and the null hypothesis is anything other than zero.  This violates the logic of hypothesis testing.  It is perfectly reasonable that it is your substantive, scientific hypothesis, but it cannot be your alternative hypothesis in a hypothesis testing situation.\n\nSo what can you do?  In this situation, you use equivalence testing.  (You might want to read through some of our threads on this topic by clicking on the \nequivalence\n tag.)  The typical strategy is to use the two one sided tests approach.  Very briefly, you select an interval within which you would consider that the true mean difference might as well be $0$ for all you could care, then you perform a one-sided test to determine if the observed value is less than the upper bound of that interval, and another one-sided test to see if it is greater than the lower bound.  If both of these tests are significant, then you have rejected the hypothesis that the true value is outside the interval you care about.  If one (or both) are non-significant, you fail to reject the hypothesis that the true value is outside the interval.\n\nFor example, suppose anything within the interval $(-0.02,\\ 0.02)$ is so close to zero that you think it is essentially the same as zero for your purposes, so you use that as your substantive hypothesis.  Now imagine that you get the first result described above.  Although $0.01$ falls within that interval, you would not be able to reject the null hypothesis on either one-sided t-test, so you would fail to reject the null hypothesis.  On the other hand, imagine that you got the second result described above.  Now you find that the observed value falls within the designated interval, and it can be shown to be both less than the upper bound and greater than the lower bound, so you can reject the null.  (It is worth noting that you can reject \nboth\n the hypothesis that the true value is $0$, \nand\n the hypothesis that the true value lies outside of the interval $(-0.02,\\ 0.02)$, which may seem perplexing at first, but is fully consistent with the logic of hypothesis testing.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/14550/402101 \n First you have to decide if you really need model selection, or you just need to model.  In the majority of situations, depending on dimensionality, fitting a flexible comprehensive model is preferred.\n\nThe bootstrap is a great way to estimate the performance of a model.  The simplest thing to estimate is variance.  More to your original point, the bootstrap can estimate the likely future performance of a given modeling procedure, on new data not yet realized.\n\nIf using resampling (bootstrap or cross-validation) to both choose model tuning parameters and to estimate the model, you will need a double bootstrap or nested cross-validation.\n\nIn general the bootstrap requires fewer model fits (often around 300) than cross-validation (10-fold cross-validation should be repeated 50-100 times for stability).\n\nSome simulation studies may be found at \nhttp://biostat.mc.vanderbilt.edu/rms",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n This is a subtle question.\n  It takes a thoughtful person \nnot\n to understand those quotations!  Although they are suggestive, it turns out that none of them is exactly or generally correct.  I haven't the time (and there isn't the space here) to give a full exposition, but I would like to share one approach and an insight that it suggests.\n\nWhere does the concept of degrees of freedom (DF) arise?\n  The contexts in which it's found in elementary treatments are:\n\nThe \nStudent t-test\n and its variants such as the Welch or Satterthwaite solutions to the Behrens-Fisher problem (where two populations have different variances).\n\n\nThe Chi-squared distribution (defined as a sum of squares of independent standard Normals), which is implicated in the \nsampling distribution of the variance.\n\n\nThe \nF-test\n (of ratios of estimated variances).\n\n\nThe \nChi-squared test\n, comprising its uses in (a) testing for independence in contingency tables and (b) testing for goodness of fit of distributional estimates.\n\nThe \nStudent t-test\n and its variants such as the Welch or Satterthwaite solutions to the Behrens-Fisher problem (where two populations have different variances).\n\nThe Chi-squared distribution (defined as a sum of squares of independent standard Normals), which is implicated in the \nsampling distribution of the variance.\n\nThe \nF-test\n (of ratios of estimated variances).\n\nThe \nChi-squared test\n, comprising its uses in (a) testing for independence in contingency tables and (b) testing for goodness of fit of distributional estimates.\n\nIn spirit, these tests run a gamut from being exact (the Student t-test and F-test for Normal variates) to being good approximations (the Student t-test and the Welch/Satterthwaite tests for not-too-badly-skewed data) to being based on asymptotic approximations (the Chi-squared test).  An interesting aspect of some of these is the appearance of non-integral \"degrees of freedom\" (the Welch/Satterthwaite tests and, as we will see, the Chi-squared test).  This is of especial interest because it is the first hint that DF is \nnot\n any of the things claimed of it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n We can dispose right away of some of the claims in the question.\n  Because \"final calculation of a statistic\" is not well-defined (it apparently depends on what algorithm one uses for the calculation), it can be no more than a vague suggestion and is worth no further criticism.  Similarly, neither \"number of independent scores that go into the estimate\" nor \"the number of parameters used as intermediate steps\" are well-defined.\n\n\"Independent pieces of information that go into [an] estimate\"\n is difficult to deal with, because there are two different but intimately related senses of \"independent\" that can be relevant here.  One is independence of random variables; the other is \nfunctional independence.\n  As an example of the latter, suppose we collect morphometric measurements of subjects--say, for simplicity, the three side lengths $X$, $Y$, $Z$, surface areas $S=2(XY+YZ+ZX)$, and volumes $V=XYZ$ of a set of wooden blocks.  The three side lengths can be considered independent random variables, but all five variables are dependent RVs.  The five are also \nfunctionally\n dependent because the \ncodomain\n (\nnot\n the \"domain\"!) of the vector-valued random variable $(X,Y,Z,S,V)$ traces out a three-dimensional manifold in $\\mathbb{R}^5$.  (Thus, locally at any point $\\omega\\in\\mathbb{R}^5$, there are two functions $f_\\omega$ and $g_\\omega$ for which $f_\\omega(X(\\psi),\\ldots,V(\\psi))=0$ and $g_\\omega(X(\\psi),\\ldots,V(\\psi))=0$ for points $\\psi$ \"near\" $\\omega$ and the derivatives of $f$ and $g$ evaluated at $\\omega$ are linearly independent.)  However--here's the kicker--for many probability measures on the blocks, subsets of the variables such as $(X,S,V)$ are \ndependent\n as random variables but functionally \nindependent.\n\nHaving been alerted by these potential ambiguities, \nlet's hold up the Chi-squared goodness of fit test for examination\n, because (a) it's simple, (b) it's one of the common situations where people really do need to know about DF to get the p-value right and (c) it's often used incorrectly.  Here's a brief synopsis of the least controversial application of this test:\n\nYou have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n Having been alerted by these potential ambiguities, \nlet's hold up the Chi-squared goodness of fit test for examination\n, because (a) it's simple, (b) it's one of the common situations where people really do need to know about DF to get the p-value right and (c) it's often used incorrectly.  Here's a brief synopsis of the least controversial application of this test:\n\nYou have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.\n\n\nYou have estimated some parameters $\\theta_1, \\ldots, \\theta_p$ of a distribution. For example, you estimated the mean $\\theta_1$ and standard deviation $\\theta_2 = \\theta_p$ of a Normal distribution, hypothesizing that the population is normally distributed but not knowing (in advance of obtaining the data) what $\\theta_1$ or $\\theta_2$ might be.\n\n\nIn advance, you created a set of $k$ \"bins\" for the data.  (It may be problematic when the bins are determined by the data, even though this is often done.)  Using these bins, the data are reduced to the set of counts within each bin.  Anticipating what the true values of $(\\theta)$ might be, you have arranged it so (hopefully) each bin will receive approximately the same count.  (Equal-probability binning assures the chi-squared distribution really is a good approximation to the true distribution of the chi-squared statistic about to be described.)\n\n\nYou have a lot of data--enough to assure that almost all bins ought to have counts of 5 or greater.  (This, we hope, will enable the sampling distribution of the $\\chi^2$ statistic to be approximated adequately by some $\\chi^2$ distribution.)\n\nYou have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.\n\nYou have estimated some parameters $\\theta_1, \\ldots, \\theta_p$ of a distribution. For example, you estimated the mean $\\theta_1$ and standard deviation $\\theta_2 = \\theta_p$ of a Normal distribution, hypothesizing that the population is normally distributed but not knowing (in advance of obtaining the data) what $\\theta_1$ or $\\theta_2$ might be.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n You have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.\n\nYou have estimated some parameters $\\theta_1, \\ldots, \\theta_p$ of a distribution. For example, you estimated the mean $\\theta_1$ and standard deviation $\\theta_2 = \\theta_p$ of a Normal distribution, hypothesizing that the population is normally distributed but not knowing (in advance of obtaining the data) what $\\theta_1$ or $\\theta_2$ might be.\n\nIn advance, you created a set of $k$ \"bins\" for the data.  (It may be problematic when the bins are determined by the data, even though this is often done.)  Using these bins, the data are reduced to the set of counts within each bin.  Anticipating what the true values of $(\\theta)$ might be, you have arranged it so (hopefully) each bin will receive approximately the same count.  (Equal-probability binning assures the chi-squared distribution really is a good approximation to the true distribution of the chi-squared statistic about to be described.)\n\nYou have a lot of data--enough to assure that almost all bins ought to have counts of 5 or greater.  (This, we hope, will enable the sampling distribution of the $\\chi^2$ statistic to be approximated adequately by some $\\chi^2$ distribution.)\n\nUsing the parameter estimates, you can compute the expected count in each bin.  The Chi-squared statistic is the sum of the ratios\n\n$$\\frac{(\\text{observed}-\\text{expected})^2}{\\text{expected}}.$$\n\nThis, many authorities tell us, should have (to a very close approximation) a Chi-squared distribution.  But there's a whole family of such distributions.  They are differentiated by a parameter $\\nu$ often referred to as the \"degrees of freedom.\"  \nThe standard reasoning\n about how to determine $\\nu$ goes like this\n\nI have $k$ counts.  That's $k$ pieces of data.  But there are (\nfunctional\n) relationships among them.  To start with, I know in advance that the sum of the counts must equal $n$.  That's one relationship.  I estimated two (or $p$, generally) parameters from the data.  That's two (or $p$) additional relationships, giving $p+1$ total relationships.  Presuming they (the parameters) are all (\nfunctionally\n) independent, that leaves only $k-p-1$ (\nfunctionally\n) independent \"degrees of freedom\": that's the value to use for $\\nu$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n The problem with this reasoning\n (which is the sort of calculation the quotations in the question are hinting at) \nis that it's wrong except when some special additional conditions hold.\n  Moreover, those conditions have \nnothing\n to do with independence (functional or statistical), with numbers of \"components\" of the data, with the numbers of parameters, nor with anything else referred to in the original question.\n\nLet me show you with an example.  (To make it as clear as possible, I'm using a small number of bins, but that's not essential.)  Let's generate 20 independent and identically distributed (iid) standard Normal variates and estimate their mean and standard deviation with the usual formulas (mean = sum/count, \netc\n.).  To test goodness of fit, create four bins with cutpoints at the quartiles of a standard normal: -0.675, 0, +0.657, and use the bin counts to generate a Chi-squared statistic.  Repeat as patience allows; I had time to do 10,000 repetitions.\n\nThe standard wisdom about DF says we have 4 bins and 1+2 = 3 constraints, implying the distribution of these 10,000 Chi-squared statistics should follow a Chi-squared distribution with 1 DF.  Here's the histogram:\n\n\n\nThe dark blue line graphs the PDF of a $\\chi^2(1)$ distribution--the one we thought would work--while the dark red line graphs that of a $\\chi^2(2)$ distribution (which would be a good guess if someone were to tell you that $\\nu=1$ is incorrect).  \nNeither fits the data.\n\nYou might expect the problem to be due to the small size of the data sets ($n$=20) or perhaps the small size of the number of bins.  However, the problem persists even with very large datasets and larger numbers of bins: it is not merely a failure to reach an asymptotic approximation.\n\nThings went wrong because I violated two requirements of the Chi-squared test:\n\nYou must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)\n\nYou must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n You must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)\n\nYou must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)\n\n\n\nThe red histogram depicts the chi-squared statistics for 10,000 separate iterations, following these requirements.  Sure enough, it visibly follows the $\\chi^2(1)$ curve (with an acceptable amount of sampling error), as we had originally hoped.\n\nThe point of this comparison--which I hope you have seen coming--is that the correct DF to use for computing the p-values depends on many things \nother\n than dimensions of manifolds, counts of functional relationships, or the geometry of Normal variates.  There is a subtle, delicate interaction between certain \nfunctional dependencies,\n as found in mathematical relationships among quantities, and \ndistributions\n of the data, their statistics, and the estimators formed from them.  Accordingly, \nit cannot be the case that DF is adequately explainable in terms of the geometry of multivariate normal distributions, or in terms of functional independence, or as counts of parameters, or anything else of this nature.\n\nWe are led to see, then, that \"degrees of freedom\" is merely a \nheuristic\n that \nsuggests\n what the sampling distribution of a (t, Chi-squared, or F) statistic ought to be, \nbut it is not dispositive.\n  Belief that it is dispositive leads to egregious errors.  (For instance, the \ntop hit\n on Google when searching \"chi squared goodness of fit\" is a \nWeb page from an Ivy League university\n that gets most of this completely wrong!  In particular, a simulation based on its instructions shows that the chi-squared value it recommends as having 7 DF actually has 9 DF.)\n\nWith this more nuanced understanding, it's worthwhile to re-read the Wikipedia article in question: in its details it gets things right, pointing out where the DF heuristic tends to work and where it is either an approximation or does not apply at all.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n With this more nuanced understanding, it's worthwhile to re-read the Wikipedia article in question: in its details it gets things right, pointing out where the DF heuristic tends to work and where it is either an approximation or does not apply at all.\n\nA good account of the phenomenon illustrated here (unexpectedly high DF in Chi-squared GOF tests) appears in \nVolume II of Kendall & Stuart, 5th edition\n.  I am grateful for the opportunity afforded by this question to lead me back to this wonderful text, which is full of such useful analyses.\n\nHere is \nR\n code to produce the figure following \"The standard wisdom about DF...\"\n\nR\n\n#\n# Simulate data, one iteration per column of `x`.\n#\nn <- 20\nn.sim <- 1e4\nbins <- qnorm(seq(0, 1, 1/4))\nx <- matrix(rnorm(n*n.sim), nrow=n)\n#\n# Compute statistics.\n#\nm <- colMeans(x)\ns <- apply(sweep(x, 2, m), 2, sd)\ncounts <- apply(matrix(as.numeric(cut(x, bins)), nrow=n), 2, tabulate, nbins=4)\nexpectations <- mapply(function(m,s) n*diff(pnorm(bins, m, s)), m, s)\nchisquared <- colSums((counts - expectations)^2 / expectations)\n#\n# Plot histograms of means, variances, and chi-squared stats.  The first\n# two confirm all is working as expected.\n#\nmfrow <- par(\"mfrow\")\npar(mfrow=c(1,3))\nred <- \"#a04040\"  # Intended to show correct distributions\nblue <- \"#404090\" # To show the putative chi-squared distribution\nhist(m, freq=FALSE)\ncurve(dnorm(x, sd=1/sqrt(n)), add=TRUE, col=red, lwd=2)\nhist(s^2, freq=FALSE)\ncurve(dchisq(x*(n-1), df=n-1)*(n-1), add=TRUE, col=red, lwd=2)\nhist(chisquared, freq=FALSE, breaks=seq(0, ceiling(max(chisquared)), 1/4), \n     xlim=c(0, 13), ylim=c(0, 0.55), \n     col=\"#c0c0ff\", border=\"#404040\")\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=2)), add=TRUE, col=red, lwd=2)\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=1)), add=TRUE, col=blue, lwd=2)\npar(mfrow=mfrow)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/894/402101 \n #\n# Simulate data, one iteration per column of `x`.\n#\nn <- 20\nn.sim <- 1e4\nbins <- qnorm(seq(0, 1, 1/4))\nx <- matrix(rnorm(n*n.sim), nrow=n)\n#\n# Compute statistics.\n#\nm <- colMeans(x)\ns <- apply(sweep(x, 2, m), 2, sd)\ncounts <- apply(matrix(as.numeric(cut(x, bins)), nrow=n), 2, tabulate, nbins=4)\nexpectations <- mapply(function(m,s) n*diff(pnorm(bins, m, s)), m, s)\nchisquared <- colSums((counts - expectations)^2 / expectations)\n#\n# Plot histograms of means, variances, and chi-squared stats.  The first\n# two confirm all is working as expected.\n#\nmfrow <- par(\"mfrow\")\npar(mfrow=c(1,3))\nred <- \"#a04040\"  # Intended to show correct distributions\nblue <- \"#404090\" # To show the putative chi-squared distribution\nhist(m, freq=FALSE)\ncurve(dnorm(x, sd=1/sqrt(n)), add=TRUE, col=red, lwd=2)\nhist(s^2, freq=FALSE)\ncurve(dchisq(x*(n-1), df=n-1)*(n-1), add=TRUE, col=red, lwd=2)\nhist(chisquared, freq=FALSE, breaks=seq(0, ceiling(max(chisquared)), 1/4), \n     xlim=c(0, 13), ylim=c(0, 0.55), \n     col=\"#c0c0ff\", border=\"#404040\")\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=2)), add=TRUE, col=red, lwd=2)\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=1)), add=TRUE, col=blue, lwd=2)\npar(mfrow=mfrow)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49065/402101 \n Overfitting comes from allowing too large a class of models. This gets a bit tricky with models with continuous parameters (like splines and polynomials), but if you discretize the parameters into some number of distinct values, you'll see that increasing the number of knots/coefficients will increase the number of available models exponentially. For every dataset there is a spline and a polynomial that fits precisely, so long as you allow enough coefficients/knots. It may be that a spline with three knots overfits more than a polynomial with three coefficients, but that's hardly a fair comparison.\n\nIf you have a low number of parameters, and a large dataset, you can be reasonably sure you're not overfitting. If you want to try higher numbers of parameters you can try cross validating within your test set to find the best number, or you can use a criterion like \nMinimum Description Length\n.\n\nEDIT\n: As requested in the comments, an example of how one would apply MDL. First you have to deal with the fact that your data is continuous, so it can't be represented in a finite code. For the sake of simplicity we'll segment the data space into boxes of side \n$\\epsilon$\n and instead of describing the data points, we'll describe the boxes that the data falls into. This means we lose some accuracy, but we can make \n$\\epsilon$\n arbitrarily small, so it doesn't matter much.\n\nNow, the task is to describe the dataset as sucinctly as possible with the help of some polynomial. First we describe the polynomial. If it's an n-th order polynomial, we just need to store (n+1) coefficients. Again, we need to discretize these values. After that we need to store first the value \n$n$\n in prefix-free coding (so we know when to stop reading) and then the \n$n+1$\n parameter values. With this information a receiver of our code could restore the polynomial. Then we add the rest of the information required to store the dataset. For each datapoint we give the x-value, and then how many boxes up or down the data point lies off the polynomial. Both values we store in prefix-free coding so that short values require few bits, and we won't need delimiters between points. (You can shorten the code for the x-values by only storing the increments between values)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49065/402101 \n The fundamental point here is the tradeoff. If I choose a 0-order polynomial (like f(x) = 3.4), then the model is very simple to store, but for the y-values, I'm essentially storing the distance to the mean. More coefficients give me a better fitting polynomial (and thus shorter codes for the y values), but I have to spend more bits describing the model. The model that gives you the shortest code for your data is the best fit by the MDL criterion.\n\n(Note that this is known as 'crude MDL', and there are some refinements you can make to solve various technical issues).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/68917/402101 \n A solution to this is to utilize a form of penalized regression. In fact, this is the original reason some of the penalized regression forms were developed (although they turned out to have other interesting properties.\n\nInstall and load package glmnet in R and you're mostly ready to go. One of the less user-friendly aspects of glmnet is that you can only feed it matrices, not formulas as we're used to. However, you can look at model.matrix and the like to construct this matrix from a data.frame and a formula...\n\nNow, when you expect that this perfect separation is not just a byproduct of your sample, but could be true in the population, you specifically \ndon't\n want to handle this: use this separating variable simply as the sole predictor for your outcome, not employing a model of any kind.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/82396/402101 \n There is nothing wrong with your current strategy.  If you have a multiple regression model with \nonly\n two explanatory variables then you could try to make a 3D-ish plot that displays the predicted regression plane, but most software don't make this easy to do.  Another possibility is to use a \ncoplot\n (see also: \ncoplot in R\n or \nthis pdf\n), which can represent three or even four variables, but many people don't know how to read them.  Essentially however, if you don't have any interactions, then the predicted \nmarginal\n relationship between \n$x_j$\n and \n$y$\n will be the same as predicted \nconditional\n relationship (plus or minus some vertical shift) at any specific level of your other \n$x$\n variables.  Thus, you can simply set all other \n$x$\n variables at their means and find the predicted line \n$\\hat y = \\hat\\beta_0 + \\cdots + \\hat\\beta_j x_j + \\cdots + \\hat\\beta_p \\bar x_p$\n and plot that line on a scatterplot of \n$(x_j, y)$\n pairs.  Moreover, you will end up with \n$p$\n such plots, although you might not include some of them if you think they are not important. (For example, it is common to have a multiple regression model with a single variable of interest and some control variables, and only present the first such plot).\n\nOn the other hand, if you \ndo\n have interactions, then you should figure out which of the interacting variables you are most interested in and plot the predicted relationship between that variable and the response variable, but with several lines on the same plot.  The other interacting variable is set to different levels for each of those lines.  Typical values would be the mean and \n$\\pm$\n 1 SD of the interacting variable.  To make this clearer, imagine you have only two variables, \n$x_1$\n and \n$x_2$\n, and you have an interaction between them, and that \n$x_1$\n is the focus of your study, then you might make a single plot with these three lines:\n\n\n\\begin{align}\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 (\\bar x_2 - s_{x_2})  + \\hat\\beta_3 x_1(\\bar x_2 - s_{x_2}) \\\\\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 \\bar x_2 \\quad\\quad\\quad\\  + \\hat\\beta_3 x_1\\bar x_2 \\\\\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 (\\bar x_2 + s_{x_2}) + \\hat\\beta_3 x_1(\\bar x_2 + s_{x_2}) \n\\end{align}\n\nAn example plot that's similar (albeit with a binary moderator) can be seen in my answer to \nPlot regression with interaction in R\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197471/402101 \n Trying to complement the other answers... What kind of information is Fisher information?  Start with the loglikelihood function\n\n$$\n   \\ell (\\theta) = \\log f(x;\\theta)\n$$\n\nas a function of \n$\\theta$\n for \n$\\theta \\in \\Theta$\n, the parameter space.\nAssuming some regularity conditions we do not discuss here, we have\n\n$\\DeclareMathOperator{\\E}{\\mathbb{E}}  \\E \\frac{\\partial}{\\partial \\theta} \\ell (\\theta) = \\E_\\theta \\dot{\\ell}(\\theta) = 0$\n (we will write derivatives with respect to the parameter as dots as here).  The variance is the Fisher information\n\n$$\n    I(\\theta) = \\E_\\theta ( \\dot{\\ell}(\\theta) )^2= -\\E_\\theta \\ddot{\\ell}(\\theta)\n$$\n\nthe last formula showing that it is the (negative) curvature of the loglikelihood function.  One often finds the maximum likelihood estimator (mle) of \n$\\theta$\n by solving the likelihood equation \n$\\dot{\\ell}(\\theta)=0$\n when the Fisher information as the variance of the score \n$\\dot{\\ell}(\\theta)$\n is large, then the solution to that equation will be very sensitive to the data, giving a hope for high precision of the mle.  That is confirmed at least asymptotically, the asymptotic variance of the mle being the inverse of Fisher information.\n\nHow can we interpret this?   \n$\\ell(\\theta)$\n is the likelihood information about the parameter \n$\\theta$\n from the sample. This can really only be interpreted in a relative sense, like when we use it to compare the plausibilities of two distinct possible parameter values via the likelihood ratio test \n$\\ell(\\theta_0) - \\ell(\\theta_1)$\n.  The rate of change of the loglikelihood is the score function \n$\\dot{\\ell}(\\theta)$\n tells us how fast the likelihood changes, and its variance \n$I(\\theta)$\n how much this varies from sample to sample, at a given parameter value, say \n$\\theta_0$\n.  The equation (which is really surprising!)\n\n$$\n    I(\\theta) = - \\E_\\theta \\ddot{\\ell}(\\theta)\n$$\n\ntells us there is a relationship (equality) between the variability in the information (likelihood) for a given parameter value, \n$\\theta_0$\n, and the curvature of the likelihood function for that parameter value.  This is a surprising relationship between the variability (variance) of ths statistic \n$\\dot{\\ell}(\\theta) \\mid_{\\theta=\\theta_0}$\n and the expected change in likelihood when we vary the parameter \n$\\theta$\n in some interval around \n$\\theta_0$\n (for the same data).   This is really both strange, surprising and powerful!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197471/402101 \n So what is the likelihood function?  We usually think of the statistical model \n$\\{ f(x;\\theta), \\theta \\in \\Theta \\} $\n as a family of probability distributions for data \n$x$\n, indexed by the parameter \n$\\theta$\n some element in the parameter space \n$\\Theta$\n. We think of this model as being true if there exists some value \n$\\theta_0 \\in \\Theta$\n such that the data \n$x$\n actually have the probability distribution \n$f(x;\\theta_0)$\n. So we get a statistical model by imbedding the true data-generating probability distribution \n$f(x;\\theta_0)$\n in a family of probability distributions. But, it is clear that such an imbedding can be done in many different ways, and each such imbedding will be a \"true\" model, and they will give different likelihood functions. And, without such an imbedding, there is no likelihood function. It seems that we really do need some help, some principles for how to choose an imbedding wisely!\n\nSo, what does this mean? It means that the choice of likelihood function tells us how we would expect the data to change, if the truth changed a little bit. But, this cannot really be verified by the data, as the data only gives information about the true model function \n$f(x;\\theta_0)$\n which actually generated the data, and not nothing about all the other elements in the choosen model. This way we see that choice of the likelihood function is similar to choice of a prior in Bayesian analysis, it injects non-data information into the analysis. Let us look at this in a simple (somewhat artificial) example, and look at the effect of imbedding \n$f(x;\\theta_0)$\n in a model in different ways.\n\nLet us assume that \n$X_1, \\dotsc, X_n$\n are iid as \n$N(\\mu=10, \\sigma^2=1)$\n. So, that is the true, data-generating distribution. Now, let us embed this in a model in two different ways, model A and model B.\n\n$$\nA \\colon X_1, \\dotsc, X_n ~\\text{iid}~N(\\mu, \\sigma^2=1),\\mu \\in \\mathbb{R} \\\\\nB \\colon X_1, \\dotsc, X_n ~\\text{iid}~N(\\mu, \\mu/10), \\mu>0\n$$\n\nyou can check that this coincides for \n$\\mu=10$\n.\n\nThe loglikelihood functions become\n\n$$\n\\ell_A(\\mu) = -\\frac{n}{2} \\log (2\\pi) -\\frac12\\sum_i (x_i-\\mu)^2 \\\\\n\\ell_B(\\mu) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2}\\log(\\mu/10) - \\frac{10}{2}\\sum_i \\frac{(x_i-\\mu)^2}{\\mu}\n$$\n\nThe score functions: (loglikelihood derivatives):\n\n$$\n\\dot{\\ell}_A(\\mu) = n (\\bar{x}-\\mu)       \\\\\n\\dot{\\ell}_B(\\mu) = -\\frac{n}{2\\mu}- \\frac{10}{2}\\sum_i \\left(\\frac{x_i}{\\mu}\\right)^2 - \n15 n\n$$\n\nand the curvatures",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197471/402101 \n you can check that this coincides for \n$\\mu=10$\n.\n\nThe loglikelihood functions become\n\n$$\n\\ell_A(\\mu) = -\\frac{n}{2} \\log (2\\pi) -\\frac12\\sum_i (x_i-\\mu)^2 \\\\\n\\ell_B(\\mu) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2}\\log(\\mu/10) - \\frac{10}{2}\\sum_i \\frac{(x_i-\\mu)^2}{\\mu}\n$$\n\nThe score functions: (loglikelihood derivatives):\n\n$$\n\\dot{\\ell}_A(\\mu) = n (\\bar{x}-\\mu)       \\\\\n\\dot{\\ell}_B(\\mu) = -\\frac{n}{2\\mu}- \\frac{10}{2}\\sum_i \\left(\\frac{x_i}{\\mu}\\right)^2 - \n15 n\n$$\n\nand the curvatures\n\n$$\n   \\ddot{\\ell}_A(\\mu) = -n   \\\\\n   \\ddot{\\ell}_B(\\mu) =  \\frac{n}{2\\mu^2} + \\frac{10}{2}\\sum_i \\frac{2 x_i^2}{\\mu^3}\n$$\n\nso, the Fisher information do really depend on the imbedding. Now, we calculate the Fisher information at the true value \n$\\mu=10$\n,\n\n$$\n  I_A(\\mu=10) = n, \\\\\n  I_B(\\mu=10) = n \\cdot \\left(\\frac1{200}+\\frac{2020}{2000}\\right) > n\n$$\n\nso the Fisher information about the parameter is somewhat larger in model B.\n\nThis illustrates that, in some sense, the Fisher information tells us how fast the information from the data about the parameter \nwould have changed\n if the governing parameter changed \nin the way postulated by the imbedding in a model family\n.  The explanation of higher information in model B is that our model family B postulates \nthat if the expectation would have increased, then the variance too would have increased\n.  So that, under model B, the sample variance will also carry information about \n$\\mu$\n, which it will not do under model A.\n\nAlso, this example illustrates that we really do need some theory for helping us in how to construct model families.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/103152/402101 \n You can perform selection and logistic regression simultaneously using the \nLASSO\n or \nElastic Net\n regression algorithms. The basic idea behind LASSO is to solve the $l_1$-penalized optimization problem\n$$\\min_{\\beta} \\{ l(\\beta) + \\lambda||\\beta||_1 \\},$$\nwhere $l(\\cdot)$ is the likelihood function. Popular implementations, e.g. \nglmnet\n, efficiently solve for a grid of $\\lambda$ values. This is useful because we usually don't know $\\lambda$ a priori and need to apply some type of cross-validation. If you have correlated features then it helps to add some $l_2$ (ridge) penalty, which is the idea behind the Elastic Net.\n\nSince you don't have a lot of data, I think this is probably your best bet. If you want to use a separate variable selection stage you will need to choose a metric (e.g. deviance of single-variable regression) and also a threshold. The LASSO gives you only one parameter to tune and operates within the context of multivariable logistic regression models directly.\n\nEDIT: The question now specifically requests an approach that is implemented in SPSS. As I don't have/use that software I don't know whether lasso logistic regression is implemented. Perhaps someone can let us know in the comments.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1602/402101 \n Here is how I would explain the basic difference to my grandma:\n\nI have misplaced my phone somewhere in the home. I can use the phone locator on the base of the instrument to locate the phone and when I press the phone locator the phone starts beeping.\n\nProblem: Which area of my home should I search?\n\nI can hear the phone beeping. I also have a mental model which helps me identify the area from which the sound is coming. Therefore, upon hearing the beep, I infer the area of my home I must search to locate the phone.\n\nI can hear the phone beeping. Now, apart from a mental model which helps me identify the area from which the sound is coming from, I also know the locations where I have misplaced the phone in the past. So, I combine my inferences using the beeps and my prior information about the locations I have misplaced the phone in the past to identify an area I must search to locate the phone.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/225977/402101 \n Are the LASSO coefficients interpreted in the same method as logistic regression?\n\nLet me rephrase: \nAre the LASSO coefficients interpreted in the same way as, for example, \nOLS\n maximum likelihood coefficients in a logistic regression?\n\nLASSO (a penalized estimation method) \naims at estimating the same quantities\n (model coefficients) as, say, \nOLS\n maximum likelihood (an unpenalized method). The model is the same, and the interpretation remains the same. The numerical values from LASSO will normally differ from those from \nOLS\n maximum likelihood: some will be closer to zero, others will be exactly zero. If a sensible amount of penalization has been applied, the LASSO estimates will lie closer to the true values than the \nOLS\n maximum likelihood estimates, which is a desirable result.\n\nWould it be appropriate to use the features selected from LASSO in logistic regression?\n\nThere is no inherent problem with that, but you could use LASSO not only for feature selection but also for coefficient estimation. As I mention above, LASSO estimates may be more accurate than, say, \nOLS\n maximum likelihood estimates.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/35125/402101 \n The standard deviation is the square root of the variance.\n\nThe standard deviation is expressed in the same units as the mean is, whereas the variance is expressed in squared units, but for looking at a distribution, you can use either just so long as you are clear about what you are using. For example, a Normal distribution with mean = 10 and sd = 3 is exactly the same thing as a Normal distribution with mean = 10 and variance = 9.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/16009/402101 \n You can find everything \nhere\n. However, here is a brief answer.\n\nLet $\\mu$ and $\\sigma^2$ be the mean and the variance of interest; you wish to estimate $\\sigma^2$ based on a sample of size $n$.\n\nNow, let us say you use the following estimator:\n\n$S^2 = \\frac{1}{n} \\sum_{i=1}^n (X_{i} - \\bar{X})^2$,\n\nwhere $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ is the estimator of $\\mu$.\n\nIt is not too difficult (see footnote) to see that $E[S^2] = \\frac{n-1}{n}\\sigma^2$.\n\nSince $E[S^2] \\neq \\sigma^2$, the estimator $S^2$ is said to be biased.\n\nBut, observe that $E[\\frac{n}{n-1} S^2] = \\sigma^2$. Therefore $\\tilde{S}^2 = \\frac{n}{n-1} S^2$ is an unbiased estimator of $\\sigma^2$.\n\nFootnote\n\nStart by writing $(X_i - \\bar{X})^2 = ((X_i - \\mu) + (\\mu - \\bar{X}))^2$ and then expand the product...\n\nEdit to account for your comments\n\nThe expected value of $S^2$ does not give $\\sigma^2$ (and hence $S^2$ is biased) but it turns out you can transform $S^2$ into $\\tilde{S}^2$ so that the expectation does give $\\sigma^2$.\n\nIn practice, one often prefers to work with $\\tilde{S}^2$ instead of $S^2$. But, if $n$ is large enough, this is not a big issue since $\\frac{n}{n-1} \\approx 1$.\n\nRemark\n Note that unbiasedness is a property of an estimator, not of an expectation as you wrote.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/580782/402101 \n Your replication of the formula is not correct. The correct formula in \nCinelli and Hazlett\n (see equation 5) is:\n\n$$\n\\beta' = \\beta + \\gamma\\delta\n$$\n\nThis is an algebraic identity that holds for any OLS model. In your example we obtain:\n\nbeta.prime <- coef(lm(y ~ x))[\"x\"]\nbeta       <- coef(lm(y ~ x + u))[\"x\"]\nimbalance  <- coef(lm(u ~ x))[\"x\"]\nimpact     <- coef(lm(y ~ x + u))[\"u\"]\n\n\n> beta.prime\n       x \n2.514337 \n\n> beta + impact*imbalance\n       x \n2.514337\n\nbeta.prime <- coef(lm(y ~ x))[\"x\"]\nbeta       <- coef(lm(y ~ x + u))[\"x\"]\nimbalance  <- coef(lm(u ~ x))[\"x\"]\nimpact     <- coef(lm(y ~ x + u))[\"u\"]\n\n\n> beta.prime\n       x \n2.514337 \n\n> beta + impact*imbalance\n       x \n2.514337\n\nYour second formula does not seem to be true in general, you seem to be assuming that a single latent variable determines both \n$X$\n and \n$Y$\n. Here is a counter-example to your formula:\n\n> n <- 1e3\n> u <- rnorm(n)\n> x <- u + rnorm(n)\n> y <- x + u + rnorm(n)\n> lm(y ~ x)\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    0.00792      1.48475\n\n> n <- 1e3\n> u <- rnorm(n)\n> x <- u + rnorm(n)\n> y <- x + u + rnorm(n)\n> lm(y ~ x)\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    0.00792      1.48475",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/31047/402101 \n To define the two terms without using too much technical language:\n\nAn estimator is \nconsistent\n if, as the sample size increases, the estimates (produced by the estimator) \"converge\" to the true value of the parameter being estimated. To be slightly more precise - consistency means that, as the sample size increases, the sampling distribution of the estimator becomes increasingly concentrated at the true parameter value.\n\n\n\n\nAn estimator is \nunbiased\n if, on average, it hits the true parameter value. That is, the mean of the sampling distribution of the estimator is equal to the true parameter value.\n\n\n\n\nThe two are not equivalent: \nUnbiasedness\n is a statement about the expected value of the sampling distribution of the estimator. \nConsistency\n is a statement about \"where the sampling distribution of the estimator is going\" as the sample size increases.\n\nAn estimator is \nconsistent\n if, as the sample size increases, the estimates (produced by the estimator) \"converge\" to the true value of the parameter being estimated. To be slightly more precise - consistency means that, as the sample size increases, the sampling distribution of the estimator becomes increasingly concentrated at the true parameter value.\n\nAn estimator is \nunbiased\n if, on average, it hits the true parameter value. That is, the mean of the sampling distribution of the estimator is equal to the true parameter value.\n\nThe two are not equivalent: \nUnbiasedness\n is a statement about the expected value of the sampling distribution of the estimator. \nConsistency\n is a statement about \"where the sampling distribution of the estimator is going\" as the sample size increases.\n\nIt certainly is possible for one condition to be satisfied but not the other - I will give two examples. For both examples consider a sample \n$X_1, ..., X_n$\n from a \n$N(\\mu, \\sigma^2)$\n population.\n\nUnbiased but not consistent:\n Suppose you're estimating \n$\\mu$\n. Then \n$X_1$\n is an unbiased estimator of \n$\\mu$\n since \n$E(X_1) = \\mu$\n. But, \n$X_1$\n is not consistent since its distribution does not become more concentrated around \n$\\mu$\n as the sample size increases - it's always \n$N(\\mu, \\sigma^2)$\n!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/31047/402101 \n Unbiased but not consistent:\n Suppose you're estimating \n$\\mu$\n. Then \n$X_1$\n is an unbiased estimator of \n$\\mu$\n since \n$E(X_1) = \\mu$\n. But, \n$X_1$\n is not consistent since its distribution does not become more concentrated around \n$\\mu$\n as the sample size increases - it's always \n$N(\\mu, \\sigma^2)$\n!\n\n\n\n\nConsistent but not unbiased:\n Suppose you're estimating \n$\\sigma^2$\n. The maximum likelihood estimator is \n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2 $$\n where \n$\\overline{X}$\n is the sample mean. It is a fact that \n$$ E(\\hat{\\sigma}^2) = \\frac{n-1}{n} \\sigma^2 $$\n which can be derived using the information \nhere\n. Therefore \n$\\hat{\\sigma}^2$\n is biased for any finite sample size. We can also easily derive that \n$${\\rm var}(\\hat{\\sigma}^2) = \\frac{ 2\\sigma^4(n-1)}{n^2}$$\n From these facts we can informally see that the distribution of \n$\\hat{\\sigma}^2$\n is becoming more and more concentrated at \n$\\sigma^2$\n as the sample size increases since the mean is converging to \n$\\sigma^2$\n and the variance is converging to \n$0$\n. (\nNote:\n This does constitute a proof of consistency, using the same argument as the one used in the answer \nhere\n)\n\nUnbiased but not consistent:\n Suppose you're estimating \n$\\mu$\n. Then \n$X_1$\n is an unbiased estimator of \n$\\mu$\n since \n$E(X_1) = \\mu$\n. But, \n$X_1$\n is not consistent since its distribution does not become more concentrated around \n$\\mu$\n as the sample size increases - it's always \n$N(\\mu, \\sigma^2)$\n!\n\nConsistent but not unbiased:\n Suppose you're estimating \n$\\sigma^2$\n. The maximum likelihood estimator is \n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2 $$\n where \n$\\overline{X}$\n is the sample mean. It is a fact that \n$$ E(\\hat{\\sigma}^2) = \\frac{n-1}{n} \\sigma^2 $$\n which can be derived using the information \nhere\n. Therefore \n$\\hat{\\sigma}^2$\n is biased for any finite sample size. We can also easily derive that \n$${\\rm var}(\\hat{\\sigma}^2) = \\frac{ 2\\sigma^4(n-1)}{n^2}$$\n From these facts we can informally see that the distribution of \n$\\hat{\\sigma}^2$\n is becoming more and more concentrated at \n$\\sigma^2$\n as the sample size increases since the mean is converging to \n$\\sigma^2$\n and the variance is converging to \n$0$\n. (\nNote:\n This does constitute a proof of consistency, using the same argument as the one used in the answer \nhere\n)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/124956/402101 \n I'm a practitioner, both producer and user of forecasting and NOT a trained statistician. Below I share some of my thoughts on why your mean forecast turned out better than ARIMA by referring to research article that rely on empirical evidence.  One book that time and time again I go back to refer is the \nPrinciples of Forecasting\n book by Armstrong and its \nwebsite\n which I would recommend as an excellent read for any forecaster, provides great insight on usage and guiding principles of extrapolation methods.\n\nTo answer you first question\n  - What I want to know is if this is unusual?\n\nThere is a chapter called Extrapolation for Time-Series and Cross-Sectional Data which also available free in the same \nwebsite\n. The following is the quote from the chapter\n\n\"For example, in the real-time M2-competition, which examined 29\n  monthly series, Box-Jenkins proved to be one of the least-accurate\n  methods and its overall median error was 17% greater than that for a\n  naive forecast\"\n\nThere lies an empirical evidence on why your mean forecasts was better than ARIMA models.\n\nThere is also been study after study in empirical competitions and the third \nM3 competition\n that show Box - Jenkins ARIMA approach fails to produce accurate forecast and lacks evidence that it performs better for univariate trend extrapolation.\n\nThere is also another paper and an ongoing study by Greene and Armstrong entitled \"\nSimple Forecasting: Avoid Tears Before Bedtime\n\" in the same website. The authors of the paper summarize as follows:\n\nIn total we identified 29 papers incorporating 94 formal comparisons\n  of the  accuracy of forecasts from complex methods with those from\n  simple\u2014but not in all cases  sophisticatedly simple\u2014methods.\n  Eighty-three percent of the comparisons found that forecasts from\n  simple methods were more accurate than, or similarly accurate to,\n  those  from complex methods. On average, the errors of forecasts from\n  complex methods were about 32 percent greater than the errors of\n  forecasts from simple methods in the 21 studies that provide\n  comparisons of errors\n\nTo answer your third question\n: does this indicate that I've set something up wrong?\nNo, I would aconsider ARIMA as complex method and Mean forecast as simple methods. There is ample evidence that simple methods like Mean forecast outperform complex methods like ARIMA.\n\nTo answer your second question\n: Does this mean the times series I'm using are strange?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/124956/402101 \n To answer your third question\n: does this indicate that I've set something up wrong?\nNo, I would aconsider ARIMA as complex method and Mean forecast as simple methods. There is ample evidence that simple methods like Mean forecast outperform complex methods like ARIMA.\n\nTo answer your second question\n: Does this mean the times series I'm using are strange?\n\nBelow are what I considered to be experts in real world forecasting:\n\nMakridakis (Pioneered Empirical competition on Forecasting called M, M2 and M3, and paved way for evidence based methods in forecasting)\n\n\nArmstrong (Provides valuable insights in the form of books/articles on Forecasting Practice)\n\n\nGardner (Invented Damped Trend exponential smoothing another simple method which works surprisingly well vs. ARIMA)\n\nAll of the above researchers advocate, simplicity (methods like your mean forecast)   vs. Complex methods like ARIMA. So you should feel comfortable that your forecasts are good and always favor simplicity over complexity based on empirical evidence. These researchers have  all contributed immensely to the field of applied forecasting.\n\nIn addition to Stephan's good list of simple forecasting method. there is also another method called \nTheta forecasting method\n which is a very simple method (basically Simple Exponential smoothing with a drift that equal 1/2 the slope of linear regression)  I would add this to your toolbox. \nForecast package in R\n implements this method.\n\nForecast package in R",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18449/402101 \n Let \n$F$\n be the CDF of the random variable \n$X$\n, so the inverse CDF can be written \n$F^{-1}$\n.  In your integral make the substitution \n$p = F(x)$\n, \n$dp = F'(x)dx = f(x)dx$\n to obtain\n\n$$\\int_0^1F^{-1}(p)dp = \\int_{-\\infty}^{\\infty}x f(x) dx = \\mathbb{E}_F[X].$$\n\nThis is valid for continuous distributions.  Care must be taken for other distributions because an inverse CDF hasn't a unique definition.\n\nWhen the variable is not continuous, it does not have a distribution that is absolutely continuous with respect to Lebesgue measure, requiring care in the definition of the inverse CDF and care in computing integrals.  Consider, for instance, the case of a discrete distribution.  By definition, this is one whose CDF \n$F$\n is a step function with steps of size \n$\\Pr_F(x)$\n at each possible value \n$x$\n.\n\n\n\nThis figure shows the CDF of a Bernoulli\n$(2/3)$\n distribution scaled by \n$2$\n.  That is, the random variable has a probability \n$1/3$\n of equalling \n$0$\n and a probability of \n$2/3$\n of equalling \n$2$\n.  The heights of the jumps at \n$0$\n and \n$2$\n give their probabilities.  The expectation of this variable evidently equals \n$0\\times(1/3)+2\\times(2/3)=4/3$\n.\n\nWe could define an \"inverse CDF\" \n$F^{-1}$\n by requiring\n\n$$F^{-1}(p) = x \\text{ if } F(x) \\ge p \\text{ and } F(x^{-}) \\lt p.$$\n\nThis means that \n$F^{-1}$\n is also a step function.  For any possible value \n$x$\n of the random variable, \n$F^{-1}$\n will attain the value \n$x$\n over an interval of length \n$\\Pr_F(x)$\n.  Therefore its integral is obtained by summing the values \n$x\\Pr_F(x)$\n, which is just the expectation.\n\n\n\nThis is the graph of the inverse CDF of the preceding example.  The jumps of \n$1/3$\n and \n$2/3$\n in the CDF become horizontal lines of these lengths at heights equal to \n$0$\n and \n$2$\n, the values to whose probabilities they correspond.  (The Inverse CDF is not defined beyond the interval \n$[0,1]$\n.)  Its integral is the sum of two rectangles, one of height \n$0$\n and base \n$1/3$\n, the other of height \n$2$\n and base \n$2/3$\n, totaling \n$4/3$\n, as before.\n\nIn general, for a mixture of a continuous and a discrete distribution, we need to define the inverse CDF to parallel this construction: at each discrete jump of height \n$p$\n we must form a horizontal line of length \n$p$\n as given by the preceding formula.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/457575/402101 \n I will assume you have a thorough grasp of the two group/two period difference-in-differences (DD) design and you now want to extend your intuition of the method to the multi-group/multi-period case. Suppose we have multiple observations of \n$i$\n units (e.g., counties) across multiple \n$t$\n periods (e.g., years). In DD applications, the data is \u2018aggregated up\u2019 to a higher-level, where some counties introduce a new policy/intervention and others do not. Note, the hypothetical policy/intervention serves as our \u201ctreatment\u201d for the purposes of this example. The \u2018generalized\u2019 DD setup is as follows:\n\n$$\ny_{it} = \\gamma_{i} + \\lambda_{t} + \\delta T_{it} + \\epsilon_{it},\n$$\n\nwhere \n$\\gamma_{i}$\n and \n$\\lambda_{t}$\n denote \ncounty\n and \nyear\n fixed effects, respectively. You may also see this referenced as a \u2018two-way\u2019 fixed effects estimator. The variable \n$T_{it}$\n is our treatment dummy, indexing the \n$i$\n counties affected by the policy/intervention during periods \n$t$\n, 0 otherwise.\n\n[Wooldridge] states that \"\n$\ud835\udc64_{\ud835\udc56\ud835\udc61}$\n can have any pattern\", i.e it could be 0 for earlier periods but 1 for later ones. It can also be always 0, which would be the case for entities from the control group under a diff-in-diff interpretation of the model.\n\nThe \u2018generalized\u2019 approach accommodates treatment exposures in multiple groups and multiple time periods. Because of this, the treatment dummy can be coded rather flexibility to account for this. Again, \n$T_{it}$\n is equal to 1 for \ntreated counties\n and only during those \n$t$\n years when treatment is actually in effect, 0 otherwise. Thus, for those counties \nnever treated\n, it is 0 across \nall years\n that the untreated county is observed in the panel. In this setting, the variable \n$T_{it}$\n does not demarcate a specific \u201ctreatment group\u201d as it would in the canonical DD approach. The indicator 'turns on' (i.e., changes from 0 to 1) during precisely those \u2018county-year\u2019 combinations when the policy/intervention is in effect. As a general framework to help us exploit variation in treatment \ntiming\n, this approach can be viewed as a weighted average of all possible two-group/two-period (2x2) DD estimators that can be constructed from the panel dataset. See this NBER working paper by \nAndrew Goodman-Bacon 2018\n which explores the \u2018two-way\u2019 fixed effects estimator in greater detail.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/457575/402101 \n I simulated a toy dataset in R and included it at the bottom of my answer. I did this to show how the treatment dummy is coded under this approach. For simplicity, it includes 3 counties observed over 10 years. The variable \ncty\n is a county identifier. Only two of the three counties are actually \"treated\" during this 10-year observation period. The treatment dummy 'turns on' (i.e., changes from 0 to 1) at \ndifferent times\n in \ndifferent counties\n. One county (i.e., County 1) serves as our control group; this county could be viewed as the \"always 0\" unit referenced in Wooldridge's text. Though there are many treatment-control comparisons made using this approach, you can think of the \u201calways 0\u2019s\u201d as the baseline history of \nnever receiving treatment.\n The treatment variable \ntrt\n (see the third column from left) is the instantiation of the variable \n$T_{it}$\n in the foregoing equation. The variable 'turns on' (equals 1) for treated counties \nand\n only when those counties enter into a post-treatment period. In this toy dataset, County 2 was exposed to treatment from the year 2013 onward. County 3, however, was a late adopter of the treatment, officially 'turning on' in 2015 before it was officially \nremoved\n (i.e., 'turned off') in the last two years of the observation period. In most cases, treatment 'turns on' in county \n$i$\n at year \n$t$\n and remains in place. In others, counties may only have a transient exposure period, in which case we can code the dummy to reflect periods of treatment withdrawal. Note, I also included the \ncounty\n and \nyear\n dummies. To avoid collinearity, I omitted one county dummy (i.e., \nc_1\n = County 1) and one year dummy (i.e., \ny_10\n = 2010).\n\ncty\n\ntrt\n\nc_1\n\ny_10\n\nThe following plot, reproduced from these \nslides\n, shows how a panel with variation in treatment timing can be\ndecomposed into \"timing groups\" reflecting observed onset of treatment.\n\n\n\nNote, we can see how a late adopter entity can serve as a counterfactual for an early adopter. Similarly, when a late adopter enters into treatment (e.g., group B), a previously treated entity (e.g., group A) can also act as a counterfactual. In other words, already treated units serve as controls in some of the two-by-two DDs underlying the weighted average. The next plot highlights the different pre-post comparisons using early versus late adopters of treatment.\n\n\n\nIt is worth noting that bias is introduced when treatment effects change over time within a unit.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/457575/402101 \n It is worth noting that bias is introduced when treatment effects change over time within a unit.\n\nNow, my question is the following: can \ud835\udc64\ud835\udc56\ud835\udc61 begin as 1 and either stay the same or change to 0?\n\nIt can, but I wouldn\u2019t advise you to incorporate such entities. If treated entities are always equal to 1, then they are \nalways treated\n. DD approaches require you to observe some units/entities pre- and post-treatment. The \nalways treated\n do not have any pre-event data. The same is true for entities that begin as 1 but then 'turn off' after some period of observation.\n\nStill, my intuition tells me that allowing that would change the interpretation of the model into something other than a general diff-in-diff since such behavior would not qualify as either part of the treatment or control group.\n\nIn general, I agree this would change the interpretation of your treatment effect. If you only acquired data on counties in the post-treatment period (i.e., beginning at 1), then you could assess the effects of units/entities \"repealing\" a policy (i.e., treatment changes from 1 to 0); this would have to occur in some counties but not others. I have seen applications where researchers conducted a DD analysis comparing all pairwise time periods. That is, they delineated pre-, during-, and after-treatment periods. The \"after\" period, in this case, is the period when treatment is \nremoved\n. To go back to your question, if the treatment variable begins as 1 and then changes to 0 (i.e., policy/law is repealed), then this would be a comparison of the \"during\" period with the \"after\" period. This becomes problematic when treatment begins and ends at different times in different units/entities. Thus, I don't think you should include units/entities where they start in the treated condition. In my opinion, I would subject the units/entities with no pre-event data to a separate analysis.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/457575/402101 \n To be transparent, the mechanics of DD can be reversed. Only a handful of academic studies have executed this well in my estimation. In this context, always treated units would constitute your control group, while the treated group is the subset of units that change their treatment status later. Maybe some exposure is removed by the second time period. The \"change\" is coded as any other treatment in a DD setting. In other words, it's a comparison of the units exposed to a treatment in all time periods (i.e., the \"always 0\" group) with the switchers (i.e., changers from 0 to 1). In this sense, DD \"in reverse\" is simply referring to the fact that the \"controls\" represent the always treated. Note, under an identification condition involving only treated responses, the identification condition may be tested by future parallel treated paths across the two groups, rather than past parallel untreated paths. Review the work of \nKim and Lee 2019\n for a neat application of this.\n\nWhat about a binary indicator that goes back and forth between 0 and 1?\n\nThe treatment variable is allowed to switch \u2018on\u2019 and \u2018off\u2019 throughout the panel. Put differently, the binary indicator may switch back and forth between 0 and 1 \nmultiple times\n over time within a unit(s). This is often the case in policy analysis, where some units can have multiple treatment histories. For example, a new law was enacted in a subset of U.S. states at the beginning of 2013, only to be repealed at the conclusion of 2016. Later, legislators in a subset of U.S. states where the law was nullified decide to reintroduce the legislation again in 2018 where it remains in effect. In practice, your treatment dummy should be coded to reflect this reality. However, this could become problematic if policymakers decide to introduce or remove laws/policies based upon past outcomes of the response variable. Review pages 4 through 7 of \nLecture 10\n for a more in-depth discussion of this.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/457575/402101 \n Research by \nAcemoglu and colleagues 2019\n investigate the effects of democracy on economic growth. They follow 184 countries from 1960\u20132010. Contrary to other research in this area, they investigate permanent \nand\n transitory transitions to democracy and nondemocracy. Thus, the dichotomous treatment variable is switching \u2018on\u2019 and \u2018off\u2019 \nmultiple times\n for a subset of countries. Their work was recently replicated by \nImai and colleagues 2021\n and a new matching estimator is now available in software (see, e.g., the \nPanelMatch\n package in R) to handle these irregular treatment patterns. For other DD applications with intermittent exposures (i.e., recurring on/off patterns), review empirical work by \nFouirnaise and Mutlu-Eren 2015\n in political economy, or my own research (see,  e.g., \nBilach et al. 2020\n) in criminology.\n\nPanelMatch\n\nIn sum, you should take good care to make sure your treatment variable is coded 1 in only those time periods when the county (or other aggregate unit) is affected by the treatment, 0 in all other time periods. There is no requirement that a treated unit stay 'turned on' for the duration of the treatment phase. And again, for counties \nnever exposed\n to the new law/policy, the treatment variable would equal 0 in all time periods it is under observation (see 'County 1' below).\n\n# Dummy coding the treatment variable in a 'generalized' DD model\n\n # N = 3 (counties)\n # T = 10 (years)\n\n# Variable labels:\n\n # cty  = unit identifier\n # year = time identifier\n # trt  = treatment variable\n # c_   = county effects (N - 1)\n # y_   = year effects (T - 1)\n\n# N x T = 30 county-year observations\n\ncty year  trt c_2 c_3 y_11 y_12 y_13 y_14 y_15 y_16 y_17 y_18 y_19\n1   2010    0   0   0    0    0    0    0    0    0    0    0    0\n1   2011    0   0   0    1    0    0    0    0    0    0    0    0\n1   2012    0   0   0    0    1    0    0    0    0    0    0    0\n1   2013    0   0   0    0    0    1    0    0    0    0    0    0\n1   2014    0   0   0    0    0    0    1    0    0    0    0    0\n1   2015    0   0   0    0    0    0    0    1    0    0    0    0\n1   2016    0   0   0    0    0    0    0    0    1    0    0    0\n1   2017    0   0   0    0    0    0    0    0    0    1    0    0 \n1   2018    0   0   0    0    0    0    0    0    0    0    1    0\n1   2019    0   0   0    0    0    0    0    0    0    0    0    1",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/457575/402101 \n 2   2010    0   1   0    0    0    0    0    0    0    0    0    0\n2   2011    0   1   0    1    0    0    0    0    0    0    0    0\n2   2012    0   1   0    0    1    0    0    0    0    0    0    0\n2   2013    1   1   0    0    0    1    0    0    0    0    0    0\n2   2014    1   1   0    0    0    0    1    0    0    0    0    0\n2   2015    1   1   0    0    0    0    0    1    0    0    0    0\n2   2016    1   1   0    0    0    0    0    0    1    0    0    0\n2   2017    1   1   0    0    0    0    0    0    0    1    0    0\n2   2018    1   1   0    0    0    0    0    0    0    0    1    0\n2   2019    1   1   0    0    0    0    0    0    0    0    0    1\n\n3   2010    0   0   1    0    0    0    0    0    0    0    0    0\n3   2011    0   0   1    1    0    0    0    0    0    0    0    0\n3   2012    0   0   1    0    1    0    0    0    0    0    0    0\n3   2013    0   0   1    0    0    1    0    0    0    0    0    0\n3   2014    0   0   1    0    0    0    1    0    0    0    0    0\n3   2015    1   0   1    0    0    0    0    1    0    0    0    0\n3   2016    1   0   1    0    0    0    0    0    1    0    0    0\n3   2017    1   0   1    0    0    0    0    0    0    1    0    0\n3   2018    0   0   1    0    0    0    0    0    0    0    1    0\n3   2019    0   0   1    0    0    0    0    0    0    0    0    1\n\n# Dummy coding the treatment variable in a 'generalized' DD model\n\n # N = 3 (counties)\n # T = 10 (years)\n\n# Variable labels:\n\n # cty  = unit identifier\n # year = time identifier\n # trt  = treatment variable\n # c_   = county effects (N - 1)\n # y_   = year effects (T - 1)\n\n# N x T = 30 county-year observations\n\ncty year  trt c_2 c_3 y_11 y_12 y_13 y_14 y_15 y_16 y_17 y_18 y_19\n1   2010    0   0   0    0    0    0    0    0    0    0    0    0\n1   2011    0   0   0    1    0    0    0    0    0    0    0    0\n1   2012    0   0   0    0    1    0    0    0    0    0    0    0\n1   2013    0   0   0    0    0    1    0    0    0    0    0    0\n1   2014    0   0   0    0    0    0    1    0    0    0    0    0\n1   2015    0   0   0    0    0    0    0    1    0    0    0    0\n1   2016    0   0   0    0    0    0    0    0    1    0    0    0\n1   2017    0   0   0    0    0    0    0    0    0    1    0    0 \n1   2018    0   0   0    0    0    0    0    0    0    0    1    0\n1   2019    0   0   0    0    0    0    0    0    0    0    0    1",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/457575/402101 \n 2   2010    0   1   0    0    0    0    0    0    0    0    0    0\n2   2011    0   1   0    1    0    0    0    0    0    0    0    0\n2   2012    0   1   0    0    1    0    0    0    0    0    0    0\n2   2013    1   1   0    0    0    1    0    0    0    0    0    0\n2   2014    1   1   0    0    0    0    1    0    0    0    0    0\n2   2015    1   1   0    0    0    0    0    1    0    0    0    0\n2   2016    1   1   0    0    0    0    0    0    1    0    0    0\n2   2017    1   1   0    0    0    0    0    0    0    1    0    0\n2   2018    1   1   0    0    0    0    0    0    0    0    1    0\n2   2019    1   1   0    0    0    0    0    0    0    0    0    1\n\n3   2010    0   0   1    0    0    0    0    0    0    0    0    0\n3   2011    0   0   1    1    0    0    0    0    0    0    0    0\n3   2012    0   0   1    0    1    0    0    0    0    0    0    0\n3   2013    0   0   1    0    0    1    0    0    0    0    0    0\n3   2014    0   0   1    0    0    0    1    0    0    0    0    0\n3   2015    1   0   1    0    0    0    0    1    0    0    0    0\n3   2016    1   0   1    0    0    0    0    0    1    0    0    0\n3   2017    1   0   1    0    0    0    0    0    0    1    0    0\n3   2018    0   0   1    0    0    0    0    0    0    0    1    0\n3   2019    0   0   1    0    0    0    0    0    0    0    0    1\n\nI hope this helps!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/10300/402101 \n It depends on exactly what you are looking for\n. Below are some brief details and references.\n\nMuch of the literature for approximations centers around the function\n\n$$\nQ(x) = \\int_x^\\infty \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, \\mathrm{d}u\n$$\n\nfor \n$x > 0$\n. This is because the function you provided can be decomposed as a simple difference of the function above (possibly adjusted by a constant). This function is referred to by many names, including \"upper-tail of the normal distribution\", \"right normal integral\", and \"Gaussian \n$Q$\n-function\", to name a few. You'll also see approximations to \nMills' ratio\n, which is\n\n$$\nR(x) = \\frac{Q(x)}{\\varphi(x)}\n$$\n\nwhere \n$\\varphi(x) = (2\\pi)^{-1/2} e^{-x^2 / 2}$\n is the Gaussian pdf.\n\nHere I list some references for various purposes that you might be interested in.\n\nComputational\n\nThe de-facto standard for computing the \n$Q$\n-function or the related complementary error function is\n\nW. J. Cody, \nRational Chebyshev Approximations for the Error Function\n, \nMath. Comp.\n, 1969, pp. 631--637.\n\nEvery\n (self-respecting) implementation uses this paper. (MATLAB, R, etc.)\n\n\"Simple\" Approximations\n\nAbramowitz and Stegun\n have one based on a polynomial expansion of a transformation of the input. Some people use it as a \"high-precision\" approximation. I don't like it for that purpose since it behaves badly around zero. For example, their approximation does \nnot\n yield \n$\\hat{Q}(0) = 1/2$\n, which I think is a big no-no. Sometimes \nbad things\n happen because of this.\n\nBorjesson and Sundberg give a simple approximation which works pretty well for most applications where one only requires a few digits of precision. The \nabsolute relative error\n is never worse than 1%, which is quite good considering its simplicity. The basic approximation is\n\n$$\n\\hat{Q}(x) = \\frac{1}{(1-a) x + a \\sqrt{x^2 + b}} \\varphi(x)\n$$\n\nand their preferred choices of the constants are \n$a = 0.339$\n and \n$b = 5.51$\n. That reference is\n\nP. O. Borjesson and C. E. Sundberg. \nSimple approximations of the error function Q(x) for communications\napplications\n. \nIEEE Trans. Commun.\n, COM-27(3):639\u2013643, March 1979.\n\nHere is a plot of its absolute relative error.\n\n\n\nThe electrical-engineering literature is awash with various such approximations and seem to take an overly intense interest in them. Many of them are poor though or expand to very strange and convoluted expressions.\n\nYou might also look at",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/10300/402101 \n P. O. Borjesson and C. E. Sundberg. \nSimple approximations of the error function Q(x) for communications\napplications\n. \nIEEE Trans. Commun.\n, COM-27(3):639\u2013643, March 1979.\n\nHere is a plot of its absolute relative error.\n\n\n\nThe electrical-engineering literature is awash with various such approximations and seem to take an overly intense interest in them. Many of them are poor though or expand to very strange and convoluted expressions.\n\nYou might also look at\n\nW. Bryc. \nA uniform approximation to the right normal integral\n. \nApplied Mathematics and Computation\n,\n127(2-3):365\u2013374, April 2002.\n\nLaplace's continued fraction\n\nLaplace has a beautiful continued fraction which yields successive upper and lower bounds for every value of \n$x > 0$\n. It is, in terms of Mills' ratio,\n\n$$\nR(x) = \\frac{1}{x+}\\frac{1}{x+}\\frac{2}{x+}\\frac{3}{x+}\\cdots ,\n$$\n\nwhere the notation I've used is fairly standard for a \ncontinued fraction\n, i.e., \n$1/(x+1/(x+2/(x+3/(x+\\cdots))))$\n. This expression doesn't converge very fast for small \n$x$\n, though, and it diverges at \n$x = 0$\n.\n\nThis continued fraction actually yields many of the \"simple\" bounds on \n$Q(x)$\n that were \"rediscovered\" in the mid-to-late 1900s. It's easy to see that for a continued fraction in \"standard\" form (i.e., composed of positive integer coefficients), truncating the fraction at odd (even) terms gives an upper (lower) bound.\n\nHence, Laplace tells us immediately that\n\n$$\n\\frac{x}{x^2 + 1} < R(x) < \\frac{1}{x} \\>,\n$$\n\nboth of which are bounds that were \"rediscovered\" in the mid-1900's. In terms of the \n$Q$\n-function, this is equivalent to\n\n$$\n\\frac{x}{x^2 + 1} \\varphi(x) < Q(x) < \\frac{1}{x} \\varphi(x) .\n$$\n\nAn alternative proof of this using simple integration by parts can be found in S. Resnick, \nAdventures in Stochastic Processes\n, Birkhauser, 1992, in Chapter 6 (Brownian motion). The absolute relative error of these bounds is no worse than \n$x^{-2}$\n, as shown in \nthis related answer\n.\n\nNotice, in particular, that the inequalities above immediately imply that \n$Q(x) \\sim \\varphi(x)/x$\n. This fact can be established using L'Hopital's rule as well. This also helps explain the choice of the functional form of the Borjesson-Sundberg approximation. Any choice of \n$a \\in [0,1]$\n maintains the asymptotic equivalence as \n$x \\to \\infty$\n. The parameter \n$b$\n serves as a \"continuity correction\" near zero.\n\nHere is a plot of the \n$Q$\n-function and the two Laplace bounds.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/10300/402101 \n Notice, in particular, that the inequalities above immediately imply that \n$Q(x) \\sim \\varphi(x)/x$\n. This fact can be established using L'Hopital's rule as well. This also helps explain the choice of the functional form of the Borjesson-Sundberg approximation. Any choice of \n$a \\in [0,1]$\n maintains the asymptotic equivalence as \n$x \\to \\infty$\n. The parameter \n$b$\n serves as a \"continuity correction\" near zero.\n\nHere is a plot of the \n$Q$\n-function and the two Laplace bounds.\n\n\n\nC-I. C. Lee has a paper from the early 1990's that does a \"correction\" for small values of \n$x$\n. See\n\nC-I. C. Lee. \nOn Laplace continued fraction for the normal integral\n. \nAnn. Inst. Statist. Math.\n, 44(1):107\u2013120,\nMarch 1992.\n\nDurrett's \nProbability: Theory and Examples\n provides the classical upper and lower bounds on \n$Q(x)$\n on pages 6\u20137 of the 3rd edition. They're meant for larger values of \n$x$\n (say, \n$x > 3$\n) and are asymptotically tight.\n\nHopefully this will get you started. If you have a more specific interest, I might be able to point you somewhere.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/19333/402101 \n As others have stated, you need to have a common frequency of measurement (i.e. the time between observations). With that in place I would identify a common model that would reasonably describe each series separately. This might be an ARIMA model or a multiply-trended  Regression Model with possible Level Shifts or a composite model integrating both memory (ARIMA) and dummy variables. This common model could be estimated globally and separately for each of the two series and then one could construct an F test to test the hypothesis of a common set of parameters.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6801/402101 \n I found this thought experiment helpful when thinking about confidence intervals. It also answers your question 3.\n\nLet $X\\sim U(0,1)$ and $Y=X+a-\\frac{1}{2}$. Consider two observations of $Y$ taking the values $y_1$ and $y_2$ corresponding to observations $x_1$ and $x_2$ of $X$, and let $y_l=\\min(y_1,y_2)$ and $y_u=\\max(y_1,y_2)$. Then $[y_l,y_u]$ is a 50% confidence interval for $a$ (since the interval includes $a$ if $x_1<\\frac12<x_2$ or $x_1>\\frac12>x_2$, each of which has probability $\\frac14$).\n\nHowever, if $y_u-y_l>\\frac12$ then we know that the probability that the interval contains $a$ is $1$, not $\\frac12$. The subtlety is that a $z\\%$ confidence interval for a parameter means that the endpoints of the interval (which are random variables) lie either side of the parameter with probability $z\\%$ \nbefore you calculate the interval\n, not that the probability of the parameter lying within the interval is $z\\%$ \nafter you have calculated the interval\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/87422/402101 \n The standard deviation calculated with a divisor of \n$n-1$\n is a standard deviation calculated from the sample as an estimate of the standard deviation of the population from which the sample was drawn. Because the observed values fall, on average, closer to the sample mean than to the population mean, the standard deviation which is calculated using deviations from the sample mean underestimates the desired standard deviation of the population. Using \n$n-1$\n instead of \n$n$\n as the divisor corrects for that by making the result a little bit bigger.\n\nNote that the correction has a larger proportional effect when \n$n$\n is small than when it is large, which is what we want because when n is larger the sample mean is likely to be a good estimator of the population mean.\n\nWhen the sample is the whole population we use the standard deviation with \n$n$\n as the divisor because the sample mean \nis\n population mean.\n\n(I note parenthetically that nothing that starts with \"second moment recentered around a known, definite mean\" is going to fulfil the questioner's request for an intuitive explanation.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/64640/402101 \n I'll start by providing the definition of \ncomonotonicity\n and \ncountermonotonicity\n. Then, I'll mention why this is relevant to compute the minimum and maximum possible correlation coefficient between two random variables. And finally, I'll compute these bounds for the lognormal random variables $X_1$ and $X_2$.\n\nComonotonicity and countermonotonicity\n\nThe random variables $X_1, \\ldots, X_d$ are said to be \ncomonotonic\n if their \ncopula\n is the \nFr\u00e9chet upper bound\n $M(u_1, \\ldots, u_d) = \\min(u_1, \\ldots, u_d)$, which is the strongest type of \"positive\" dependence.\n\nIt can be shown that $X_1, \\ldots, X_d$ are comonotonic if and only if\n$$\n(X_1, \\ldots, X_d) \\stackrel{\\mathrm{d}}{=} (h_1(Z), \\ldots, h_d(Z)),\n$$\nwhere $Z$ is some random variable, $h_1, \\ldots, h_d$ are increasing functions, and \n$\\stackrel{\\mathrm{d}}{=}$ denotes equality in distribution.\nSo, comonotonic random variables are only functions of a single random variable.\n\nThe random variables $X_1, X_2$ are said to be \ncountermonotonic\n if their copula is the \nFr\u00e9chet lower bound\n $W(u_1, u_2) = \\max(0, u_1 + u_2 - 1)$, which is the strongest type of \"negative\" dependence in the bivariate case. Countermonotonocity doesn't generalize to higher dimensions.\n\nIt can be shown that $X_1, X_2$ are countermonotonic if and only if\n$$\n(X_1, X_2) \\stackrel{\\mathrm{d}}{=} (h_1(Z), h_2(Z)),\n$$\nwhere $Z$ is some random variable, and $h_1$ and $h_2$ are respectively an increasing and a decreasing function, or vice versa.\n\nAttainable correlation\n\nLet $X_1$ and $X_2$ be two random variables with strictly positive and finite variances, and let $\\rho_{\\min}$ and $\\rho_{\\max}$ denote the minimum and maximum possible correlation coefficient between $X_1$ and $X_2$.\nThen, it can be shown that\n\n${\\rm \\rho}(X_1, X_2) = \\rho_{\\min}$ if and only if $X_1$ and $X_2$ are countermonotonic;\n\n\n${\\rm \\rho}(X_1, X_2) = \\rho_{\\max}$ if and only if $X_1$ and $X_2$ are comonotonic.\n\nAttainable correlation for lognormal random variables\n\nTo obtain $\\rho_{\\max}$ we use the fact that the maximum correlation is attained if and only if $X_1$ and $X_2$ are comonotonic. The random variables $X_1 = e^{Z}$ and $X_2 = e^{\\sigma Z}$ where $Z \\sim {\\rm N} (0, 1)$ are comonotonic since the exponential function is a (strictly) increasing function, and thus $\\rho_{\\max} = {\\rm corr} \\left (e^Z, e^{\\sigma Z} \\right )$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/64640/402101 \n Attainable correlation for lognormal random variables\n\nTo obtain $\\rho_{\\max}$ we use the fact that the maximum correlation is attained if and only if $X_1$ and $X_2$ are comonotonic. The random variables $X_1 = e^{Z}$ and $X_2 = e^{\\sigma Z}$ where $Z \\sim {\\rm N} (0, 1)$ are comonotonic since the exponential function is a (strictly) increasing function, and thus $\\rho_{\\max} = {\\rm corr} \\left (e^Z, e^{\\sigma Z} \\right )$.\n\nUsing the properties of \nlognormal random variables\n, we have\n${\\rm E}(e^Z) = e^{1/2}$,\n${\\rm E}(e^{\\sigma Z}) = e^{\\sigma^2/2}$,\n${\\rm var}(e^Z) = e(e - 1)$, \n${\\rm var}(e^{\\sigma Z}) = e^{\\sigma^2}(e^{\\sigma^2} - 1)$, and the covariance is\n\\begin{align}\n{\\rm cov}\\left (e^Z, e^{\\sigma Z}\\right )\n &= {\\rm E}\\left (e^{(\\sigma + 1) Z}\\right ) - {\\rm E}\\left (e^{\\sigma Z}\\right ){\\rm E}\\left (e^Z\\right ) \\\\\n &= e^{(\\sigma + 1)^2/2} - e^{(\\sigma^2 + 1)/2} \\\\\n &= e^{(\\sigma^2 + 1)/2} ( e^{\\sigma} -1 ).\n\\end{align}\nThus, \n\\begin{align}\n\\rho_{\\max} \n & = \\frac{ e^{(\\sigma^2 + 1)/2} ( e^{\\sigma} -1 ) }\n          { \\sqrt{ e(e - 1) e^{\\sigma^2}(e^{\\sigma^2} - 1) } } \\\\\n & = \\frac{ ( e^{\\sigma} -1 ) }\n          { \\sqrt{ (e - 1) (e^{\\sigma^2} - 1) } }.\n\\end{align}\n\nSimilar computations with $X_2 = e^{-\\sigma Z}$ yield\n\\begin{align}\n\\rho_{\\min} \n & = \\frac{ ( e^{-\\sigma} -1 ) }\n          { \\sqrt{ (e - 1) (e^{\\sigma^2} - 1) } }.\n\\end{align}\n\nComment\n\nThis example shows that it is possible to have a pair of random variable that are strongly dependent \u2014 comonotonicity and countermonotonicity are the strongest kind of dependence \u2014 but that have a very low correlation.\nThe following chart shows these bounds as a function of $\\sigma$.\n\n\n\nThis is the R code I used to produce the above chart.\n\ncurve((exp(x)-1)/sqrt((exp(1) - 1)*(exp(x^2) - 1)), from = 0, to = 5,\n      ylim = c(-1, 1), col = 2, lwd = 2, main = \"Lognormal attainable correlation\",\n      xlab = expression(sigma), ylab = \"Correlation\", cex.lab = 1.2)\ncurve((exp(-x)-1)/sqrt((exp(1) - 1)*(exp(x^2) - 1)), col = 4, lwd = 2, add = TRUE)\nlegend(x = \"bottomright\", col = c(2, 4), lwd = c(2, 2), inset = 0.02,\n       legend = c(\"Correlation upper bound\", \"Correlation lower bound\"))\nabline(h = 0, lty = 2)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/64640/402101 \n curve((exp(x)-1)/sqrt((exp(1) - 1)*(exp(x^2) - 1)), from = 0, to = 5,\n      ylim = c(-1, 1), col = 2, lwd = 2, main = \"Lognormal attainable correlation\",\n      xlab = expression(sigma), ylab = \"Correlation\", cex.lab = 1.2)\ncurve((exp(-x)-1)/sqrt((exp(1) - 1)*(exp(x^2) - 1)), col = 4, lwd = 2, add = TRUE)\nlegend(x = \"bottomright\", col = c(2, 4), lwd = c(2, 2), inset = 0.02,\n       legend = c(\"Correlation upper bound\", \"Correlation lower bound\"))\nabline(h = 0, lty = 2)\n\ncurve((exp(x)-1)/sqrt((exp(1) - 1)*(exp(x^2) - 1)), from = 0, to = 5,\n      ylim = c(-1, 1), col = 2, lwd = 2, main = \"Lognormal attainable correlation\",\n      xlab = expression(sigma), ylab = \"Correlation\", cex.lab = 1.2)\ncurve((exp(-x)-1)/sqrt((exp(1) - 1)*(exp(x^2) - 1)), col = 4, lwd = 2, add = TRUE)\nlegend(x = \"bottomright\", col = c(2, 4), lwd = c(2, 2), inset = 0.02,\n       legend = c(\"Correlation upper bound\", \"Correlation lower bound\"))\nabline(h = 0, lty = 2)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45063/402101 \n One very common cause is mis-specification. For example, let $y$ be grocery sales and $\\varepsilon$ be an \nunobserved\n (to the analyst) coupon campaign that varies in intensity over time. At any point in time, there may be several \"vintages\" of coupons circulating as people use them, throw them away, and receive new ones. Shocks can also have persistent (but gradually weakening) effects. Take natural disasters or simply bad weather. Battery sales go up before the storm, then fall during, and then jump again as people people realize that disaster kits may be a good idea for the future.\n\nSimilarly, data manipulation (like smoothing or interpolation) can induce this effect.\n\nI also have \"inherently smooth behavior of time series data (inertia) can cause $MA(1)$\" in my notes, but that one no longer makes sense to me.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/357275/402101 \n So if that's the case, does statistical independence automatically\n  mean lack of causation?\n\nNo, and here's a simple counter example with a multivariate normal,\n\nset.seed(100)\nn <- 1e6\na <- 0.2\nb <- 0.1\nc <- 0.5\nz <- rnorm(n)\nx <- a*z + sqrt(1-a^2)*rnorm(n)\ny <- b*x - c*z + sqrt(1- b^2 - c^2 +2*a*b*c)*rnorm(n)\ncor(x, y)\n\nset.seed(100)\nn <- 1e6\na <- 0.2\nb <- 0.1\nc <- 0.5\nz <- rnorm(n)\nx <- a*z + sqrt(1-a^2)*rnorm(n)\ny <- b*x - c*z + sqrt(1- b^2 - c^2 +2*a*b*c)*rnorm(n)\ncor(x, y)\n\nWith corresponding graph,\n\n\n\nHere we have that $x$ and $y$ are marginally independent (in the multivariate normal case, zero correlation implies independence). This happens because the backdoor path via $z$ exactly cancels out the direct path from $x$ to $y$, that is, $cov(x,y) = b - a*c = 0.1 - 0.1 = 0$. Thus $E[Y|X =x] =E[Y] =0$. Yet, $x$ directly causes $y$, and we have that $E[Y|do(X= x)] = bx$, which is different from $E[Y]=0$.\n\nAssociations, interventions and counterfactuals\n\nI think it's important to make some clarifications here regarding associations, interventions and counterfactuals.\n\nCausal models entail statements about the behavior of the system: (i) under passive observations, (ii) under interventions, as well as (iii) counterfactuals. And independence on one level does not necessarily translate to the other.\n\nAs the example above shows, we can have no association between $X$ and $Y$, that is, $P(Y|X) = P(Y)$, and still be the case that manipulations on $X$ changes the distribution of $Y$, that is, $P(Y|do(x)) \\neq P(Y)$.\n\nNow, we can go one step further. We can have causal models where  intervening on $X$ does not change the population distribution of $Y$, but that does not mean lack of counterfactual causation! That is, even though $P(Y|do(x)) = P(Y)$, for every individual their outcome $Y$ would have been different had you changed his $X$. This is precisely the case described by user20160, as well as in my previous answer \nhere.\n\nThese three levels make a \nhierarchy of causal inference tasks\n, in terms of the information needed to answer queries on each of them.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4095/402101 \n I think it's useful to review what we know about cross-validation.  Statistical results around CV fall into two classes: efficiency and consistency.\n\nEfficiency is what we're usually concerned with when building predictive models.  The idea is that we use CV to determine a model with asymtptotic guarantees concerning the loss function.  The most famous result here is due to \nAn asymptotic equivalence of choice of model by cross\u2010validation and Akaike's criterion\n (Stone 1977) and shows that LOO CV is asymptotically equivalent to AIC.  But, Brett provides a good example where you can find a predictive model which doesn't inform you on the causal mechanism.\n\nConsistency is what we're concerned with if our goal is to find the \"true\" model.  The idea is that we use CV to determine a model with asymptotic guarantees that, given that our model space includes the true model, we'll discover it with a large enough sample.  The most famous result here is due to \nLinear Model Selection by Cross-Validation\n (Shao 1993) concerning linear models, but as he states in his abstract, his \"shocking discovery\" is opposite of the result for LOO.  For linear models, you can achieve consistency using LKO CV as long as \n$k/n \\rightarrow 1$\n as \n$n \\rightarrow \\infty$\n.  Beyond linear mdoels, it's harder to derive statistical results.\n\nBut suppose you can meet the consistency criteria and your CV procedure leads to the true model: \n$Y = \\beta X + e$\n.  What have we learned about the causal mechanism?  We simply know that there's a well defined correlation between \n$Y$\n and \n$X$\n, which doesn't say much about causal claims.  From a traditional perspective, you need to bring in experimental design with the mechanism of control/manipulation to make causal claims.  From the perspective of Judea Pearl's framework, you can bake causal assumptions into a structural model and use the probability based calculus of counterfactuals to derive some claims, but you'll need to satisfy \ncertain properties\n.\n\nPerhaps you could say that CV can help with causal inference by identifying the true model (provided you can satisfy consistency criteria!).  But it only gets you so far; CV by itself isn't doing any of the work in either framework of causal inference.\n\nIf you're interested further in what we can say with cross-validation, I would recommend Shao 1997 over the widely cited 1993 paper:\n\nAn Asymptotic Theory for Linear Model Selection\n (Shao, 1997)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4095/402101 \n Perhaps you could say that CV can help with causal inference by identifying the true model (provided you can satisfy consistency criteria!).  But it only gets you so far; CV by itself isn't doing any of the work in either framework of causal inference.\n\nIf you're interested further in what we can say with cross-validation, I would recommend Shao 1997 over the widely cited 1993 paper:\n\nAn Asymptotic Theory for Linear Model Selection\n (Shao, 1997)\n\nYou can skim through the major results, but it's interesting to read the discussion that follows.  I thought the comments by Rao & Tibshirani, and by Stone, were particularly insightful.  But note that while they discuss consistency, no claims are ever made regarding causality.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n This is a subtle question.\n  It takes a thoughtful person \nnot\n to understand those quotations!  Although they are suggestive, it turns out that none of them is exactly or generally correct.  I haven't the time (and there isn't the space here) to give a full exposition, but I would like to share one approach and an insight that it suggests.\n\nWhere does the concept of degrees of freedom (DF) arise?\n  The contexts in which it's found in elementary treatments are:\n\nThe \nStudent t-test\n and its variants such as the Welch or Satterthwaite solutions to the Behrens-Fisher problem (where two populations have different variances).\n\n\nThe Chi-squared distribution (defined as a sum of squares of independent standard Normals), which is implicated in the \nsampling distribution of the variance.\n\n\nThe \nF-test\n (of ratios of estimated variances).\n\n\nThe \nChi-squared test\n, comprising its uses in (a) testing for independence in contingency tables and (b) testing for goodness of fit of distributional estimates.\n\nThe \nStudent t-test\n and its variants such as the Welch or Satterthwaite solutions to the Behrens-Fisher problem (where two populations have different variances).\n\nThe Chi-squared distribution (defined as a sum of squares of independent standard Normals), which is implicated in the \nsampling distribution of the variance.\n\nThe \nF-test\n (of ratios of estimated variances).\n\nThe \nChi-squared test\n, comprising its uses in (a) testing for independence in contingency tables and (b) testing for goodness of fit of distributional estimates.\n\nIn spirit, these tests run a gamut from being exact (the Student t-test and F-test for Normal variates) to being good approximations (the Student t-test and the Welch/Satterthwaite tests for not-too-badly-skewed data) to being based on asymptotic approximations (the Chi-squared test).  An interesting aspect of some of these is the appearance of non-integral \"degrees of freedom\" (the Welch/Satterthwaite tests and, as we will see, the Chi-squared test).  This is of especial interest because it is the first hint that DF is \nnot\n any of the things claimed of it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n We can dispose right away of some of the claims in the question.\n  Because \"final calculation of a statistic\" is not well-defined (it apparently depends on what algorithm one uses for the calculation), it can be no more than a vague suggestion and is worth no further criticism.  Similarly, neither \"number of independent scores that go into the estimate\" nor \"the number of parameters used as intermediate steps\" are well-defined.\n\n\"Independent pieces of information that go into [an] estimate\"\n is difficult to deal with, because there are two different but intimately related senses of \"independent\" that can be relevant here.  One is independence of random variables; the other is \nfunctional independence.\n  As an example of the latter, suppose we collect morphometric measurements of subjects--say, for simplicity, the three side lengths $X$, $Y$, $Z$, surface areas $S=2(XY+YZ+ZX)$, and volumes $V=XYZ$ of a set of wooden blocks.  The three side lengths can be considered independent random variables, but all five variables are dependent RVs.  The five are also \nfunctionally\n dependent because the \ncodomain\n (\nnot\n the \"domain\"!) of the vector-valued random variable $(X,Y,Z,S,V)$ traces out a three-dimensional manifold in $\\mathbb{R}^5$.  (Thus, locally at any point $\\omega\\in\\mathbb{R}^5$, there are two functions $f_\\omega$ and $g_\\omega$ for which $f_\\omega(X(\\psi),\\ldots,V(\\psi))=0$ and $g_\\omega(X(\\psi),\\ldots,V(\\psi))=0$ for points $\\psi$ \"near\" $\\omega$ and the derivatives of $f$ and $g$ evaluated at $\\omega$ are linearly independent.)  However--here's the kicker--for many probability measures on the blocks, subsets of the variables such as $(X,S,V)$ are \ndependent\n as random variables but functionally \nindependent.\n\nHaving been alerted by these potential ambiguities, \nlet's hold up the Chi-squared goodness of fit test for examination\n, because (a) it's simple, (b) it's one of the common situations where people really do need to know about DF to get the p-value right and (c) it's often used incorrectly.  Here's a brief synopsis of the least controversial application of this test:\n\nYou have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n Having been alerted by these potential ambiguities, \nlet's hold up the Chi-squared goodness of fit test for examination\n, because (a) it's simple, (b) it's one of the common situations where people really do need to know about DF to get the p-value right and (c) it's often used incorrectly.  Here's a brief synopsis of the least controversial application of this test:\n\nYou have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.\n\n\nYou have estimated some parameters $\\theta_1, \\ldots, \\theta_p$ of a distribution. For example, you estimated the mean $\\theta_1$ and standard deviation $\\theta_2 = \\theta_p$ of a Normal distribution, hypothesizing that the population is normally distributed but not knowing (in advance of obtaining the data) what $\\theta_1$ or $\\theta_2$ might be.\n\n\nIn advance, you created a set of $k$ \"bins\" for the data.  (It may be problematic when the bins are determined by the data, even though this is often done.)  Using these bins, the data are reduced to the set of counts within each bin.  Anticipating what the true values of $(\\theta)$ might be, you have arranged it so (hopefully) each bin will receive approximately the same count.  (Equal-probability binning assures the chi-squared distribution really is a good approximation to the true distribution of the chi-squared statistic about to be described.)\n\n\nYou have a lot of data--enough to assure that almost all bins ought to have counts of 5 or greater.  (This, we hope, will enable the sampling distribution of the $\\chi^2$ statistic to be approximated adequately by some $\\chi^2$ distribution.)\n\nYou have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.\n\nYou have estimated some parameters $\\theta_1, \\ldots, \\theta_p$ of a distribution. For example, you estimated the mean $\\theta_1$ and standard deviation $\\theta_2 = \\theta_p$ of a Normal distribution, hypothesizing that the population is normally distributed but not knowing (in advance of obtaining the data) what $\\theta_1$ or $\\theta_2$ might be.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n You have a collection of data values $(x_1, \\ldots, x_n)$, considered as a sample of a population.\n\nYou have estimated some parameters $\\theta_1, \\ldots, \\theta_p$ of a distribution. For example, you estimated the mean $\\theta_1$ and standard deviation $\\theta_2 = \\theta_p$ of a Normal distribution, hypothesizing that the population is normally distributed but not knowing (in advance of obtaining the data) what $\\theta_1$ or $\\theta_2$ might be.\n\nIn advance, you created a set of $k$ \"bins\" for the data.  (It may be problematic when the bins are determined by the data, even though this is often done.)  Using these bins, the data are reduced to the set of counts within each bin.  Anticipating what the true values of $(\\theta)$ might be, you have arranged it so (hopefully) each bin will receive approximately the same count.  (Equal-probability binning assures the chi-squared distribution really is a good approximation to the true distribution of the chi-squared statistic about to be described.)\n\nYou have a lot of data--enough to assure that almost all bins ought to have counts of 5 or greater.  (This, we hope, will enable the sampling distribution of the $\\chi^2$ statistic to be approximated adequately by some $\\chi^2$ distribution.)\n\nUsing the parameter estimates, you can compute the expected count in each bin.  The Chi-squared statistic is the sum of the ratios\n\n$$\\frac{(\\text{observed}-\\text{expected})^2}{\\text{expected}}.$$\n\nThis, many authorities tell us, should have (to a very close approximation) a Chi-squared distribution.  But there's a whole family of such distributions.  They are differentiated by a parameter $\\nu$ often referred to as the \"degrees of freedom.\"  \nThe standard reasoning\n about how to determine $\\nu$ goes like this\n\nI have $k$ counts.  That's $k$ pieces of data.  But there are (\nfunctional\n) relationships among them.  To start with, I know in advance that the sum of the counts must equal $n$.  That's one relationship.  I estimated two (or $p$, generally) parameters from the data.  That's two (or $p$) additional relationships, giving $p+1$ total relationships.  Presuming they (the parameters) are all (\nfunctionally\n) independent, that leaves only $k-p-1$ (\nfunctionally\n) independent \"degrees of freedom\": that's the value to use for $\\nu$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n The problem with this reasoning\n (which is the sort of calculation the quotations in the question are hinting at) \nis that it's wrong except when some special additional conditions hold.\n  Moreover, those conditions have \nnothing\n to do with independence (functional or statistical), with numbers of \"components\" of the data, with the numbers of parameters, nor with anything else referred to in the original question.\n\nLet me show you with an example.  (To make it as clear as possible, I'm using a small number of bins, but that's not essential.)  Let's generate 20 independent and identically distributed (iid) standard Normal variates and estimate their mean and standard deviation with the usual formulas (mean = sum/count, \netc\n.).  To test goodness of fit, create four bins with cutpoints at the quartiles of a standard normal: -0.675, 0, +0.657, and use the bin counts to generate a Chi-squared statistic.  Repeat as patience allows; I had time to do 10,000 repetitions.\n\nThe standard wisdom about DF says we have 4 bins and 1+2 = 3 constraints, implying the distribution of these 10,000 Chi-squared statistics should follow a Chi-squared distribution with 1 DF.  Here's the histogram:\n\n\n\nThe dark blue line graphs the PDF of a $\\chi^2(1)$ distribution--the one we thought would work--while the dark red line graphs that of a $\\chi^2(2)$ distribution (which would be a good guess if someone were to tell you that $\\nu=1$ is incorrect).  \nNeither fits the data.\n\nYou might expect the problem to be due to the small size of the data sets ($n$=20) or perhaps the small size of the number of bins.  However, the problem persists even with very large datasets and larger numbers of bins: it is not merely a failure to reach an asymptotic approximation.\n\nThings went wrong because I violated two requirements of the Chi-squared test:\n\nYou must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)\n\nYou must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n You must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)\n\nYou must use the \nMaximum Likelihood\n estimate of the parameters.  (This requirement can, in practice, be slightly violated.)\n\nYou must base that estimate \non the counts,\n not on the actual data!  (This is \ncrucial\n.)\n\n\n\nThe red histogram depicts the chi-squared statistics for 10,000 separate iterations, following these requirements.  Sure enough, it visibly follows the $\\chi^2(1)$ curve (with an acceptable amount of sampling error), as we had originally hoped.\n\nThe point of this comparison--which I hope you have seen coming--is that the correct DF to use for computing the p-values depends on many things \nother\n than dimensions of manifolds, counts of functional relationships, or the geometry of Normal variates.  There is a subtle, delicate interaction between certain \nfunctional dependencies,\n as found in mathematical relationships among quantities, and \ndistributions\n of the data, their statistics, and the estimators formed from them.  Accordingly, \nit cannot be the case that DF is adequately explainable in terms of the geometry of multivariate normal distributions, or in terms of functional independence, or as counts of parameters, or anything else of this nature.\n\nWe are led to see, then, that \"degrees of freedom\" is merely a \nheuristic\n that \nsuggests\n what the sampling distribution of a (t, Chi-squared, or F) statistic ought to be, \nbut it is not dispositive.\n  Belief that it is dispositive leads to egregious errors.  (For instance, the \ntop hit\n on Google when searching \"chi squared goodness of fit\" is a \nWeb page from an Ivy League university\n that gets most of this completely wrong!  In particular, a simulation based on its instructions shows that the chi-squared value it recommends as having 7 DF actually has 9 DF.)\n\nWith this more nuanced understanding, it's worthwhile to re-read the Wikipedia article in question: in its details it gets things right, pointing out where the DF heuristic tends to work and where it is either an approximation or does not apply at all.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n With this more nuanced understanding, it's worthwhile to re-read the Wikipedia article in question: in its details it gets things right, pointing out where the DF heuristic tends to work and where it is either an approximation or does not apply at all.\n\nA good account of the phenomenon illustrated here (unexpectedly high DF in Chi-squared GOF tests) appears in \nVolume II of Kendall & Stuart, 5th edition\n.  I am grateful for the opportunity afforded by this question to lead me back to this wonderful text, which is full of such useful analyses.\n\nHere is \nR\n code to produce the figure following \"The standard wisdom about DF...\"\n\nR\n\n#\n# Simulate data, one iteration per column of `x`.\n#\nn <- 20\nn.sim <- 1e4\nbins <- qnorm(seq(0, 1, 1/4))\nx <- matrix(rnorm(n*n.sim), nrow=n)\n#\n# Compute statistics.\n#\nm <- colMeans(x)\ns <- apply(sweep(x, 2, m), 2, sd)\ncounts <- apply(matrix(as.numeric(cut(x, bins)), nrow=n), 2, tabulate, nbins=4)\nexpectations <- mapply(function(m,s) n*diff(pnorm(bins, m, s)), m, s)\nchisquared <- colSums((counts - expectations)^2 / expectations)\n#\n# Plot histograms of means, variances, and chi-squared stats.  The first\n# two confirm all is working as expected.\n#\nmfrow <- par(\"mfrow\")\npar(mfrow=c(1,3))\nred <- \"#a04040\"  # Intended to show correct distributions\nblue <- \"#404090\" # To show the putative chi-squared distribution\nhist(m, freq=FALSE)\ncurve(dnorm(x, sd=1/sqrt(n)), add=TRUE, col=red, lwd=2)\nhist(s^2, freq=FALSE)\ncurve(dchisq(x*(n-1), df=n-1)*(n-1), add=TRUE, col=red, lwd=2)\nhist(chisquared, freq=FALSE, breaks=seq(0, ceiling(max(chisquared)), 1/4), \n     xlim=c(0, 13), ylim=c(0, 0.55), \n     col=\"#c0c0ff\", border=\"#404040\")\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=2)), add=TRUE, col=red, lwd=2)\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=1)), add=TRUE, col=blue, lwd=2)\npar(mfrow=mfrow)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17148/402101 \n #\n# Simulate data, one iteration per column of `x`.\n#\nn <- 20\nn.sim <- 1e4\nbins <- qnorm(seq(0, 1, 1/4))\nx <- matrix(rnorm(n*n.sim), nrow=n)\n#\n# Compute statistics.\n#\nm <- colMeans(x)\ns <- apply(sweep(x, 2, m), 2, sd)\ncounts <- apply(matrix(as.numeric(cut(x, bins)), nrow=n), 2, tabulate, nbins=4)\nexpectations <- mapply(function(m,s) n*diff(pnorm(bins, m, s)), m, s)\nchisquared <- colSums((counts - expectations)^2 / expectations)\n#\n# Plot histograms of means, variances, and chi-squared stats.  The first\n# two confirm all is working as expected.\n#\nmfrow <- par(\"mfrow\")\npar(mfrow=c(1,3))\nred <- \"#a04040\"  # Intended to show correct distributions\nblue <- \"#404090\" # To show the putative chi-squared distribution\nhist(m, freq=FALSE)\ncurve(dnorm(x, sd=1/sqrt(n)), add=TRUE, col=red, lwd=2)\nhist(s^2, freq=FALSE)\ncurve(dchisq(x*(n-1), df=n-1)*(n-1), add=TRUE, col=red, lwd=2)\nhist(chisquared, freq=FALSE, breaks=seq(0, ceiling(max(chisquared)), 1/4), \n     xlim=c(0, 13), ylim=c(0, 0.55), \n     col=\"#c0c0ff\", border=\"#404040\")\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=2)), add=TRUE, col=red, lwd=2)\ncurve(ifelse(x <= 0, Inf, dchisq(x, df=1)), add=TRUE, col=blue, lwd=2)\npar(mfrow=mfrow)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/q/49052/402101 \n Overfitting comes from allowing too large a class of models. This gets a bit tricky with models with continuous parameters (like splines and polynomials), but if you discretize the parameters into some number of distinct values, you'll see that increasing the number of knots/coefficients will increase the number of available models exponentially. For every dataset there is a spline and a polynomial that fits precisely, so long as you allow enough coefficients/knots. It may be that a spline with three knots overfits more than a polynomial with three coefficients, but that's hardly a fair comparison.\n\nIf you have a low number of parameters, and a large dataset, you can be reasonably sure you're not overfitting. If you want to try higher numbers of parameters you can try cross validating within your test set to find the best number, or you can use a criterion like \nMinimum Description Length\n.\n\nEDIT\n: As requested in the comments, an example of how one would apply MDL. First you have to deal with the fact that your data is continuous, so it can't be represented in a finite code. For the sake of simplicity we'll segment the data space into boxes of side \n$\\epsilon$\n and instead of describing the data points, we'll describe the boxes that the data falls into. This means we lose some accuracy, but we can make \n$\\epsilon$\n arbitrarily small, so it doesn't matter much.\n\nNow, the task is to describe the dataset as sucinctly as possible with the help of some polynomial. First we describe the polynomial. If it's an n-th order polynomial, we just need to store (n+1) coefficients. Again, we need to discretize these values. After that we need to store first the value \n$n$\n in prefix-free coding (so we know when to stop reading) and then the \n$n+1$\n parameter values. With this information a receiver of our code could restore the polynomial. Then we add the rest of the information required to store the dataset. For each datapoint we give the x-value, and then how many boxes up or down the data point lies off the polynomial. Both values we store in prefix-free coding so that short values require few bits, and we won't need delimiters between points. (You can shorten the code for the x-values by only storing the increments between values)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/q/49052/402101 \n The fundamental point here is the tradeoff. If I choose a 0-order polynomial (like f(x) = 3.4), then the model is very simple to store, but for the y-values, I'm essentially storing the distance to the mean. More coefficients give me a better fitting polynomial (and thus shorter codes for the y values), but I have to spend more bits describing the model. The model that gives you the shortest code for your data is the best fit by the MDL criterion.\n\n(Note that this is known as 'crude MDL', and there are some refinements you can make to solve various technical issues).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/251128/402101 \n The sample standard deviation \n$S=\\sqrt{\\frac{\\sum (X - \\bar{X})^2}{n-1}}$\n is complete and sufficient for \n$\\sigma$\n so the set of unbiased estimators of \n$\\sigma^k$\n given by\n\n$$\n\\frac{(n-1)^\\frac{k}{2}}{2^\\frac{k}{2}} \\cdot \\frac{\\Gamma\\left(\\frac{n-1}{2}\\right)}{\\Gamma\\left(\\frac{n+k-1}{2}\\right)} \\cdot S^k = \\frac{S^k}{c_k}\n$$\n\n(See \nWhy is sample standard deviation a biased estimator of \n$\\sigma$\n?\n) are, by the Lehmann\u2013Scheff\u00e9 theorem, UMVUE. Consistent, though biased, estimators of \n$\\sigma^k$\n can also be formed as\n\n$$\n\\tilde{\\sigma}^k_j= \\left(\\frac{S^j}{c_j}\\right)^\\frac{k}{j}\n$$\n\n(the unbiased estimators being specified when \n$j=k$\n). The bias of each is given by\n\n$$\\operatorname{E}\\tilde{\\sigma}^k_j - \\sigma^k =\\left( \\frac{c_k}{c_j^\\frac{k}{j}} -1 \\right) \\sigma^k$$\n\n& its variance by\n\n$$\\operatorname{Var}\\tilde{\\sigma}^{k}_j=\\operatorname{E}\\tilde{\\sigma}^{2k}_j - \\left(\\operatorname{E}\\tilde{\\sigma}^k_j\\right)^2=\\frac{c_{2k}-c_k^2}{c_j^\\frac{2k}{j}} \\sigma^{2k}$$\n\nFor the two estimators of \n$\\sigma$\n you've considered, \n$\\tilde{\\sigma}^1_1=\\frac{S}{c_1}$\n & \n$\\tilde{\\sigma}^1_2=S$\n, the lack of bias of \n$\\tilde{\\sigma}_1$\n is more than offset by its larger variance when compared to \n$\\tilde{\\sigma}_2$\n:\n\n$$\\begin{align}\n\\operatorname{E}\\tilde{\\sigma}_1 - \\sigma &= 0 \\\\\n\\operatorname{E}\\tilde{\\sigma}_2 - \\sigma &=(c_1 -1) \\sigma \\\\\n\\operatorname{Var}\\tilde{\\sigma}_1 =\\operatorname{E}\\tilde{\\sigma}^{2}_1 - \\left(\\operatorname{E}\\tilde{\\sigma}^1_1\\right)^2 &=\\frac{c_{2}-c_1^2}{c_1^2} \\sigma^{2} = \\left(\\frac{1}{c_1^2}-1\\right) \\sigma^2 \\\\\n\\operatorname{Var}\\tilde{\\sigma}_2 =\\operatorname{E}\\tilde{\\sigma}^{2}_1 - \\left(\\operatorname{E}\\tilde{\\sigma}_2\\right)^2 &=\\frac{c_{2}-c_1^2}{c_2} \\sigma^{2}=(1-c_1^2)\\sigma^2\n\\end{align}$$\n\n(Note that \n$c_2=1$\n, as \n$S^2$\n is already an unbiased estimator of \n$\\sigma^2$\n.)\n\n\n\nThe mean square error of \n$a_k S^k$\n as an estimator of \n$\\sigma^2$\n is given by\n\n$$\n\\begin{align}\n(\\operatorname{E} a_k S^k - \\sigma^k)^2 + \\operatorname{E} (a_k S^k)^2 - (\\operatorname{E} a_k S^k)^2\n&= [ (a_k c_k -1)^2 +  a_k^2 c_{2k}  - a_k^2 c_k^2 ] \\sigma^{2k}\\\\\n&= ( a_k^2 c_{2k} -2  a_k c_k + 1 ) \\sigma^{2k}\n\\end{align}\n$$\n\n& therefore minimized when\n\n$$a_k  = \\frac{c_k}{c_{2k}}$$\n\n, allowing the definition of another set of estimators of potential interest:\n\n$$\n\\hat{\\sigma}^k_j= \\left(\\frac{c_j S^j}{c_{2j}}\\right)^\\frac{k}{j}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/251128/402101 \n $$\n\\begin{align}\n(\\operatorname{E} a_k S^k - \\sigma^k)^2 + \\operatorname{E} (a_k S^k)^2 - (\\operatorname{E} a_k S^k)^2\n&= [ (a_k c_k -1)^2 +  a_k^2 c_{2k}  - a_k^2 c_k^2 ] \\sigma^{2k}\\\\\n&= ( a_k^2 c_{2k} -2  a_k c_k + 1 ) \\sigma^{2k}\n\\end{align}\n$$\n\n& therefore minimized when\n\n$$a_k  = \\frac{c_k}{c_{2k}}$$\n\n, allowing the definition of another set of estimators of potential interest:\n\n$$\n\\hat{\\sigma}^k_j= \\left(\\frac{c_j S^j}{c_{2j}}\\right)^\\frac{k}{j}\n$$\n\nCuriously, \n$\\hat{\\sigma}^1_1=c_1S$\n, so the same constant that divides \n$S$\n to remove bias multiplies \n$S$\n to reduce MSE. Anyway, when \n$j=k$\n these are the uniformly minimum-square-error location-invariant & scale-equivariant estimators of \n$\\sigma^k$\n (you don't want your estimate to change at all if you measure in kelvins rather than degrees Celsius, & you want it to change by a factor of \n$\\left(\\frac{9}{5}\\right)^k$\n if you measure in Fahrenheit).\n\nNone of the above has any bearing on the construction of hypothesis tests or confidence intervals (see e.g. \nWhy does this excerpt say that unbiased estimation of standard deviation usually isn't relevant?\n). And \n$\\tilde{\\sigma}^k_j$\n & \n$\\hat{\\sigma}^k_j$\n exhaust neither estimators nor parameter scales of potential interest\u2014consider the maximum-likelihood estimator\n\u2020\n \n$\\sqrt{\\frac{n-1}{n}}S$\n, or the median-unbiased estimator \n$\\sqrt{\\frac{n-1}{\\chi^2_{n-1}(0.5)}}S$\n; or the geometric standard deviation of a lognormal distribution \n$\\mathrm{e}^\\sigma$\n. It may be worth showing a few more-or-less popular estimates made from a small sample (\n$n=2$\n) together with the upper & lower bounds, \n$\\sqrt{\\frac{(n-1)s^2}{\\chi^2_{n-1}(\\alpha)}}$\n & \n$\\sqrt{\\frac{(n-1)s^2}{\\chi^2_{n-1}(1-\\alpha)}}$\n, of the equal-tailed confidence interval having coverage \n$1-\\alpha$\n:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/251128/402101 \n The span between the most divergent estimates is negligible in comparison with the width of any confidence interval having decent coverage. (The 95% C.I., for instance, is \n$(0.45s,31.9s)$\n.) There's no sense in being finicky about the properties of a point estimator unless you're prepared to be fairly explicit about what you want you want to use it for\u2014most explicitly you can define a custom loss function for a particular application. A reason you might prefer an exactly (or almost) unbiased estimator is that you're going to use it in subsequent calculations during which you don't want bias to accumulate: your illustration of averaging biased estimates of standard deviation is a simple example of such (a more complex example might be using them as a response in a linear regression). In principle an all-encompassing model should obviate the need for unbiased estimates as an intermediate step, but might be considerably more tricky to specify & fit.\n\n\u2020 The value of \n$\\sigma$\n that makes the observed data most probable has an appeal as an estimate independent of consideration of its sampling distribution.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/268772/402101 \n Let $x$ by any number.  Consider the event $\\min(X,Y)\\le x$.  It can be expressed as the union of two events\n\n$$\\min(X,Y)\\le x = (X\\le x) \\cup (Y \\le x),$$\n\nshown by the overlapping yellow and green regions in this figure, respectively:\n\n\n\nThe intersection of these events (shown in the bottom left corner where they overlap) obviously is $\\{X\\le x,\\,Y\\le x\\}=\\max(X,Y)\\le x$.  Therefore (by the \nPIE\n),\n\n$$\\Pr\\left(\\min(X,Y)\\le x\\right) = \\Pr(X\\le x) + \\Pr (Y\\le x) - \\Pr\\left(\\max(X,Y)\\le x\\right).$$\n\nAll three probabilities are given directly by $F$\n (answering the main question):\n\n$$\\eqalign{\\Pr\\left(\\min(X,Y)\\le x\\right) &= F_{X,Y}(x,\\infty) + F_{X,Y}(\\infty, x) - F_{X,Y}(x,x)\\\\&= F_X(x) + F_Y(x) - F_{X,Y}(x,x).\\tag{1}}$$\n\nThe use of \"$\\infty$\" as an argument refers to the limit; thus, \ne.g.\n, $F_X(x)=F_{X,Y}(x,\\infty)=\\lim_{y\\to\\infty} F_{X,Y}(x,y).$\n\nThe result can be expressed in terms of the marginal distributions (only) when $X$ and $Y$ are independent, for then $(1)$ becomes\n\n$$\\eqalign{\\Pr\\left(\\min(X,Y)\\le x\\right) &= F_X(x) + F_Y(x) - F_X(x)F_Y(x) \\\\&= 1 - (1-F_X(x))(1-F_Y(x)).\\tag{2}}$$\n\nThe latter expression is recognizable as computing the chance that independent variables $X$ and $Y$ are both \nnot\n less than or equal to $x$, given by $(1-F_X(x))(1-F_Y(x))$: the subtraction from $1$ then gives the complementary chance that at least one of those variables is less than or equal to $x$, which is precisely what $\\min(X,Y)\\le x$ means.  Thus $(1)$ is the natural generalization of $(2)$ to \nall\n bivariate distributions.\n\nAs a final comment, please note that care is needed in the use of \"$\\le$\" and \"$\\lt$\".  They can be interchanged in all the preceding calculations when $F$ is continuous, but otherwise they make a difference.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/490081/402101 \n ...what does the word \"uniform\" mean?\n\nUniform refers to the topology (metric, if you'd like) of uniform convergence.\nThe uniform metric \n$\\|\\cdot\\|_{\\infty}$\n between two functions \n$f, g :X \\rightarrow \\mathbb{R}$\n is\n\n$$\n\\| f-g \\|_{\\infty} = \\sup_{x \\in X} |f(x) - g(x)|,\n$$\n\ni.e. \n$f$\n and \n$g$\n are close when, well, their values are uniformly close across \n$x$\n.\n\nFor a sequence of random functions \n$f_n$\n and a random function \n$f$\n, a uniform LLN is a statement of the\nform \"\n$\\| f_n-f \\|_{\\infty}$\n converges to zero (say) in probability\n\".\nThat is, for \n$n$\n sufficiently large, \n$f_n$\n is uniformly close to \n$f$\n with high probability.\n\nOne example of the Uniform LLN is the Glivenko\u2013Cantelli Theorem, i.e. convergence of empirical CDF's to the population CDF, uniformly in probability. In this case, \n$X$\n is the real line---possible outcomes of a random variable. This in turn leads to the Kolmogorov-Smirnov test. (You can find some related simulation \nin this question\n).\n\nAnother context in which one encounters Uniform LLN is MLE. In this case, \n$X$\n is the parameter space and likelihood functions are random functions defined on \n$X$\n. MLE is consistent because the likelihood functions converge uniformly in probability to a non-random function that is maximized at the true parameter, with curvature at the maximum being Fisher information.\n\n...the difference between uniform laws of large numbers and law of\nlarge numbers?\n\nOne can view LLN as a (very) special case of ULLN, where the functions are constant, but this is perhaps not the best perspective.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/96000/402101 \n A physical, intuitive model of a random variable\n is to write down the name of every member of a population on one or more slips of paper--\"tickets\"--and put those tickets into a box. The process of thoroughly mixing the contents of the box, followed by blindly pulling out one ticket--exactly as in a lottery--models randomness.  Non-uniform probabilities are modeled by introducing variable numbers of tickets in the box: more tickets for the more probable members, fewer for the less probable.\n\nA \nrandom variable\n is a number associated with each member of the population. (Therefore, for consistency, every ticket for a given member has to have the same number written on it.)  Multiple random variables are modeled by reserving spaces on the tickets for more than one number.  We usually give those spaces names like $X,$ $Y,$ and $Z$.  The \nsum\n of those random variables is the usual sum: reserve a new space on every ticket for the sum, read off the values of $X,$ $Y,$ \netc.\n on each ticket, and write their sum in that new space.  This is a consistent way of writing numbers on the tickets, so it's another random variable.\n\n\n\nThis figure portrays a box representing a population $\\Omega=\\{\\alpha,\\beta,\\gamma\\}$ and three random variables $X$, $Y$, and $X+Y$.  It contains six tickets: the three for $\\alpha$ (blue) give it a probability of $3/6$, the two for $\\beta$ (yellow) give it a probability of $2/6$, and the one for $\\gamma$ (green) give it a probability of $1/6$.  In order to display what is written on the tickets, they are shown before being mixed.\n\nThe beauty of this approach\n is that all the paradoxical parts of the question turn out to be correct:\n\nthe sum of random variables is indeed a single, definite number (for each member of the population), \n\n\nyet it also leads to a distribution (given by the frequencies with which the sum appears in the box), and \n\n\nit still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).\n\nthe sum of random variables is indeed a single, definite number (for each member of the population),\n\nyet it also leads to a distribution (given by the frequencies with which the sum appears in the box), and\n\nit still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/96000/402101 \n it still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).\n\nthe sum of random variables is indeed a single, definite number (for each member of the population),\n\nyet it also leads to a distribution (given by the frequencies with which the sum appears in the box), and\n\nit still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).\n\nIn this fashion the sum can simultaneously have a definite value (given by the rules of addition as applied to numbers on each of the tickets) while the \nrealization\n--which will be a ticket drawn from the box--does not have a value until it is carried out.\n\nThis physical model of drawing tickets from a box is adopted in the theoretical literature and made rigorous with the definitions of sample space (the population), sigma algebras (with their associated probability measures), and random variables as measurable functions defined on the sample space.\n\nThis account of random variables is elaborated, with realistic examples, at \n\"What is meant by a random variable?\"\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/103064/402101 \n You can perform selection and logistic regression simultaneously using the \nLASSO\n or \nElastic Net\n regression algorithms. The basic idea behind LASSO is to solve the $l_1$-penalized optimization problem\n$$\\min_{\\beta} \\{ l(\\beta) + \\lambda||\\beta||_1 \\},$$\nwhere $l(\\cdot)$ is the likelihood function. Popular implementations, e.g. \nglmnet\n, efficiently solve for a grid of $\\lambda$ values. This is useful because we usually don't know $\\lambda$ a priori and need to apply some type of cross-validation. If you have correlated features then it helps to add some $l_2$ (ridge) penalty, which is the idea behind the Elastic Net.\n\nSince you don't have a lot of data, I think this is probably your best bet. If you want to use a separate variable selection stage you will need to choose a metric (e.g. deviance of single-variable regression) and also a threshold. The LASSO gives you only one parameter to tune and operates within the context of multivariable logistic regression models directly.\n\nEDIT: The question now specifically requests an approach that is implemented in SPSS. As I don't have/use that software I don't know whether lasso logistic regression is implemented. Perhaps someone can let us know in the comments.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/134706/402101 \n How can a random number converge to a constant?\n\nLet's say you have $N$ balls in the box. You can pick them one by one. After you picked $k$ balls, I ask you: what's the mean weight of the balls in the box? Your best answer would be $\\bar x_k=\\frac{1}{k}\\sum_{i=1}^kx_i$. You realize that $\\bar x_k$ itself is the random value? It depends on which $k$ balls you picked first.\n\nNow, if you keep pulling the balls, at some point there'll be no balls left in the box, and you'll get $\\bar x_N\\equiv\\mu$.\n\nSo, what we've got is the random sequence $$\\bar x_1,\\dots,\\bar x_k, \\dots, \\bar x_N ,\\bar x_N, \\bar x_N, \\dots $$ which converges to the constant $\\bar x_N = \\mu$. So, the key to understanding your issue with convergence in probability is realizing that we're talking about \na sequence of random variables, constructed in a certain way\n.\n\nNext, let's get uniform random numbers $e_1,e_2,\\dots$, where $e_i\\in [0,1]$. Let's look at the random sequence $\\xi_1,\\xi_2,\\dots$, where $\\xi_k=\\frac{1}{\\sqrt{\\frac{k}{12}}}\\sum_{i=1}^k \\left(e_i- \\frac{1}{2} \\right)$. The $\\xi_k$ is a random value, because all its terms are random values. We can't predict what is $\\xi_k$ going to be. However, it turns out that we can claim that the probability distributions of $\\xi_k$ will look more and more like the standard normal $\\mathcal{N}(0,1)$. That's how the distributions converge.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/311990/402101 \n In maximum likelihood estimation, we calculate\n\n$$\\hat \\beta_{ML}:  \\sum \\frac {\\partial \\ln f(\\epsilon_i)}{\\partial \\beta} = \\mathbf 0 \\implies \\sum \\frac {f'(\\epsilon_i)}{f(\\epsilon_i)}\\mathbf x_i = \\mathbf 0$$\n\nthe last relation taking into account the linearity structure of the regression equation.\n\nIn comparison , the OLS estimator satisfies\n\n$$\\sum \\epsilon_i\\mathbf x_i = \\mathbf 0$$\n\nIn order to obtain identical algebraic expressions for the slope coefficients we need to have a density for the error term such that\n\n$$\\frac {f'(\\epsilon_i)}{f(\\epsilon_i)} = \\pm \\;c\\epsilon_i \\implies f'(\\epsilon_i)= \\pm \\;c\\epsilon_if(\\epsilon_i)$$\n\nThese are differential equations of the form $y' = \\pm\\; xy$ that have solutions\n\n$$\\int \\frac 1 {y}dy = \\pm \\int x dx\\implies \\ln y = \\pm\\;\\frac 12 x^2$$\n\n$$ \\implies y = f(\\epsilon) = \\exp\\left \\{\\pm\\;\\frac 12 c\\epsilon^2\\right\\}$$\n\nAny function that has this kernel and integrates to unity over an appropriate domain, will make the MLE and OLS for the slope coefficients identical. Namely we are looking for\n\n$$g(x)= A\\exp\\left \\{\\pm\\;\\frac 12 cx^2\\right\\} : \\int_a^b g(x)dx =1$$\n\nIs there such a $g$ that is not the normal density (or the half-normal or the derivative of the error function)?\n\nCertainly. But one more thing one has to consider is the following: if one uses the plus sign in the exponent, and a symmetric support around zero for example, one will get a density that has a unique \nminimum\n in the middle, and two local maxima at the boundaries of the support.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/188666/402101 \n What happens if the residuals are not homoscedastic? If the residuals show an increasing or decreasing pattern in Residuals vs. Fitted plot.\n\nIf the error term is not homoscedastic (we use the residuals as a proxy for the unobservable error term), the OLS estimator is still consistent and unbiased but is no longer the most efficient in the class of linear estimators. It is the GLS estimator now that enjoys this property.\n\nWhat happens if the residuals are not normally distributed, and fail the Shapiro-Wilk test? Shapiro-Wilk test of normality is a very strict test, and sometimes even if the Normal-QQ plot looks somewhat reasonable, the data fails the test.\n\nNormality is not required by the Gauss-Markov theorem. The OLS estimator is still BLUE but without normality you will have difficulty doing inference, i.e. hypothesis testing and confidence intervals, at least for finite sample sizes. There is still the bootstrap, however.\n\nAsymptotically this is less of a problem since the OLS estimator has a limiting normal distribution under mild regularity conditions.\n\nWhat happens if one or more predictors are not normally distributed, do not look right on the Normal-QQ plot or if the data fails the Shapiro-Wilk test?\n\nAs far as I know the predictors are either considered fixed or the regression is conditional on them. This limits the effect of non-normality.\n\nWhat does failing the normality means for a model that is a good fit according to the R-Squared value. Does it become less reliable, or completely useless?\n\nThe R-squared is the proportion of the variance explained by the model. It does not require the normality assumption and it's a measure of goodness of fit regardless. If you want to use it for a partial F-test though, that is quite another story.\n\nTo what extent, the deviation is acceptable, or is it acceptable at all?\n\nDeviation from normality you mean, right? It really depends on your purposes because as I said, inference becomes hard in the absence of normality but is not impossible (bootstrap!).\n\nWhen applying transformations on the data to meet the normality criteria, does the model gets better if the data is more normal (higher P-value on Shapiro-Wilk test, better looking on normal Q-Q plot), or it is useless (equally good or bad compared to the original) until the data passes normality test?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/188666/402101 \n Deviation from normality you mean, right? It really depends on your purposes because as I said, inference becomes hard in the absence of normality but is not impossible (bootstrap!).\n\nWhen applying transformations on the data to meet the normality criteria, does the model gets better if the data is more normal (higher P-value on Shapiro-Wilk test, better looking on normal Q-Q plot), or it is useless (equally good or bad compared to the original) until the data passes normality test?\n\nIn short, if you have all the Gauss-Markov assumptions \nplus\n normality then the OLS estimator is Best Unbiased (BUE), i.e. the most efficient \nin all\n classes of estimators - the Cramer-Rao Lower Bound is attained. This is desirable of course but it's not the end of world if it does not happen. The above remarks apply.\n\nRegarding transformations, bear in mind that while the distribution of the response might be brought closer to normality, interpretation might not be straightforward afterwards.\n\nThese are just some short answers to your questions. You seem to be particularly concerned with the implications of non-normality. Overall, I would say that it is not as catastrophic as people (have been made to?) believe and there are workarounds. The two references I have included are a good starting point for further reading, the first one being of theoretical nature.\n\nReferences\n:\n\nHayashi, Fumio. : \"Econometrics.\", Princeton University Press, 2000\n\nKutner, Michael H., et al. \"Applied linear statistical models.\", McGraw-Hill Irwin, 2005.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/29624/402101 \n The general rule of thumb (based on stuff in Frank Harrell's book, \nRegression Modeling Strategies\n) is that \nif you expect to be able to detect reasonable-size effects with reasonable power\n, you need 10-20 observations per parameter (covariate) estimated.  Harrell discusses a lot of options for \"dimension reduction\" (getting your number of covariates down to a more reasonable size), such as PCA, but the most important thing is that in order to have any confidence in the results \ndimension reduction must be done without looking at the response variable\n. Doing the regression again with just the significant variables, as you suggest above, is in almost every case a bad idea.\n\nHowever, since you're stuck with a data set and a set of covariates you're interested in, I don't think that running the multiple regression this way is inherently wrong. I think the best thing would be to accept the results as they are, from the full model (don't forget to look at the point estimates and confidence intervals to see whether the significant effects are estimated to be \"large\" in some real-world sense, and whether the non-significant effects are actually estimated to be smaller than the significant effects or not).\n\nAs to whether it makes any sense to do an analysis without the predictor that your field considers important: I don't know. It depends what kind of inferences you want to make based on the model. In the narrow sense, the regression model is still well-defined (\"what are the marginal effects of these predictors on this response?\"), but someone in your field might quite rightly say that the analysis just doesn't make sense.  It would help a little bit if you knew that the predictors you have are uncorrelated from the well-known predictor (whatever it is), or that well-known predictor is constant or nearly constant for your data: then at least you could say that something other than the well-known predictor does have an effect on the response.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/185070/402101 \n If the values lie along a line the distribution has the same shape (up to location and scale) as the theoretical distribution we have supposed.\n\nLocal behaviour\n: When  looking at sorted sample values on the y-axis and (approximate) expected quantiles on the x-axis, we can identify from how the values in some section of the plot differ  locally from an overall linear trend by seeing whether the values are more or less concentrated than the theoretical distribution would suppose in that section of a plot:\n\n\n\nAs we see, less concentrated points increase more and more concentrated points increase less rapidly than an overall linear relation would suggest, and in the extreme cases correspond to a gap in the density of the sample (shows as a near-vertical jump) or a spike of constant values (values aligned horizontally). This allows us to spot a heavy tail or a light tail and hence, skewness greater or smaller than the theoretical distribution, and so on.\n\nOverall apppearance:\n\nHere's what QQ-plots look like (for particular choices of distribution) \non average\n:\n\n\n\nBut randomness tends to obscure things, especially with small samples:\n\n\n\nNote that at \n$n=21$\n the results may be much more variable than shown there - I generated several such sets of six plots and chose a 'nice' set where you could kind of see the shape in all six plots at the same time. Sometimes straight relationships look curved, curved relationships look straight, heavy-tails just look skew, and so on - with such small samples, often the situation may be much less clear:\n\n\n\nIt's possible to discern more features than those (such as discreteness, for one example), but with \n$n=21$\n, even such basic features may be hard to spot; we shouldn't try to 'over-interpret' every little wiggle. As sample sizes become larger, generally speaking the plots 'stabilize' and the features become more clearly interpretable rather than representing noise. [With some very heavy-tailed distributions, the rare large outlier might prevent the picture stabilizing nicely even at quite large sample sizes.]\n\nYou may also find the suggestion \nhere\n useful when trying to decide how much you should worry about a particular amount of curvature or wiggliness.\n\nA more suitable guide for interpretation in general would also include displays at smaller and larger sample sizes.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/46896/402101 \n (Technically, the P-value is the probability of observing data \nat least as extreme\n as that actually observed, given the null hypothesis.)\n\nQ1. A decision to reject the null hypothesis on the basis of a small P-value typically depends on 'Fisher's disjunction': Either a rare event has happened or the null hypothesis is false. In effect, it is rarity of the event is what the P-value tells you rather than the probability that the null is false.\n\nThe probability that the null is false can be obtained from the experimental data only by way of Bayes' theorem, which requires specification of the 'prior' probability of the null hypothesis (presumably what Gill is referring to as \"marginal distributions\").\n\nQ2. This part of your question is much harder than it might seem. There is a great deal of confusion regarding P-values and error rates which is, presumably, what Gill is referring to with \"but is typically treated as such.\" The combination of Fisherian P-values with Neyman-Pearsonian error rates has been called an incoherent mishmash, and it is unfortunately very widespread. No short answer is going to be completely adequate here, but I can point you to a couple of good papers (yes, one is mine). Both will help you make sense of the Gill paper.\n\nHurlbert, S., & Lombardi, C. (2009). Final collapse of the Neyman-Pearson decision theoretic framework and rise of the neoFisherian. Annales Zoologici Fennici, 46(5), 311\u2013349. \n(Link to paper)\n\nLew, M. J. (2012). Bad statistical practice in pharmacology (and other basic biomedical disciplines): you probably don't know P. British Journal of Pharmacology, 166(5), 1559\u20131567. doi:10.1111/j.1476-5381.2012.01931.x \n(Link to paper)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45143/402101 \n If you work in terms of indicator variables (i.e. \n$Z_i = 1$\n if \n$X_i \\leq x$\n and \n$0$\n otherwise), you can directly apply the Central limit theorem to a mean of \n$Z$\n's, and by using the \nDelta method\n, turn that into an asymptotic normal distribution for \n$F_X^{-1}(\\bar{Z})$\n, which in turn means that you get asymptotic normality for fixed quantiles of \n$X$\n.\n\nSo not just the median, but quartiles, 90th percentiles, ... etc.\n\nLoosely, if we're talking about the \n$q$\nth  sample quantile in sufficiently large samples, we get that it will approximately have a normal distribution with mean the \n$q$\nth population quantile \n$x_q$\n and variance \n$q(1-q)/(nf_X(x_q)^2)$\n.\n\nHence for the median (\n$q = 1/2$\n), the variance in sufficiently large samples will be approximately \n$1/(4nf_X(\\tilde{\\mu})^2)$\n.\n\nYou need all the conditions along the way to hold, of course, so it doesn't work in all situations, but for continuous distributions where the density at the population quantile is positive and differentiable, etc, ...\n\nFurther, it doesn't hold for extreme quantiles, because the CLT doesn't kick in there (the average of Z's won't be asymptotically normal). You need different theory for extreme values.\n\nEdit: whuber's critique is correct; this would work if \n$x$\n were a population median rather than a sample median. The argument needs to be modified to actually work properly.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4221/402101 \n That Wiki page is abusing language by referring to this number as a probability.  You are correct that it is not.  It is actually a \nprobability per foot\n.  Specifically, the value of 1.5789 (for a height of 6 feet) implies that the probability of a height between, say, 5.99 and 6.01 feet is close to the following unitless value:\n\n$$1.5789\\, [1/\\text{foot}] \\times (6.01 - 5.99)\\, [\\text{feet}] = 0.0316$$\n\nThis\n value \nmust\n not exceed 1, as you know.  (The small range of heights (0.02 in this example) is a crucial part of the probability apparatus.  It is the \"differential\" of height, which I will abbreviate $d(\\text{height})$.)  Probabilities per unit of something are called \ndensities\n by analogy to other densities, like mass per unit volume.\n\nBona fide\n probability \ndensities\n can have arbitrarily large values, even infinite ones.\n\n\n\nThis example shows the probability density function for a Gamma distribution (with shape parameter of $3/2$ and scale of $1/5$). Because most of the density is less than $1$, the curve has to rise higher than $1$ in order to have a total area of $1$ as required for all probability distributions.\n\n\n\nThis density (for a beta distribution with parameters $1/2, 1/10$) becomes infinite at $0$ and at $1$.  The total area still is finite (and equals $1$)!\n\nThe value of 1.5789 /foot is obtained in that example by estimating that the heights of males have a normal distribution with mean 5.855 feet and variance 3.50e-2 square feet.  (This can be found in a previous table.) The square root of that variance is the standard deviation, 0.18717 feet.  We re-express 6 feet as the number of SDs from the mean:\n\n$$z = (6 - 5.855) / 0.18717 = 0.7747$$\n\nThe division by the standard deviation produces a relation\n\n$$dz = d(\\text{height})/0.18717$$\n\nThe Normal probability density, by definition, equals\n\n$$\\frac{1}{\\sqrt{2 \\pi}}\\exp(-z^2/2)dz = 0.29544\\ d(\\text{height}) / 0.18717 = 1.5789\\  d(\\text{height}).$$\n\n(Actually, I cheated: I simply asked Excel to compute NORMDIST(6, 5.855, 0.18717, FALSE).  But then I really did check it against the formula, just to be sure.)  When we strip the \nessential\n differential $d(\\text{height})$ from the formula only the number $1.5789$ remains, like the Cheshire Cat's smile.  We, the readers, need to understand that the number has to be multiplied by a small difference in heights in order to produce a probability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197364/402101 \n In an unpenalized regression, you can often get a ridge* in parameter space, where many different values along the ridge all do as well or nearly as well on the least squares criterion.\n\n*  (at least, it's a ridge in the \nlikelihood function\n -- they're actually \nvalleys\n$ in the RSS criterion, but I'll continue to call it a ridge, as this seems to be conventional -- or even, as Alexis points out in comments, I could call that a \nthalweg\n, being the valley's counterpart of a ridge)\n\nIn the presence of a ridge in the least squares criterion in parameter space, the penalty you get with ridge regression gets rid of those ridges by pushing the criterion up as the parameters head away from the origin:\n\n[\nClearer image\n]\n\nIn the first plot, a large change in parameter values (along the ridge) produces a miniscule change in the RSS criterion. This can cause numerical instability; it's very sensitive to small changes (e.g. a tiny change in a data value, even truncation or rounding error). The parameter estimates are almost perfectly correlated. You may get parameter estimates that are very large in magnitude.\n\nBy contrast, by lifting up the thing that ridge regression minimizes (by adding the $L_2$ penalty) when the parameters are far from 0, small changes in conditions (such as a little rounding or truncation error) can't produce gigantic changes in the resulting estimates. The penalty term results in shrinkage toward 0 (resulting in some bias). A small amount of bias can buy a substantial improvement in the variance (by eliminating that ridge).\n\nThe uncertainty of the estimates are reduced (the standard errors are inversely related to the second derivative, which is made larger by the penalty).\n\nCorrelation in parameter estimates is reduced. You now won't get parameter estimates that are very large in magnitude if the RSS for small parameters would not be much worse.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/69582/402101 \n In an a sample $x$ of $n$ independent values from a distribution $F$ with pdf $f$, the pdf of the joint distribution of the extremes $\\min(x)=x_{[1]}$ and $\\max(x)=x_{[n]}$ is proportional to\n\n$$f(x_{[1]})\\left(F(x_{[n]})-F(x_{[1]})\\right)^{n-2}f(x_{[n]})dx_{[1]}dx_{[n]} = H_F(x_{[1]}, x_{[n]})dx_{[1]}dx_{[n]}.$$\n\n(The constant of proportionality is the reciprocal of the multinomial coefficient $\\binom{n}{1,n-2,1} = n(n-1)$.  Intuitively, this joint PDF expresses the chance of finding the smallest value in the range $[x_{[1]},x_{[1]}+dx_{[1]})$, the largest value in the range $[x_{[n]},x_{[n]}+dx_{[n]})$, and the middle $n-2$ values between them within the range $[x_{[1]}+dx_{[1]}, x_{[n]})$.  When $F$ is continuous, we may replace that middle range by $(x_{[1]}, x_{[n]}]$, thereby neglecting only an \"infinitesimal\" amount of probability. The associated probabilities, to first order in the differentials, are $f(x_{[1]})dx_{[1]},$  $f(x_{[n]})dx_{[n]},$ and $F(x_{[n]})-F(x_{[1]}),$respectively, now making it obvious where the formula comes from.)\n\nTaking the expectation of the range $x_{[n]} - x_{[1]}$ gives $2.53441\\ \\sigma$ for any Normal distribution with standard deviation $\\sigma$ and $n=6$. The expected range as a multiple of $\\sigma$ depends on the sample size $n$:\n\n\n\nThese values were computed by numerically integrating $\\binom{n}{1,n-2,1}\\left(y-x\\right)H_F(x,y)dxdy$ over $\\{(x,y)\\in\\mathbb{R}^2|x\\le y\\}$, with $F$ set to the standard Normal CDF, and dividing by the standard deviation of $F$ (which is just $1$).\n\nA similar multiplicative relationship between the expected range and the standard deviation will hold for any location-scale family of distributions, because it is a property of the \nshape\n of the distribution alone.  For instance, here is a comparable plot for uniform distributions:\n\n\n\nand exponential distributions:\n\n\n\nThe values in the preceding two plots were obtained by exact--not numerical--integration, which is possible due to the relatively simple algebraic forms of $f$ and $F$ in each case.  For the uniform distributions they equal $\\frac{n-1}{(n+1)}\\sqrt{12}$ and for the exponential distributions they are $\\gamma + \\psi(n) = \\gamma + \\frac{\\Gamma'(n)}{\\Gamma(n)}$ where $\\gamma$ is Euler's constant and $\\psi$ is the \"polygamma\" function, the logarithmic derivative of Euler's Gamma function.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/69582/402101 \n The values in the preceding two plots were obtained by exact--not numerical--integration, which is possible due to the relatively simple algebraic forms of $f$ and $F$ in each case.  For the uniform distributions they equal $\\frac{n-1}{(n+1)}\\sqrt{12}$ and for the exponential distributions they are $\\gamma + \\psi(n) = \\gamma + \\frac{\\Gamma'(n)}{\\Gamma(n)}$ where $\\gamma$ is Euler's constant and $\\psi$ is the \"polygamma\" function, the logarithmic derivative of Euler's Gamma function.\n\nAlthough they differ (because these distributions display a wide range of shapes), the three roughly agree around $n=6$, showing that the multiplier $2.5$ does not depend heavily on the shape and therefore can serve as an omnibus, robust assessment of the standard deviation when ranges of small subsamples are known.  (Indeed, the very heavy-tailed Student $t$ distribution with three degrees of freedom still has a multiplier around $2.3$ for $n=6$, not far at all from $2.5$.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/493559/402101 \n No, those equations come directly from the mean and variance formulae in terms of expected value, considering the collected data as a population.\n\n$$\\mu = \\mathbb{E}\\big[X\\big]$$\n\n$$\\sigma^2 = \\mathbb{E}\\big[\\big(X-\\mu\\big)^2\\big]$$\n\nSince you have a finite number of observations, the distribution is discrete,\n$^{\\dagger}$\n and the expected value is a sum.\n\n$$\\mu = \\mathbb{E}\\big[X\\big] = \\sum_{i=1}^N p(x_i)x_i = \\sum_{i=1}^N \\dfrac{1}{N}x_i = \\dfrac{1}{N}\\sum_{i=1}^Nx_i$$\n\n$$\\sigma^2 = \\mathbb{E}\\big[\\big(X-\\mu\\big)^2\\big] = \\sum_{i=1}^N p(x_i)(x_i - \\mu)^2 = \\sum_{i=1}^N \\dfrac{1}{N}(x_i - \\mu)^2 = \\dfrac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2$$\n\n(To get from \n$p(x_i)$\n to \n$\\dfrac{1}{N}$\n, note that each individual \n$x_i$\n has probability \n$1/N$\n.)\n\nThis is why the \n$\\dfrac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2$\n gets called the \"population\" variance. It literally is the population variance if you consider the observed data to be the population.\n\n$^{\\dagger}$\n This is a sufficient, but not necessary, condition for a discrete distribution. A Poisson distribution is an example of a discrete distribution with infinitely many values.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163935/402101 \n Rarely if ever a parametric test and a non-parametric test actually have the same null. The parametric $t$-test is testing the mean of the distribution, assuming the first two moments exist. The Wilcoxon rank sum test does not assume any moments, and tests equality of distributions instead. Its implied parameter is a weird functional of distributions, the probability that the observation from one sample is lower than the observation from the other. You can sort of talk about comparisons between the two tests under the completely specified null of identical distributions... but you have to recognize that the two tests are testing different hypotheses.\n\nThe information that parametric tests bring in along with their assumption helps improving the power of the tests. Of course that information better be right, but there are few if any domains of human knowledge these days where such preliminary information does not exist. An interesting exception that explicitly says \"I don't want to assume anything\" is the courtroom where non-parametric methods continue to be widely popular -- and it makes perfect sense for the application. There's probably a good reason, pun intended, that Phillip Good authored good books on both \nnon-parametric statistics\n and \ncourtroom statistics\n.\n\nThere are also testing situations where you don't have access to the microdata necessary for the nonparametric test. Suppose you were asked to compare two groups of people to gauge whether one is more obese than the other. In an ideal world, you will have height and weight measurements for everybody, and you could form a permutation test stratifying by height. In a less than ideal (i.e., real) world, you may only have the mean height and mean weight in each group (or may be some ranges or variances of these characteristics on top of the sample means). Your best bet is then to compute the mean BMI for each group and compare them if you only have the means; or assume a bivariate normal for height and weight if you have means and variances (you'd probably have to take a correlation from some external data if it did not come with your samples), form some sort of regression lines of weight on height within each group, and check whether one line is above the other.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/355042/402101 \n You should think of the algorithm as producing draws from a random variable, to show that the algorithm works, it suffices to show that the algorithm draws from the random variable you want it to.\n\nLet $X$ and $Y$ be scalar random variables with pdfs $f_X$ and $f_Y$ respectively, where $Y$ is something we already know how to sample from. We can also know that we can bound $f_X$ by $Mf_Y$ where $M\\ge1$.\n\nWe now form a new random variable $A$ where $A | y \\sim \\text{Bernoulli } \\left (\\frac{f_X(y)}{Mf_Y(y)}\\right )$, this takes the value $1$ with probability $\\frac{f_X(y)}{Mf_Y(y)} $ and $0$ otherwise. This represents the algorithm 'accepting' a draw from $Y$.\n\nNow we run the algorithm and collect all the draws from $Y$ that are accepted, lets call this random variable $Z = Y|A=1$.\n\nTo show that $Z \\equiv X$, for any event $E$, we must show that $P(Z \\in E) =P(X \\in E)$.\n\nSo let's try that, first use Bayes' rule:\n\n$P(Z \\in E) = P(Y \\in E | A =1) = \\frac{P(Y \\in E \\& A=1)}{P(A=1)}$,\n\nand the top part we write as\n\n\\begin{align*}P(Y \\in E \\& A=1) &= \\int_E f_{Y, A}(y,1) \\, dy \\\\ &= \\int_E f_{A|Y}(1,y)f_Y(y) \\, dy =\\int_E f_Y(y) \\frac{f_X(y)}{Mf_Y(y)} \\, dy  =\\frac{P(X \\in E)}{M}.\\end{align*}\n\nAnd then the bottom part is simply\n\n$P(A=1) = \\int_{-\\infty}^{\\infty}f_{Y,A}(y,1) \\, dy = \\frac{1}{M}$,\n\nby the same reasoning as above, setting $E=(-\\infty, +\\infty)$.\n\nAnd these combine to give $P(X \\in E)$, which is what we wanted, $Z \\equiv X$.\n\nThat is how the algorithm works, but at the end of your question you seem to be concerned about a more general idea, that is when does an empirical distribution converge to the distribution sampled from? This is a general phenomenon concerning any sampling whatsoever if I understand you correctly.\n\nIn this case, let $X_1, \\dots, X_n$ be iid random variables all with distribution $\\equiv X$. Then for any event $E$, $\\frac{\\sum_{i=1}^n1_{X_i \\in E}}{n}$ has expectation $P(X \\in E)$ by the linearity of expectation.\n\nFurthermore, given suitable assumptions you could use the \nstrong law of large numbers\n to show that the empirical probability converges almost surely to the true probability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18189/402101 \n Sometimes we can \"augment knowledge\" with an unusual or different approach.  I would like this reply to be accessible to kindergartners and also have some fun, so \neverybody get out your crayons!\n\nGiven paired \n$(x,y)$\n data, draw their scatterplot.  (The younger students may need a teacher to produce this for them. :-)  Each pair of points \n$(x_i,y_i)$\n, \n$(x_j,y_j)$\n in that plot determines a rectangle: it's the smallest rectangle, whose sides are parallel to the axes, containing those points.  Thus the points are either at the upper right and lower left corners (a \"positive\" relationship) or they are at the upper left and lower right corners (a \"negative\" relationship).\n\nDraw all possible such rectangles.\n Color them transparently, making the positive rectangles red (say) and the negative rectangles \"anti-red\" (blue).  In this fashion, wherever rectangles overlap, their colors are either enhanced when they are the same (blue and blue or red and red) or cancel out when they are different.\n\n\n\n(\nIn this illustration of a positive (red) and negative (blue) rectangle, the overlap ought to be white; unfortunately, this software does not have a true \"anti-red\" color.   The overlap is gray, so it will darken the plot, but on the whole the\n net \namount of red is correct.\n)\n\nNow we're ready for the explanation of covariance.\n\nThe covariance is the net amount of red in the plot\n (treating blue as negative values).\n\nHere are some examples with 32 binormal points drawn from distributions with the given covariances, ordered from most negative (bluest) to most positive (reddest).\n\n\n\nThey are drawn on common axes to make them comparable.  The rectangles are lightly outlined to help you see them.  This is an updated (2019) version of the original: it uses software that properly cancels the red and cyan colors in overlapping rectangles.\n\nLet's deduce some properties of covariance.\n  Understanding of these properties will be accessible to anyone who has actually drawn a few of the rectangles. :-)\n\nBilinearity.\n Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.\n\n\n\n\nCorrelation.\n Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18189/402101 \n Bilinearity.\n Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.\n\n\n\n\nCorrelation.\n Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.\n\n\n\n\nRelationship to linear associations.\n Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances.  Linear associations can be fully interpreted by means of the preceding two characterizations.\n\n\n\n\nSensitivity to outliers.\n A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points.  It alone can create a net positive or negative amount of red in the overall picture.\n\nBilinearity.\n Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.\n\nCorrelation.\n Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.\n\nRelationship to linear associations.\n Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances.  Linear associations can be fully interpreted by means of the preceding two characterizations.\n\nSensitivity to outliers.\n A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points.  It alone can create a net positive or negative amount of red in the overall picture.\n\nIncidentally, this definition of covariance differs from the usual one only by a constant of proportionality.  The mathematically inclined will have no trouble performing the algebraic demonstration that the formula given here is always twice the usual covariance.  For a full explanation, see the follow-up thread at \nhttps://stats.stackexchange.com/a/222091/919\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2251/402101 \n This is a broad question, but given the Box, Hunter and Hunter quote is true I think what it comes down to is\n\nThe quality of the experimental design:\n\n\n\n\nrandomization, sample sizes, control of confounders,...\n\n\n\n\nThe quality of the implementation of the design:\n\n\n\n\nadherance to protocol, measurement error, data handling, ...\n\n\n\n\nThe quality of the model to accurately reflect the design:\n\n\n\n\nblocking structures are accurately represented, proper degrees of freedom are associated with effects, estimators are unbiased, ...\n\nThe quality of the experimental design:\n\nrandomization, sample sizes, control of confounders,...\n\nThe quality of the implementation of the design:\n\nadherance to protocol, measurement error, data handling, ...\n\nThe quality of the model to accurately reflect the design:\n\nblocking structures are accurately represented, proper degrees of freedom are associated with effects, estimators are unbiased, ...\n\nAt the risk of stating the obvious I'll try to hit on the key points of each:\n\nis a large sub-field of statistics, but in it's most basic form I think it comes down to the fact that when making causal inference we ideally start with identical units that are monitored in identical environments other than being assigned to a treatment.  Any systematic differences between groups after assigment are then logically attributable to the treatment (we can infer cause).  But, the world isn't that nice and units differ prior to treatment and evironments during experiments are not perfectly controlled.  So we \"control what we can and randomize what we can't\", which helps to insure that there won't be systematic bias due to the confounders that we controlled or randomized.  One problem is that experiments tend to be difficult (to impossible) and expensive and a large variety of designs have been developed to efficiently extract as much information as possible in as carefully controlled a setting as possible, given the costs.  Some of these are quite rigorous (e.g. in medicine the double-blind, randomized, placebo-controlled trial) and others less so (e.g. various forms of 'quasi-experiments').",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2251/402101 \n is also a big issue and one that statisticians generally don't think about...though we should.  In applied statistical work I can recall incidences where 'effects' found in the data were spurious results of inconsistency of data collection or handling.  I also wonder how often information on true causal effects of interest is lost due to these issues (I believe students in the applied sciences generally have little-to-no training about ways that data can become corrupted - but I'm getting off topic here...)\n\n\nis another large technical subject, and another necessary step in objective causal inference.  To a certain degree this is taken care of because the design crowd develop designs and models together (since inference from a model is the goal, the attributes of the estimators drive design). But this only gets us so far because in the 'real world' we end up analysing experimental data from non-textbook designs and then we have to think hard about things like the appropriate controls and how they should enter the model and what associated degrees of freedom should be and whether assumptions are met if if not how to adjust of violations and how robust the estimators are to any remaining violations and...\n\nis a large sub-field of statistics, but in it's most basic form I think it comes down to the fact that when making causal inference we ideally start with identical units that are monitored in identical environments other than being assigned to a treatment.  Any systematic differences between groups after assigment are then logically attributable to the treatment (we can infer cause).  But, the world isn't that nice and units differ prior to treatment and evironments during experiments are not perfectly controlled.  So we \"control what we can and randomize what we can't\", which helps to insure that there won't be systematic bias due to the confounders that we controlled or randomized.  One problem is that experiments tend to be difficult (to impossible) and expensive and a large variety of designs have been developed to efficiently extract as much information as possible in as carefully controlled a setting as possible, given the costs.  Some of these are quite rigorous (e.g. in medicine the double-blind, randomized, placebo-controlled trial) and others less so (e.g. various forms of 'quasi-experiments').",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2251/402101 \n is also a big issue and one that statisticians generally don't think about...though we should.  In applied statistical work I can recall incidences where 'effects' found in the data were spurious results of inconsistency of data collection or handling.  I also wonder how often information on true causal effects of interest is lost due to these issues (I believe students in the applied sciences generally have little-to-no training about ways that data can become corrupted - but I'm getting off topic here...)\n\nis another large technical subject, and another necessary step in objective causal inference.  To a certain degree this is taken care of because the design crowd develop designs and models together (since inference from a model is the goal, the attributes of the estimators drive design). But this only gets us so far because in the 'real world' we end up analysing experimental data from non-textbook designs and then we have to think hard about things like the appropriate controls and how they should enter the model and what associated degrees of freedom should be and whether assumptions are met if if not how to adjust of violations and how robust the estimators are to any remaining violations and...\n\nAnyway, hopefully some of the above helps in thinking about considerations in making causal inference from a model.  Did I forget anything big?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/13113/402101 \n The two approaches do differ.\n\nLet the estimated standard errors of the two regressions be $s_1$ and $s_2$.  Then, because the combined regression (with all coefficient-dummy interactions) fits the same coefficients, it has the same residuals, whence its standard error can be computed as\n\n$$s = \\sqrt{\\frac{(n_1-p) s_1^2 + (n_2-p) s_2^2)}{n_1 + n_2 - 2 p}}.$$\n\nThe number of parameters $p$ equals $6$ in the example: five slopes and an intercept in each regression.\n\nLet $b_1$ estimate a parameter in one regression, $b_2$ estimate the same parameter in the other regression, and $b$ estimate their \ndifference\n in the combined regression.  Then their standard errors are related by\n\n$$SE(b) =  s \\sqrt{(SE(b_1)/s_1)^2 + (SE(b_2)/s_2)^2}.$$\n\nIf you haven't done the combined regression, but only have statistics for the separate regressions, plug in the preceding equation for $s$.  This will be the denominator for the t-test.  Evidently it is not the same as the denominator presented in the question.\n\nThe assumption made by the combined regression is that the variances of the residuals are essentially the same in both separate regressions.  If this is not the case, however, the z-test isn't going to be good, either (unless the sample sizes are large): you would want to use a \nCABF test\n or \nWelch-Satterthwaite t-test.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/109840/402101 \n From \nSklar's Theorem\n, it follows that you can construct the joint distribution using a copula:\n\n$$H(x,y) = C(F(x),G(y)).$$\n\nSo, you need two ingredients: the marginal distributions $(F,G)$, and the copula $C$. You mentioned that you know the marginals, so this ingredient is done. Now, you need information to construct the copula. So, if you cannot come up with enough information to select/estimate/guess/divine the copula, then you cannot construct the joint distribution.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/84085/402101 \n To add to @Peter Flom's answer, it is worth defining the other terms that were used:\n\nDeductive reasoning:\n  Derive conclusions or predictions about specific cases from fundamental rules or theories.\n\nInductive reasoning:\n  Derive universal rules or theories from observation of many cases.\n\nInferential statistics use both inductive and deductive reasoning.  You are trying to establish rules about the behaviour of a system based on evidence, but you are testing models against probability theories derived deductively (i.e., probability distributions in parametric models or the combinatorics that are the basis of non-parametric models).\n\nDescriptive statistics don't really qualify as \"reasoning\" in my book.  Saying the average of something is \nx\n and the standard deviation is \ns\n isn't any more of an argument than saying the colour of something is blue.  You're describing what you have in front of you, not drawing any conclusions beyond it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2363/402101 \n Very quickly, I would say: 'multiple' applies to the number of predictors that enter the model (or equivalently the design matrix) with a single outcome (Y response), while 'multivariate' refers to a matrix of response vectors. Cannot remember the author who starts its introductory section on multivariate modeling with that consideration, but I think it is Brian Everitt in his textbook \nAn R and S-Plus Companion to Multivariate Analysis\n. For a thorough discussion about this, I would suggest to look at his latest book, \nMultivariable Modeling and Multivariate Analysis for the Behavioral Sciences\n.\n\nFor 'variate', I would say this is a common way to refer to any random variable that follows a known or hypothesized distribution, e.g. we speak of gaussian variates $X_i$ as a series of observations drawn from a normal distribution (with parameters $\\mu$ and $\\sigma^2$). In probabilistic terms, we said that these are some random \nrealizations\n of X, with mathematical expectation $\\mu$, and about 95% of them are expected to lie on the range $[\\mu-2\\sigma;\\mu+2\\sigma]$ .",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2360/402101 \n Very quickly, I would say: 'multiple' applies to the number of predictors that enter the model (or equivalently the design matrix) with a single outcome (Y response), while 'multivariate' refers to a matrix of response vectors. Cannot remember the author who starts its introductory section on multivariate modeling with that consideration, but I think it is Brian Everitt in his textbook \nAn R and S-Plus Companion to Multivariate Analysis\n. For a thorough discussion about this, I would suggest to look at his latest book, \nMultivariable Modeling and Multivariate Analysis for the Behavioral Sciences\n.\n\nFor 'variate', I would say this is a common way to refer to any random variable that follows a known or hypothesized distribution, e.g. we speak of gaussian variates $X_i$ as a series of observations drawn from a normal distribution (with parameters $\\mu$ and $\\sigma^2$). In probabilistic terms, we said that these are some random \nrealizations\n of X, with mathematical expectation $\\mu$, and about 95% of them are expected to lie on the range $[\\mu-2\\sigma;\\mu+2\\sigma]$ .",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1830/402101 \n Consider the following situation:\n\nI want to catch the subway to go to my office. My plan is to take my car, park at the subway and then take the train to go to my office. My goal is to catch the train at 8.15 am every day so that I can reach my office on time. I need to decide the following: (a) the time at which I need to leave from my home and (b) the route I will take to drive to the station.\n\nIn the above example, I have two parameters (i.e., time of departure from home and route to take to the station) and I need to choose these parameters such that I reach the station by 8.15 am.\n\nIn order to solve the above problem I may try out different sets of 'parameters' (i.e., different combination of times of departure and route) on Mondays, Wednesdays, and Fridays, to see which combination is the 'best' one. The idea is that once I have identified the best combination I can use it every day so that I achieve my objective.\n\nProblem of Overfitting\n\nThe problem with the above approach is that I may overfit which essentially means that the best combination I identify may in some sense may be unique to Mon, Wed and Fridays and that combination may not work for Tue and Thu. Overfitting may happen if in my search for the best combination of times and routes I exploit some aspect of the traffic situation on Mon/Wed/Fri which does not occur on Tue and Thu.\n\nOne Solution to Overfitting: Cross-Validation\n\nCross-validation is one solution to overfitting. The idea is that once we have identified our best combination of parameters (in our case time and route) we test the performance of that set of parameters in a different context. Therefore, we may want to test on Tue and Thu as well to ensure that our choices work for those days as well.\n\nExtending the analogy to statistics\n\nIn statistics, we have a similar issue. We often use a limited set of data to estimate the unknown parameters we do not know. If we overfit then our parameter estimates will work very well for the existing data but not as well for when we use them in another context. Thus, cross-validation helps in avoiding the above issue of overfitting by proving us some reassurance that the parameter estimates are not unique to the data we used to estimate them.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1830/402101 \n Extending the analogy to statistics\n\nIn statistics, we have a similar issue. We often use a limited set of data to estimate the unknown parameters we do not know. If we overfit then our parameter estimates will work very well for the existing data but not as well for when we use them in another context. Thus, cross-validation helps in avoiding the above issue of overfitting by proving us some reassurance that the parameter estimates are not unique to the data we used to estimate them.\n\nOf course, cross validation is not perfect. Going back to our example of the subway, it can happen that even after cross-validation, our best choice of parameters may not work one month down the line because of various issues (e.g., construction, traffic volume changes over time etc).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1876/402101 \n You really need to figure out what is the question that you are trying to answer- or what question is management most interested in. Then you can select the survey questions that are most relevant to your problem.\n\nWithout knowing anything about your problem or dataset, here are some generic solutions:\n\nVisually represent the answers as clusters. My favorite is by either using dendrograms or just plotting on an xy axis (Google \"cluster analysis r\" and go to the first result by statmethods.net)\n\n\nRank the questions from greatest to least \"daily or more frequently\" responses. This is an example that may not exactly work for you but perhaps it will inspire you \nhttp://www.programmingr.com/content/building-scoring-and-ranking-systems-r\n\n\nCrosstabs: if for example, you have a question \"How often do you come in late for work?\" and \"How often do you use Facebook?,\" by crosstabbing the two questions you can find out the percentage of people who rarely do both, or who do both everyday.(Google \"r frequency crosstabs\" or go to the aforementioned statmethods.net)\n\n\nCorrelograms. I don't have any experience with these but I saw it also on the statmethods.net website. Basically you find which questions have the highest correlation and then create a table. You may find this useful although it looks kind of \"busy.\"",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/311958/402101 \n In maximum likelihood estimation, we calculate\n\n$$\\hat \\beta_{ML}:  \\sum \\frac {\\partial \\ln f(\\epsilon_i)}{\\partial \\beta} = \\mathbf 0 \\implies \\sum \\frac {f'(\\epsilon_i)}{f(\\epsilon_i)}\\mathbf x_i = \\mathbf 0$$\n\nthe last relation taking into account the linearity structure of the regression equation.\n\nIn comparison , the OLS estimator satisfies\n\n$$\\sum \\epsilon_i\\mathbf x_i = \\mathbf 0$$\n\nIn order to obtain identical algebraic expressions for the slope coefficients we need to have a density for the error term such that\n\n$$\\frac {f'(\\epsilon_i)}{f(\\epsilon_i)} = \\pm \\;c\\epsilon_i \\implies f'(\\epsilon_i)= \\pm \\;c\\epsilon_if(\\epsilon_i)$$\n\nThese are differential equations of the form $y' = \\pm\\; xy$ that have solutions\n\n$$\\int \\frac 1 {y}dy = \\pm \\int x dx\\implies \\ln y = \\pm\\;\\frac 12 x^2$$\n\n$$ \\implies y = f(\\epsilon) = \\exp\\left \\{\\pm\\;\\frac 12 c\\epsilon^2\\right\\}$$\n\nAny function that has this kernel and integrates to unity over an appropriate domain, will make the MLE and OLS for the slope coefficients identical. Namely we are looking for\n\n$$g(x)= A\\exp\\left \\{\\pm\\;\\frac 12 cx^2\\right\\} : \\int_a^b g(x)dx =1$$\n\nIs there such a $g$ that is not the normal density (or the half-normal or the derivative of the error function)?\n\nCertainly. But one more thing one has to consider is the following: if one uses the plus sign in the exponent, and a symmetric support around zero for example, one will get a density that has a unique \nminimum\n in the middle, and two local maxima at the boundaries of the support.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/50788/402101 \n For OLS, you can imagine that you're using the estimated variance of the residuals (under the assumption of independence and homoscedasticity) as an estimate for the conditional variance of the $Y_i$s. In the sandwich based estimator, you're using the observed squared residuals as a plug-in estimate of the same variance which can vary between observations.\n\n\\begin{equation}\n\\mbox{var}\\left(\\hat{\\beta}\\right) = \\left(X^TX\\right)^{-1}\\left(X^T\\mbox{diag}\\left(\\mbox{var}\\left(Y|X\\right)\\right)X\\right)\\left(X^TX\\right)^{-1}\n\\end{equation}\n\nIn the ordinary least squares standard error estimate for the regression coefficient estimate, the conditional variance of the outcome is treated as constant and independent, so that it can be estimated consistently.\n\n\\begin{equation}\n\\widehat{\\mbox{var}}_{OLS}\\left(\\hat{\\beta}\\right) = \\left(X^TX\\right)^{-1}\\left(r^2X^TX\\right)\\left(X^TX\\right)^{-1}\n\\end{equation}\n\nFor the sandwich, we eschew consistent estimation of the conditional variance and instead use a plug-in estimate of the variance of each component using the squared residual\n\n\\begin{equation}\n\\widehat{\\mbox{var}}_{RSE}\\left(\\hat{\\beta}\\right) = \\left(X^TX\\right)^{-1}\\left(X^T\\mbox{diag}\\left(r_i^2\\right)X\\right)\\left(X^TX\\right)^{-1}\n\\end{equation}\n\nBy using the plug-in variance estimate, we get consistent estimates of the variance of $\\hat{\\beta}$ by the Lyapunov Central Limit Theorem.\n\nIntuitively, these observed squared residuals will mop up any unexplained error due to heteroscedasticity that would have otherwise been unexpected under the assumption of constant variance.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2630/402101 \n There may be varying opinions on this, but I would treat the population data as a sample and assume a hypothetical population, then make inferences in the usual way.  One way to think about this is that there is an underlying data generating process responsible for the collected data, the \"population\" distribution.\n\nIn your particular case, this might make even more sense since you will have cohorts in the future.  Then your population is really cohorts who take the test even in the future.  In this way, you could account for time based variations if you have data for more than a year, or try to account for latent factors through your error model.  In short, you can develop richer models with greater explanatory power.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/207/402101 \n First, we need to understand what is a Markov chain. Consider the following \nweather\n example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:\n\n$P(\\text{Next day is Sunny}\\,\\vert \\,\\text{Given today is Rainy)}=0.50$\n\nSince, the next day's weather is either sunny or rainy it follows that:\n\n$P(\\text{Next day is Rainy}\\,\\vert \\,\\text{Given today is Rainy)}=0.50$\n\nSimilarly, let:\n\n$P(\\text{Next day is Rainy}\\,\\vert \\,\\text{Given today is Sunny)}=0.10$\n\nTherefore, it follows that:\n\n$P(\\text{Next day is Sunny}\\,\\vert \\,\\text{Given today is Sunny)}=0.90$\n\nThe above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:\n\n$P = \\begin{bmatrix}\n& S & R \\\\\nS& 0.9 & 0.1 \\\\\nR& 0.5 & 0.5\n\\end{bmatrix}$\n\nWe might ask several questions whose answers follow:\n\nQ1:\n If the weather is sunny today then what is the weather likely to be tomorrow?\n\nA1:\n Since, we do not know what is going to happen for sure, the best we can say is that there is a $90\\%$ chance that it is likely to be sunny and $10\\%$ that it will be rainy.\n\nQ2:\n What about two days from today?\n\nA2:\n One day prediction: $90\\%$ sunny, $10\\%$ rainy. Therefore, two days from now:\n\nFirst day it can be sunny and the next day also it can be sunny. Chances of this happening are: $0.9 \\times 0.9$.\n\nOr\n\nFirst day it can be rainy and second day it can be sunny. Chances of this happening are: $0.1 \\times 0.5$.\n\nTherefore, the probability that the weather will be sunny in two days is:\n\n$P(\\text{Sunny 2 days from now} = 0.9 \\times 0.9 + 0.1 \\times 0.5 = 0.81 + 0.05 = 0.86$\n\nSimilarly, the probability that it will be rainy is:\n\n$P(\\text{Rainy 2 days from now} = 0.1 \\times 0.5 + 0.9 \\times 0.1 = 0.05 + 0.09 = 0.14$\n\nIn linear algebra (transition matrices) these calculations correspond to all the permutations in transitions from one step to the next (sunny-to-sunny ($S_2S$), sunny-to-rainy ($S_2R$), rainy-to-sunny ($R_2S$) or rainy-to-rainy ($R_2R$)) with their calculated probabilities:\n\n\n\nOn the lower part of the image we see how to calculate the probability of a future state ($t+1$ or $t+2$) given the probabilities (probability mass function, $PMF$) for every state (sunny or rainy) at time zero (now or $t_0$) as simple matrix multiplication.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/207/402101 \n On the lower part of the image we see how to calculate the probability of a future state ($t+1$ or $t+2$) given the probabilities (probability mass function, $PMF$) for every state (sunny or rainy) at time zero (now or $t_0$) as simple matrix multiplication.\n\nIf you keep forecasting weather like this you will notice that eventually the $n$-th day forecast, where $n$ is very large (say $30$), settles to the following 'equilibrium' probabilities:\n\n$P(\\text{Sunny}) = 0.833$\n\nand\n\n$P(\\text{Rainy}) = 0.167$\n\nIn other words, your forecast for the $n$-th day and the $n+1$-th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.\n\nThe above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' Markov chain (nice = transition probabilities satisfy conditions):\n\nIrrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.\n\nMarkov Chain Monte Carlo exploits the above feature as follows:\n\nWe want to generate random draws from a target distribution. We then identify a way to construct a 'nice' Markov chain such that its equilibrium probability distribution is our target distribution.\n\nIf we can construct such a chain then we arbitrarily start from some point and iterate the Markov chain many times (like how we forecast the weather $n$ times). Eventually, the draws we generate would appear as if they are coming from our target distribution.\n\nWe then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the Monte Carlo component.\n\nThere are several ways to construct 'nice' Markov chains (e.g., Gibbs sampler, Metropolis-Hastings algorithm).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4518/402101 \n Yes, it is possible.  What you're interested is is called \"Multivariate Multiple Regression\" or just \"Multivariate Regression\".  I don't know what software you are using, but you can do this in R.\n\nHere's a link that provides examples\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/362422/402101 \n The reviewer should have told you why the Spearman $\\rho$ is not appropriate.  Here is one version of that:  Let the data be $(Z_i, I_i)$ where $Z$ is the measured variable and $I$ is the gender indicator, say it is 0 (man), 1 (woman). Then Spearman's $\\rho$ is calculated based on the ranks of $Z, I$ respectively. Since there are only two possible values for the indicator $I$, there will be a lot of ties, so this formula is not appropriate. If you replace rank with mean rank,  then you will get only two different values, one for men, another for women.  Then $\\rho$ will become basically some rescaled version of the mean ranks between the two groups. It would be simpler (more interpretable) to simply compare the means!   Another approach is the following.\n\nLet $X_1, \\dots, X_n$ be the observations of the continuous variable among men, $Y_1, \\dots, Y_m$ same among women. Now, if the distribution of $X$ and of $Y$ are the same, then $P(X>Y)$ will be 0.5 (let's assume the distribution is purely absolutely continuous, so there are no ties). In the general case, define\n$$\n   \\theta = P(X>Y)\n$$\nwhere $X$ is a random draw among men, $Y$ among women. Can we estimate $\\theta$ from our sample? Form all pairs $(X_i, Y_j)$ (assume no ties) and count for how many we have  \"man is larger\" ($X_i > Y_j$)($M$) and for how many \"woman is larger\"  ($ X_i < Y_j$) ($W$).  Then one sample estimate of $\\theta$ is\n$$\n  \\frac{M}{M+W}\n$$\nThat is one reasonable measure of correlation!  (If there are only a few ties, just ignore them).  But I am not sure what that is called,  if it has a name. \nThis one may be close:       \nhttps://en.wikipedia.org/wiki/Goodman_and_Kruskal%27s_gamma",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27374/402101 \n apparently the Bayes factor somehow uses likelihoods that represent the likelihood of each model integrated over it's entire parameter space (i.e. not just at the MLE). How is this integration actually achieved typically? Does one really just try to calculate the likelihood at each of thousands (millions?) of random samples from the parameter space, or are there analytic methods to integrating the likelihood across the parameter space?\n\nFirst, any situation where you consider a term such as $P(D|M)$ for data $D$ and model $M$ is considered a \nlikelihood\n model. This is often the bread and butter of any statistical analysis, frequentist or Bayesian, and this is the portion that your analysis is meant to suggest is either a good fit or a bad fit. So Bayes factors are not doing anything fundamentally different than likelihood ratios.\n\nIt's important to put Bayes factors in their right setting. When you have two models, say, and you convert from probabilities to odds, then Bayes factors act like an operator on prior beliefs:\n\n$$ Posterior Odds = Bayes Factor * Prior Odds $$\n$$ \\frac{P(M_{1}|D)}{P(M_{2}|D)} = B.F. \\times \\frac{P(M_{1})}{P(M_{2})} $$\n\nThe real difference is that likelihood ratios are cheaper to compute and generally conceptually easier to specify. The likelihood at the MLE is just a point estimate of the Bayes factor numerator and denominator, respectively. Like most frequentist constructions, it can be viewed as a special case of Bayesian analysis with a contrived prior that's hard to get at. But mostly it arose because it's analytically tractable and easier to compute (in the era before approximate Bayesian computational approaches arose).\n\nTo the point on computation, yes: you will evaluate the different likelihood integrals in the Bayesian setting with a large-scale Monte Carlo procedure in almost any case of practical interest. There are some specialized simulators, such as GHK, that work if you assume certain distributions, and if you make these assumptions, sometimes you can find analytically tractable problems for which fully analytic Bayes factors exist.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27374/402101 \n To the point on computation, yes: you will evaluate the different likelihood integrals in the Bayesian setting with a large-scale Monte Carlo procedure in almost any case of practical interest. There are some specialized simulators, such as GHK, that work if you assume certain distributions, and if you make these assumptions, sometimes you can find analytically tractable problems for which fully analytic Bayes factors exist.\n\nBut no one uses these; there is no reason to. With optimized Metropolis/Gibbs samplers and other MCMC methods, it's totally tractable to approach these problems in a fully data driven way and compute your integrals numerically. In fact, one will often do this hierarchically and further integrate the results over meta-priors that relate to data collection mechanisms, non-ignorable experimental designs, etc.\n\nI recommend the book \nBayesian Data Analysis\n for more on this. Although, the author, Andrew Gelman, \nseems not to care too much for Bayes factors\n. As an aside, I agree with Gelman. If you're going to go Bayesian, then exploit the full posterior. Doing model selection with Bayesian methods is like handicapping them, because model selection is a weak and mostly useless form of inference. I'd rather know distributions over model choices if I can... who cares about quantizing it down to \"model A is better than model B\" sorts of statements when you do not have to?\n\nAdditionally, when computing the Bayes factor, does one apply correction for complexity (automatically via cross-validated estimation of likelihood or analytically via AIC) as one does with the likelihood ratio?\n\nThis is one of the nice things about Bayesian methods. Bayes factors \nautomatically\n account for model complexity in a technical sense. You can set up a simple scenario with two models, $M_{1}$ and $M_{2}$ with assumed model complexities $d_{1}$ and $d_{2}$, respectively, with $d_{1} < d_{2}$ and a sample size $N$.\n\nThen if $B_{1,2}$ is the Bayes factor with $M_{1}$ in the numerator, under the assumption that $M_{1}$ is true one can prove that as $N\\to\\infty$, $B_{1,2}$ approaches $\\infty$ at a rate that depends on the difference in model complexity, and that the Bayes factor favors the simpler model. More specifically, you can show that under all of the above assumptions, $$ B_{1,2} = \\mathcal{O}(N^{\\frac{1}{2}(d_{2}-d_{1})}) $$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27374/402101 \n Then if $B_{1,2}$ is the Bayes factor with $M_{1}$ in the numerator, under the assumption that $M_{1}$ is true one can prove that as $N\\to\\infty$, $B_{1,2}$ approaches $\\infty$ at a rate that depends on the difference in model complexity, and that the Bayes factor favors the simpler model. More specifically, you can show that under all of the above assumptions, $$ B_{1,2} = \\mathcal{O}(N^{\\frac{1}{2}(d_{2}-d_{1})}) $$\n\nI'm familiar with this derivation and the discussion from the book \nFinite Mixture and Markov Switching Models\n by Sylvia Fr\u00fchwirth-Schnatter, but there are likely more directly statistical accounts that dive more into the epistemology underlying it.\n\nI don't know the details well enough to give them here, but I believe there are some fairly deep theoretical connections between this and the derivation of AIC. The Information Theory book by Cover and Thomas hinted at this at least.\n\nAlso, what are the philosophical differences between the likelihood ratio and the Bayes factor (n.b. I'm not asking about the philosophical differences between the likelihood ratio and Bayesian methods in general, but the Bayes factor as a representation of the objective evidence specifically). How would one go about characterizing the meaning of the Bayes factor as compared to the likelihood ratio?\n\nThe \nWikipedia article's section on \"Interpretation\"\n does a good job of discussing this (especially the chart showing Jeffreys' strength of evidence scale).\n\nLike usual, there's not too much philosophical stuff beyond the basic differences between Bayesian methods and frequentist methods (which you seem already familiar with).\n\nThe main thing is that the likelihood ratio is not coherent in a Dutch book sense. You can concoct scenarios where the model selection inference from likelihood ratios will lead one to accept losing bets. The Bayesian method is coherent, but operates on a prior which could be extremely poor and has to be chosen subjectively. Tradeoffs.. tradeoffs...",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27374/402101 \n The main thing is that the likelihood ratio is not coherent in a Dutch book sense. You can concoct scenarios where the model selection inference from likelihood ratios will lead one to accept losing bets. The Bayesian method is coherent, but operates on a prior which could be extremely poor and has to be chosen subjectively. Tradeoffs.. tradeoffs...\n\nFWIW, I think this kind of heavily parameterized model selection is not very good inference. I prefer Bayesian methods and I prefer to organize them more hierarchically, and I want the inference to center on the full posterior distribution if it is at all computationally feasible to do so. I think Bayes factors have some neat mathematical properties, but as a Bayesian myself, I am not impressed by them. They conceal the really useful part of Bayesian analysis, which is that it forces you to deal with your priors out in the open instead of sweeping them under the rug, and allows you to do inference on full posteriors.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2501/402101 \n It's not an argument. It is a (a bit strongly stated) fact that formal normality tests always reject on the huge sample sizes we work with today. It's even easy to prove that when n gets large, even the smallest deviation from perfect normality will lead to a significant result. And as every dataset has some degree of randomness, no single dataset will be a perfectly normally distributed sample. But in applied statistics the question is not whether the data/residuals ... are perfectly normal, but normal enough for the assumptions to hold.\n\nLet me illustrate with \nthe Shapiro-Wilk test\n. The code below constructs a set of distributions that approach normality but aren't completely normal. Next, we test with \nshapiro.test\n whether a sample from these almost-normal distributions deviate from normality. In R:\n\nshapiro.test\n\nx <- replicate(100, { # generates 100 different tests on each distribution\n                     c(shapiro.test(rnorm(10)+c(1,0,2,0,1))$p.value,   #$\n                       shapiro.test(rnorm(100)+c(1,0,2,0,1))$p.value,  #$\n                       shapiro.test(rnorm(1000)+c(1,0,2,0,1))$p.value, #$\n                       shapiro.test(rnorm(5000)+c(1,0,2,0,1))$p.value) #$\n                    } # rnorm gives a random draw from the normal distribution\n               )\nrownames(x) <- c(\"n10\",\"n100\",\"n1000\",\"n5000\")\n\nrowMeans(x<0.05) # the proportion of significant deviations\n  n10  n100 n1000 n5000 \n 0.04  0.04  0.20  0.87\n\nx <- replicate(100, { # generates 100 different tests on each distribution\n                     c(shapiro.test(rnorm(10)+c(1,0,2,0,1))$p.value,   #$\n                       shapiro.test(rnorm(100)+c(1,0,2,0,1))$p.value,  #$\n                       shapiro.test(rnorm(1000)+c(1,0,2,0,1))$p.value, #$\n                       shapiro.test(rnorm(5000)+c(1,0,2,0,1))$p.value) #$\n                    } # rnorm gives a random draw from the normal distribution\n               )\nrownames(x) <- c(\"n10\",\"n100\",\"n1000\",\"n5000\")\n\nrowMeans(x<0.05) # the proportion of significant deviations\n  n10  n100 n1000 n5000 \n 0.04  0.04  0.20  0.87\n\nThe last line checks which fraction of the simulations for every sample size deviate significantly from normality. So in 87% of the cases, a sample of 5000 observations deviates significantly from normality according to Shapiro-Wilks. Yet, if you see the qq plots, you would never ever decide on a deviation from normality. Below you see as an example the qq-plots for one set of random samples\n\n\n\nwith p-values",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2501/402101 \n The last line checks which fraction of the simulations for every sample size deviate significantly from normality. So in 87% of the cases, a sample of 5000 observations deviates significantly from normality according to Shapiro-Wilks. Yet, if you see the qq plots, you would never ever decide on a deviation from normality. Below you see as an example the qq-plots for one set of random samples\n\n\n\nwith p-values\n\nn10  n100 n1000 n5000 \n0.760 0.681 0.164 0.007\n\nn10  n100 n1000 n5000 \n0.760 0.681 0.164 0.007",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/619893/402101 \n This is impossible. Suppose \n$X$\n is the distribution, and let \n$a,b,c$\n be its first three moments, \n$a=E[X], b=E[X^2], c=E[X^3]$\n.\n\nWe derive three relationships for these variables, and show that they can not be satisfied simulateously.\n\nFirst, by the \nCauchy-Schwarz\n inequality:\n\n\\begin{align}\nE[\\sqrt{X^\\phantom{1}}\\!\\! \\sqrt{X^3}] &\\le \\sqrt{E[X]}\\sqrt{E[X^3]}\\\\\n(E[X^2])^2 &\\le E[X]E[X^3]\\\\\nb^2 &\\le ac \\\\\n\\end{align}\n\nwhere \n$\\sqrt{X}$\n and \n$\\sqrt{X^3}$\n are positive by the positivity of \n$X$\n.\n\nSecond, since skewness is 0,\n\n\\begin{align}\n\\phantom{skewness = skewness = = = }0 &= E[(X-a)^3]\\\\\n&= E[X^3]-3aE[X^2]+3a^2E[X]-a^3\\\\\n&= c-3ab+3a^3-a^3\\\\\n&= c-3ab+2a^3\n\\end{align}\n\nThird, since the standard deviation is greater than the mean,\n\n\\begin{align}\n\\text{variance} &> \\text{mean}^2 \\phantom{+mean} \\\\\nb - a^2 &> a^2\\\\\nb &> 2a^2\\\\\n\\end{align}\n\nand, since \n$a^2$\n is positive, also \n$b>a^2$\n.\n\nTogether these yield\n\n$$0\\ge b^2-ac=(b-2a^2)(b-a^2)>0$$\n\nwhich is impossible.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/13115/402101 \n The two approaches do differ.\n\nLet the estimated standard errors of the two regressions be $s_1$ and $s_2$.  Then, because the combined regression (with all coefficient-dummy interactions) fits the same coefficients, it has the same residuals, whence its standard error can be computed as\n\n$$s = \\sqrt{\\frac{(n_1-p) s_1^2 + (n_2-p) s_2^2)}{n_1 + n_2 - 2 p}}.$$\n\nThe number of parameters $p$ equals $6$ in the example: five slopes and an intercept in each regression.\n\nLet $b_1$ estimate a parameter in one regression, $b_2$ estimate the same parameter in the other regression, and $b$ estimate their \ndifference\n in the combined regression.  Then their standard errors are related by\n\n$$SE(b) =  s \\sqrt{(SE(b_1)/s_1)^2 + (SE(b_2)/s_2)^2}.$$\n\nIf you haven't done the combined regression, but only have statistics for the separate regressions, plug in the preceding equation for $s$.  This will be the denominator for the t-test.  Evidently it is not the same as the denominator presented in the question.\n\nThe assumption made by the combined regression is that the variances of the residuals are essentially the same in both separate regressions.  If this is not the case, however, the z-test isn't going to be good, either (unless the sample sizes are large): you would want to use a \nCABF test\n or \nWelch-Satterthwaite t-test.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/612518/402101 \n Here's an example of how one analyzes such a situation.  Suppose that the model\n\n$$E[Y\\mid X_1,X_2] = \\beta_1 X_1 + \\beta_2 X_2$$\n\nholds where \n$X_1,$\n \n$X_2,$\n and \n$Y$\n are random \n$n$\n-vectors.  If you omit the second variable and use the wrong model (omitting the \n$X_2$\n variable)\n\n$$E[Y\\mid X] = \\gamma_1 X_1,$$\n\nwe may ask what error you might expect in using an estimate of \n$\\gamma_1$\n to estimate \n$\\beta_1.$\n  When you use ordinary least squares regression to estimate \n$\\gamma_1,$\n the formula is\n\n$$\\hat\\gamma_1 = \\frac{Y\\cdot X_1}{X_1\\cdot X_1}.$$\n\nUsing the correct model formulation you may compute\n\n$$E[\\hat\\gamma_1\\mid X_1, X_2] = E\\left[\\frac{Y\\cdot X_1}{X_1\\cdot X_1}\\mid X_1,X_2\\right] = E\\left[\\frac{(\\beta_1 X_1 + \\beta_2 X_2)\\cdot X_1}{X_1\\cdot X_1}\\mid X_1,X_2\\right].$$\n\nBasic properties of expectation (linearity) and conditional expectation (taking out what is known) allow you to simplify the right hand side to\n\n$$E[\\hat\\gamma_1\\mid X_1, X_2] = \\beta_1 + \\beta_2 \\frac{X_2\\cdot X_1}{X_1\\cdot X_1}.$$\n\nBy definition, the (conditional) \nbias\n in an estimate is the difference between its expectation and estimand,\n\n$$\\text{bias} = E[\\hat\\gamma_1\\mid X_1,X_2] - \\beta_1 = \\beta_2 \\frac{X_2\\cdot X_1}{X_1\\cdot X_1}.$$\n\nThat is a general formula, applicable for fixed \n$X_i$\n or for random \n$X_i$\n where \n$X_1\\cdot X_1$\n is almost surely nonzero.  Already the result is helpful, because it implies that when \n$X_1$\n is \northogonal\n to \n$X_2$\n (which is just a way of saying the numerator is zero), the bias is \nzero;\n and otherwise it shows that the bias is nonzero and it gives you information about its sign and magnitude.\n\nIf, additionally, you arrange for the \n$X_i$\n to be standardized (which means their components sum to zero and the squares of their components sum to unity), the fraction on the right could be called the \"correlation\" of \n$X_1$\n and \n$X_2,$\n \nunderstanding this term to be a shorthand for\n\n$$\\operatorname{correlation}(X_1,X_2) = \\frac{\\sum_{i=1}^n X_{1i}X_{2i}}{\\sum_{i=1}^n X_{1i}X_{1i}} = \\frac{\\sum_{i=1}^n X_{1i}X_{2i}}{1} = \\sum_{i=1}^n X_{1i}X_{2i},$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/612518/402101 \n If, additionally, you arrange for the \n$X_i$\n to be standardized (which means their components sum to zero and the squares of their components sum to unity), the fraction on the right could be called the \"correlation\" of \n$X_1$\n and \n$X_2,$\n \nunderstanding this term to be a shorthand for\n\n$$\\operatorname{correlation}(X_1,X_2) = \\frac{\\sum_{i=1}^n X_{1i}X_{2i}}{\\sum_{i=1}^n X_{1i}X_{1i}} = \\frac{\\sum_{i=1}^n X_{1i}X_{2i}}{1} = \\sum_{i=1}^n X_{1i}X_{2i},$$\n\nwhich is the Pearson correlation of standardized vectors.  But please note that this is \nnot\n the correlation in the sense that \n$X_1$\n and \n$X_2$\n might be random vectors: the bias was computed \nconditionally\n and still depends on \n$X_1$\n and \n$X_2.$\n  If, for instance, each \n$X_i$\n were an \niid\n sequence of random values drawn from a bivariate distribution, that underlying distribution can have a correlation but it's unlikely to equal the value computed in the bias formula.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/14549/402101 \n First you have to decide if you really need model selection, or you just need to model.  In the majority of situations, depending on dimensionality, fitting a flexible comprehensive model is preferred.\n\nThe bootstrap is a great way to estimate the performance of a model.  The simplest thing to estimate is variance.  More to your original point, the bootstrap can estimate the likely future performance of a given modeling procedure, on new data not yet realized.\n\nIf using resampling (bootstrap or cross-validation) to both choose model tuning parameters and to estimate the model, you will need a double bootstrap or nested cross-validation.\n\nIn general the bootstrap requires fewer model fits (often around 300) than cross-validation (10-fold cross-validation should be repeated 50-100 times for stability).\n\nSome simulation studies may be found at \nhttp://biostat.mc.vanderbilt.edu/rms",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/602086/402101 \n For \n$1 \\leq i < n$\n, denote random vectors \n$(X_1, \\ldots, X_i)$\n and \n$(X_{i + 1}, \\ldots, X_n)$\n by \n$X$\n and \n$Y$\n respectively.  Furthermore, denote product measures \n$\\mu_1 \\times \\cdots \\times \\mu_i$\n (on \n$\\mathcal{X}^i$\n) and \n$\\mu_{i + 1} \\times \\cdots \\times \\mu_n$\n (on \n$\\mathcal{X}^{n - i}$\n) by \n$\\pi_1$\n and \n$\\pi_2$\n respectively. Under these notations, your first equality is then:\n\n\\begin{align*}\nE[f(X, Y)|X] = \\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy). \\tag{1}\n\\end{align*}\n\nAs the right hand side of \n$(1)$\n is a function of \n$X$\n, it is \n$\\sigma(X)$\n-measurable.  To prove \n$(1)$\n, it is therefore sufficient to show that for any \n$G \\in \\sigma(X)$\n, it holds that\n\n\\begin{align}\n\\int_G f(X, Y)dP = \\int_G\\left[\\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy)\\right]dP. \\tag{2}\n\\end{align}\n\nSince every \n$G$\n in \n$\\sigma(X)$\n has the form \n$\\{\\omega \\in \\Omega: X(\\omega) \\in H\\} =: X^{-1}(H)$\n for some \n$H \\in \\mathscr{X}^i$\n (Theorem 20.1, \nProbability and Measure\n by Patrick Billingsley. Here \n$\\mathscr{X}^i$\n denotes the \n$\\sigma$\n-field generated by open sets in \n$\\mathcal{X}^i$\n), \n$(2)$\n is equivalent to:\n\n\n\\begin{align}\n\\int_{X^{-1}(H)}f(X, Y)dP = \\int_{X^{-1}(H)}\\left[\\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy)\\right]dP. \\tag{2}\n\\end{align}\n\nBy the change of variable theorem (Theorem 16.13, \nProbability and Measure\n by Patrick Billingsley. Or refer to \nthis Wikipedia page\n) and the independence of \n$X$\n and \n$Y$\n (hence the distribution of \n$(X, Y)$\n is the product measure \n$\\pi_1 \\times \\pi_2$\n), the left hand side of \n$(2)$\n is:\n\n\\begin{align}\n\\int_{X^{-1}(H)}f(X, Y)dP &= \n\\int_{(X, Y)^{-1}(H \\times \\mathcal{X}^{n - i})}f(X, Y)dP \\\\\n&=\\int_{H \\times \\mathcal{X}^{n - i}} f(x, y) (\\pi_1 \\times \\pi_2)(d(x, y)). \\tag{3}\n\\end{align}\n\nSimilarly, applying the change of variable theorem to the right hand side of \n$(2)$\n yields:\n\n\\begin{align}\n\\int_{X^{-1}(H)}\\left[\\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy)\\right]dP\n= \\int_H\\left[\\int_{\\mathcal{X}^{n - i}}f(x, y)\\pi_2(dy)\\right]\\pi_1(dx). \\tag{4}\n\\end{align}\n\nNow the coincidence of the right hand side of \n$(3)$\n and the right hand side of \n$(4)$\n is exactly the consequence of the Fubini's theorem.  This completes the proof.\n\nThe proof to your second equality is completely analogous (with slightly more verbose notations).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7633/402101 \n In the \"marginalisation\" integral, the lower limit for $x_1$ is not $0$ but $x_2$ (because of the $0<x_2<x_1$ condition).\n\nSo the integral should be:\n\n$$p(x_2)=\\int p(x_1,x_2) dx_1=\\int \\frac{I(0\\leq x_2\\leq x_1\\leq 1)}{x_1} dx_1=\\int_{x_2}^{1} \\frac{dx_1}{x_1}=log\\big(\\frac{1}{x_2}\\big)$$\n\nYou have stumbled across, what I think is one of the hardest parts of statistical integrals - determining the limits of integration.\n\nNOTE: This is consistent with Henry's answer, mine is the PDF, and his is the CDF.  Differentiating his answer gives you mine, which shows we are both right.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163975/402101 \n Rarely if ever a parametric test and a non-parametric test actually have the same null. The parametric $t$-test is testing the mean of the distribution, assuming the first two moments exist. The Wilcoxon rank sum test does not assume any moments, and tests equality of distributions instead. Its implied parameter is a weird functional of distributions, the probability that the observation from one sample is lower than the observation from the other. You can sort of talk about comparisons between the two tests under the completely specified null of identical distributions... but you have to recognize that the two tests are testing different hypotheses.\n\nThe information that parametric tests bring in along with their assumption helps improving the power of the tests. Of course that information better be right, but there are few if any domains of human knowledge these days where such preliminary information does not exist. An interesting exception that explicitly says \"I don't want to assume anything\" is the courtroom where non-parametric methods continue to be widely popular -- and it makes perfect sense for the application. There's probably a good reason, pun intended, that Phillip Good authored good books on both \nnon-parametric statistics\n and \ncourtroom statistics\n.\n\nThere are also testing situations where you don't have access to the microdata necessary for the nonparametric test. Suppose you were asked to compare two groups of people to gauge whether one is more obese than the other. In an ideal world, you will have height and weight measurements for everybody, and you could form a permutation test stratifying by height. In a less than ideal (i.e., real) world, you may only have the mean height and mean weight in each group (or may be some ranges or variances of these characteristics on top of the sample means). Your best bet is then to compute the mean BMI for each group and compare them if you only have the means; or assume a bivariate normal for height and weight if you have means and variances (you'd probably have to take a correlation from some external data if it did not come with your samples), form some sort of regression lines of weight on height within each group, and check whether one line is above the other.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/140626/402101 \n In the paper, and in general, (random) \nvariables\n are everything which is drawn from a probability distribution. \nLatent\n (random) \nvariables\n are the ones you don't directly observe (\n$y$\n is observed, \n$\\beta$\n is not, but both are r.v). From a latent random variable you can get a posterior distribution, which is its probability distribution conditioned to the observed data.\n\nOn the other hand, a \nparameter\n is fixed, even if you don't know its value. Maximum Likelihood Estimation, for instance, gives you the most likely value of your parameter. But it gives you a point, not a full distribution, because fixed things do not have distributions! (You can put a distribution on how sure you are about this value, or in what range you thing this value is, but this is is not the same as the distribution of the value itself, which only exists if the value is actually a random variable)\n\nIn a Bayesian setting, you can have all of them. Here, parameters are things like the number of clusters; you give this value to the model, and the model considers it a fixed number. \n$y$\n is a random variable because it is drawn from a distribution, and \n$\\beta$\n and \n$w$\n are latent random variables because they are drawn from probability distributions as well. The fact that \n$y$\n depends on \n$\\beta$\n and \n$w$\n doesn't make them \"parameters\", it just makes \n$y$\n dependent on two random variables.\n\nIn the paper they consider that \n$\\beta$\n and \n$w$\n are random variables.\n\nIn this sentence:\n\nThese update equations need to be run iteratively until all parameters\n  and the complete log likelihood converge to steady values\n\nin theory they talk about the two parameters, not the ones that are random variables, since in EM this is what you do, optimizing over parameters.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/291414/402101 \n To add to the previous responses. You should definitely check out the recent work by Tibshirani and colleagues. They have developed a rigorous framework for inferring selection-corrected p-values and confidence intervals for lasso-type methods and also provide an R-package.\n\nSee:\n\nLee, Jason D., et al. \"Exact post-selection inference, with application to the lasso.\" The Annals of Statistics 44.3 (2016): 907-927.\n(\nhttps://projecteuclid.org/euclid.aos/1460381681\n)\n\nTaylor, Jonathan, and Robert J. Tibshirani. \"Statistical learning and selective inference.\" Proceedings of the National Academy of Sciences 112.25 (2015): 7629-7634.\n\nR-package:\n\nhttps://cran.r-project.org/web/packages/selectiveInference/index.html",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6885/402101 \n Failing to look at (plot) the data.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/184023/402101 \n How to decide which regularization (L1 or L2) to use?\n\nWhat is your goal? Both can improve model generalization by penalizing coefficients, since features with opposite relationship to the outcome can \"offset\" each other (a large positive value is counterbalanced by a large negative value). This can arise when there are collinear features. Small changes in the data can result in dramatically different parameter estimates (high variance estimates). Penalization can restrain both coefficients to be smaller. (Hastie et al, \nElements of Statistical Learning\n, 2nd edition, p. 63)\n\nWhat are the pros & cons of each of L1 / L2 regularization?\n\nL1 regularization can address the multicollinearity problem by constraining the coefficient norm and pinning some coefficient values to 0. Computationally, Lasso regression (regression with an L1 penalty) is a quadratic program which requires some special tools to solve. \nWhen you have more features than observations \n$N$\n, lasso will keep at most \n$N$\n non-zero coefficients\n. Depending on context, that might not be what you want.\n\nL1 regularization is sometimes used as a feature selection method. Suppose you have some kind of hard cap on the number of features you can use (because data collection for \nall\n features is expensive, or you have tight engineering constraints on how many values you can store, etc.). You can try to tune the L1 penalty to hit your desired number of non-zero features.\n\nL2 regularization can address the multicollinearity problem by constraining the coefficient norm and keeping all the variables. It's unlikely to estimate a coefficient to be exactly 0. This isn't necessarily a drawback, unless a sparse coefficient vector is important for some reason.\n\nIn the regression setting, it's the \"classic\" solution to the problem of estimating a regression with more features than observations. L2 regularization can estimate a coefficient for each feature even if there are more features than observations (indeed, this was the original motivation for \"ridge regression\").\n\nAs an alternative, \nelastic net allows L1 and L2 regularization as special cases.\n A typical use-case in for a data scientist in industry is that you just want to pick the best model, but don't necessarily care if it's penalized using L1, L2 or both. Elastic net is nice in situations like these.\n\nIs it recommended to 1st do feature selection using L1 & then apply L2 on these selected variables?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/184023/402101 \n As an alternative, \nelastic net allows L1 and L2 regularization as special cases.\n A typical use-case in for a data scientist in industry is that you just want to pick the best model, but don't necessarily care if it's penalized using L1, L2 or both. Elastic net is nice in situations like these.\n\nIs it recommended to 1st do feature selection using L1 & then apply L2 on these selected variables?\n\nI'm not familiar with a publication proposing an L1-then-L2 pipeline, but this is probably just ignorance on my part. There doesn't seem to be anything wrong with it. I'd conduct a literature review.\n\nA few examples of similar \"phased\" pipelines exist. One is the \"relaxed lasso\", which applies lasso regression \ntwice\n, once to down-select from a large group to a small group of features, and second to estimate coefficients for use in a model. This uses cross-validation at each step to choose the magnitude of the penalty. The reasoning is that in the first step, you cross-validate and will likely choose a large penalty to screen out irrelevant predictors; in the second step, you cross-validate and will likely pick a smaller penalty (and hence larger coefficients). This is mentioned briefly in \nElements of Statistical Learning\n with a citation to Nicolai Meinshausen (\"Relaxed Lasso.\" \nComputational Statistics & Data Analysis\n Volume 52, Issue 1, 15 September 2007, pp 374-393).\n\nUser @amoeba also suggests an L1-then-OLS pipeline; this might be nice because it only has 1 hyperparameter for the magnitude of the L1 penalty, so less fiddling would be required.\n\nOne problem that can arise with any \"phased\" analysis pipeline (that is, a pipeline which does some steps, and then some other steps separately) is that there's no \"visibility\" between those different phases (algorithms applied at each step). This means that one process inherits any data snooping that happened at the previous steps. This effect is not negligible; poorly-conceived modeling can result in garbage models.\n\nOne way to hedge against data-snooping side-effects is to cross-validate all of your choices. However, the increased computational costs can be prohibitive, depending on the scale of the data and the complexity of each step.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74587/402101 \n Let's consider a very simple model: \n$y = \\beta x + e$\n, with an L1 penalty on \n$\\hat{\\beta}$\n and a least-squares loss function on \n$\\hat{e}$\n.  We can expand the expression to be minimized as:\n\n$\\min y^Ty -2 y^Tx\\hat{\\beta} + \\hat{\\beta} x^Tx\\hat{\\beta} + 2\\lambda|\\hat{\\beta}|$\n\nKeep in mind this is a univariate example, with \n$\\beta$\n and \n$x$\n being scalars, to show how LASSO can send a coefficient to zero. This can be generalized to the multivariate case.\n\nLet us assume the least-squares solution is some \n$\\hat{\\beta} > 0$\n, which is equivalent to assuming that \n$y^Tx > 0$\n, and see what happens when we add the L1 penalty.  With \n$\\hat{\\beta}>0$\n, \n$|\\hat{\\beta}| = \\hat{\\beta}$\n, so the penalty term is equal to \n$2\\lambda\\beta$\n.  The derivative of the objective function w.r.t. \n$\\hat{\\beta}$\n is:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} + 2\\lambda$\n\nwhich evidently has solution \n$\\hat{\\beta} = (y^Tx - \\lambda)/(x^Tx)$\n.\n\nObviously by increasing \n$\\lambda$\n we can drive \n$\\hat{\\beta}$\n to zero (at \n$\\lambda = y^Tx$\n).   However, once \n$\\hat{\\beta} = 0$\n, increasing \n$\\lambda$\n won't drive it negative, because, writing loosely, the instant \n$\\hat{\\beta}$\n becomes negative, the derivative of the objective function changes to:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} - 2\\lambda$\n\nwhere the flip in the sign of \n$\\lambda$\n is due to the absolute value nature of the penalty term; when \n$\\beta$\n becomes negative, the penalty term becomes equal to \n$-2\\lambda\\beta$\n, and taking the derivative w.r.t. \n$\\beta$\n results in \n$-2\\lambda$\n.  This leads to the solution \n$\\hat{\\beta} = (y^Tx + \\lambda)/(x^Tx)$\n, which is obviously inconsistent with \n$\\hat{\\beta} < 0$\n (given that the least squares solution \n$> 0$\n, which implies \n$y^Tx > 0$\n, and \n$\\lambda > 0$\n).  There is an increase in the L1 penalty AND an increase in the squared error term (as we are moving farther from the least squares solution) when moving \n$\\hat{\\beta}$\n from \n$0$\n to \n$ < 0$\n, so we don't, we just stick at \n$\\hat{\\beta}=0$\n.\n\nIt should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with \n$\\hat{\\beta} < 0$\n.\n\nWith the least squares penalty \n$\\lambda\\hat{\\beta}^2$\n, however, the derivative becomes:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} + 2\\lambda\\hat{\\beta}$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74587/402101 \n It should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with \n$\\hat{\\beta} < 0$\n.\n\nWith the least squares penalty \n$\\lambda\\hat{\\beta}^2$\n, however, the derivative becomes:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} + 2\\lambda\\hat{\\beta}$\n\nwhich evidently has solution \n$\\hat{\\beta} = y^Tx/(x^Tx + \\lambda)$\n.  Obviously no increase in \n$\\lambda$\n will drive this all the way to zero.  So the L2 penalty can't act as a variable selection tool without some mild ad-hockery such as \"set the parameter estimate equal to zero if it is less than \n$\\epsilon$\n\".\n\nObviously things can change when you move to multivariate models, for example, moving one parameter estimate around might force another one to change sign, but the general principle is the same: the L2 penalty function can't get you all the way to zero, because, writing very heuristically, it in effect adds to the \"denominator\" of the expression for \n$\\hat{\\beta}$\n, but the L1 penalty function can, because it in effect adds to the \"numerator\".",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27824/402101 \n apparently the Bayes factor somehow uses likelihoods that represent the likelihood of each model integrated over it's entire parameter space (i.e. not just at the MLE). How is this integration actually achieved typically? Does one really just try to calculate the likelihood at each of thousands (millions?) of random samples from the parameter space, or are there analytic methods to integrating the likelihood across the parameter space?\n\nFirst, any situation where you consider a term such as $P(D|M)$ for data $D$ and model $M$ is considered a \nlikelihood\n model. This is often the bread and butter of any statistical analysis, frequentist or Bayesian, and this is the portion that your analysis is meant to suggest is either a good fit or a bad fit. So Bayes factors are not doing anything fundamentally different than likelihood ratios.\n\nIt's important to put Bayes factors in their right setting. When you have two models, say, and you convert from probabilities to odds, then Bayes factors act like an operator on prior beliefs:\n\n$$ Posterior Odds = Bayes Factor * Prior Odds $$\n$$ \\frac{P(M_{1}|D)}{P(M_{2}|D)} = B.F. \\times \\frac{P(M_{1})}{P(M_{2})} $$\n\nThe real difference is that likelihood ratios are cheaper to compute and generally conceptually easier to specify. The likelihood at the MLE is just a point estimate of the Bayes factor numerator and denominator, respectively. Like most frequentist constructions, it can be viewed as a special case of Bayesian analysis with a contrived prior that's hard to get at. But mostly it arose because it's analytically tractable and easier to compute (in the era before approximate Bayesian computational approaches arose).\n\nTo the point on computation, yes: you will evaluate the different likelihood integrals in the Bayesian setting with a large-scale Monte Carlo procedure in almost any case of practical interest. There are some specialized simulators, such as GHK, that work if you assume certain distributions, and if you make these assumptions, sometimes you can find analytically tractable problems for which fully analytic Bayes factors exist.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27824/402101 \n To the point on computation, yes: you will evaluate the different likelihood integrals in the Bayesian setting with a large-scale Monte Carlo procedure in almost any case of practical interest. There are some specialized simulators, such as GHK, that work if you assume certain distributions, and if you make these assumptions, sometimes you can find analytically tractable problems for which fully analytic Bayes factors exist.\n\nBut no one uses these; there is no reason to. With optimized Metropolis/Gibbs samplers and other MCMC methods, it's totally tractable to approach these problems in a fully data driven way and compute your integrals numerically. In fact, one will often do this hierarchically and further integrate the results over meta-priors that relate to data collection mechanisms, non-ignorable experimental designs, etc.\n\nI recommend the book \nBayesian Data Analysis\n for more on this. Although, the author, Andrew Gelman, \nseems not to care too much for Bayes factors\n. As an aside, I agree with Gelman. If you're going to go Bayesian, then exploit the full posterior. Doing model selection with Bayesian methods is like handicapping them, because model selection is a weak and mostly useless form of inference. I'd rather know distributions over model choices if I can... who cares about quantizing it down to \"model A is better than model B\" sorts of statements when you do not have to?\n\nAdditionally, when computing the Bayes factor, does one apply correction for complexity (automatically via cross-validated estimation of likelihood or analytically via AIC) as one does with the likelihood ratio?\n\nThis is one of the nice things about Bayesian methods. Bayes factors \nautomatically\n account for model complexity in a technical sense. You can set up a simple scenario with two models, $M_{1}$ and $M_{2}$ with assumed model complexities $d_{1}$ and $d_{2}$, respectively, with $d_{1} < d_{2}$ and a sample size $N$.\n\nThen if $B_{1,2}$ is the Bayes factor with $M_{1}$ in the numerator, under the assumption that $M_{1}$ is true one can prove that as $N\\to\\infty$, $B_{1,2}$ approaches $\\infty$ at a rate that depends on the difference in model complexity, and that the Bayes factor favors the simpler model. More specifically, you can show that under all of the above assumptions, $$ B_{1,2} = \\mathcal{O}(N^{\\frac{1}{2}(d_{2}-d_{1})}) $$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27824/402101 \n Then if $B_{1,2}$ is the Bayes factor with $M_{1}$ in the numerator, under the assumption that $M_{1}$ is true one can prove that as $N\\to\\infty$, $B_{1,2}$ approaches $\\infty$ at a rate that depends on the difference in model complexity, and that the Bayes factor favors the simpler model. More specifically, you can show that under all of the above assumptions, $$ B_{1,2} = \\mathcal{O}(N^{\\frac{1}{2}(d_{2}-d_{1})}) $$\n\nI'm familiar with this derivation and the discussion from the book \nFinite Mixture and Markov Switching Models\n by Sylvia Fr\u00fchwirth-Schnatter, but there are likely more directly statistical accounts that dive more into the epistemology underlying it.\n\nI don't know the details well enough to give them here, but I believe there are some fairly deep theoretical connections between this and the derivation of AIC. The Information Theory book by Cover and Thomas hinted at this at least.\n\nAlso, what are the philosophical differences between the likelihood ratio and the Bayes factor (n.b. I'm not asking about the philosophical differences between the likelihood ratio and Bayesian methods in general, but the Bayes factor as a representation of the objective evidence specifically). How would one go about characterizing the meaning of the Bayes factor as compared to the likelihood ratio?\n\nThe \nWikipedia article's section on \"Interpretation\"\n does a good job of discussing this (especially the chart showing Jeffreys' strength of evidence scale).\n\nLike usual, there's not too much philosophical stuff beyond the basic differences between Bayesian methods and frequentist methods (which you seem already familiar with).\n\nThe main thing is that the likelihood ratio is not coherent in a Dutch book sense. You can concoct scenarios where the model selection inference from likelihood ratios will lead one to accept losing bets. The Bayesian method is coherent, but operates on a prior which could be extremely poor and has to be chosen subjectively. Tradeoffs.. tradeoffs...",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27824/402101 \n The main thing is that the likelihood ratio is not coherent in a Dutch book sense. You can concoct scenarios where the model selection inference from likelihood ratios will lead one to accept losing bets. The Bayesian method is coherent, but operates on a prior which could be extremely poor and has to be chosen subjectively. Tradeoffs.. tradeoffs...\n\nFWIW, I think this kind of heavily parameterized model selection is not very good inference. I prefer Bayesian methods and I prefer to organize them more hierarchically, and I want the inference to center on the full posterior distribution if it is at all computationally feasible to do so. I think Bayes factors have some neat mathematical properties, but as a Bayesian myself, I am not impressed by them. They conceal the really useful part of Bayesian analysis, which is that it forces you to deal with your priors out in the open instead of sweeping them under the rug, and allows you to do inference on full posteriors.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/120227/402101 \n There are two different typical situations for these kind of problems:\n\ni) you want to generate a sample from a given distribution whose population characteristics match the ones specified (but due to sampling variation, you don't have the sample characteristics exactly matching).\n\nii) you want to generate a sample whose sample characteristics match the ones specified (but, due to the constraints of exactly matching sample quantities to a prespecified set of values, don't really come from the distribution you want).\n\nYou want the second case -- but you get it by following the same approach as the first case, with an extra standardization step.\n\nSo for multivariate normals, either can be done in a fairly straightforward manner:\n\nWith first case you could use random normals without the population structure (such as iid standard normal which have expectation 0 and identity covariance matrix) and then impose it - transform to get the covariance matrix and mean you want. If $\\mu$ and $\\Sigma$ are the population mean and covariance you need and $z$ are iid standard normal, you calculate $y=Lz+\\mu$, for some $L$ where $LL'=\\Sigma$ (e.g. a suitable $L$ could be obtained via Cholesky decomposition). Then $y$ has the desired population characteristics.\n\nWith the second, you have to first transform your random normals to remove even the random variation away from the zero mean and identity covariance (making the sample mean zero and sample covariance $I_n$), then proceed as before. But that initial step of removing the sample deviation from exact mean $0$, variance $I$ interferes with the distribution. (In small samples it can be quite severe.)\n\nThis can be done by subtracting the sample mean of $z$ ($z^*=z-\\bar z$) and calculating the Cholesky decomposition of $z^*$. If $L^*$ is the left Cholesky factor, then $z^{(0)}=(L^*)^{-1}z^*$ should have sample mean 0 and identity sample covariance. You can then calculate $y=Lz^{(0)}+\\mu$ and have a sample with the desired sample moments. (Depending on how your sample quantities are defined, there may be an extra small fiddle involved with multiplying/dividing by factors like $\\sqrt{\\frac{n-1}{n}}$, but it's easy enough to identify that need.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/30160/402101 \n The bivariate normal distribution is the \nexception\n, not the rule!\n\nIt is important to recognize that \"almost all\" joint distributions with normal marginals are \nnot\n the bivariate normal distribution. That is, the common viewpoint that joint distributions with normal marginals that are not the bivariate normal are somehow \"pathological\", is a bit misguided.\n\nCertainly, the multivariate normal is \nextremely\n important due to its stability under linear transformations, and so receives the bulk of attention in applications.\n\nExamples\n\nIt is useful to start with some examples. The figure below contains heatmaps of six bivariate distributions, \nall\n of which have standard normal marginals. The left and middle ones in the top row are bivariate normals, the remaining ones are not (as should be apparent). They're described further below.\n\n\n\nThe bare bones of copulas\n\nProperties of dependence are often efficiently analyzed using \ncopulas\n. A \nbivariate copula\n is just a fancy name for a probability distribution on the unit square \n$[0,1]^2$\n with \nuniform\n marginals.\n\nSuppose \n$C(u,v)$\n is a bivariate copula. Then, immediately from the above, we know that \n$C(u,v) \\geq 0$\n, \n$C(u,1) = u$\n and \n$C(1,v) = v$\n, for example.\n\nWe can construct bivariate random variables on the Euclidean plane with \nprespecified\n marginals by a simple transformation of a bivariate copula. Let \n$F_1$\n and \n$F_2$\n be prescribed marginal distributions for a pair of random variables \n$(X,Y)$\n. Then, if \n$C(u,v)$\n is a bivariate copula,\n\n$$\nF(x,y) = C(F_1(x), F_2(y))\n$$\n\nis a bivariate distribution function with marginals \n$F_1$\n and \n$F_2$\n. To see this last fact, just note that\n\n$$\n\\renewcommand{\\Pr}{\\mathbb P}\n\\Pr(X \\leq x) = \\Pr(X \\leq x, Y < \\infty) = C(F_1(x), F_2(\\infty)) = C(F_1(x),1) = F_1(x) \\>.\n$$\n\nThe same argument works for \n$F_2$\n.\n\nFor continuous \n$F_1$\n and \n$F_2$\n, \nSklar's theorem\n asserts a converse implying uniqueness. That is, given a bivariate distribution \n$F(x,y)$\n with continuous marginals \n$F_1$\n, \n$F_2$\n, the corresponding copula is unique (on the appropriate range space).\n\nThe bivariate normal is \nexceptional\n\nSklar's theorem tells us (essentially) that there is only one copula that produces the bivariate normal distribution. This is, aptly named, the \nGaussian copula\n which has density on \n$[0,1]^2$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/30160/402101 \n For continuous \n$F_1$\n and \n$F_2$\n, \nSklar's theorem\n asserts a converse implying uniqueness. That is, given a bivariate distribution \n$F(x,y)$\n with continuous marginals \n$F_1$\n, \n$F_2$\n, the corresponding copula is unique (on the appropriate range space).\n\nThe bivariate normal is \nexceptional\n\nSklar's theorem tells us (essentially) that there is only one copula that produces the bivariate normal distribution. This is, aptly named, the \nGaussian copula\n which has density on \n$[0,1]^2$\n\n\n$$\nc_\\rho(u,v) := \\frac{\\partial^2}{\\partial u \\, \\partial v} C_\\rho(u,v) = \\frac{\\varphi_{2,\\rho}(\\Phi^{-1}(u),\\Phi^{-1}(v))}{\\varphi(\\Phi^{-1}(u)) \\varphi(\\Phi^{-1}(v))} \\>,\n$$\n\nwhere the numerator is the bivariate normal distribution with correlation \n$\\rho$\n evaluated at \n$\\Phi^{-1}(u)$\n and \n$\\Phi^{-1}(v)$\n.\n\nBut, there are \nlots\n of other copulas and \nall\n of them will give a bivariate distribution with normal marginals which is \nnot\n the bivariate normal by using the transformation described in the previous section.\n\nSome details on the examples\n\nNote that if \n$C(u,v)$\n is an \narbitrary\n copula with density \n$c(u,v)$\n, the corresponding bivariate density with standard normal marginals under the transformation \n$F(x,y) = C(\\Phi(x),\\Phi(y))$\n is\n\n$$\nf(x,y) = \\varphi(x) \\varphi(y) c(\\Phi(x), \\Phi(y)) \\> .\n$$\n\nNote that by applying the Gaussian copula in the above equation, we recover the bivariate normal density. But, for any other choice of \n$c(u,v)$\n, we will not.\n\nThe examples in the figure were constructed as follows (going across each row, one column at a time):\n\nBivariate normal with independent components.\n\n\nBivariate normal with \n$\\rho = -0.4$\n.\n\n\nThe \nexample given in this answer\n of \nDilip Sarwate\n. It can easily be seen to be induced by the copula \n$C(u,v)$\n with density \n$c(u,v) = 2 (\\mathbf 1_{(0 \\leq u \\leq 1/2, 0 \\leq v \\leq 1/2)} + \\mathbf 1_{(1/2 < u \\leq 1, 1/2 < v \\leq 1)})$\n.\n\n\nGenerated from the \nFrank copula\n with parameter \n$\\theta = 2$\n.\n\n\nGenerated from the \nClayton copula\n with parameter \n$\\theta = 1$\n.\n\n\nGenerated from an asymmetric modification of the Clayton copula with parameter \n$\\theta = 3$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45078/402101 \n What does ARIMA(1, 0, 12) mean?\n\nSpecifically for your model, ARIMA(1, 0, 12) means that it you are describing some response variable (Y) by combining a 1st order Auto-Regressive model and a 12th order Moving Average model. A good way to think about it is (AR, I, MA). This makes your model look the following, in simple terms:\n\nY = (Auto-Regressive Parameters) + (Moving Average Parameters)\n\nThe 0 between the 1 and the 12 represents the 'I' part of the model (the Integrative part) and it signifies a model where you're taking the difference between response variable data - this can be done with non-stationary data and it doesn't seem like you're dealing with that, so you can just ignore it.\n\nThe link that DanTheMan posted shows a nice mix of models that could help you understand yours by comparing it to those.\n\nWhat values can be assigned to p, d, q?\n\nLots of different whole numbers. There are diagnostic tests you can do to try to find the best values of p,d,q (see part 3).\n\nWhat is the process to find the values of p, d, q?\n\nThere are a number of ways, and I don't intend this to be exhaustive:\n\nlook at an autocorrelation graph of the data (will help if Moving Average (MA) model is appropriate)\n\n\nlook at a partial autocorrelation graph of the data (will help if AutoRegressive (AR) model is appropriate)\n\n\nlook at extended autocorrelation chart of the data (will help if a combination of AR and MA are needed)\n\n\ntry Akaike's Information Criterion (AIC) on a set of models and investigate the models with the lowest AIC values\n\n\ntry the Schwartz Bayesian Information Criterion (BIC) and investigate the models with the lowest BIC values\n\nWithout knowing how much more you need to know, I can't go too much farther, but if you have more questions, feel free to ask and maybe I, or someone else, can help.\n\n* \nEdit\n: All of the ways to find p, d, q that I listed here can be found in the R package TSA if you are familiar with R.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/12657/402101 \n First, we need to understand what is a Markov chain. Consider the following \nweather\n example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:\n\n$P(\\text{Next day is Sunny}\\,\\vert \\,\\text{Given today is Rainy)}=0.50$\n\nSince, the next day's weather is either sunny or rainy it follows that:\n\n$P(\\text{Next day is Rainy}\\,\\vert \\,\\text{Given today is Rainy)}=0.50$\n\nSimilarly, let:\n\n$P(\\text{Next day is Rainy}\\,\\vert \\,\\text{Given today is Sunny)}=0.10$\n\nTherefore, it follows that:\n\n$P(\\text{Next day is Sunny}\\,\\vert \\,\\text{Given today is Sunny)}=0.90$\n\nThe above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:\n\n$P = \\begin{bmatrix}\n& S & R \\\\\nS& 0.9 & 0.1 \\\\\nR& 0.5 & 0.5\n\\end{bmatrix}$\n\nWe might ask several questions whose answers follow:\n\nQ1:\n If the weather is sunny today then what is the weather likely to be tomorrow?\n\nA1:\n Since, we do not know what is going to happen for sure, the best we can say is that there is a $90\\%$ chance that it is likely to be sunny and $10\\%$ that it will be rainy.\n\nQ2:\n What about two days from today?\n\nA2:\n One day prediction: $90\\%$ sunny, $10\\%$ rainy. Therefore, two days from now:\n\nFirst day it can be sunny and the next day also it can be sunny. Chances of this happening are: $0.9 \\times 0.9$.\n\nOr\n\nFirst day it can be rainy and second day it can be sunny. Chances of this happening are: $0.1 \\times 0.5$.\n\nTherefore, the probability that the weather will be sunny in two days is:\n\n$P(\\text{Sunny 2 days from now} = 0.9 \\times 0.9 + 0.1 \\times 0.5 = 0.81 + 0.05 = 0.86$\n\nSimilarly, the probability that it will be rainy is:\n\n$P(\\text{Rainy 2 days from now} = 0.1 \\times 0.5 + 0.9 \\times 0.1 = 0.05 + 0.09 = 0.14$\n\nIn linear algebra (transition matrices) these calculations correspond to all the permutations in transitions from one step to the next (sunny-to-sunny ($S_2S$), sunny-to-rainy ($S_2R$), rainy-to-sunny ($R_2S$) or rainy-to-rainy ($R_2R$)) with their calculated probabilities:\n\n\n\nOn the lower part of the image we see how to calculate the probability of a future state ($t+1$ or $t+2$) given the probabilities (probability mass function, $PMF$) for every state (sunny or rainy) at time zero (now or $t_0$) as simple matrix multiplication.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/12657/402101 \n On the lower part of the image we see how to calculate the probability of a future state ($t+1$ or $t+2$) given the probabilities (probability mass function, $PMF$) for every state (sunny or rainy) at time zero (now or $t_0$) as simple matrix multiplication.\n\nIf you keep forecasting weather like this you will notice that eventually the $n$-th day forecast, where $n$ is very large (say $30$), settles to the following 'equilibrium' probabilities:\n\n$P(\\text{Sunny}) = 0.833$\n\nand\n\n$P(\\text{Rainy}) = 0.167$\n\nIn other words, your forecast for the $n$-th day and the $n+1$-th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.\n\nThe above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' Markov chain (nice = transition probabilities satisfy conditions):\n\nIrrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.\n\nMarkov Chain Monte Carlo exploits the above feature as follows:\n\nWe want to generate random draws from a target distribution. We then identify a way to construct a 'nice' Markov chain such that its equilibrium probability distribution is our target distribution.\n\nIf we can construct such a chain then we arbitrarily start from some point and iterate the Markov chain many times (like how we forecast the weather $n$ times). Eventually, the draws we generate would appear as if they are coming from our target distribution.\n\nWe then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the Monte Carlo component.\n\nThere are several ways to construct 'nice' Markov chains (e.g., Gibbs sampler, Metropolis-Hastings algorithm).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/558950/402101 \n I'm taking a lot of my answer from Agresti's \"Categorical Data Analysis\".  For some context, let's say we want to predict if an email is spam or ham using a single binary variable, namely if the subject line contains the word \"Viagra\".  Let's call this predictor \n$x$\n. This is a simplification of our more general problem, but will suffice.\n\nUnder/Oversampling can be thought of as a case-control study.  Viewing the spam/ham problem as a case-control design, we would fix the marginal distribution of ham to spam (via oversampling in this case) and the outcome of the study would be if the email subject line had the word \"Viagra\".  Before oversampling, our estimate of \n$Pr(\\mbox{spam})=\\delta$\n (its just the proportion of our sample which is spam).  After oversampling, our estimate of \n$Pr(\\mbox{spam})=\\delta^\\star >\\delta$\n.  This will be important later.\n\nIn most other studies, we would want to know \n$Pr(y=\\mbox{spam} \\vert x)$\n. This is referred to as the \nconditional distribution\n of spam (conditional on \n$x$\n in this case).  However, because of the sampling design fixing the marginal distribution of ham/spam, we can't estimate the conditional distribution of spam, but we can estimate the conditional distribution of \n$x$\n,  \n$Pr(x \\vert y=\\mbox{spam})$\n.\n\nIn order to get the conditional distribution of spam, we would need to account for the prevalence of spam.  Following Lachin from chapter 5 of his book \nBiostatistical Methods: The Assessment of Relative Risks, 2nd Edition\n and an application of Bayes' Rule, the conditional distribution of spam would be calculated as\n\n$$Pr(\\mbox{spam} \\vert x) = \\dfrac{Pr(x\\vert \\mbox{spam})\\cdot \\delta}{Pr(x \\vert \\mbox{spam}) \\cdot \\delta + Pr(x \\vert \\mbox{ham}) \\cdot (1-\\delta)}$$\n\nCan you spot the problem now?\n\nHere is the problem: \nthe sampling design fixes the prevalence in our sample to be something else other than \n$\\delta$\n.  In essence, we have forced the prevalence to be \n$\\delta^\n\\star > \\delta$\n via oversampling.  Hence, any estimate of the risk of spam from the data we have using oversampling is biased precisely because the prevalence is biased by design.\n\ndoesn't that just mean it has learnt the wrong things during training\n\nSome of what you have learned would be wrong, but surprisingly not everything.  The prevalence would certainly be wrong, hence the estimated risk would be wrong, but the relationship between \n$x$\n and the risk of spam is unaffected.  From Agresti (edited to align with our example),",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/558950/402101 \n doesn't that just mean it has learnt the wrong things during training\n\nSome of what you have learned would be wrong, but surprisingly not everything.  The prevalence would certainly be wrong, hence the estimated risk would be wrong, but the relationship between \n$x$\n and the risk of spam is unaffected.  From Agresti (edited to align with our example),\n\nWe can find the odds ratio, however, because it treats the variables symmetrically, taking the same value using the conditional distribution of [\n$x$\n] given [spam] as it does using the conditional distribution of [spam] given [\n$x$\n].\n\nSo our model would learn the correct relationships between inputs and outputs, but the probabilities would be biased.\n\nLet's make this more concrete by modelling \n$Pr(\\mbox{spam} \\vert x)$\n with a logistic regression.  Our model would be\n\n$$ \\operatorname{logit}(Pr(\\mbox{spam} \\vert x)) = \\beta_0 + \\beta_1 x$$\n\nIf we were to run a case-control study on the spam/ham problem, \n$\\beta_0$\n would be biased, but not \n$\\beta_1$\n.  Its easy to demonstrate this too via simulation.  I will simulate data from the model\n\n$$ \\operatorname{logit}(p) = -2.2 + 0.46x $$\n\nand upsample the minority class.  Then, I will compute the difference between the estimated coefficients.  I'll do this 1000 times and plot a histogram of the differences.  We will see that \n$\\beta_1 - 0.46$\n will be centred around 0 (hence unbiased) whereas \n$\\beta_0 - (-2.2)$\n will not be centered around 0 (hence biased) due to the upsampling.  I've added a red line at the point of 0 difference for reference.\n\n\n\nBecause the intercept is biased, the entire risk estimate is biased.  Not performing the upsampling and fitting the model on the raw data fixes this bias (shown below, though it should be noted that the estimates are asymptotically unbiased, so we would need \nenough\n data for this to work).\n\n\n\nCode to reproduce the plots:\n\nlibrary(tidyverse)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/558950/402101 \n Because the intercept is biased, the entire risk estimate is biased.  Not performing the upsampling and fitting the model on the raw data fixes this bias (shown below, though it should be noted that the estimates are asymptotically unbiased, so we would need \nenough\n data for this to work).\n\n\n\nCode to reproduce the plots:\n\nlibrary(tidyverse)\n\nz = rerun(1000, {\n  # sample data to fit the model to\n  n = 1000\n  x = rbinom(n, 1, 0.5)\n  b0 = qlogis(0.1)\n  b1 = (qlogis(0.15) - qlogis(0.1))\n  b = c(b0, b1)\n  p = plogis(b0 + b1*x)\n  y = rbinom(n, 1, p)\n  \n  d = tibble(x, y)\n  \n  # upsample postivie cases\n  nsamp = length(y[y==0]) - length(y[y==1])\n  yd = filter(d, y==1) %>% \n       sample_n(size=nsamp, replace = T)\n  \n  newd = bind_rows(yd, d)\n  \n  # switch data to newd if you want to get the biased estimates\n  model = glm(y~x, data=d, family=binomial())\n  estbeta = coef(model)\n  tibble(coef = c('Intercept','slope'), difference = b - estbeta)\n})\n\n\nbind_rows(z) %>% \n  ggplot(aes(difference))+\n  geom_histogram(color = 'black', fill = 'dark gray')+\n  facet_wrap(~coef, ncol = 1)+\n  geom_vline(aes(xintercept=0), color = 'red')+\n  theme_light()+\n  theme(aspect.ratio = 1/1.61)\n\n```\n\nlibrary(tidyverse)\n\nz = rerun(1000, {\n  # sample data to fit the model to\n  n = 1000\n  x = rbinom(n, 1, 0.5)\n  b0 = qlogis(0.1)\n  b1 = (qlogis(0.15) - qlogis(0.1))\n  b = c(b0, b1)\n  p = plogis(b0 + b1*x)\n  y = rbinom(n, 1, p)\n  \n  d = tibble(x, y)\n  \n  # upsample postivie cases\n  nsamp = length(y[y==0]) - length(y[y==1])\n  yd = filter(d, y==1) %>% \n       sample_n(size=nsamp, replace = T)\n  \n  newd = bind_rows(yd, d)\n  \n  # switch data to newd if you want to get the biased estimates\n  model = glm(y~x, data=d, family=binomial())\n  estbeta = coef(model)\n  tibble(coef = c('Intercept','slope'), difference = b - estbeta)\n})\n\n\nbind_rows(z) %>% \n  ggplot(aes(difference))+\n  geom_histogram(color = 'black', fill = 'dark gray')+\n  facet_wrap(~coef, ncol = 1)+\n  geom_vline(aes(xintercept=0), color = 'red')+\n  theme_light()+\n  theme(aspect.ratio = 1/1.61)\n\n```",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86804/402101 \n If you work in terms of indicator variables (i.e. \n$Z_i = 1$\n if \n$X_i \\leq x$\n and \n$0$\n otherwise), you can directly apply the Central limit theorem to a mean of \n$Z$\n's, and by using the \nDelta method\n, turn that into an asymptotic normal distribution for \n$F_X^{-1}(\\bar{Z})$\n, which in turn means that you get asymptotic normality for fixed quantiles of \n$X$\n.\n\nSo not just the median, but quartiles, 90th percentiles, ... etc.\n\nLoosely, if we're talking about the \n$q$\nth  sample quantile in sufficiently large samples, we get that it will approximately have a normal distribution with mean the \n$q$\nth population quantile \n$x_q$\n and variance \n$q(1-q)/(nf_X(x_q)^2)$\n.\n\nHence for the median (\n$q = 1/2$\n), the variance in sufficiently large samples will be approximately \n$1/(4nf_X(\\tilde{\\mu})^2)$\n.\n\nYou need all the conditions along the way to hold, of course, so it doesn't work in all situations, but for continuous distributions where the density at the population quantile is positive and differentiable, etc, ...\n\nFurther, it doesn't hold for extreme quantiles, because the CLT doesn't kick in there (the average of Z's won't be asymptotically normal). You need different theory for extreme values.\n\nEdit: whuber's critique is correct; this would work if \n$x$\n were a population median rather than a sample median. The argument needs to be modified to actually work properly.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/196996/402101 \n I will try to add to the other answer. First, completeness is a technical condition which is justified mainly by the theorems that use it. So let us start with some related concepts and theorems where they occur.\n\nLet \n$X=(X_1,X_2,\\dotsc,X_n)$\n represent a vector of iid data, which we model as having a distribution \n$f(x;\\theta), \\theta \\in \\Theta$\n where the parameter \n$\\theta$\n governing the data is unknown. \n$T=T(X)$\n is \nsufficient\n if the conditional distribution of \n$X \\mid T$\n does not depend on the parameter \n$\\theta$\n.  \n$V=V(X)$\n is \nancillary\n if the distribution of \n$V$\n does not depend on \n$\\theta$\n (within the family \n$f(x;\\theta)$\n). \n$U=U(X)$\n is an \nunbiased estimator of zero\n if its expectation is zero, irrespective of \n$\\theta$\n. \n$S=S(X)$\n is a \ncomplete statistic\n if any unbiased estimator of zero based on \n$S$\n is identically zero, that is, if \n$\\DeclareMathOperator{\\E}{\\mathbb{E}} \\E g(S)=0 (\\text{for all $\\theta$})$\n then \n$g(S)=0$\n a.e. (for all \n$\\theta$\n).\n\nNow, suppose you have two different unbiased estimators of \n$\\theta$\n based on the sufficient statistic \n$T$\n, \n$g_1(T), g_2(T)$\n. That is, in symbols\n\n$$\n    \\E g_1(T)=\\theta ,\\\\\n     \\E g_2(T)=\\theta\n$$\n\nand \n$\\DeclareMathOperator{\\P}{\\mathbb{P}} \\P(g_1(T) \\not= g_2(T) ) > 0$\n (for all \n$\\theta$\n). Then \n$g_1(T)-g_2(T)$\n is an unbiased estimator of zero, which is not identically zero, proving  that \n$T$\n is not complete. So, completeness of an sufficient statistic \n$T$\n gives us that there exists only one unique unbiased estimator of \n$\\theta$\n based on \n$T$\n. That is already very close to the Lehmann\u2013Scheff\u00e9 theorem.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/196996/402101 \n $$\n    \\E g_1(T)=\\theta ,\\\\\n     \\E g_2(T)=\\theta\n$$\n\nand \n$\\DeclareMathOperator{\\P}{\\mathbb{P}} \\P(g_1(T) \\not= g_2(T) ) > 0$\n (for all \n$\\theta$\n). Then \n$g_1(T)-g_2(T)$\n is an unbiased estimator of zero, which is not identically zero, proving  that \n$T$\n is not complete. So, completeness of an sufficient statistic \n$T$\n gives us that there exists only one unique unbiased estimator of \n$\\theta$\n based on \n$T$\n. That is already very close to the Lehmann\u2013Scheff\u00e9 theorem.\n\nLet us look at some examples. Suppose \n$X_1, \\dotsc, X_n$\n now are iid uniform on the interval \n$(\\theta, \\theta+1)$\n. We can show that (\n$X_{(1)} < X_{(2)} < \\dotsm < X_{(n)}$\n are the order statistics) the pair \n$(X_{(1)}, X_{(n)})$\n is sufficient, but it is not complete, because the difference \n$X_{(n)}-X_{(1)}$\n is ancillary; we can compute its expectation, let it be \n$c$\n (which is a function of \n$n$\n only), and then \n$X_{(n)}-X_{(1)} -c$\n will be an unbiased estimator of zero which is not identically zero. So our sufficient statistic, in this case, is not complete and sufficient. And we can see what that means: there exist functions of the sufficient statistic which are not informative about \n$\\theta$\n (in the context of the model). This cannot happen with a complete sufficient statistic; it is in a sense maximally informative, in that no functions of it are uninformative. On the other hand, if there is some function of the minimally sufficient statistic that has expectation zero, that could be seen as a \nnoise term\n; disturbance/noise terms in models have expectation zero. So we could say that non-complete sufficient statistics \ndo contain some noise\n.\n\nLook again at the range \n$R=X_{(n)}-X_{(1)}$\n in this example. Since its distribution does not depend on \n$\\theta$\n, it doesn't \nby itself alone\n contain any information about \n$\\theta$\n. But, together with the sufficient statistic, it does! How? Look at the case where \n$R=1$\n is observed.Then, in the context of our (known to be true) model, we have perfect knowledge of \n$\\theta$\n! Namely, we can say with certainty that \n$\\theta = X_{(1)}$\n. You can check that any other value for \n$\\theta$\n then leads to either \n$X_{(1)}$\n or \n$X_{(n)}$\n being an impossible observation, under the assumed model. On the other hand, if we observe \n$R=0.1$\n, then the range of possible values for \n$\\theta$\n is rather large (exercise ...).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/196996/402101 \n In this sense, the ancillary statistic \n$R$\n does contain some information about the precision with which we can estimate \n$\\theta$\n based on this data and model. In this example, and others, the ancillary statistic \n$R$\n \"takes over the role of the sample size\". Usually, confidence intervals and such need the sample size \n$n$\n, but in this example, we can make a \nconditional confidence interval\n this is computed using only \n$R$\n, not \n$n$\n (exercise.)\nThis was an idea of Fisher, that inference should be conditional on some ancillary statistic.\n\nNow, Basu's theorem: If \n$T$\n is complete sufficient, then it is independent of any ancillary statistic. That is, inference based on a complete sufficient statistic is simpler, in that we do not need to consider conditional inference.\nConditioning on a statistic which is independent of \n$T$\n does not change anything, of course.\n\nThen, a last example to give some more intuition. Change our uniform distribution example to a uniform distribution on the interval \n$(\\theta_1, \\theta_2)$\n (with \n$\\theta_1<\\theta_2$\n).  In this case the statistic \n$(X_{(1)}, X_{(n)})$\n \nis\n complete and sufficient. What changed? We can see that completeness is really a property of the \nmodel\n. In the former case, we had a restricted parameter space. This restriction destroyed completeness by introducing relationships on the order statistics. By removing this restriction we got completeness! So, in a sense, lack of completeness means that the parameter space is not big enough, and by enlarging it we can hope to restore completeness (and thus, easier inference).\n\nSome other examples where lack of completeness is caused by restrictions on the parameter space,\n\nsee my answer to:  \nWhat kind of information is Fisher information?\n\n\n\n\nLet \n$X_1, \\dotsc, X_n$\n be iid \n$\\mathcal{Cauchy}(\\theta,\\sigma)$\n (a location-scale model). Then the order statistics are sufficient but not complete. But now enlarge this model to a fully nonparametric model, still iid but from some completely unspecified distribution \n$F$\n. Then the order statistics are sufficient and complete.\n\n\n\n\nFor exponential families with canonical parameter space (that is, as large as possible) the minimal sufficient statistic is also complete. But in many cases, introducing restrictions on the parameter space, as with \ncurved exponential families\n, destroys completeness.\n\nsee my answer to:  \nWhat kind of information is Fisher information?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/196996/402101 \n For exponential families with canonical parameter space (that is, as large as possible) the minimal sufficient statistic is also complete. But in many cases, introducing restrictions on the parameter space, as with \ncurved exponential families\n, destroys completeness.\n\nsee my answer to:  \nWhat kind of information is Fisher information?\n\nLet \n$X_1, \\dotsc, X_n$\n be iid \n$\\mathcal{Cauchy}(\\theta,\\sigma)$\n (a location-scale model). Then the order statistics are sufficient but not complete. But now enlarge this model to a fully nonparametric model, still iid but from some completely unspecified distribution \n$F$\n. Then the order statistics are sufficient and complete.\n\nFor exponential families with canonical parameter space (that is, as large as possible) the minimal sufficient statistic is also complete. But in many cases, introducing restrictions on the parameter space, as with \ncurved exponential families\n, destroys completeness.\n\nA very relevant paper is \nLehmann (1981), \nJ. Am. Stat. Assoc.\n, \n76\n, 374, \"An Interpretation of Completeness and\nBasu's Theorem\".",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7792/402101 \n For a univariate KDE, you are better off using something other than Silverman's rule which is based on a normal approximation. One excellent approach is the Sheather-Jones method, easily implemented in R; for example,\n\nplot(density(precip, bw=\"SJ\"))\n\nplot(density(precip, bw=\"SJ\"))\n\nThe situation for multivariate KDE is not so well studied, and the tools are not so mature. Rather than a bandwidth, you need a bandwidth matrix. To simplify the problem, most people assume a diagonal matrix, although this may not lead to the best results. The \nks package in R\n provides some very useful tools including allowing a full (not necessarily diagonal) bandwidth matrix.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6072/402101 \n Balance in the Training Set\n\nFor logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward:   Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept.  Details are in \nKing and Zeng (2001)\n [\nPDF\n].\n\nThese 'rare event corrections' were designed for case control research designs, mostly used in epidemiology, that select cases by choosing a fixed, usually balanced number of 0 cases and 1 cases, and then need to correct for the resulting sample selection bias.  Indeed, you might train your classifier the same way.  Pick a nice balanced sample and then correct the intercept to take into account the fact that you've selected on the dependent variable to learn more about rarer classes than a random sample would be able to tell you.\n\nMaking Predictions\n\nOn a related but distinct topic: Don't forget that you should be thresholding intelligently to make predictions.  It is not always best to predict 1 when the model probability is greater 0.5.  Another threshold may be better.  To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/69856/402101 \n You have defined causality incorrectly, yes.  Probably, you have heard the saying \"correlation isn't causation.\"  You have essentially defined causality as correlation.  The problem is worse than that, though.  Causality is not a statistical or probabilistic concept at all, at least as those topics are normally taught.  There is no statistical or probabilistic definition of causality: nothing involving conditional expectations or conditional distributions or suchlike.  It is hard to pick up this fact from courses in statistics or econometrics, though.\n\nUnfortunately, we tend to do a better job saying what causality isn't than what causality is.  Causality always and everywhere comes from theory, from a priori reasoning, from assumptions.  You mentioned econometrics.  If you have been taught instrumental variables competently, then you know that causal effects can only be measured if you have an \"exclusion restriction.\"  And you know that exclusion restrictions always come from theory.\n\nYou said you wanted math, though.  The guy you want to read is \nJudea Pearl\n.  It's not easy math, and the math sometimes wanders off into philosophy, but that's because causality is a hard subject.  Here is \na page\n with more links on the subject.  Here is \na free online book\n I just came across.  Finally, here is \na previous question\n where I gave an answer you might find useful.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32385/402101 \n To complete the answer to the question, Ocram nicely addressed standard error but did not contrast it to standard deviation and did not mention the dependence on sample size.  As a special case for the estimator consider the sample mean.  The standard error for the mean is $\\sigma \\, / \\, \\sqrt{n}$ where $\\sigma$ is the population standard deviation.  So in this example we see explicitly how the standard error decreases with increasing sample size. The standard deviation is most often used to refer to the individual observations. So standard deviation describes the variability of the individual observations while standard error shows the variability of the estimator. Good estimators are consistent which means that they converge to the true parameter value. When their standard error decreases to 0 as the sample size increases the estimators are consistent which in most cases happens because the standard error goes to 0 as we see explicitly with the sample mean.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/164188/402101 \n [The following perhaps seems a little technical because of the use of equations but it builds mainly on the arrow charts to provide the intuition which only requires very basic understanding of OLS - so don't be repulsed.]\n\nSuppose you want to estimate the causal effect of $x_i$ on $y_i$ given by the estimated coefficient for $\\beta$, but for some reason there is a correlation between your explanatory variable and the error term:\n\n$$\\begin{matrix}y_i &=& \\alpha &+& \\beta x_i &+&   \\epsilon_i & \\\\ & && & & \\hspace{-1cm}\\nwarrow & \\hspace{-0.8cm} \\nearrow \\\\ &  & & & & corr &   \\end{matrix}$$\n\nThis might have happened because we forgot to include an important variable that also correlates with $x_i$. This problem is known as omitted variable bias and then your $\\widehat{\\beta}$ will not give you the causal effect (see \nhere\n for the details). This is a case when you would want to use an instrument because only then can you find the true causal effect.\n\nAn instrument is a new variable $z_i$ which is uncorrelated with $\\epsilon_i$, but that correlates well with $x_i$ and which only influences $y_i$ through $x_i$ - so our instrument is what is called \"exogenous\". It's like in this chart here:\n\n$$\\begin{matrix}\nz_i & \\rightarrow & x_i & \\rightarrow & y_i \\newline\n  &   & \\uparrow & \\nearrow & \\newline\n & & \\epsilon_i &  \n\\end{matrix}$$\n\nSo how do we use this new variable?\n\nMaybe you remember the ANOVA type idea behind regression where you split the total variation of a dependent variable into an explained and an unexplained component. For example, if you regress your $x_i$ on the instrument,\n\n$$\\underbrace{x_i}_{\\text{total variation}} = \\underbrace{a \\quad + \\quad \\pi z_i}_{\\text{explained variation}} \\quad + \\underbrace{\\eta_i}_{\\text{unexplained variation}}$$\n\nthen you know that the explained variation here is exogenous to our original equation because it depends on the exogenous variable $z_i$ only. So in this sense, we split our $x_i$ up into a part that we can claim is certainly exogenous (that's the part that depends on $z_i$) and some unexplained part $\\eta_i$ that keeps all the bad variation which correlates with $\\epsilon_i$. Now we take the exogenous part of this regression, call it $\\widehat{x_i}$,\n\n$$x_i \\quad = \\underbrace{a \\quad + \\quad \\pi z_i}_{\\text{good variation} \\: = \\: \\widehat{x}_i } \\quad + \\underbrace{\\eta_i}_{\\text{bad variation}}$$\n\nand put this into our original regression:\n$$y_i = \\alpha + \\beta \\widehat{x}_i + \\epsilon_i$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/164188/402101 \n $$x_i \\quad = \\underbrace{a \\quad + \\quad \\pi z_i}_{\\text{good variation} \\: = \\: \\widehat{x}_i } \\quad + \\underbrace{\\eta_i}_{\\text{bad variation}}$$\n\nand put this into our original regression:\n$$y_i = \\alpha + \\beta \\widehat{x}_i + \\epsilon_i$$\n\nNow since $\\widehat{x}_i$ is not correlated anymore with $\\epsilon_i$ (remember, we \"filtered out\" this part from $x_i$ and left it in $\\eta_i$), we can consistently estimate our $\\beta$ because the instrument has helped us to break the correlation between the explanatory variably and the error. This was one way how you can apply instrumental variables. This method is actually called 2-stage least squares, where our regression of $x_i$ on $z_i$ is called the \"first stage\" and the last equation here is called the \"second stage\".\n\nIn terms of our original picture (I leave out the $\\epsilon_i$ to not make a mess but remember that it is there!), instead of taking the direct but flawed route between $x_i$ to $y_i$ we took an intermediate step via $\\widehat{x}_i$\n\n$$\\begin{matrix}\n& &  &     &             & \\widehat{x}_i \\newline\n& &  &     &    \\nearrow         & \\downarrow \\newline\n& z_i & \\rightarrow  & x_i & \\rightarrow & y_i \n\\end{matrix}$$\n\nThanks to this slight diversion of our road to the causal effect we were able to consistently estimate $\\beta$ by using the instrument. The cost of this diversion is that instrumental variables models are generally less precise, meaning that they tend to have larger standard errors.\n\nHow do we find instruments?\n\nThat's not an easy question because you need to make a good case as to why your $z_i$ would not be correlated with $\\epsilon_i$ - this cannot be tested formally because the true error is unobserved. The main challenge is therefore to come up with something that can be plausibly seen as exogenous such as natural disasters, policy changes, or sometimes you can even run a randomized experiment. The other answers had some very good examples for this so I won't repeat this part.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/70910/402101 \n A square matrix is singular, that is, its \ndeterminant\n is zero, if it contains rows or columns which are proportionally interrelated; in other words, one or more of its rows (columns) is exactly expressible as a linear combination of all or some other its rows (columns), the combination being without a constant term.\n\nImagine, for example, a \n$3 \\times 3$\n matrix \n$A$\n - symmetric, like correlaton matrix, or asymmetric. If in terms of its entries it appears that \n$\\text {col}_3 = 2.15 \\cdot \\text {col}_1$\n for example, then the matrix \n$A$\n is singular. If, as another example, its \n$\\text{row}_2 = 1.6 \\cdot \\text{row}_1 - 4 \\cdot \\text{row}_3$\n, then \n$A$\n is again singular. As a particular case, if any row contains just \nzeros\n, the matrix is also singular because any column then is a linear combination of the other columns. In general, if any row (column) of a square matrix is a weighted sum of the other rows (columns), then any of the latter is also a weighted sum of the other rows (columns).\n\nSingular or near-singular matrix is often referred to as \"ill-conditioned\" matrix because it delivers problems in many statistical data analyses.\n\nWhat must multivariate \ndata\n look like in order for its correlation or covariance matrix to be a singular matrix as described above? It is when there is linear interdependances among the variables. If some variable is an exact linear combination of the other variables, with constant term allowed, the correlation and covariance matrces of the variables will be singular. The dependency observed in such matrix between its columns is actually that \nsame\n dependency as the dependency between the variables in the data observed after the variables have been centered (their means brought to 0) or standardized (if we mean correlation rather than covariance matrix).\n\nSome frequent \nparticular\n situations when the correlation/covariance matrix of variables is singular: (1) Number of variables is equal or greater than the number of cases; (2) Two or more variables sum up to a constant; (3) Two variables are identical or differ merely in mean (level) or variance (scale).\n\nAlso, duplicating observations in a dataset will lead the matrix towards singularity. The more times you clone a case the closer is singularity. So, when doing some sort of imputation of missing values it is always beneficial (from both statistical and mathematical view) to add some noise to the imputed data.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/70910/402101 \n Also, duplicating observations in a dataset will lead the matrix towards singularity. The more times you clone a case the closer is singularity. So, when doing some sort of imputation of missing values it is always beneficial (from both statistical and mathematical view) to add some noise to the imputed data.\n\nIn geometrical viewpoint, singularity is \n(multi)collinearity\n (or \"complanarity\"): variables displayed as vectors (arrows) in space lie in the space of dimentionality lesser than the number of variables - in a reduced space. (That dimensionality is known as the \nrank\n of the matrix; it is equal to the number of non-zero \neigenvalues\n of the matrix.)\n\nIn a more distant or \"transcendental\" geometrical view, singularity or zero-definiteness (presense of zero eigenvalue) is the bending point between positive definiteness and non-positive definiteness of a matrix. When some of the vectors-variables (which \nis\n the correlation/covariance matrix) \"go beyond\" lying even in the reduced euclidean space - so that they cannot \"converge in\" or \"perfectly span\" \neuclidean\n space anymore, non-positive definiteness appears, i.e. some eigenvalues of the correlation matrix become negative. (See about non-positive definite matrix, aka non-gramian \nhere\n.) Non-positive definite matrix is also \"ill-conditioned\" for some kinds of statistical analysis.\n\nThe first picture below shows a \nnormal regression situation\n with two predictors (we'll speek of linear regression). The picture is copied from \nhere\n where it is explained in more details. In short, moderately correlated (= having acute angle between them) predictors \n$X_1$\n and \n$X_2$\n span 2-dimesional space \"plane X\". The dependent variable \n$Y$\n is projected onto it orthogonally, leaving the predicted variable \n$Y'$\n and the residuals with st. deviation equal to the length of \n$e$\n. R-square of the regression is the angle between \n$Y$\n and \n$Y'$\n, and the two regression coefficients are directly related to the skew coordinates \n$b_1$\n and \n$b_2$\n, respectively.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/70910/402101 \n The picture below shows regression situation with \ncompletely collinear\n predictors. \n$X_1$\n and \n$X_2$\n correlate perfectly and therefore these two vectors coincide and form the line, a 1-dimensional space. This is a reduced space. Mathematically though, plane X \nmust\n exist in order to solve regression with two predictors, - but the plane is not defined anymore, alas. Fortunately, if we \ndrop\n any one of the two collinear predictors out of analysis the regression is then simply solved because one-predictor regression needs one-dimensional predictor space. We see prediction \n$Y'$\n and error \n$e$\n of that (one-predictor) regression, drawn on the picture. There exist other approaches as well, besides dropping variables, to get rid of collinearity.\n\n\n\nThe final picture below displays a situation with \nnearly collinear\n predictors. This situation is different and a bit more complex and nasty. \n$X_1$\n and \n$X_2$\n (both shown again in blue) tightly correlate and thence almost coincide. But there is still a tiny angle between, and because of the non-zero angle, plane X is defined (this plane on the picture looks like the plane on the first picture). So, \nmathematically\n there is no problem to solve the regression. The problem which arises here is a \nstatistical\n one.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/70910/402101 \n Usually we do regression to infer about the R-square and the coefficients in the population. From sample to sample, data varies a bit. So, if we took another sample, the juxtaposition of the two predictor vectors would change slightly, which is normal. Not \"normal\" is that under near collinearity it leads to devastating consequences. Imagine that \n$X_1$\n deviated just a little down, beyond plane X - as shown by grey vector. \nBecause\n the angle between the two predictors was so small, plane X which will come through \n$X_2$\n and through that drifted \n$X_1$\n will \ndrastically\n diverge from old plane X. Thus, because \n$X_1$\n and \n$X_2$\n are so much correlated we expect very different plane X in different samples from the same population. As plane X is different, predictions, R-square, residuals, coefficients - everything become different, too. It is well seen on the picture, where plane X swung somewhere 40 degrees. In a situation like that, estimates (coefficients, R-square etc.) are very \nunreliable\n which fact is expressed by their huge standard errors. And in contrast, with predictors far from collinear, estimates are reliable because the space spanned by the predictors is robust to those sampling fluctuations of data.\n\nEven a high correlation between two variables, if it is below 1, doesn't necessarily make the whole correlation matrix singular; it depends on the rest correlations as well. For example this correlation matrix:\n\n1.000     .990     .200\n .990    1.000     .100\n .200     .100    1.000\n\n1.000     .990     .200\n .990    1.000     .100\n .200     .100    1.000\n\nhas determinant \n.00950\n which is yet enough different from 0 to be considered eligible in many statistical analyses. But this matrix:\n\n.00950\n\n1.000     .990     .239\n .990    1.000     .100\n .239     .100    1.000\n\n1.000     .990     .239\n .990    1.000     .100\n .239     .100    1.000\n\nhas determinant \n.00010\n, a degree closer to 0.\n\n.00010\n\nStatistical data analyses, such as regressions, incorporate special indices and tools to detect collinearity strong enough to consider dropping some of the variables or cases from the analysis, or to undertake other healing means. Please search (including this site) for \"collinearity diagnostics\", \"multicollinearity\", \"singularity/collinearity tolerance\", \"condition indices\", \"variance decomposition proportions\", \"variance inflation factors (VIF)\".",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/107903/402101 \n I often hear the claim that Bayesian statistics can be highly subjective.\n\nSo do I.  But notice that there's a major ambiguity in calling something subjective.\n\nSubjectivity (both senses)\n\nSubjective can mean (at least) one of\n\ndepends on the idiosyncracies of the researcher\n\n\nexplicitly concerned with the state of knowledge of an individual\n\nBayesianism is subjective in the second sense because it is always offering a way to update beliefs represented by probability distributions by conditioning on information.  (Note that whether those beliefs are beliefs that some subject actually has or just beliefs that a subject \ncould\n have is irrelevant to deciding whether it is 'subjective'.)\n\nThe main argument being that inference depends on the choice of a prior\n\nActually, if a prior represents your personal belief about something then you almost certainly didn't \nchoose\n it at any more than you chose most of your beliefs.  And if it represents somebody's beliefs then it can be a more or less accurate representation of those beliefs, so ironically there will be a rather 'objective' fact about how well it represents them.\n\n(even though one could use the principle of indifference o maximum entropy to choose a prior).\n\nOne could, though this doesn't tend to generalize very smoothly to continuous domains.  Also, arguably it's impossible to be flat or 'indifferent' in all parameterisations at once (though I've never been quite sure why you'd want to be).\n\nIn comparison, the claim goes, frequentist statistics is in general more objective. How much truth is there in this statement?\n\nSo how might we evaluate this claim?\n\nI suggest that in the second second sense of subjective: it's mostly correct.  And in the first sense of subjective: it's probably false.\n\nFrequentism as subjective (second sense)\n\nSome historical detail is helpful to map the issues\n\nFor Neyman and Pearson there is only inductive \nbehaviour\n not inductive \ninference\n and all statistical evaluation works with long run sampling properties of estimators.  (Hence alpha and power analysis, but not p values).  That's pretty unsubjective in both senses.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/107903/402101 \n Frequentism as subjective (second sense)\n\nSome historical detail is helpful to map the issues\n\nFor Neyman and Pearson there is only inductive \nbehaviour\n not inductive \ninference\n and all statistical evaluation works with long run sampling properties of estimators.  (Hence alpha and power analysis, but not p values).  That's pretty unsubjective in both senses.\n\nIndeed it's possible, and I think quite reasonable, to argue along these lines that Frequentism is actually not an inference framework at all but rather a collection of \nevaluation criteria\n for all  possible inference procedures that emphasises their behaviour in repeated application.  Simple examples would be consistency, unbiasedness, etc.  This makes it obviously unsubjective in sense 2.  However, it also risks being subjective in sense 1 when we have to decide what to do when those crteria do not apply (e.g. when there isn't an unbiased estimator to be had) or when they apply but contradict.\n\nFisher offered a less unsubjective Frequentism that is interesting.  For Fisher, there is such a thing as inductive inference, in the sense that a subject, the scientist, makes inferences on the basis of a data analysis, done by the statistician. (Hence p-values but not alpha and power analysis).  However, the decisions about how to behave, whether to carry on with research etc. are made by the scientist on the basis of her understanding of domain theory, not by the statistician applying the inference paradigm.  Because of this Fisherian division of labour, both the subjectiveness (sense 2) and the individual subject (sense 1) sit on the science side, not the statistical side.\n\nLegalistically speaking, the Fisher's Frequentism \nis\n subjective. It's just that the subject who is subjective is not the statistician.\n\nThere are various syntheses of these available, both the barely coherent mix of these two you find in applied statistics textbooks and more nuanced versions, e.g. the 'Error Statistics' pushed by Deborah Mayo.  This latter is pretty unsubjective in sense 2, but highly subjective in sense 1, because the researcher has to use scientific judgement - Fisher style - to figure out what error probabilities matter and shoudl be tested.\n\nFrequentism as subjective (first sense)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/107903/402101 \n There are various syntheses of these available, both the barely coherent mix of these two you find in applied statistics textbooks and more nuanced versions, e.g. the 'Error Statistics' pushed by Deborah Mayo.  This latter is pretty unsubjective in sense 2, but highly subjective in sense 1, because the researcher has to use scientific judgement - Fisher style - to figure out what error probabilities matter and shoudl be tested.\n\nFrequentism as subjective (first sense)\n\nSo is Frequentism less subjective in the first sense?  It depends.  Any inference procedure can be riddled with idiosyncracies as actually applied.  So perhaps it's more useful to ask whether Frequentism \nencourages\n a less subjective (first sense) approach?  I doubt it - I think the self conscious application of subjective (second sense) methods leads to less subjective (first sense) outcomes, but it can be argued either way.\n\nAssume for a moment that subjectiveness (first sense) sneaks into an analysis via 'choices'.  Bayesianism does seem to involve more 'choices'.  In the simplest case the choices tally up as: one set of potentially idiosyncratic assumptions for the Frequentist (the Likelihood function or equivalent) and two sets for the Bayesian (the Likelihood and a prior over the unknowns).\n\nHowever, Bayesians \nknow\n they're being subjective (in the second sense) about all these choices so they are liable to be more self conscious about the implications which should lead to less subjectiveness (in the first sense).\n\nIn contrast, if one looks up a test in a big book of tests, then one could get the feeling that the result is less subjective (first sense), but arguably that's a result of substituting some other subject's understanding of the problem for one's own.  It's not clear that one has gotten less subjective this way, but it might feel that way.  I think most would agree that that's unhelpful.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/132832/402101 \n AUC = Area Under the Curve.\n\n\nAUROC = \nArea Under the Receiver Operating Characteristic curve\n.\n\nAUC is used most of the time to mean AUROC, which is a bad practice since as Marc Claesen pointed out AUC is ambiguous (could be any curve) while AUROC is not.\n\nThe AUROC has \nseveral equivalent interpretations\n:\n\nThe expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\n\n\nThe expected proportion of positives ranked before a uniformly drawn random negative.\n\n\nThe expected true positive rate if the ranking is split just before a uniformly drawn random negative.\n\n\nThe expected proportion of negatives ranked after a uniformly drawn random positive.\n\n\nThe expected false positive rate if the ranking is split just after a uniformly drawn random positive.\n\nGoing further: \nHow to derive the probabilistic interpretation of the AUROC?\n\nAssume we have a probabilistic, binary classifier such as logistic regression.\n\nBefore presenting the ROC curve (= Receiver Operating Characteristic curve), the concept of \nconfusion matrix\n must be understood. When we make a binary prediction, there can be 4 types of outcomes:\n\nWe predict 0 while the true class is actually 0: this is called a \nTrue Negative\n, i.e. we correctly predict that the class is negative (0). For example, an antivirus did not detect a harmless file as a virus .\n\n\nWe predict 0 while the true class is actually 1: this is called a \nFalse Negative\n, i.e. we incorrectly predict that the class is negative (0). For example, an antivirus failed to detect a virus.\n\n\nWe predict 1 while the true class is actually 0: this is called a \nFalse Positive\n, i.e. we incorrectly predict that the class is positive (1). For example, an antivirus considered a harmless file to be a virus.\n\n\nWe predict 1 while the true class is actually 1: this is called a \nTrue Positive\n, i.e. we correctly predict that the class is positive (1). For example, an antivirus rightfully detected a virus.\n\nTo get the confusion matrix, we go over all the predictions made by the model, and count how many times each of those 4 types of outcomes occur:\n\n\n\nIn this example of a confusion matrix, among the 50 data points that are classified, 45 are correctly classified and the 5 are misclassified.\n\nSince to compare two different models it is often more convenient to have a single metric rather than several ones, we compute two metrics from the confusion matrix, which we will later combine into one:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/132832/402101 \n To get the confusion matrix, we go over all the predictions made by the model, and count how many times each of those 4 types of outcomes occur:\n\n\n\nIn this example of a confusion matrix, among the 50 data points that are classified, 45 are correctly classified and the 5 are misclassified.\n\nSince to compare two different models it is often more convenient to have a single metric rather than several ones, we compute two metrics from the confusion matrix, which we will later combine into one:\n\nTrue positive rate\n (\nTPR\n), aka. sensitivity, \nhit rate\n, and \nrecall\n, which is defined as \n$ \\frac{TP}{TP+FN}$\n. Intuitively this metric corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points. In other words, the higher TPR, the fewer positive data points we will miss.\n\n\nFalse positive rate\n (\nFPR\n), aka. \nfall-out\n, which is defined as \n$ \\frac{FP}{FP+TN}$\n. Intuitively this metric corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.  In other words, the higher FPR, the more negative data points will be missclassified.\n\nTo combine the FPR and the TPR into one single metric, we first compute the two former metrics with many different threshold (for example \n$0.00; 0.01, 0.02, \\dots, 1.00$\n) for the logistic regression, then plot them on a single graph, with the FPR values on the abscissa and the TPR values on the ordinate. The resulting curve is called ROC curve, and the metric we consider is the AUC of this curve, which we call AUROC.\n\nThe following figure shows the AUROC graphically:\n\n\n\nIn this figure, the blue area corresponds to the Area Under the curve of the Receiver Operating Characteristic (AUROC). The dashed line in the diagonal we present the ROC curve of a random predictor: it has an AUROC of 0.5. The random predictor is commonly used as a baseline to see whether the model is useful.\n\nIf you want to get some first-hand experience:\n\nPython: \nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n\n\nMATLAB: \nhttp://www.mathworks.com/help/stats/perfcurve.html",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/179551/402101 \n Correlation measures linear association between two given variables and it has no obligation to detect any other form of association else.\n\nSo those two variables might be associated in several other non-linear ways and correlation could not distinguish from independent case.\n\nAs a very didactic, artificial and non realistic example, one can consider $X$  such that $P(X=x)=1/3$ for $x=-1, 0, 1$ and $Y=X^2$. Notice that they are not only associated, but one is a function of the other. Nonetheless, their correlation is 0, for their association is orthogonal to the association that correlation can detect.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/211311/402101 \n According to the paper \n\"First (?) Occurrence of Common Terms in Mathematical Statistics\"\n by H.A. David, the first use of the word 'moment' in this situation was in a 1893 letter to \nNature\n by Karl Pearson entitled \n\"Asymmetrical Frequency Curves\"\n.\n\nNeyman's 1938 \nBiometrika\n paper \n\"A Historical Note on Karl Pearson's Deduction of the Moments of the Binomial\"\n gives a good synopsis of the letter and Pearson's subsequent work on moments of the binomial distribution and the method of moments.  It's a really good read.  Hopefully you have access JSTOR for I don't have the time now to give a good summary of the paper (though I will this weekend).  Though I will mention one piece that may give insight as to why the term 'moment' was used.  From Neyman's paper:\n\nIt [Pearson's memoir] deals primarily with methods of approximating \n      continuous frequency curves by means of some processes involving the \n      calculation of easy formulae.  One of these formulae considered was the \n      \"point-binomial\" or the \"binomial with loaded ordinates\".  The formula\n\n      differs from what to-day we call a binomial, viz. (4), only by a factor \n      $\\alpha$, representing the area under the continuous curve which it is desired\n      to fit.\n\nThis is what eventually led to the 'method of moments.' Neyman goes over the Pearson's derivation of the binomial moments in the above paper.\n\nAnd from Pearson's letter:\n\nWe shall now proceed to find the first four moments of the system of \n      rectangles round GN. If the inertia of each rectangle might be considered \n      as concentrated along its mid vertical, we should have for the $s^{\\text{th}}$ moment\n      round NG, writing $d = c(1 + nq)$.\n\nThis hints at the fact that Pearson used the term 'moment' as an allusion to \n'moment of inertia,'\n a term common in physics.\n\nHere's a scan of most of Pearson's \nNature\n letter:\n\n\n\n\n\nYou can view the entire article on page 615 \nhere\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/26238/402101 \n You can find everything \nhere\n. However, here is a brief answer.\n\nLet $\\mu$ and $\\sigma^2$ be the mean and the variance of interest; you wish to estimate $\\sigma^2$ based on a sample of size $n$.\n\nNow, let us say you use the following estimator:\n\n$S^2 = \\frac{1}{n} \\sum_{i=1}^n (X_{i} - \\bar{X})^2$,\n\nwhere $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ is the estimator of $\\mu$.\n\nIt is not too difficult (see footnote) to see that $E[S^2] = \\frac{n-1}{n}\\sigma^2$.\n\nSince $E[S^2] \\neq \\sigma^2$, the estimator $S^2$ is said to be biased.\n\nBut, observe that $E[\\frac{n}{n-1} S^2] = \\sigma^2$. Therefore $\\tilde{S}^2 = \\frac{n}{n-1} S^2$ is an unbiased estimator of $\\sigma^2$.\n\nFootnote\n\nStart by writing $(X_i - \\bar{X})^2 = ((X_i - \\mu) + (\\mu - \\bar{X}))^2$ and then expand the product...\n\nEdit to account for your comments\n\nThe expected value of $S^2$ does not give $\\sigma^2$ (and hence $S^2$ is biased) but it turns out you can transform $S^2$ into $\\tilde{S}^2$ so that the expectation does give $\\sigma^2$.\n\nIn practice, one often prefers to work with $\\tilde{S}^2$ instead of $S^2$. But, if $n$ is large enough, this is not a big issue since $\\frac{n}{n-1} \\approx 1$.\n\nRemark\n Note that unbiasedness is a property of an estimator, not of an expectation as you wrote.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2522/402101 \n It is not true.  If the null hypothesis is true then it will not be rejected more frequently at large sample sizes than small. There is an erroneous rejection rate that's usually set to 0.05 (alpha) but it is independent of sample size. Therefore, taken literally the statement is false. Nevertheless, it's possible that in some situations (even whole fields) all nulls are false and therefore all will be rejected if N is high enough. But is this a bad thing?\n\nWhat is true is that trivially small effects can be found to be \"significant\" with very large sample sizes.  That does not suggest that you shouldn't have such large samples sizes.  What it means is that the way you interpret your finding is dependent upon the effect size and sensitivity of the test.  If you have a very small effect size and highly sensitive test you have to recognize that the statistically significant finding may not be meaningful or useful.\n\nGiven some people don't believe that a test of the null hypothesis, when the null is \ntrue\n, always has an error rate equal to the cutoff point selected for any sample size, here's a simple simulation in \nR\n proving the point. Make N as large as you like and the rate of Type I errors will remain constant.\n\nR\n\n# number of subjects in each condition\nn <- 100\n# number of replications of the study in order to check the Type I error rate\nnsamp <- 10000\n\nps <- replicate(nsamp, {\n    # population mean = 0, sd = 1 for both samples, therefore, no real effect\n    y1 <- rnorm(n, 0, 1) \n    y2 <- rnorm(n, 0, 1)\n    tt <- t.test(y1, y2, var.equal = TRUE)\n    tt$p.value\n})\nsum(ps < .05) / nsamp\n\n# ~ .05 no matter how big n is. Note particularly that it is \n# not an increasing value always finding effects when n is very large.\n\n# number of subjects in each condition\nn <- 100\n# number of replications of the study in order to check the Type I error rate\nnsamp <- 10000\n\nps <- replicate(nsamp, {\n    # population mean = 0, sd = 1 for both samples, therefore, no real effect\n    y1 <- rnorm(n, 0, 1) \n    y2 <- rnorm(n, 0, 1)\n    tt <- t.test(y1, y2, var.equal = TRUE)\n    tt$p.value\n})\nsum(ps < .05) / nsamp\n\n# ~ .05 no matter how big n is. Note particularly that it is \n# not an increasing value always finding effects when n is very large.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/103725/402101 \n Here are some links which may interest you comparing frequentist and Bayesian methods:\n\nhttp://www.stat.ufl.edu/archived/casella/Talks/BayesRefresher.pdf\n\n\n\n\nArchived here: \nhttps://web.archive.org/web/20140308021414/https://stat.ufl.edu/archived/casella/Talks/BayesRefresher.pdf\n\n\n\n\nhttp://www.bayesian-inference.com/advantagesbayesian\n\n\nhttp://www.researchgate.net/post/Bayesian_vs_frequentist_statistics2\n\nArchived here: \nhttps://web.archive.org/web/20140308021414/https://stat.ufl.edu/archived/casella/Talks/BayesRefresher.pdf\n\nIn a nutshell, the way I have understood it, given a specific set of data, the frequentist believes that there is a true, underlying distribution from which said data was generated. The inability to get the exact parameters is a function of finite sample size. The Bayesian, on the other hand, think that we start with some assumption about the parameters (even if unknowingly) and use the data to refine our opinion about those parameters. Both are trying to develop a model which can explain the observations and make predictions; the difference is in the assumptions (both actual and philosophical). As a pithy, non-rigorous, statement, one can say the frequentist believes that the parameters are fixed and the data is random; the Bayesian believes the data is fixed and the parameters are random. Which is better or preferable? To answer that you have to dig in and realize just \nwhat\n assumptions each entails (e.g. are parameters asymptotically normal?).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/370378/402101 \n A physical, intuitive model of a random variable\n is to write down the name of every member of a population on one or more slips of paper--\"tickets\"--and put those tickets into a box. The process of thoroughly mixing the contents of the box, followed by blindly pulling out one ticket--exactly as in a lottery--models randomness.  Non-uniform probabilities are modeled by introducing variable numbers of tickets in the box: more tickets for the more probable members, fewer for the less probable.\n\nA \nrandom variable\n is a number associated with each member of the population. (Therefore, for consistency, every ticket for a given member has to have the same number written on it.)  Multiple random variables are modeled by reserving spaces on the tickets for more than one number.  We usually give those spaces names like $X,$ $Y,$ and $Z$.  The \nsum\n of those random variables is the usual sum: reserve a new space on every ticket for the sum, read off the values of $X,$ $Y,$ \netc.\n on each ticket, and write their sum in that new space.  This is a consistent way of writing numbers on the tickets, so it's another random variable.\n\n\n\nThis figure portrays a box representing a population $\\Omega=\\{\\alpha,\\beta,\\gamma\\}$ and three random variables $X$, $Y$, and $X+Y$.  It contains six tickets: the three for $\\alpha$ (blue) give it a probability of $3/6$, the two for $\\beta$ (yellow) give it a probability of $2/6$, and the one for $\\gamma$ (green) give it a probability of $1/6$.  In order to display what is written on the tickets, they are shown before being mixed.\n\nThe beauty of this approach\n is that all the paradoxical parts of the question turn out to be correct:\n\nthe sum of random variables is indeed a single, definite number (for each member of the population), \n\n\nyet it also leads to a distribution (given by the frequencies with which the sum appears in the box), and \n\n\nit still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).\n\nthe sum of random variables is indeed a single, definite number (for each member of the population),\n\nyet it also leads to a distribution (given by the frequencies with which the sum appears in the box), and\n\nit still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/370378/402101 \n it still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).\n\nthe sum of random variables is indeed a single, definite number (for each member of the population),\n\nyet it also leads to a distribution (given by the frequencies with which the sum appears in the box), and\n\nit still effectively models a \nrandom\n process (because the tickets are still blindly drawn from the box).\n\nIn this fashion the sum can simultaneously have a definite value (given by the rules of addition as applied to numbers on each of the tickets) while the \nrealization\n--which will be a ticket drawn from the box--does not have a value until it is carried out.\n\nThis physical model of drawing tickets from a box is adopted in the theoretical literature and made rigorous with the definitions of sample space (the population), sigma algebras (with their associated probability measures), and random variables as measurable functions defined on the sample space.\n\nThis account of random variables is elaborated, with realistic examples, at \n\"What is meant by a random variable?\"\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/56884/402101 \n If all the assumptions hold and you have the correct form for $R^2$ then the usual F statistic can be computed as $F = \\frac{ R^2 }{ 1- R^2} \\times \\frac{ \\text{df}_2 }{ \\text{df}_1 }$.  This value can then be compared to the appropriate F distribution to do an F test.  This can be derived/confirmed with basic algebra.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/71979/402101 \n Coming from a behavioural sciences background, I associate this terminology particularly with introductory statistics textbooks. In this context the distinction is that :\n\nDescriptive statistics\n  are functions of  the sample data that are intrinsically interesting in describing some feature of the data. Classic descriptive statistics include mean, min, max, standard deviation, median, skew, kurtosis.\n\n\nInferential statistics\n are a function of the sample data that assists you to draw an inference regarding an hypothesis about a population parameter. Classic inferential statistics include z, t, $\\chi^2$, F-ratio, etc.\n\nThe important point is that any statistic, inferential or descriptive, is a function of the sample data. A parameter is a function of the population, where the term population is the same as saying the underlying data generating process.\n\nFrom this perspective the status of a given function of the data as a descriptive or inferential statistic depends on the purpose for which you are using it.\n\nThat said, some statistics are clearly more useful in describing  relevant features of the data, and some are well suited to aiding inference.\n\nInferential statistics:\n  Standard test statistics like t and z, for a given data generating process, where the null hypothesis is false, the expected value is strongly influenced by sample size. Most researchers would not see such statistics as estimating a population parameter of intrinsic interest.\n\n\nDescriptive statistics\n: In contrast descriptive statistics do estimate population parameters that are typically of intrinsic interest. For example the sample mean and standard deviation provide estimates of the equivalent population parameters. Even descriptive statistics like the minimum and maximum provide information about equivalent or similar population parameters, although of course in this case, much more care is required. Furthermore, many descriptive statistics might be biased or otherwise less than ideal estimators. However, they still have some utility in estimating a population parameter of interest.\n\nSo from this perspective, the important things to understand are:\n\nstatistic\n: function of the sample data\n\n\nparameter\n: function of the population (data generating process)\n\n\nestimator\n: function of the sample data used to provide an estimate of a parameter\n\n\ninference\n: process of reaching a conclusion about a parameter",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/71979/402101 \n So from this perspective, the important things to understand are:\n\nstatistic\n: function of the sample data\n\n\nparameter\n: function of the population (data generating process)\n\n\nestimator\n: function of the sample data used to provide an estimate of a parameter\n\n\ninference\n: process of reaching a conclusion about a parameter\n\nThus, you could either define the distinction between descriptive and inferential based on the intention of the researcher using the statistic, or you could define a statistic based on how it is typically used.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/30909/402101 \n They mainly differ in the link function.\n\nIn Logit:\n\n$\\Pr(Y=1 \\mid X) = [1 + e^{-X'\\beta}]^{-1} $\n\nIn Probit:\n\n$\\Pr(Y=1 \\mid X) = \\Phi(X'\\beta)$\n   (Cumulative standard normal pdf)\n\nIn other way, logistic has slightly flatter tails. i.e the probit curve approaches the axes more quickly than the logit curve.\n\nLogit has easier interpretation than probit. Logistic regression can be interpreted as modelling log odds (i.e those who smoke >25 cigarettes a day are 6 times more likely to die before 65 years of age). Usually people start the modelling with logit. You could use the likelihood value of each model to decide for logit vs probit.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/5427/402101 \n This is a good question.\n\nHere are some common pitfalls:\n\nUsing standard likelihood theory, we may derive a test to compare two nested\nhypotheses, $H_0$ and $H_1$, by computing the likelihood ratio test statistic. The null distribution of this test statistic is approximately chi-squared with degrees of freedom equal to difference in the dimensions of the two parameters spaces. Unfortunately, this test is only approximate and requires several assumptions. One crucial assumption is that the parameters under the null are not on the boundary of the parameter space. Since we are often interested in testing hypotheses about the random effects that take the form:\n$$H_0: \\sigma^2=0$$\r\n\nThis a real concern.\n The way to get around this problem is using REML. But still, the p-values will tend to be larger than they should be. This means that if you observe a significant effect using the \u03c72 approximation, you can be fairly confident that it is actually significant. Small, but not significant, p-values might spur one to use more accurate, but time-consuming, bootstrap methods.\n\n\nComparing fixed effects: If you plan to use the likelihood ratio test to compare two nested models that differ only in their fixed effects, you cannot use the REML estimation method. The reason is that REML estimates the random effects by considering linear combinations of the data that remove the fixed effects. If these fixed effects are changed, the likelihoods of the two models will not be directly comparable.\n\n\nP-values: The p-values generated by the likelihood ratio test for fixed effects are approximate and unfortunately tend to be too small, thereby sometimes overstating the importance of some effects. We may use \nnonparametric\n bootstrap methods to find more accurate p-values for the likelihood ratio test.\n\n\nThere are other concerns about p-values for the fixed effects test which are highlighted by Dr. Doug Bates [\nhere\n].",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/5427/402101 \n P-values: The p-values generated by the likelihood ratio test for fixed effects are approximate and unfortunately tend to be too small, thereby sometimes overstating the importance of some effects. We may use \nnonparametric\n bootstrap methods to find more accurate p-values for the likelihood ratio test.\n\n\nThere are other concerns about p-values for the fixed effects test which are highlighted by Dr. Doug Bates [\nhere\n].\n\nUsing standard likelihood theory, we may derive a test to compare two nested\nhypotheses, $H_0$ and $H_1$, by computing the likelihood ratio test statistic. The null distribution of this test statistic is approximately chi-squared with degrees of freedom equal to difference in the dimensions of the two parameters spaces. Unfortunately, this test is only approximate and requires several assumptions. One crucial assumption is that the parameters under the null are not on the boundary of the parameter space. Since we are often interested in testing hypotheses about the random effects that take the form:\n$$H_0: \\sigma^2=0$$\r\n\nThis a real concern.\n The way to get around this problem is using REML. But still, the p-values will tend to be larger than they should be. This means that if you observe a significant effect using the \u03c72 approximation, you can be fairly confident that it is actually significant. Small, but not significant, p-values might spur one to use more accurate, but time-consuming, bootstrap methods.\n\nComparing fixed effects: If you plan to use the likelihood ratio test to compare two nested models that differ only in their fixed effects, you cannot use the REML estimation method. The reason is that REML estimates the random effects by considering linear combinations of the data that remove the fixed effects. If these fixed effects are changed, the likelihoods of the two models will not be directly comparable.\n\nP-values: The p-values generated by the likelihood ratio test for fixed effects are approximate and unfortunately tend to be too small, thereby sometimes overstating the importance of some effects. We may use \nnonparametric\n bootstrap methods to find more accurate p-values for the likelihood ratio test.\n\nThere are other concerns about p-values for the fixed effects test which are highlighted by Dr. Doug Bates [\nhere\n].\n\nI am sure other members of the forum will have better answers.\n\nSource: Extending linear models with R -- Dr. Julain Faraway.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/134732/402101 \n How can a random number converge to a constant?\n\nLet's say you have $N$ balls in the box. You can pick them one by one. After you picked $k$ balls, I ask you: what's the mean weight of the balls in the box? Your best answer would be $\\bar x_k=\\frac{1}{k}\\sum_{i=1}^kx_i$. You realize that $\\bar x_k$ itself is the random value? It depends on which $k$ balls you picked first.\n\nNow, if you keep pulling the balls, at some point there'll be no balls left in the box, and you'll get $\\bar x_N\\equiv\\mu$.\n\nSo, what we've got is the random sequence $$\\bar x_1,\\dots,\\bar x_k, \\dots, \\bar x_N ,\\bar x_N, \\bar x_N, \\dots $$ which converges to the constant $\\bar x_N = \\mu$. So, the key to understanding your issue with convergence in probability is realizing that we're talking about \na sequence of random variables, constructed in a certain way\n.\n\nNext, let's get uniform random numbers $e_1,e_2,\\dots$, where $e_i\\in [0,1]$. Let's look at the random sequence $\\xi_1,\\xi_2,\\dots$, where $\\xi_k=\\frac{1}{\\sqrt{\\frac{k}{12}}}\\sum_{i=1}^k \\left(e_i- \\frac{1}{2} \\right)$. The $\\xi_k$ is a random value, because all its terms are random values. We can't predict what is $\\xi_k$ going to be. However, it turns out that we can claim that the probability distributions of $\\xi_k$ will look more and more like the standard normal $\\mathcal{N}(0,1)$. That's how the distributions converge.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123010/402101 \n The \"frequency\" is the number of observations per \"cycle\" (normally a year, but sometimes a week, a day, an hour, etc). This is the opposite of the definition of frequency in physics, or in Fourier analysis, where \"period\" is the length of the cycle, and \"frequency\" is the inverse of period. When using the \nts()\n function in R, the following choices should be used.\n\nts()\n\nData      frequency\nAnnual     1\nQuarterly  4\nMonthly   12\nWeekly    52\n\nData      frequency\nAnnual     1\nQuarterly  4\nMonthly   12\nWeekly    52\n\nActually, there are not 52 weeks in a year, but 365.25/7 = 52.18 on average. But most functions which use \nts\n objects require integer frequency.\n\nts\n\nOnce the frequency of observations is smaller than a week, then there is usually more than one way of handling the frequency. For example, data observed every minute might have an hourly seasonality (frequency=60), a daily seasonality (frequency=24x60=1440), a weekly seasonality (frequency=24x60x7=10080) and an annual seasonality (frequency=24x60x365.25=525960). If you want to use a \nts\n object, then you need to decide which of these is the most important.\n\nts\n\nAn alternative is to use a \nmsts\n object (defined in the \nforecast\n package) which handles multiple seasonality time series. Then you can specify all the frequencies that might be relevant. It is also flexible enough to handle non-integer frequencies.\n\nmsts\n\nforecast\n\nYou won't necessarily want to include all of these frequencies --- just the ones that are likely to be present in the data. As you have only 180 days of data, you can probably ignore the annual seasonality. If the data are measurements of a natural phenomenon (e.g., temperature), you might also be able to ignore the weekly seasonality.\n\nWith multiple seasonalities, you could use a TBATS model, or Fourier terms in a regression or ARIMA model. The \nfourier\n function from the forecast package will handle \nmsts\n objects.\n\nfourier\n\nmsts",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/430444/402101 \n The \"generalized variance inflation factors\" (GVIF) implemented in the \nvif()\n function of the R \ncar\n package\n were designed by \nFox and Monette\n specifically to handle situations like this, where there are groups of predictor variables that should be considered together rather than separately. Such situations include multi-level categorical variables and polynomial terms in a single variable.\n\nvif()\n\ncar\n\nThe standard VIF calculation described on the \nWikipedia page\n (and evidently as implemented in the Python \nvariance_inflation_factor()\n function) treats each predictor separately. A \n$k$\n-level categorical variable then counts as \n$k-1$\n predictors, and the result of that type of VIF calculation will depend on how that variable is coded, specifically which category is considered the reference level. Allison alluded to that in the post you linked, recommending use of the most frequent category as the reference when performing that type of VIF calculation.\n\nvariance_inflation_factor()\n\nThe GVIF approach provides a combined measure of collinearity for each group of predictors that should be considered together, like each of your multi-level categorical variables. It does this in a way that is independent of the details of how those predictors are coded. The \nGVIF^(1/(2*Df))\n calculation then provides comparability among predictor sets having different dimensions.\n\nGVIF^(1/(2*Df))\n\nSo in your case the GVIF approach is most applicable, and there seems to be no substantial evidence of multicollinearity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n Causal Inference is an important topic in statistics generally, for both observational research and controlled experiments such as clinical trials.\n\nA \nDAG\n is a \nD\nirected \nA\ncyclic \nG\nraph.\n\nA \u201c\nG\nraph\u201d is a structure with nodes (which are usually variables in statistics) and arcs (lines) connecting nodes to other nodes. \u201c\nD\nirected\u201d means that all the arcs have a direction, where one end of the arc has an arrow head, and the other does not, which usually refers to causation. \u201c\nA\ncyclic\u201d means that the graph is not cyclic \u2013 that means there can be no path from any node that leads back to the same node.  In statistics a DAG is a very powerful tool to aid in causal inference \u2013 to estimate the causal effect of one variable (often called the main exposure) on another (often called the outcome) in the presence of other variables which may be competing exposures, confounders or mediators.  The DAG can be used to identify a minimal sufficient set of variables to be used in a multivariable regression model for the estimation of said causal effect.  For example it is usually a very bad idea to condition on a mediator (a variable that lies on the causal path between the main exposure and the outcome), while it is usually a very good idea to condition on a confounder (a variable that is a cause, or a proxy for a cause, of both the main exposure and the outcome). It is also a bad idea to condition on a collider (to be defined below).\n\nBut first, what is the problem we want to overcome?  This is what a multivariable regression model looks like to your statistical software:\n\n\n\nThe software does not \u201cknow\u201d which variables are our main exposure, competing exposures, confounders or mediators. It treats them all the same. In the real world it is far more common for the variables to be inter-related. For example, knowledge of the particular area of research may indicate a structure such as:\n\n\n\nIt is the responsibility of the researchers to determine the causal pathways by utilising their expertise in the relevant subject matter.   DAGs are graphical representations of a collection of causal ideas that are typically abstracted and relevant to certain causal relationships. It is not unusual for one researcher's DAG to differ from another researcher's DAG. Similarly, a researcher may develop several DAGs for the same study design. Employing DAGs in a systematic manner, as I will try to show below, serves as a means to acquire knowledge or substantiate a certain theory.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n Let\u2019s suppose that our primary interest is in the causal effect of \n$X7$\n on \n$Y$\n. What are we to do? A very naive approach is simply to put all the variables into a regression model, and take the estimated coefficient for \n$X7$\n as our \u201canswer\u201d. This would be a big mistake. It turns out that the \nonly\n variable that should be adjusted for in this DAG is \n$X3$\n, because it is a confounder.  But what if our interest was in the effect of \n$X3$\n, not \n$X7$\n ? Do we simply use the same model (also containing \n$X7$\n) and just take the estimate of \n$X3$\n as our \u201canswer\u201d? No! In this case, we do not adjust for \n$X7$\n because it is a mediator. No adjustment is needed at all. In both cases, we may also adjust for \n$X1$\n because this is a competing exposure and will improve the precision of our casual inferences in both models. In both models we should not adjust for \n$X2$\n, \n$X4$\n, \n$X5$\n and \n$X6$\n because all of them are mediators for the effect of \n$X7$\n on \n$Y$\n.\n\nSo, getting back to the question, how do DAGs actually enable us to do this?  First we need to establish a few ground truths.\n\nA collider is a variable which has more than 1 cause \u2013 that is, at least 2 arrows are pointing at it (hence the incoming arrows \u201ccollide\u201d). \n$X5$\n in the above DAG is a collider\n\n\n\n\nIf there are no variables being conditioned on, a path is blocked if and only if it contains a collider. The path \n$X4 \\rightarrow X5 \\leftarrow X6$\n is blocked by the collider \n$X5$\n.\n\n\nNote: when we talk about \"conditioning\" on a variable this could refer to a few things, for example stratifying, but perhaps more commonly including the variable as a covariate in a multivariable regression model. Other synonymous terms are \"controlling for\" and \"adjusting for\".\n\n\n\n\nAny path that contains a non-collider that has been conditioned on is blocked. The path \n$Y \\leftarrow X3 \\rightarrow X7$\n will be blocked if we condition on \n$X3$\n.\n\n\n\n\nA collider (or a descendant of a collider) that has been conditioned on does not block a path. If we condition on \n$X5$\n we will open the path \n$X4 \\rightarrow X5 \\leftarrow X6$\n\n\n\n\nA backdoor path is a non-causal path between an outcome and a cause. It is non-causal because it contains an arrow pointing at both the cause and the outcome. For example the path \n$Y \\leftarrow X3 \\rightarrow X7$\n is a backdoor path from \n$Y$\n to \n$X3$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n A collider (or a descendant of a collider) that has been conditioned on does not block a path. If we condition on \n$X5$\n we will open the path \n$X4 \\rightarrow X5 \\leftarrow X6$\n\n\n\n\nA backdoor path is a non-causal path between an outcome and a cause. It is non-causal because it contains an arrow pointing at both the cause and the outcome. For example the path \n$Y \\leftarrow X3 \\rightarrow X7$\n is a backdoor path from \n$Y$\n to \n$X3$\n.\n\n\n\n\nConfounding of a causal path occurs where a common cause for both variables is present. In other words confounding occurs where an unblocked backdoor path is present. Again, \n$Y \\leftarrow X3 \\rightarrow X7$\n is such a path.\n\nA collider is a variable which has more than 1 cause \u2013 that is, at least 2 arrows are pointing at it (hence the incoming arrows \u201ccollide\u201d). \n$X5$\n in the above DAG is a collider\n\nIf there are no variables being conditioned on, a path is blocked if and only if it contains a collider. The path \n$X4 \\rightarrow X5 \\leftarrow X6$\n is blocked by the collider \n$X5$\n.\n\nNote: when we talk about \"conditioning\" on a variable this could refer to a few things, for example stratifying, but perhaps more commonly including the variable as a covariate in a multivariable regression model. Other synonymous terms are \"controlling for\" and \"adjusting for\".\n\nAny path that contains a non-collider that has been conditioned on is blocked. The path \n$Y \\leftarrow X3 \\rightarrow X7$\n will be blocked if we condition on \n$X3$\n.\n\nA collider (or a descendant of a collider) that has been conditioned on does not block a path. If we condition on \n$X5$\n we will open the path \n$X4 \\rightarrow X5 \\leftarrow X6$\n\nA backdoor path is a non-causal path between an outcome and a cause. It is non-causal because it contains an arrow pointing at both the cause and the outcome. For example the path \n$Y \\leftarrow X3 \\rightarrow X7$\n is a backdoor path from \n$Y$\n to \n$X3$\n.\n\nConfounding of a causal path occurs where a common cause for both variables is present. In other words confounding occurs where an unblocked backdoor path is present. Again, \n$Y \\leftarrow X3 \\rightarrow X7$\n is such a path.\n\nSo, armed with this knowledge, let\u2019s see how DAGs help us with removing bias:\n\nConfounding\n\nThe definition of confounding is 6 above.  If we apply 4 and condition on the confounder we will block the backdoor path from the outcome to the cause, thereby removing confounding bias.  The example is the association of carrying a lighter and lung cancer:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n So, armed with this knowledge, let\u2019s see how DAGs help us with removing bias:\n\nConfounding\n\nThe definition of confounding is 6 above.  If we apply 4 and condition on the confounder we will block the backdoor path from the outcome to the cause, thereby removing confounding bias.  The example is the association of carrying a lighter and lung cancer:\n\n\n\nCarrying a lighter has no causal effect on lung cancer, however, they share a common cause - smoking - so applying rule 5 above, a backdoor path from Lung cancer to carrying a lighter is present which induces an association between carrying a lighter and Lung cancer. Conditioning on Smoking will remove this association, which can be demonstrate with a simple simulation where I use continuous variables for simplicity:\n\n> set.seed(15)\n> N       <- 100\n> Smoking <- rnorm(N, 10, 2)\n> Cancer  <- Smoking + rnorm(N)\n> Lighter <- Smoking + rnorm(N)\n\n> summary(lm(Cancer ~ Lighter)) \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.66263    0.76079   0.871    0.386    \nLighter      0.91076    0.07217  12.620   <2e-16 ***\n\n> set.seed(15)\n> N       <- 100\n> Smoking <- rnorm(N, 10, 2)\n> Cancer  <- Smoking + rnorm(N)\n> Lighter <- Smoking + rnorm(N)\n\n> summary(lm(Cancer ~ Lighter)) \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.66263    0.76079   0.871    0.386    \nLighter      0.91076    0.07217  12.620   <2e-16 ***\n\nwhich shows the spurious association between Lighter and Cancer, but now when we condition on Smoking:\n\n> summary(lm(Cancer ~ Lighter + Smoking))  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.42978    0.60363  -0.712    0.478    \nLighter      0.07781    0.11627   0.669    0.505    \nSmoking      0.95215    0.11658   8.168 1.18e-12 ***\n\n> summary(lm(Cancer ~ Lighter + Smoking))  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.42978    0.60363  -0.712    0.478    \nLighter      0.07781    0.11627   0.669    0.505    \nSmoking      0.95215    0.11658   8.168 1.18e-12 ***\n\n...the bias is removed.\n\nMediation",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n > summary(lm(Cancer ~ Lighter + Smoking))  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.42978    0.60363  -0.712    0.478    \nLighter      0.07781    0.11627   0.669    0.505    \nSmoking      0.95215    0.11658   8.168 1.18e-12 ***\n\n...the bias is removed.\n\nMediation\n\nA mediator is a variable that lies on the causal path between the cause and the outcome.  This means that the outcome is a collider. Therefore, applying rule 3 means that we should not condition on the mediator otherwise the indirect effect of the cause on the outcome (i.e., that mediated by the mediator) will be blocked. A good example example is the grades of a student and their happiness. A mediating variable is self-esteem:\n\n\n\nHere, Grades has a direct effect on Happiness, but it also has an indirect effect mediated by self-esteem. We want to estimate the total causal effect of Grades on Happiness. Rule 3 says that a path that contains a non-collider that has been conditioned on is blocked. Since we want the total effect (i.e., including the indirect effect) we should not condition on self-esteem otherwise the mediated path will be blocked, as we can see in the following simulation:\n\n> set.seed(15)\n> N          <- 100\n> Grades     <- rnorm(N, 10, 2)\n> SelfEsteem <- Grades + rnorm(N)\n> Happiness  <- Grades + SelfEsteem + rnorm(N)\n\n> set.seed(15)\n> N          <- 100\n> Grades     <- rnorm(N, 10, 2)\n> SelfEsteem <- Grades + rnorm(N)\n> Happiness  <- Grades + SelfEsteem + rnorm(N)\n\nSo the total effect should be 2:\n\n> summary(m0 <- lm(Happiness ~ Grades)) # happy times\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.05650    0.79509   1.329    0.187    \nGrades       1.90003    0.07649  24.840   <2e-16 ***\n\n> summary(m0 <- lm(Happiness ~ Grades)) # happy times\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.05650    0.79509   1.329    0.187    \nGrades       1.90003    0.07649  24.840   <2e-16 ***\n\nwhich is what we do find. But if we now condition on self esteem:\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.39804    0.50783   2.753  0.00705 ** \nGrades       0.81917    0.10244   7.997 2.73e-12 ***\nSelfEsteem   1.05907    0.08826  11.999  < 2e-16 ***\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n which is what we do find. But if we now condition on self esteem:\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.39804    0.50783   2.753  0.00705 ** \nGrades       0.81917    0.10244   7.997 2.73e-12 ***\nSelfEsteem   1.05907    0.08826  11.999  < 2e-16 ***\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.39804    0.50783   2.753  0.00705 ** \nGrades       0.81917    0.10244   7.997 2.73e-12 ***\nSelfEsteem   1.05907    0.08826  11.999  < 2e-16 ***\n\nonly the direct effect for grades is estimated, due to blocking the indirect effect by conditioning on the mediator \nSelfEsteem\n.\n\nSelfEsteem\n\nCollider bias\n\nThis is probably the most difficult one to understand, but with the aid of a very simple DAG we can easily see the problem:\n\n\n\nHere, there is no causal path between X and Y. However, both cause C, the collider. If we condition on C, then applying rule 4 above we will invoke collider bias by opening up the (non causal) path between X, and Y. This may be a little hard to grasp at first, but it should become apparent by thinking in terms of equations. We have X + Y = C.  Let X and Y be binary variables taking the values 1 or zero. Hence, C can only take the values of 0, 1 or 2. Now, when we condition on C we fix its value. Say we fix it at 1. This immediately means that if X is zero then Y must be 1, and if Y is zero then X must be one. That is, X = -Y, so they are perfectly (negatively) correlated, conditional on C= 1. We can also see this in action with the following simulation:\n\n> set.seed(16)\n> N <- 100\n> X <- rnorm(N, 10, 2)\n> Y <- rnorm(N, 15, 3)\n> C <- X + Y + rnorm(N)\n\n> set.seed(16)\n> N <- 100\n> X <- rnorm(N, 10, 2)\n> Y <- rnorm(N, 15, 3)\n> C <- X + Y + rnorm(N)\n\nSo, X and Y are independent so we should find no association:\n\n> summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\n> summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\nand indeed no association is found. But now condition on C\n\n> summary(m1 <- lm(Y ~ X + C))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n > summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\n> summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\nand indeed no association is found. But now condition on C\n\n> summary(m1 <- lm(Y ~ X + C))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.10461    0.61206   1.805   0.0742 .  \nX           -0.92633    0.05435 -17.043   <2e-16 ***\nC            0.92454    0.02881  32.092   <2e-16 ***\n\n> summary(m1 <- lm(Y ~ X + C))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.10461    0.61206   1.805   0.0742 .  \nX           -0.92633    0.05435 -17.043   <2e-16 ***\nC            0.92454    0.02881  32.092   <2e-16 ***\n\nand now we have a spurious association between X and Y.\n\nNow let\u2019s consider a slightly more complex situation:\n\n\n\nHere we are interested in the causal effect of Activity on Cervical Cancer. Hypochondria is an unmeasured variable which is a psychological condition that is characterized by fears of minor and sometimes non-existent medical symptoms being an indication of major illness. Lesion is also an unobserved variable that indicates the presence of a pre-cancerous lesion. Test is a diagnostic test for early stage cervical cancer. Here we hypothesise that both the unmeasured variables affect Test, obviously in the case of Lesion, and by making frequent visits to the doctor in the case of Hypochondria. Lesion also (obviously causes Cancer) and Hypochondria causes more physical activity (because persons with hypochondria are worried about a sedentary lifestyle leading to disease in later life.\n\nFirst notice that if the collider, Test, was removed and replace with an arc either from Lesion to Hypochondria or vice versa, then our causal path of interest, Activity to Cancer, would be confounded, but due to rule 2 above, the collider blocks the backdoor path \n$\\text{Cancer}\\leftarrow \\text{Lesion} \\rightarrow \\text{Test} \\leftarrow \\text{Hypochondria} \\rightarrow \\text{Activity}$\n, as we can see with a simple simulation:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n First notice that if the collider, Test, was removed and replace with an arc either from Lesion to Hypochondria or vice versa, then our causal path of interest, Activity to Cancer, would be confounded, but due to rule 2 above, the collider blocks the backdoor path \n$\\text{Cancer}\\leftarrow \\text{Lesion} \\rightarrow \\text{Test} \\leftarrow \\text{Hypochondria} \\rightarrow \\text{Activity}$\n, as we can see with a simple simulation:\n\n> set.seed(16)\n> N            <- 100\n> Lesion       <- rnorm(N, 10, 2)\n> Hypochondria <- rnorm(N, 10, 2)\n> Test         <- Lesion + Hypochondria + rnorm(N)\n> Activity     <- Hypochondria + rnorm(N)\n> Cancer       <- Lesion + 0.25 * Activity + rnorm(N)\n\n> set.seed(16)\n> N            <- 100\n> Lesion       <- rnorm(N, 10, 2)\n> Hypochondria <- rnorm(N, 10, 2)\n> Test         <- Lesion + Hypochondria + rnorm(N)\n> Activity     <- Hypochondria + rnorm(N)\n> Cancer       <- Lesion + 0.25 * Activity + rnorm(N)\n\nwhere we hypothesize a much smaller effect of Activity on Cancer than Lesion on Cancer\n\n> summary(lm(Cancer ~ Activity))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.47570    1.01150  10.357   <2e-16 ***\nActivity     0.21103    0.09667   2.183   0.0314 *\n\n> summary(lm(Cancer ~ Activity))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.47570    1.01150  10.357   <2e-16 ***\nActivity     0.21103    0.09667   2.183   0.0314 *\n\nAnd indeed we obtain a reasonable estimate.\n\nNow, also observe the association of Activity and Cancer with Test (due to their common, but unmeasured causes:\n\n> cor(Test, Activity); cor(Test, Cancer)\n[1] 0.6245565\n[1] 0.7200811\n\n> cor(Test, Activity); cor(Test, Cancer)\n[1] 0.6245565\n[1] 0.7200811\n\nThe traditional definition of confounding is that a confounder is variable that is \nassociated\n with both the exposure and the outcome. So, we might mistakenly think that Test is a confounder and condition on it. However, we then open up the backdoor path \n$\\text{Cancer}\\leftarrow \\text{Lesion} \\rightarrow \\text{Test} \\leftarrow \\text{Hypochondria} \\rightarrow \\text{Activity}$\n, and introduce confounding which would otherwise not be present, as we can see from:\n\n> summary(lm(Cancer ~ Activity + Test))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.77204    0.98383   1.801   0.0748 .  \nActivity    -0.37663    0.07971  -4.725 7.78e-06 ***\nTest         0.72716    0.06160  11.804  < 2e-16 ***",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445606/402101 \n > summary(lm(Cancer ~ Activity + Test))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.77204    0.98383   1.801   0.0748 .  \nActivity    -0.37663    0.07971  -4.725 7.78e-06 ***\nTest         0.72716    0.06160  11.804  < 2e-16 ***\n\n> summary(lm(Cancer ~ Activity + Test))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.77204    0.98383   1.801   0.0748 .  \nActivity    -0.37663    0.07971  -4.725 7.78e-06 ***\nTest         0.72716    0.06160  11.804  < 2e-16 ***\n\nNow not only is the estimate for Activity biased, but it is of larger magnitude and of the opposite sign!\n\nSelection bias\n\nThe preceding example can also be used to demonstrate selection bias. A researcher may identify Test as a potential confounder, and then only conduct the analysis on those that have tested negative (or positive).\n\n> dtPos <- data.frame(Lesion, Hypochondria, Test, Activity, Cancer)\n> dtNeg <- dtPos[dtPos\n$Test <  22, ]\n> dtPos <- dtPos[dtPos$\nTest >= 22, ]\n> summary(lm(Cancer ~ Activity, data = dtPos))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.15915    3.07604   4.278 0.000242 ***\nActivity     0.08662    0.25074   0.345 0.732637\n\n> dtPos <- data.frame(Lesion, Hypochondria, Test, Activity, Cancer)\n> dtNeg <- dtPos[dtPos\n$Test <  22, ]\n> dtPos <- dtPos[dtPos$\nTest >= 22, ]\n> summary(lm(Cancer ~ Activity, data = dtPos))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.15915    3.07604   4.278 0.000242 ***\nActivity     0.08662    0.25074   0.345 0.732637\n\nSo for those that test positive we obtain a very small positive effect, that is not statistically significant at the 5% level\n\n> summary(lm(Cancer ~ Activity, data = dtNeg))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.18865    1.12071  10.876   <2e-16 ***\nActivity    -0.01553    0.11541  -0.135    0.893\n\n> summary(lm(Cancer ~ Activity, data = dtNeg))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.18865    1.12071  10.876   <2e-16 ***\nActivity    -0.01553    0.11541  -0.135    0.893\n\nAnd for those that test negative we obtain a very small negative association which is also not significant.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/103595/402101 \n (Answer partially updated in 2023.)\n\nThis is a very hard problem in general, though your variables are apparently only 1d so that helps. Of course, the first step (when possible) should be to plot the data and see if anything pops out at you; you're in 2d so this should be easy.\n\nHere are a few approaches that work in \n$\\mathbb{R}^d$\n or even more general settings, to match the general title of the question.\n\nOne general category is, related to the suggestion here, to estimate the mutual information:\n\nEstimate mutual information via entropies, as mentioned. In low dimensions with sufficient samples, histograms / KDE / nearest-neighbour estimators should work okay, but expect them to behave very poorly as the dimension increases. In particular, the following simple estimator has finite-sample bounds (compared to most approaches' asymptotic-only properties):\n\nSricharan, Raich, and Hero. \nEmpirical estimation of entropy functionals with confidence.\n arXiv:1012.4188 [math.ST]\n\nSimilar direct estimators of mutual information, e.g. the following based on nearest neighbours:\n\nP\u00e1l, P\u00f3czos, and Svepes\u00e1ri. \nEstimation of R\u00e9nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs\n, NeurIPS 2010.\n\nVariational estimators of mutual information, based on optimizing some function parameterized typically as a neural network; this is probably the \"default\" modern approach in high dimensions. The following paper gives a nice overview of the relationship between various estimators. Be aware, however, that these approaches are highly dependent on the neural network class and optimization scheme, and can have \nparticularly surprising behaviour in their bias/variance tradeoffs\n.\n\nPoole, Ozair, van den Oord, Alemi, and Tucker. \nOn Variational Bounds of Mutual Information\n, ICML 2019.\n\nThere are also other approaches, based on measures other than the mutual information.\n\nThe Schweizer-Wolff approach is a classic one based on copula transformations, and so is invariant to monotone increasing transformations. I'm not very familiar with this one, but I think it's computationally simpler but also maybe less powerful than most of the other approaches here. (I vaguely expect it can be framed as a special case of some of the other approaches but haven't really thought about it.)\n\nSchweizer and Wolff, \nOn Nonparametric Measures of Dependence for Random Variables\n, Annals of Statistics 1981.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/103595/402101 \n Schweizer and Wolff, \nOn Nonparametric Measures of Dependence for Random Variables\n, Annals of Statistics 1981.\n\nThe Hilbert-Schmidt independence criterion (HSIC): a kernel (in the sense of RKHS, not KDE)-based approach, based on measuring the norm of \n$\\operatorname{Cov}(\\phi(X), \\psi(Y))$\n for kernel features \n$\\phi$\n and \n$\\psi$\n.  In fact, the HSIC with kernels defined by a deep network is related to one of the more common variational estimators, InfoNCE; see \ndiscussion here\n.\n\nGretton, Bousqet, Smola, and Sch\u00f6lkopf, \nMeasuring Statistical Independence with Hilbert-Schmidt Norms\n, Algorithmic Learning Theory 2005.\n\nStatisticians are probably more familiar with the distance covariance/correlation as mentioned here previously; this is in fact \na special case\n of the HSIC with a particular choice of kernel, but that choice is maybe often a better kernel choice than the default Gaussian kernel typically used for HSIC.\n\nSz\u00e9kely, Rizzo, and Bakirov, \nMeasuring and testing dependence by correlation of distances\n, Annals of Statistics 2007.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18522/402101 \n You're going to have a little bit of trouble modeling a series with 2 levels of seasonality using an ARIMA model.  Getting this right is going highly dependent on setting things up correctly.  Have you considered a simple linear model yet?  They're a lot faster and easier to fit than ARIMA models, and if you use dummy variables for your different seasonality levels they are often quite accurate.\n\nI'm assuming you have hourly data, so make sure your TS object is setup with a frequency of 24.\n\n\nYou can model other levels of seasonality using dummy variables.  For example, you might want a set of 0/1 dummies representing the month of the year.\n\n\nInclude the dummy variables in the \nxreg\n argument, along with any covariates (like temperature).\n\n\nFit the model with the arima function in base R.  This function can handle ARMAX models through the use of the \nxreg\n argument.\n\n\nTry the \nArima\n and \nauto.arima\n functions in the forecast package.  auto.arima is nice because it will automatically find good parameters for your arima model. However, it will take FOREVER to fit on your dataset.\n\n\nTry the tslm function in the arima package, using dummy variables for each level of seasonality.  This will fit a lot faster than the Arima model, and may even work better in your situation.\n\n\nIf 4/5/6 don't work, THEN start worrying about transfer functions.  You have to crawl before you can walk.\n\n\nIf you are planning to forecast into the future, you will first need to forecast your xreg variables.  This is easy for seasonal dummies, but you'll have to think about how to make a good weather forecasts.  Maybe use the median of historical data?\n\nxreg\n\nxreg\n\nHere is an example of how I would approach this:\n\n#Setup a fake time series\nset.seed(1)\nlibrary(lubridate)\nindex <- ISOdatetime(2010,1,1,0,0,0)+1:8759*60*60\nmonth <- month(index)\nhour <- hour(index)\nusage <- 1000+10*rnorm(length(index))-25*(month-6)^2-(hour-12)^2\nusage <- ts(usage,frequency=24)\n\n#Create monthly dummies.  Add other xvars to this matrix\nxreg <- model.matrix(~as.factor(month))[,2:12]\ncolnames(xreg) <- c('Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')\n\n#Fit a model\nlibrary(forecast)\nmodel <- Arima(usage, order=c(0,0,0), seasonal=list(order=c(1,0,0), period=24), xreg=xreg)\nplot(usage)\nlines(fitted(model),col=2)\n\n#Benchmark against other models\nmodel2 <- tslm(usage~as.factor(month)+as.factor(hour))\nmodel3 <- tslm(usage~as.factor(month))\nmodel4 <- rep(mean(usage),length(usage))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18522/402101 \n #Fit a model\nlibrary(forecast)\nmodel <- Arima(usage, order=c(0,0,0), seasonal=list(order=c(1,0,0), period=24), xreg=xreg)\nplot(usage)\nlines(fitted(model),col=2)\n\n#Benchmark against other models\nmodel2 <- tslm(usage~as.factor(month)+as.factor(hour))\nmodel3 <- tslm(usage~as.factor(month))\nmodel4 <- rep(mean(usage),length(usage))\n\n#Compare the 4 models\nlibrary(plyr) #for rbind.fill\nACC <- rbind.fill(  data.frame(t(accuracy(model))),\n                    data.frame(t(accuracy(model2))),\n                    data.frame(t(accuracy(model3))),\n                    data.frame(t(accuracy(model4,usage)))\n                )\nACC <- round(ACC,2)\nACC <- cbind(Type=c('Arima','LM1','Monthly Mean','Mean'),ACC)\nACC[order(ACC$MAE),]\n\n#Setup a fake time series\nset.seed(1)\nlibrary(lubridate)\nindex <- ISOdatetime(2010,1,1,0,0,0)+1:8759*60*60\nmonth <- month(index)\nhour <- hour(index)\nusage <- 1000+10*rnorm(length(index))-25*(month-6)^2-(hour-12)^2\nusage <- ts(usage,frequency=24)\n\n#Create monthly dummies.  Add other xvars to this matrix\nxreg <- model.matrix(~as.factor(month))[,2:12]\ncolnames(xreg) <- c('Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')\n\n#Fit a model\nlibrary(forecast)\nmodel <- Arima(usage, order=c(0,0,0), seasonal=list(order=c(1,0,0), period=24), xreg=xreg)\nplot(usage)\nlines(fitted(model),col=2)\n\n#Benchmark against other models\nmodel2 <- tslm(usage~as.factor(month)+as.factor(hour))\nmodel3 <- tslm(usage~as.factor(month))\nmodel4 <- rep(mean(usage),length(usage))\n\n#Compare the 4 models\nlibrary(plyr) #for rbind.fill\nACC <- rbind.fill(  data.frame(t(accuracy(model))),\n                    data.frame(t(accuracy(model2))),\n                    data.frame(t(accuracy(model3))),\n                    data.frame(t(accuracy(model4,usage)))\n                )\nACC <- round(ACC,2)\nACC <- cbind(Type=c('Arima','LM1','Monthly Mean','Mean'),ACC)\nACC[order(ACC$MAE),]",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/155846/402101 \n Here's an algorithm to sample from an arbitrary mixture $f(x) = \\frac1N \\sum_{i=1}^N f_i(x)$:\n\nPick a mixture component $i$ uniformly at random.\n\n\nSample from $f_i$.\n\nIt should be clear that this produces an exact sample.\n\nA Gaussian kernel density estimate is a mixture $\\frac1N \\sum_{i=1}^N \\mathcal{N}(x; x_i, h^2)$. So you can take a sample of size $N$ by picking a bunch of $x_i$s and adding normal noise with zero mean and variance $h^2$ to it.\n\nYour code snippet is selecting a bunch of $x_i$s, but then it's doing something slightly different:\n\nchanging $x_i$ to\n$\n\\hat\\mu + \\frac{x_i - \\hat\\mu}{\\sqrt{1 + h^2 / \\hat\\sigma^2}}\n$\n\n\nadding zero-mean normal noise with variance $\\frac{h^2}{1 + h^2/\\hat\\sigma^2} = \\frac{1}{\\frac{1}{h^2} + \\frac{1}{\\hat\\sigma^2}}$, the harmonic mean of $h^2$ and $\\sigma^2$.\n\nWe can see that the expected value of a sample according to this procedure is\n$$\n\\frac1N \\sum_{i=1}^N \\frac{x_i}{\\sqrt{1 + h^2/\\hat\\sigma^2}}\n+ \\hat\\mu\n- \\frac{1}{\\sqrt{1 + h^2 /\\hat\\sigma^2}} \\hat\\mu\n= \\hat\\mu\n$$\nsince $\\hat\\mu = \\frac1N \\sum_{i=1}^N x_i$.\n\nI don't think the sampling disribution is the same, though.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/103719/402101 \n Here are some links which may interest you comparing frequentist and Bayesian methods:\n\nhttp://www.stat.ufl.edu/archived/casella/Talks/BayesRefresher.pdf\n\n\n\n\nArchived here: \nhttps://web.archive.org/web/20140308021414/https://stat.ufl.edu/archived/casella/Talks/BayesRefresher.pdf\n\n\n\n\nhttp://www.bayesian-inference.com/advantagesbayesian\n\n\nhttp://www.researchgate.net/post/Bayesian_vs_frequentist_statistics2\n\nArchived here: \nhttps://web.archive.org/web/20140308021414/https://stat.ufl.edu/archived/casella/Talks/BayesRefresher.pdf\n\nIn a nutshell, the way I have understood it, given a specific set of data, the frequentist believes that there is a true, underlying distribution from which said data was generated. The inability to get the exact parameters is a function of finite sample size. The Bayesian, on the other hand, think that we start with some assumption about the parameters (even if unknowingly) and use the data to refine our opinion about those parameters. Both are trying to develop a model which can explain the observations and make predictions; the difference is in the assumptions (both actual and philosophical). As a pithy, non-rigorous, statement, one can say the frequentist believes that the parameters are fixed and the data is random; the Bayesian believes the data is fixed and the parameters are random. Which is better or preferable? To answer that you have to dig in and realize just \nwhat\n assumptions each entails (e.g. are parameters asymptotically normal?).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/175265/402101 \n ANOVA and linear regression are equivalent when the two models test against the same hypotheses and use an identical encoding. The models differ in their basic aim: ANOVA is mostly concerned to present differences between categories' means in the data while linear regression is mostly concern to estimate a sample mean response and an associated $\\sigma^2$.\n\nSomewhat aphoristically one can describe ANOVA as a regression with dummy variables. We can easily see that this is the case in the simple regression with categorical variables. A categorical variable will be encoded as a indicator matrix (a matrix of \n0/1\n depending on whether a subject is part of a given group or not) and then used directly for the solution of the linear system described by a linear regression.\nLet's see an example with 5 groups. For the sake of argument I will assume that the mean of \ngroup1\n equals 1,  the mean of \ngroup2\n equals 2, ... and the mean of \ngroup5\n equals 5. (I use MATLAB, but the exact same thing is equivalent in R.)\n\n0/1\n\ngroup1\n\ngroup2\n\ngroup5\n\nrng(123);               % Fix the seed\nX = randi(5,100,1);     % Generate 100 random integer U[1,5]\nY = X + randn(100,1);   % Generate my response sample\nXcat = categorical(X);  % Treat the integers are categories\n\n% One-way ANOVA\n[anovaPval,anovatab,stats] = anova1(Y,Xcat);\n% Linear regression\nfitObj = fitlm(Xcat,Y);\n\n% Get the group means from the ANOVA\nANOVAgroupMeans = stats.means\n% ANOVAgroupMeans =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Get the beta coefficients from the linear regression\nLRbetas = [fitObj.Coefficients.Estimate'] \n% LRbetas =\n% 1.0953    0.7468    1.6398    3.1368    3.9565\n\n% Rescale the betas according the intercept\nscaledLRbetas = [LRbetas(1) LRbetas(1)+LRbetas(2:5)]\n% scaledLRbetas =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Check if the two results are numerically equivalent\nabs(max( scaledLRbetas - ANOVAgroupMeans)) \n% ans =\n% 2.6645e-15\n\nrng(123);               % Fix the seed\nX = randi(5,100,1);     % Generate 100 random integer U[1,5]\nY = X + randn(100,1);   % Generate my response sample\nXcat = categorical(X);  % Treat the integers are categories\n\n% One-way ANOVA\n[anovaPval,anovatab,stats] = anova1(Y,Xcat);\n% Linear regression\nfitObj = fitlm(Xcat,Y);\n\n% Get the group means from the ANOVA\nANOVAgroupMeans = stats.means\n% ANOVAgroupMeans =\n% 1.0953    1.8421    2.7350    4.2321    5.0517",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/175265/402101 \n rng(123);               % Fix the seed\nX = randi(5,100,1);     % Generate 100 random integer U[1,5]\nY = X + randn(100,1);   % Generate my response sample\nXcat = categorical(X);  % Treat the integers are categories\n\n% One-way ANOVA\n[anovaPval,anovatab,stats] = anova1(Y,Xcat);\n% Linear regression\nfitObj = fitlm(Xcat,Y);\n\n% Get the group means from the ANOVA\nANOVAgroupMeans = stats.means\n% ANOVAgroupMeans =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Get the beta coefficients from the linear regression\nLRbetas = [fitObj.Coefficients.Estimate'] \n% LRbetas =\n% 1.0953    0.7468    1.6398    3.1368    3.9565\n\n% Rescale the betas according the intercept\nscaledLRbetas = [LRbetas(1) LRbetas(1)+LRbetas(2:5)]\n% scaledLRbetas =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Check if the two results are numerically equivalent\nabs(max( scaledLRbetas - ANOVAgroupMeans)) \n% ans =\n% 2.6645e-15\n\nAs it can be seen in this scenario the results where exactly the same. The minute numerical difference is due to the design not being perfectly balanced as well as the underlaying estimation procedure; the ANOVA accumulates numerical errors a bit more aggressively. To that respect we fit an intercept, \nLRbetas(1)\n; we could fit an intercept-free model but that would not be a \"standard\" linear regression. (The results would be even closer to ANOVA in that case though.)\n\nLRbetas(1)\n\nThe $F$-statistic (a ratio of the means) in the case of the ANOVA and in the case of linear regression will be also be the same for the above example:\n\nabs( fitObj.anova.F(1) - anovatab{2,5} )\n% ans =\n% 2.9132e-13\n\nabs( fitObj.anova.F(1) - anovatab{2,5} )\n% ans =\n% 2.9132e-13\n\nThis is because procedures test the same hypothesis but with different wordings: ANOVA will qualitatively check if \"\nthe ratio is high enough to suggest that no grouping is implausible\n\" while linear regression will qualitatively check if \"\nthe ratio is high enough to suggest an intercept only model is possibly inadequate\n\".\n\n(This is a somewhat free interpretation of the \"\npossibility to see a value equal or greater than the one observed under the null hypothesis\n\" and it is not meant to be a text-book definition.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/175265/402101 \n (This is a somewhat free interpretation of the \"\npossibility to see a value equal or greater than the one observed under the null hypothesis\n\" and it is not meant to be a text-book definition.)\n\nComing back to the final part of your question about \"\nANOVA tell(ing) you nothing about the coefficients of the linear model (assuming the means are not equal\n\") I hope you can now see that the ANOVA, in the case that your design is simple/\nbalanced\n enough, tells you everything that a linear model would. The confidence intervals for group means will be the same you have for your $\\beta$, etc. Clearly when ones starts adding multiple covariate in his regression model, a simple one-way ANOVA does not have a direct equivalence. In that case one augments the information used to calculate the linear regression's mean response with information that are not directly available for a one way ANOVA. I believe that one can re-express things in ANOVA terms once more but it is mostly an academic exercise.\n\nAn interesting paper on the matter is Gelman's 2005 paper titled: \nAnalysis of Variance - Why it is more important than ever\n. Some important points raised; I am not fully supportive of the paper (I think I personally align much more with McCullach's view) but it can be a constructive read.\n\nAs a final note: The plot thickens when you have \nmixed effects models\n. There you have different concepts about what can be considered a nuisance or actual information regarding the grouping of your data. These issues are outside the scope of this question but I think they are worthy of a nod.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/13698/402101 \n There are several alternatives to \nStepwise Regression\n. The most used I have seen are:\n\nExpert opinion\n to decide which variables to include in the model.\n\n\nPartial Least Squares Regression\n. You essentially get latent variables and do a regression with them. You could also do \nPCA\n yourself and then use the principal variables.\n\n\nLeast Absolute Shrinkage and Selection Operator\n (LASSO).\n\nBoth \nPLS Regression\n and \nLASSO\n are implemented in R packages like\n\nPLS\n: \nhttp://cran.r-project.org/web/packages/pls/\n and\n\nLARS\n: \nhttp://cran.r-project.org/web/packages/lars/index.html\n\nIf you only want to \nexplore\n the relationship between your dependent variable and the independent variables (e.g. you do not need statistical significance tests), I would also recommend \nMachine Learning\n methods like \nRandom Forests\n or \nClassification/Regression Trees\n. \nRandom Forests\n can also approximate complex non-linear relationships between your dependent and independent variables, which might not have been revealed by linear techniques (like \nLinear Regression\n).\n\nA good starting point to \nMachine Learning\n might be the Machine Learning task view on CRAN:\n\nMachine Learning Task View\n: \nhttp://cran.r-project.org/web/views/MachineLearning.html",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/396681/402101 \n The problem is that there isn't really \na\n $R^2$ for logistic regression.  Instead there are \nmany\n different \"pseudo-$R^2$s\" that may be similar to the $R^2$ from a linear model in different ways.  You can get a list of some at UCLA's statistics help website \nhere\n.\n\nIn addition, the effect (e.g., odds ratio) of the added variable, $x_2$, isn't sufficient to determine your power to detect that effect.  It matters how $x_2$ is distributed:  The more widely spread the values are, the more powerful your test, even if the odds ratio is held constant.  It further matters what the correlation between $x_2$ and $x_1$ is:  The more correlated they are, the more data would be required to achieve the same power.\n\nAs a result of these facts, the way I try to calculate the power in these more complicated situations is to simulate.  In that vein, it may help you to read my answer here:  \nSimulation of logistic regression power analysis - designed experiments\n.\n\nLooking at G*Power's documentation, they use a method based on Hsieh, Bloch, & Larsen (1998).  The idea is that you first regress $x_2$ on $x_1$ (or whatever predictor variables went into the first model) using a linear regression.  You use the regular $R^2$ for that.  (That value should lie in the interval $[0,\\ 1]$.)  It goes in the \nR\u00b2 other X\n field you are referring to.  Then you specify the distribution of $x_2$ in the next couple of fields (\nX distribution\n, \nX parm \u03bc\n, and \nZ parm \u03c3\n).\n\nR\u00b2 other X\n\nX distribution\n\nX parm \u03bc\n\nZ parm \u03c3\n\nHsieh, F.Y., Bloch, D.A., & Larsen, M.D. (1998).  \nA simple method of sample size calculation for linear and logistic regression\n.  \nStatistics in Medicine, 17\n, 1623-1634.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/92087/402101 \n When you fit a regression model such as $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1x_i + \\hat\\beta_2x^2_i$, the model and the OLS estimator doesn't 'know' that $x^2_i$ is simply the square of $x_i$, it just 'thinks' it's another variable.  Of course there is some collinearity, and that gets incorporated into the fit (e.g., the standard errors are larger than they might otherwise be), but lots of pairs of variables can be somewhat collinear without one of them being a function of the other.\n\nWe don't recognize that there are really two separate variables in the model, because \nwe\n know that $x^2_i$ is ultimately the same variable as $x_i$ that we transformed and included in order to capture a curvilinear relationship between $x_i$ and $y_i$.  That knowledge of the true nature of $x^2_i$, coupled with our belief that there is a curvilinear relationship between $x_i$ and $y_i$ is what makes it difficult for us to understand the way that it is still linear from the model's perspective.  In addition, we visualize $x_i$ and $x^2_i$ together by looking at the marginal projection of the 3D function onto the 2D $x, y$ plane.\n\nIf you only have $x_i$ and $x^2_i$, you can try to visualize them in the full 3D space (although it is still rather hard to really see what is going on).  If you did look at the fitted function in the full 3D space, you would see that the fitted function is a 2D plane, and moreover that it is a flat plane.  As I say, it is hard to see well because the $x_i, x^2_i$ data exist only along a curved line going through that 3D space (that fact is the visual manifestation of their collinearity).  We can try to do that here.  Imagine this is the fitted model:\n\nx     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")\n\nx     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/92087/402101 \n x     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")\n\nx     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")\n\n\n\n# 3D plot\nlibrary(scatterplot3d)\ns = scatterplot3d(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, color=\"gray\", pch=1, \n              xlab=\"X1\", ylab=\"X2\", zlab=\"Y\", xlim=c(0, 11), ylim=c(0,101), \n              zlim=c(0, 11), type=\"h\", main=\"In pseudo-3D space\")\ns$points(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, col=\"red\", pch=1)\ns$plane3d(Intercept=3, x.coef=1, y.coef=-.05, col=\"lightblue\")\n\n# 3D plot\nlibrary(scatterplot3d)\ns = scatterplot3d(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, color=\"gray\", pch=1, \n              xlab=\"X1\", ylab=\"X2\", zlab=\"Y\", xlim=c(0, 11), ylim=c(0,101), \n              zlim=c(0, 11), type=\"h\", main=\"In pseudo-3D space\")\ns$points(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, col=\"red\", pch=1)\ns$plane3d(Intercept=3, x.coef=1, y.coef=-.05, col=\"lightblue\")\n\n\n\nIt may be easier to see in these images, which are screenshots of a rotated 3D figure made with the same data using the \nrgl\n package.\n\nrgl\n\n\n\nWhen we say that a model that is \"linear in the parameters\" really is linear, this isn't just some mathematical sophistry.  With $p$ variables, you are fitting a $p$-dimensional hyperplane in a $p\\!+\\!1$-dimensional hyperspace (in our example a 2D plane in a 3D space).  That hyperplane really is 'flat' / 'linear'; it isn't just a metaphor.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/289221/402101 \n It's really just a convenience for loglikelihood, nothing more.\n\nI mean the convenience of the sums vs. products: $\\ln (\\prod_i x_i) =\\sum_i\\ln x_i$, the sums are easier to deal with in many respects, such as differentialtion or integration. It's not a convenience for only exponential families, I'm trying to say.\n\nWhen you deal with a random sample, the likelihoods are of the form: $\\mathrm{L}=\\prod_ip_i$, so the loglikelihood would break this product into the sum instead, which is easier to manipulate and analyze. It helps that all we care is the point of the maximum, the value at the maximum is not important, se we can apply any monotonous transformation such as the logarithm.\n\nOn the curvature intuition. It's basically the same thing in the end as the second derivative of loglikelihood.\n\nUPDATE:\nThis is what I meant on the curvature. If you have a function $y=f(x)$, then it's curvature would be (\nsee (14)\n on Wolfram):\n$$\\kappa=\\frac{f''(x)}{(1+f'(x)^2)^{3/2}}$$\n\nThe second derivative of the log likelihood:\n$$A=(\\ln f(x))''=\\frac{f''(x)}{f(x)}-\\left(\\frac{f'(x)}{f(x)}\\right)^2$$\n\nAt the point of the maximum, the first derivative is obviously zero, so we get:\n$$\\kappa_{max}=f''(x_{max})=Af(x_{max})$$\nHence, my quip that the curvature of the likelihood and the second derivative of loglikelihood are the same thing, sort of.\n\nOn the other hand, if the first derivative of likelihood is small not only at but around the point of maximum, i.e. the likelihood function is flat then we get:\n$$\\kappa\\approx f''(x)\\approx A f(x)$$\nNow the flat likelihood is not a good thing for us, because it makes finding the maximum more difficult numerically, and the maximum likelihood is not that better than other points around it, i.e. the parameter estimation errors are high.\n\nAnd again, we still have the curvature and second derivative relationship. So why didn't Fisher look at the curvature of the likelihood function? I think it's for the same reason of convenience. It's easier to manipulate the loglikelihood because of sums instead of the product. So, he could study the curvature of the likelihood by analyzing the second derivative of the loglikelihood. Although the equation looks very simple for the curvature $\\kappa_{max}=f''(x_{max})$, in actuality you're taking a second derivative of the product, which is messier than the sum of second derivatives.\n\nUPDATE 2:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/289221/402101 \n UPDATE 2:\n\nHere's a demonstration. I draw a (completely made up) likelihood function, its a) curvature and b) the 2nd derivative of its log. On the left side you see the narrow likelihood and on the right side it's wide. You see how at the point of the max likelihood a) and b) converge, as they should. More importantly though, you can study the width (or flatness) of the likelihood function by examining the 2nd derivative of its log-likelihood. As I wrote earlier the latter is technically simpler than the former to analyze.\n\nNot surprisingly deeper 2nd derivative of loglikelihood signals flatter likelihood function around its max, which is not desired for it causes bigger parameter estimation error.\n\n\n\nMATLAB code in case you want to reproduce the plots:\n\nf=@(x,a)a.^2./(a.^2+x.^2);\nc = @(x,a)(-2*a.^2.*(a.^2-3*x.^2)./(a.^2+x.^2).^3/(4*a.^4.*x.^2/(a.^2+x.^2).^4+1).^(3/2));\nll2d = @(x,a)(2*(x.^2-a.^2)./(a.^2+x.^2).^2);\n\nh = 0.1;\nx=-10:h:10;\n\n% narrow peak\nfigure\nsubplot(1,2,1)\na = 1;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Narrow Likelihood'\nylim([-2 1])\n\n% wide peak\nsubplot(1,2,2)\na=2;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Wide Likelihood'\nlegend('likelihood','curvature','2nd derivative LogL','location','best')\nylim([-2 1])\n\nf=@(x,a)a.^2./(a.^2+x.^2);\nc = @(x,a)(-2*a.^2.*(a.^2-3*x.^2)./(a.^2+x.^2).^3/(4*a.^4.*x.^2/(a.^2+x.^2).^4+1).^(3/2));\nll2d = @(x,a)(2*(x.^2-a.^2)./(a.^2+x.^2).^2);\n\nh = 0.1;\nx=-10:h:10;\n\n% narrow peak\nfigure\nsubplot(1,2,1)\na = 1;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Narrow Likelihood'\nylim([-2 1])\n\n% wide peak\nsubplot(1,2,2)\na=2;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Wide Likelihood'\nlegend('likelihood','curvature','2nd derivative LogL','location','best')\nylim([-2 1])\n\nUPDATE 3:\n\nIn the code above I plugged some arbitrary bell shaped function into the curvature equation, then calculated the second derivative of its log. I didn't re-scale anything, the values are straight from equations to show the equivalence that I mentioned earlier.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/289221/402101 \n UPDATE 3:\n\nIn the code above I plugged some arbitrary bell shaped function into the curvature equation, then calculated the second derivative of its log. I didn't re-scale anything, the values are straight from equations to show the equivalence that I mentioned earlier.\n\nHere's\n the very first paper on likelihood that Fisher published while still in the university, \"On an Absolute Criterion for Fitting Frequency Curves\", Messenger of Mathmatics, 41: 155-160 (1912)\n\nAs I was insisting all along he doesn't mention any \"deeper\" connections of log probabilities to entropy and other fancy subjects, neither does he offer his information criterion yet. He simply puts the equation $\\log P'=\\sum_1^n\\log p$ on p.54 then proceeds to talk about maximizing the probabilities. In my opinion, this shows that he was using the logarithm just as a convenient method of analyzing the joint probabilities themselves. It is especially useful in the continuous curve fitting, for which he gives an obvious formula on p.55:\n$$\\log P=\\int_{-\\infty}^\\infty\\log fdx$$\nGood luck analyzing this likelihood (or probability $P$ as per Fisher) without the log!\n\nOne thing to note when reading the paper he was only starting with maximum likelihood estimation work, and did more work in subsequent 10 years, so even the term MLE wasn't coined yet, as far as I know.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/90761/402101 \n First a general comment: Note that the Anderson-Darling test is for completely specified distributions, while the Shapiro-Wilk is for normals with any mean and variance. However, as noted in D'Agostino & Stephens\n$^{[1]}$\n the Anderson-Darling adapts in a very convenient way to the estimation case, akin to (but converges faster and is modified in a way that's simpler to deal with than) the Lilliefors test for the Kolmogorov-Smirnov case. Specifically, at the normal, by \n$n=5$\n, tables of the asymptotic value of \n$A^*=A^2\\left(1+\\frac{4}{n}-\\frac{25}{n^2}\\right)$\n may be used (don't be testing goodness of fit for n<5).\n\nI have read somewhere in the literature that the Shapiro\u2013Wilk test is considered to be the best normality test because for a given significance level, \u03b1, the probability of rejecting the null hypothesis if it's false is higher than in the case of the other normality tests.\n\nAs a general statement this is false.\n\nWhich normality tests are \"better\" depends on which classes of alternatives you're interested in. One reason the Shapiro-Wilk is popular is that it tends to have very good power under a broad range of useful alternatives. It comes up in many studies of power, and usually performs very well, but it's not universally best.\n\nIt's quite easy to find alternatives under which it's less powerful.\n\nFor example, against light tailed alternatives it often has less power than the studentized range \n$u=\\frac{\\max(x)\u2212\\min(x)}{sd(x)}$\n (compare them on a test of normality on uniform data, for example - at \n$n=30$\n, a test based on \n$u$\n has power of about 63% compared to a bit over 38% for the Shapiro Wilk).\n\nThe Anderson-Darling (adjusted for parameter estimation) does better at the double exponential. Moment-skewness does better against some skew alternatives.\n\nCould you please explain to me, using mathematical arguments if possible, how exactly it works compared to some of the other normality tests (say the Anderson\u2013Darling test)?\n\nI will explain in general terms (if you want more specific details the original papers and some of the later papers that discuss them would be your best bet):",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/90761/402101 \n The Anderson-Darling (adjusted for parameter estimation) does better at the double exponential. Moment-skewness does better against some skew alternatives.\n\nCould you please explain to me, using mathematical arguments if possible, how exactly it works compared to some of the other normality tests (say the Anderson\u2013Darling test)?\n\nI will explain in general terms (if you want more specific details the original papers and some of the later papers that discuss them would be your best bet):\n\nConsider a simpler but closely related test, the Shapiro-Francia; it's effectively a function of the correlation between the order statistics and the expected order statistics under normality (and as such, a pretty direct measure of \"how straight the line is\" in the normal Q-Q plot). As I recall, the Shapiro-Wilk is more powerful because it also takes into account the covariances between the order statistics, producing a best linear estimator of \n$\\sigma$\n from the Q-Q plot, which is then scaled by \n$s$\n. When the distribution is far from normal, the ratio isn't close to 1.\n\nBy comparison the Anderson-Darling, like the Kolmogorov-Smirnov and the Cram\u00e9r-von Mises, is based on the empirical CDF. Specifically, it's based on weighted deviations between ECDF and theoretical ECDF (the weighting-for-variance makes it more sensitive to deviations in the tail).\n\nThe test by Shapiro and Chen\n$^{[2]}$\n (1995) (based on spacings between order statistics) often exhibits slightly more power than the Shapiro-Wilk (but not always); they often perform very similarly.\n\n--\n\nUse the Shapiro Wilk because it's often powerful, widely available and many people are familiar with it (removing the need to explain in detail what it is if you use it in a paper) -- just don't use it under the illusion that it's \"the best normality test\". There isn't one best normality test.\n\n[1]: D\u2019Agostino, R. B. and Stephens, M. A. (1986)\n\n\nGoodness of Fit Techniques\n,\n\nMarcel Dekker, New York.\n\n[2]: Chen, L. and Shapiro, S. (1995)\n\n\"An Alternative test for normality based on normalized spacings.\"\n\n\nJournal of Statistical Computation and Simulation\n \n53\n, 269-287.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/28493/402101 \n Although collinearity (of predictor variables) is a possible explanation, I would like to suggest it is not an illuminating explanation because we know collinearity is related to \"common information\" among the predictors, so there is nothing mysterious or counter-intuitive about the side effect of introducing a second correlated predictor into the model.\n\nLet us then consider the case of \ntwo predictors that are truly orthogonal\n: there is absolutely no collinearity among them.  A remarkable change in significance can still happen.\n\nDesignate the predictor variables $X_1$ and $X_2$ and let $Y$ name the predictand.  The regression of $Y$ against $X_1$ will fail to be significant when the variation in $Y$ around its mean is not appreciably reduced when $X_1$ is used as the independent variable.  \nWhen that variation is strongly associated with a second variable $X_2$,\n however, the situation changes.  Recall that multiple regression of $Y$ against $X_1$ and $X_2$ is equivalent to\n\nSeparately regress $Y$ and $X_1$ against $X_2$.\n\n\nRegress the $Y$ residuals against the $X_1$ residuals.\n\nSeparately regress $Y$ and $X_1$ against $X_2$.\n\nRegress the $Y$ residuals against the $X_1$ residuals.\n\nThe residuals from the first step have removed the effect of $X_2$.  When $X_2$ is closely correlated with $Y$, this can expose a relatively small amount of variation that had previously been masked.  If \nthis\n variation is associated with $X_1$, we obtain a significant result.\n\nAll this might perhaps be clarified with a concrete example.\n  To begin, let's use \nR\n to generate two orthogonal independent variables along with some independent random error $\\varepsilon$:\n\nR\n\nn <- 32\nset.seed(182)\nu <-matrix(rnorm(2*n), ncol=2)\nu0 <- cbind(u[,1] - mean(u[,1]), u[,2] - mean(u[,2]))\nx <- svd(u0)$u\neps <- rnorm(n)\n\nn <- 32\nset.seed(182)\nu <-matrix(rnorm(2*n), ncol=2)\nu0 <- cbind(u[,1] - mean(u[,1]), u[,2] - mean(u[,2]))\nx <- svd(u0)$u\neps <- rnorm(n)\n\n(The \nsvd\n step assures the two columns of matrix \nx\n (representing $X_1$ and $X_2$) are orthogonal, ruling out collinearity as a possible explanation of any subsequent results.)\n\nsvd\n\nx\n\nNext, create $Y$ as a linear combination of the $X$'s and the error.  I have adjusted the coefficients to produce the counter-intuitive behavior:\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\nThis is a realization of the model $Y \\sim_{iid} N(0.05 X_1 + 1.00 X_2, 0.01^2)$ with $n=32$ cases.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/28493/402101 \n svd\n\nx\n\nNext, create $Y$ as a linear combination of the $X$'s and the error.  I have adjusted the coefficients to produce the counter-intuitive behavior:\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\ny <-  x %*% c(0.05, 1) + eps * 0.01\n\nThis is a realization of the model $Y \\sim_{iid} N(0.05 X_1 + 1.00 X_2, 0.01^2)$ with $n=32$ cases.\n\nLook at the two regressions in question.  \nFirst\n, regress $Y$ against $X_1$ only:\n\n> summary(lm(y ~ x[,1]))\n...\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.002576   0.032423  -0.079    0.937\nx[, 1]       0.068950   0.183410   0.376    0.710\n\n> summary(lm(y ~ x[,1]))\n...\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.002576   0.032423  -0.079    0.937\nx[, 1]       0.068950   0.183410   0.376    0.710\n\nThe high p-value of 0.710 shows that $X_1$ is completely non-significant.\n\nNext\n, regress $Y$ against $X_1$ and $X_2$:\n\n> summary(lm(y ~ x))\n...\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.002576   0.001678  -1.535    0.136    \nx1           0.068950   0.009490   7.265 5.32e-08 ***\nx2           1.003276   0.009490 105.718  < 2e-16 ***\n\n> summary(lm(y ~ x))\n...\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.002576   0.001678  -1.535    0.136    \nx1           0.068950   0.009490   7.265 5.32e-08 ***\nx2           1.003276   0.009490 105.718  < 2e-16 ***\n\nSuddenly, in the presence of $X_2$, $X_1$ is \nstrongly\n significant, as indicated by the near-zero p-values for both variables.\n\nWe can visualize this behavior\n by means of a scatterplot matrix of the variables $X_1$, $X_2$, and $Y$ along with the \nresiduals\n used in the two-step characterization of multiple regression above.  Because $X_1$ and $X_2$ are orthogonal, the $X_1$ residuals will be the same as $X_1$ and therefore need not be redrawn.  We will include the residuals of $Y$ against $X_2$ in the scatterplot matrix, giving this figure:\n\nlmy <- lm(y ~ x[,2])\nd <- data.frame(X1=x[,1], X2=x[,2], Y=y, RY=residuals(lmy))\nplot(d)\n\nlmy <- lm(y ~ x[,2])\nd <- data.frame(X1=x[,1], X2=x[,2], Y=y, RY=residuals(lmy))\nplot(d)\n\nHere is a rendering of it (with a little prettification):\n\n\n\nThis matrix of graphics has four rows and four columns, which I will count down from the top and from left to right.\n\nNotice:\n\nThe $(X_1, X_2)$ scatterplot in the second row and first column confirms the orthogonality of these predictors: the least squares line is horizontal and correlation is zero.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/28493/402101 \n lmy <- lm(y ~ x[,2])\nd <- data.frame(X1=x[,1], X2=x[,2], Y=y, RY=residuals(lmy))\nplot(d)\n\nHere is a rendering of it (with a little prettification):\n\n\n\nThis matrix of graphics has four rows and four columns, which I will count down from the top and from left to right.\n\nNotice:\n\nThe $(X_1, X_2)$ scatterplot in the second row and first column confirms the orthogonality of these predictors: the least squares line is horizontal and correlation is zero.\n\n\nThe $(X_1, Y)$ scatterplot in the third row and first column exhibits the slight but completely insignificant relationship reported by the first regression of $Y$ against $X_1$.  (The correlation coefficient, $\\rho$, is only $0.07$).\n\n\nThe $(X_2, Y)$ scatterplot in the third row and second column shows the strong relationship between $Y$ and the second independent variable.  (The correlation coefficient is $0.996$).\n\n\nThe fourth row examines the relationships between the \nresiduals\n of $Y$ (regressed against $X_2$) and other variables:\n\n\n\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.\n\n\nThe residuals \nare\n strongly correlated with $X_1$ ($\\rho = 0.80$).  The regression against $X_2$ has unmasked this previously hidden behavior.\n\n\nBy construction, there is no remaining correlation between the residuals and $X_2$.\n\n\nThere is little correlation between $Y$ and these residuals ($\\rho = 0.09$).  This shows how the residuals can behave entirely differently than $Y$ itself.  \nThat's\n how $X_1$ can suddenly be revealed as a significant contributor to the regression.\n\nThe $(X_1, X_2)$ scatterplot in the second row and first column confirms the orthogonality of these predictors: the least squares line is horizontal and correlation is zero.\n\nThe $(X_1, Y)$ scatterplot in the third row and first column exhibits the slight but completely insignificant relationship reported by the first regression of $Y$ against $X_1$.  (The correlation coefficient, $\\rho$, is only $0.07$).\n\nThe $(X_2, Y)$ scatterplot in the third row and second column shows the strong relationship between $Y$ and the second independent variable.  (The correlation coefficient is $0.996$).\n\nThe fourth row examines the relationships between the \nresiduals\n of $Y$ (regressed against $X_2$) and other variables:\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/28493/402101 \n The $(X_2, Y)$ scatterplot in the third row and second column shows the strong relationship between $Y$ and the second independent variable.  (The correlation coefficient is $0.996$).\n\nThe fourth row examines the relationships between the \nresiduals\n of $Y$ (regressed against $X_2$) and other variables:\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.\n\n\nThe residuals \nare\n strongly correlated with $X_1$ ($\\rho = 0.80$).  The regression against $X_2$ has unmasked this previously hidden behavior.\n\n\nBy construction, there is no remaining correlation between the residuals and $X_2$.\n\n\nThere is little correlation between $Y$ and these residuals ($\\rho = 0.09$).  This shows how the residuals can behave entirely differently than $Y$ itself.  \nThat's\n how $X_1$ can suddenly be revealed as a significant contributor to the regression.\n\nThe vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.\n\nThe residuals \nare\n strongly correlated with $X_1$ ($\\rho = 0.80$).  The regression against $X_2$ has unmasked this previously hidden behavior.\n\nBy construction, there is no remaining correlation between the residuals and $X_2$.\n\nThere is little correlation between $Y$ and these residuals ($\\rho = 0.09$).  This shows how the residuals can behave entirely differently than $Y$ itself.  \nThat's\n how $X_1$ can suddenly be revealed as a significant contributor to the regression.\n\nFinally, it is worth remarking that the two estimates of the $X_1$ coefficient (both equal to $0.06895$, not far from the intended value of $0.05$) agree \nonly\n because $X_1$ and $X_2$ are orthogonal.  Except in designed experiments, it is rare for orthogonality to hold exactly.  A departure from orthogonality usually causes the coefficient estimates to change.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414906/402101 \n This question can almost certainly not be answered well for you by readers at CrossValidated. \nThere is no context-free way to decide whether model metrics such as \n$R^2$\n are good or not\n. At the extremes, it is usually possible to get a consensus from a wide variety of experts: an \n$R^2$\n of almost 1 generally indicates a good model, and of close to 0 indicates a terrible one. In between lies a range where assessments are inherently subjective. In this range, it takes more than just statistical expertise to answer whether your model metric is any good. It takes additional expertise in your area, which CrossValidated readers probably do not have.\n\nWhy is this? Let me illustrate with an example from my own experience (minor details changed).\n\nI used to do microbiology lab experiments. I would set up flasks of cells at different levels of nutrient concentration, and measure the growth in cell density (i.e. slope of cell density against time, though this detail is not important). When I then modelled this growth/nutrient relationship, it was common to achieve \n$R^2$\n values of >0.90.\n\nI am now an environmental scientist. I work with datasets containing measurements from nature. If I try to fit the exact same model described above to these \u2018field\u2019 datasets, I\u2019d be surprised if I the \n$R^2$\n was as high as 0.4.\n\nThese two cases involve exactly the same parameters, with very similar measurement methods, models written and fitted using the same procedures - and even the same person doing the fitting! But in one case, an \n$R^2$\n of 0.7 would be worryingly low, and in the other it would be suspiciously high.\n\nFurthermore, we would take some chemistry measurements alongside the biological measurements. Models for the chemistry standard curves would have \n$R^2$\n around 0.99, and a value of 0.90 would be worryingly \nlow\n.\n\nWhat leads to these big differences in expectations? Context. That vague term covers a vast area, so let me try to separate it into some more specific factors (this is likely incomplete):\n\n1. What is the payoff / consequence / application?\n\nThis is where the nature of your field are likely to be most important. However valuable I think my work is, bumping up my model \n$R^2$\ns by 0.1 or 0.2 is not going to revolutionize the world. But there are applications where that magnitude of change would be a huge deal! A much smaller improvement in a stock forecast model could mean tens of millions of dollars to the firm that develops it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414906/402101 \n 1. What is the payoff / consequence / application?\n\nThis is where the nature of your field are likely to be most important. However valuable I think my work is, bumping up my model \n$R^2$\ns by 0.1 or 0.2 is not going to revolutionize the world. But there are applications where that magnitude of change would be a huge deal! A much smaller improvement in a stock forecast model could mean tens of millions of dollars to the firm that develops it.\n\nThis is even easier to illustrate for classifiers, so I\u2019m going to switch my discussion of metrics from \n$R^2$\n to accuracy for the following example (ignoring \nthe weakness of the accuracy metric\n for the moment). Consider the strange and lucrative world of \nchicken sexing\n. After years of training, a human can rapidly tell the difference between a male and female chick when they are just 1 day old. Males and females are fed differently to optimize meat & egg production, so high accuracy saves huge amounts in misallocated investment in \nbillions\n of birds. Till a few decades ago, accuracies of about 85% were considered high in the US. Nowadays, the value of achieving the very highest accuracy, of around 99%? A salary that can apparently range as high as \n60,000\n to possibly \n180,000\n dollars per year (based on some quick googling). Since humans are still limited in the speed at which they work, machine learning algorithms that can achieve similar accuracy but allow sorting to take place faster could be worth millions.\n\n(I hope you enjoyed the example \u2013 the alternative was a depressing one about very questionable algorithmic identification of terrorists).\n\n2. How strong is the influence of unmodelled factors in your system?\n\nIn many experiments, you have the luxury of isolating the system from all other factors that may influence it (that\u2019s partly the goal of experimentation, after all). Nature is messier. To continue with the earlier microbiology example: cells grow when nutrients are available but other things affect them too \u2013 how hot it is, how many predators there are to eat them, whether there are toxins in the water. All of those covary with nutrients and with each other in complex ways. Each of those other factors drives variation in the data that is not being captured by your model. Nutrients may be unimportant in driving variation \nrelative\n to the other factors, and so if I exclude those other factors, my model of my field data will necessarily have a lower \n$R^2$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414906/402101 \n 3. How precise and accurate are your measurements?\n\nMeasuring the concentration of cells and chemicals can be \nextremely\n precise and accurate. Measuring (for example) the emotional state of a community based on trending twitter hashtags is likely to be\u2026less so. If you cannot be precise in your measurements, it is unlikely that your model can ever achieve a high \n$R^2$\n. How precise are measurements in your field? We probably do not know.\n\n4. Model complexity and generalizability\n\nIf you add more factors to your model, even random ones, you will on average increase the model \n$R^2$\n (adjusted \n$R^2$\n partly addresses this). This is \noverfitting\n. An overfit model will not generalize well to new data i.e. will have higher prediction error than expected based on the fit to the original (training) dataset. This is because it has fit the \nnoise\n in the original dataset. This is partly why models are penalized for complexity in model selection procedures, or subjected to regularization.\n\nIf overfitting is ignored or not successfully prevented, the estimated \n$R^2$\n will be biased upward i.e. higher than it ought to be. In other words, your \n$R^2$\n value can give you a misleading impression of your model\u2019s performance if it is overfit.\n\nIMO, overfitting is surprisingly common in many fields. How best to avoid this is a complex topic, and I recommend reading about \nregularization\n procedures and \nmodel selection\n on this site if you are interested in this.\n\n5. Data range and extrapolation\n\nDoes your dataset extend across a substantial portion of the range of X values you are interested in? Adding new data points outside the existing data range can have a large effect on estimated \n$R^2$\n, since it is a metric based on the variance in X and Y.\n\nAside from this, if you fit a model to a dataset and need to predict a value outside the X range of that dataset (i.e. \nextrapolate\n), you might find that its performance is lower than you expect. This is because the relationship you have estimated might well change outside the data range you fitted. In the figure below, if you took measurements only in the range indicated by the green box, you might imagine that a straight line (in red) described the data well. But if you attempted to predict a value outside that range with that red line, you would be quite incorrect.\n\n\n\n[The figure is an edited version of \nthis one\n, found via a quick google search for 'Monod curve'.]\n\n6. Metrics only give you a piece of the picture",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414906/402101 \n [The figure is an edited version of \nthis one\n, found via a quick google search for 'Monod curve'.]\n\n6. Metrics only give you a piece of the picture\n\nThis is not really a criticism of the metrics \u2013 they are \nsummaries\n, which means that they also throw away information by design. But it does mean that any single metric leaves out information that can be crucial to its interpretation. A good analysis takes into consideration more than a single metric.\n\nSuggestions, corrections and other feedback welcome. And other answers too, of course.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/8872/402101 \n The \"t value\" is the ratio of the coefficient to the standard error. The degrees of freedom (ndf) would be the number of observations minus the max order of difference in the model minus the number of estimated coefficients. The \"F value \" would be the square of the \"t value\" In order to exactly compute probability you would have to call a non-central chi-square function and pass in the F value and the degrees of freedom (1,ndf) or perhaps simply call an F function lookup.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3411/402101 \n At the beginning of an article aiming at promoting the use of PSs in epidemiology, Oakes and Church (1) cited Hern\u00e1n and Robins's claims about confounding effect in epidemiology (2):\n\nCan you guarantee that the results\n  from your observational study are\n  unaffected by unmeasured confounding?\n  The only answer an epidemiologist can\n  provide is \u2018no\u2019.\n\nThis is not just to say that we cannot ensure that results from observational studies are unbiased or useless (because, as @propofol said, their results can be useful for designing RCTs), but also that PSs do certainly not offer a complete solution to this problem, or at least do not necessarily yield better results than other matching or multivariate methods (see e.g. (10)).\n\nPropensity scores (PS) are, by construction, \nprobabilistic\n not \ncausal\n indicators. The choice of the covariates that enter the propensity score function is a key element for ensuring its reliability, and their weakness, as has been said, mainly stands from not controlling for unobserved confounders (which is quite likely in retrospective or \ncase-control\n studies). Others factors have to be considered: (a) model misspecification will impact direct effect estimates (not really more than in the OLS case, though), (b) there may be missing data at the level of the covariates, (c) PSs do not overcome synergistic effects which are know to affect causal interpretation (8,9).\n\nAs for references, I found Roger Newson's slides -- \nCausality, confounders, and propensity scores\n -- relatively well-balanced about the pros and cons of using propensity scores, with illustrations from real studies.\nThere were also several good papers discussing the use of propensity scores in observational studies or environmental epidemiology two years ago in \nStatistics in Medicine\n, and I enclose a couple of them at the end (3-6). But I like Pearl's review (7) because it offers a larger perspective on causality issues (PSs are discussed p. 117 and 130). Obviously, you will find many more illustrations by looking at applied research. I would like to add two recent articles from William R Shadish that came across Andrew Gelman's website (11,12). The use of propensity scores is discussed, but the two papers more largely focus on causal inference in observational studies (and how it compare to randomized settings).\n\nReferences",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3411/402101 \n References\n\nOakes, J.M. and Church, T.R. (2007). \nInvited Commentary: Advancing Propensity Score Methods in Epidemiology\n. \nAmerican Journal of Epidemiology\n, 165(10), 1119-1121.\n\n\nHernan M.A. and Robins J.M. (2006). \nInstruments for causal inference: an epidemiologist's dream?\n \nEpidemiology\n, 17, 360-72.\n\n\nRubin, D. (2007). \nThe design versus the analysis of observational studies for causal effects: Parallels with the design of randomized trials\n. \nStatistics in Medicine\n, 26, 20\u201336.\n\n\nShrier, I. (2008). \nLetter to the editor\n. \nStatistics in Medicine\n, 27, 2740\u20132741.\n\n\nPearl, J. (2009). \nRemarks on the method of propensity score\n. \nStatistics in Medicine\n, 28, 1415\u20131424.\n\n\nStuart, E.A. (2008). \nDeveloping practical recommendations for the use of propensity scores: Discussion of \u2018A critical appraisal of propensity score matching in the medical literature between 1996 and 2003\u2019 by Peter Austin\n. \nStatistics in Medicine\n, 27, 2062\u20132065. \n\n\nPearl, J. (2009). \nCausal inference in statistics: An overview\n. \nStatistics Surveys\n, 3, 96-146.\n\n\nOakes, J.M. and Johnson, P.J. (2006). \nPropensity score matching for social epidemiology\n. In \nMethods in Social Epidemiology\n, J.M. Oakes and S. Kaufman (Eds.), pp. 364-386. Jossez-Bass.\n\n\nH\u00f6fler, M (2005). \nCausal inference based on counterfactuals\n. \nBMC Medical Research Methodology\n, 5, 28.\n\n\nWinkelmayer, W.C. and Kurth, T. (2004). \nPropensity scores: help or hype?\n \nNephrology Dialysis Transplantation\n, 19(7), 1671-1673.\n\n\nShadish, W.R., Clark, M.H., and Steiner, P.M. (2008). \nCan Nonrandomized Experiments Yield Accurate Answers? A Randomized Experiment Comparing Random and Nonrandom Assignments\n. \nJASA\n, 103(484), 1334-1356.\n\n\nCook, T.D., Shadish, W.R., and Wong, V.C. (2008). \nThree Conditions under Which Experiments and Observational Studies Produce Comparable Causal Estimates: New Findings from Within-Study Comparisons\n. \nJournal of Policy Analysis and Management\n, 27(4), 724\u2013750.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18259/402101 \n Yes, there are some simple relationships between confidence interval comparisons and hypothesis tests in a wide range of practical settings.\n  However, in addition to verifying the CI procedures and t-test are appropriate for our data, we must check that the sample sizes are not too different and that the two sets have similar standard deviations.  We also should not attempt to derive highly precise p-values from comparing two confidence intervals, but should be glad to develop effective approximations.\n\nIn trying to reconcile the two replies already given (by @John and @Brett), it helps to be mathematically explicit.  A formula for a symmetric two-sided confidence interval appropriate for the setting of this question is\n\n$$\\text{CI} = m \\pm \\frac{t_\\alpha(n) s}{\\sqrt{n}}$$\n\nwhere \n$m$\n is the sample mean of \n$n$\n independent observations, \n$s$\n is the sample standard deviation, \n$2\\alpha$\n is the desired test size (maximum false positive rate), and \n$t_\\alpha(n)$\n is the upper \n$1-\\alpha$\n percentile of the Student t distribution with \n$n-1$\n degrees of freedom.  (This slight deviation from conventional notation simplifies the exposition by obviating any need to fuss over the \n$n$\n \nvs\n \n$n-1$\n distinction, which will be inconsequential anyway.)\n\nUsing subscripts \n$1$\n and \n$2$\n to distinguish two independent sets of data for comparison, with \n$1$\n corresponding to the larger of the two means, a \nnon\n-overlap of confidence intervals is expressed by the inequality (lower confidence limit 1) \n$\\gt$\n (upper confidence limit 2); \nviz.\n,\n\n$$m_1 - \\frac{t_\\alpha(n_1) s_1}{\\sqrt{n_1}} \\gt m_2 + \\frac{t_\\alpha(n_2) s_2}{\\sqrt{n_2}}.$$\n\nThis can be made to look like the t-statistic of the corresponding hypothesis test (to compare the two means) with simple algebraic manipulations, yielding\n\n$$\\frac{m_1-m_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}} \\gt \\frac{s_1\\sqrt{n_2}t_\\alpha(n_1) + s_2\\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 s_2^2 + n_2 s_1^2}}.$$\n\nThe left hand side is the statistic used in the hypothesis test; it is usually compared to a percentile of a Student t distribution with \n$n_1+n_2$\n degrees of freedom: that is, to \n$t_\\alpha(n_1+n_2)$\n.  The right hand side is a biased weighted average of the original t distribution percentiles.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18259/402101 \n $$\\frac{m_1-m_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}} \\gt \\frac{s_1\\sqrt{n_2}t_\\alpha(n_1) + s_2\\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 s_2^2 + n_2 s_1^2}}.$$\n\nThe left hand side is the statistic used in the hypothesis test; it is usually compared to a percentile of a Student t distribution with \n$n_1+n_2$\n degrees of freedom: that is, to \n$t_\\alpha(n_1+n_2)$\n.  The right hand side is a biased weighted average of the original t distribution percentiles.\n\nThe analysis so far justifies the reply by @Brett: there appears to be no simple relationship available.\n  However, let's probe further.  I am inspired to do so because, intuitively, a non-overlap of confidence intervals \nought\n to say something!\n\nFirst, notice that this form of the hypothesis test is valid only when we expect \n$s_1$\n and \n$s_2$\n to be at least approximately equal.  (Otherwise we face the notorious \nBehrens-Fisher problem\n and its complexities.)  Upon checking the approximate equality of the \n$s_i$\n, we could then create an approximate simplification in the form\n\n$$\\frac{m_1-m_2}{s\\sqrt{1/n_1 + 1/n_2}} \\gt \\frac{\\sqrt{n_2}t_\\alpha(n_1) + \\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 + n_2}}.$$\n\nHere, \n$s \\approx s_1 \\approx s_2$\n.  Realistically, we should not expect this informal comparison of confidence limits to have the same size as \n$\\alpha$\n.  Our question then is whether there exists an \n$\\alpha'$\n such that the right hand side is (at least approximately) equal to the correct t statistic.  Namely, for what \n$\\alpha'$\n is it the case that\n\n$$t_{\\alpha'}(n_1+n_2) = \\frac{\\sqrt{n_2}t_\\alpha(n_1) + \\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 + n_2}}\\text{?}$$\n\nIt turns out that for equal sample sizes, \n$\\alpha$\n and \n$\\alpha'$\n are connected (to pretty high accuracy) by a power law.\n  For instance, here is a log-log plot of the two for the cases \n$n_1=n_2=2$\n (lowest blue line), \n$n_1=n_2=5$\n (middle red line), \n$n_1=n_2=\\infty$\n (highest gold line).  The middle green dashed line is an approximation described below.  The straightness of these curves belies a power law.  It varies with \n$n=n_1=n_2$\n, but not much.\n\n\n\nThe answer does depend on the set \n$\\{n_1, n_2\\}$\n, but it is natural to wonder how much it really varies with changes in the sample sizes.  In particular, we could hope that for moderate to large sample sizes (maybe \n$n_1 \\ge 10, n_2 \\ge 10$\n or thereabouts) the sample size makes little difference.  In this case, we could develop a \nquantitative\n way to relate \n$\\alpha'$\n to \n$\\alpha$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18259/402101 \n The answer does depend on the set \n$\\{n_1, n_2\\}$\n, but it is natural to wonder how much it really varies with changes in the sample sizes.  In particular, we could hope that for moderate to large sample sizes (maybe \n$n_1 \\ge 10, n_2 \\ge 10$\n or thereabouts) the sample size makes little difference.  In this case, we could develop a \nquantitative\n way to relate \n$\\alpha'$\n to \n$\\alpha$\n.\n\nThis approach turns out to work provided the sample sizes are not too different from each other.  In the spirit of simplicity, I will report an omnibus formula for computing the test size \n$\\alpha'$\n corresponding to the confidence interval size \n$\\alpha$\n.  It is\n\n$$\\alpha' \\approx e \\alpha^{1.91};$$\n\nthat is,\n\n$$\\alpha' \\approx \\exp(1 + 1.91\\log(\\alpha)).$$\n\nThis formula works reasonably well in these common situations:\n\nBoth sample sizes are close to each other, \n$n_1 \\approx n_2$\n, and \n$\\alpha$\n is not too extreme (\n$\\alpha \\gt .001$\n or so).\n\n\n\n\nOne sample size is within about three times the other and the smallest isn't too small (roughly, greater than \n$10$\n) and again \n$\\alpha$\n is not too extreme.\n\n\n\n\nOne sample size is within three times the other and \n$\\alpha \\gt .02$\n or so.\n\nBoth sample sizes are close to each other, \n$n_1 \\approx n_2$\n, and \n$\\alpha$\n is not too extreme (\n$\\alpha \\gt .001$\n or so).\n\nOne sample size is within about three times the other and the smallest isn't too small (roughly, greater than \n$10$\n) and again \n$\\alpha$\n is not too extreme.\n\nOne sample size is within three times the other and \n$\\alpha \\gt .02$\n or so.\n\nThe relative error (correct value divided by the approximation) in the first situation is plotted here, with the lower (blue) line showing the case \n$n_1=n_2=2$\n, the middle (red) line the case \n$n_1=n_2=5$\n, and the upper (gold) line the case \n$n_1=n_2=\\infty$\n.  Interpolating between the latter two, we see that the approximation is excellent for a wide range of practical values of \n$\\alpha$\n when sample sizes are moderate (around 5-50) and otherwise is reasonably good.\n\n\n\nThis is more than good enough for eyeballing a bunch of confidence intervals.\n\nTo summarize,\n the failure of two \n$2\\alpha$\n-size confidence intervals of means to overlap is significant evidence of a difference in means at a level equal to \n$2e \\alpha^{1.91}$\n, provided the two samples have approximately equal standard deviations and are approximately the same size.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18259/402101 \n This is more than good enough for eyeballing a bunch of confidence intervals.\n\nTo summarize,\n the failure of two \n$2\\alpha$\n-size confidence intervals of means to overlap is significant evidence of a difference in means at a level equal to \n$2e \\alpha^{1.91}$\n, provided the two samples have approximately equal standard deviations and are approximately the same size.\n\nI'll end with a tabulation of the approximation for common values of \n$2\\alpha$\n.  In the left hand column is the nominal size \n$2\\alpha$\n of the original confidence interval; in the right hand column is the actual size \n$2\\alpha^\\prime$\n of the comparison of two such intervals:\n\n$$\\begin{array}{ll}\n2\\alpha & 2\\alpha^\\prime \\\\   \\hline \n0.1   &0.02\\\\  \n0.05  &0.005\\\\  \n0.01  &0.0002\\\\  \n0.005 &0.00006\\\\  \n\\end{array}$$\n\nFor example, when a pair of two-sided 95% CIs (\n$2\\alpha=.05$\n) for samples of approximately equal sizes do not overlap, we should take the means to be significantly different, \n$p \\lt .005$\n.  The correct p-value (for equal sample sizes \n$n$\n) actually lies between \n$.0037$\n (\n$n=2$\n) and \n$.0056$\n (\n$n=\\infty$\n).\n\nThis result justifies (and I hope improves upon) the reply by @John.  Thus, although the previous replies appear to be in conflict, both are (in their own ways) correct.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/306188/402101 \n You have defined causality incorrectly, yes.  Probably, you have heard the saying \"correlation isn't causation.\"  You have essentially defined causality as correlation.  The problem is worse than that, though.  Causality is not a statistical or probabilistic concept at all, at least as those topics are normally taught.  There is no statistical or probabilistic definition of causality: nothing involving conditional expectations or conditional distributions or suchlike.  It is hard to pick up this fact from courses in statistics or econometrics, though.\n\nUnfortunately, we tend to do a better job saying what causality isn't than what causality is.  Causality always and everywhere comes from theory, from a priori reasoning, from assumptions.  You mentioned econometrics.  If you have been taught instrumental variables competently, then you know that causal effects can only be measured if you have an \"exclusion restriction.\"  And you know that exclusion restrictions always come from theory.\n\nYou said you wanted math, though.  The guy you want to read is \nJudea Pearl\n.  It's not easy math, and the math sometimes wanders off into philosophy, but that's because causality is a hard subject.  Here is \na page\n with more links on the subject.  Here is \na free online book\n I just came across.  Finally, here is \na previous question\n where I gave an answer you might find useful.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32353/402101 \n It is possible that this question is homework but I felt this classical elementary probability question was still lacking a complete answer after several months, so I'll give one here.\n\nFrom the problem statement, we want the distribution of\n\n$$Y = \\max \\{ X_1, ..., X_n \\}$$\n\nwhere \n$X_1, ..., X_n$\n are iid \n${\\rm Uniform}(a,b)$\n. We know that \n$Y < x$\n if and only if every element of the sample is less than \n$x$\n. Then this, as indicated in @varty's hint, combined with the fact that the \n$X_i$\n's are independent, allows us to deduce\n\n$$ P(Y \\leq x) = P(X_1 \\leq x, ..., X_n \\leq x) = \\prod_{i=1}^{n} P(X_i \\leq x) = F_{X}(x)^n$$\n\nwhere \n$F_{X}(x)$\n is the \nCDF of the uniform distribution\n that is \n$\\frac{y-a}{b-a}$\n. Therefore the CDF of \n$Y$\n is\n\n$$F_{Y}(y) = P(Y \\leq y) = \\begin{cases} \n0 & y \\leq a \\\\ \n\\phantom{} \\left( \\frac{y-a}{b-a} \\right)^n & y\\in(a,b) \\\\\n1 & y \\geq b \\\\ \n\\end{cases}$$\n\nSince \n$Y$\n has an absolutely continuous distribution \nwe can derive its density by differentiating the CDF\n. Therefore the density of \n$Y$\n is\n\n$$ p_{Y}(y) = \\frac{n(y-a)^{n-1}}{(b-a)^{n}}$$\n\nIn the special case where \n$a=0,b=1$\n, we have that \n$p_{Y}(y)=ny^{n-1}$\n, which is the density of a \nBeta distribution\n with \n$\\alpha=n$\n and \n$\\beta=1$\n, since \n${\\rm Beta}(n,1) = \\frac{\\Gamma(n+1)}{\\Gamma(n)\\Gamma(1)}=\\frac{n!}{(n-1)!} = n$\n.\n\nAs a note, the sequence you get if you were to sort your sample in increasing order - \n$X_{(1)}, ..., X_{(n)}$\n - are called the \norder statistics\n. A generalization of this answer is that \nall order statistics of a \n${\\rm Uniform}(0,1)$\n distributed sample have a Beta distribution\n, as noted in @bnaul's answer.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473981/402101 \n The result is correct, but the reasoning is somewhat inaccurate. You need to keep track of the property that the density is zero outside \n$[0,\\theta]$\n. This implies that the likelihood is zero to the left of the sample maximum, and jumps to \n$\\theta^n$\n in the maximum. It indeed decreases afterwards, so that the maximum is the MLE.\n\nThis also entails that the likelihood is not differentiable in this point, so that finding the MLE via the \"canonical\" route of the score function is not the way to go here.\n\nA more detailed formal derivation is, e.g., given \nhere",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/463010/402101 \n Rephrasing the opposite of a confounder: It is definitely possible that an unobserved variable yields the impression that there is no relationship, when there \nis\n one.\n\nConfounding\n usually refers to a situation where an unobserved variable yields the illusion that there exists a relationship between two variables where there is none:\n\n\n\nThis is a special case of \nomitted-variable bias\n, which more generally refers to any situation wherein an unobserved variable biases the observed relationship:\n\n\n\nIt's easy to imagine a scenario where this would have a canceling effect on the estimate instead:\n\n\n\n(I wrote \n$\\rho=0$\n for the illustration, but the unobserved relationship does not have to be linear.)\n\nYou could call this phenomenon omitted-variable bias, cancellation, or masking. Confounding usually refers to the kind of causal relationship shown in the first figure.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/189758/402101 \n A (metric) distance $D$ must be symmetric, i.e. $D(P,Q) = D(Q,P)$.\nBut, from definition, $KL$ is not.\n\nExample: $\\Omega = \\{A,B\\}$, $P(A) = 0.2, P(B) = 0.8$, $Q(A) = Q(B) = 0.5$.\n\nWe have:\n\n$$KL(P,Q) = P(A)\\log \\frac{P(A)}{Q(A)} + P(B) \\log \\frac{P(B)}{Q(B)} \\approx 0.19$$\n\nand\n\n$$KL(Q,P) = Q(A)\\log \\frac{Q(A)}{P(A)} + Q(B) \\log \\frac{Q(B)}{P(B)} \\approx 0.22$$\n\nthus $KL(P,Q) \\neq KL(Q,P)$ and therefore $KL$ is not a (metric) distance.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/8870/402101 \n The \"t value\" is the ratio of the coefficient to the standard error. The degrees of freedom (ndf) would be the number of observations minus the max order of difference in the model minus the number of estimated coefficients. The \"F value \" would be the square of the \"t value\" In order to exactly compute probability you would have to call a non-central chi-square function and pass in the F value and the degrees of freedom (1,ndf) or perhaps simply call an F function lookup.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/525516/402101 \n One very common cause is mis-specification. For example, let $y$ be grocery sales and $\\varepsilon$ be an \nunobserved\n (to the analyst) coupon campaign that varies in intensity over time. At any point in time, there may be several \"vintages\" of coupons circulating as people use them, throw them away, and receive new ones. Shocks can also have persistent (but gradually weakening) effects. Take natural disasters or simply bad weather. Battery sales go up before the storm, then fall during, and then jump again as people people realize that disaster kits may be a good idea for the future.\n\nSimilarly, data manipulation (like smoothing or interpolation) can induce this effect.\n\nI also have \"inherently smooth behavior of time series data (inertia) can cause $MA(1)$\" in my notes, but that one no longer makes sense to me.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/651236/402101 \n The \"generalized variance inflation factors\" (GVIF) implemented in the \nvif()\n function of the R \ncar\n package\n were designed by \nFox and Monette\n specifically to handle situations like this, where there are groups of predictor variables that should be considered together rather than separately. Such situations include multi-level categorical variables and polynomial terms in a single variable.\n\nvif()\n\ncar\n\nThe standard VIF calculation described on the \nWikipedia page\n (and evidently as implemented in the Python \nvariance_inflation_factor()\n function) treats each predictor separately. A \n$k$\n-level categorical variable then counts as \n$k-1$\n predictors, and the result of that type of VIF calculation will depend on how that variable is coded, specifically which category is considered the reference level. Allison alluded to that in the post you linked, recommending use of the most frequent category as the reference when performing that type of VIF calculation.\n\nvariance_inflation_factor()\n\nThe GVIF approach provides a combined measure of collinearity for each group of predictors that should be considered together, like each of your multi-level categorical variables. It does this in a way that is independent of the details of how those predictors are coded. The \nGVIF^(1/(2*Df))\n calculation then provides comparability among predictor sets having different dimensions.\n\nGVIF^(1/(2*Df))\n\nSo in your case the GVIF approach is most applicable, and there seems to be no substantial evidence of multicollinearity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72800/402101 \n This is a recipe to learn EM with a practical and (in my opinion) very intuitive 'Coin-Toss' example:\n\nRead this short \nEM tutorial paper\n by Do and Batzoglou. This is the schema where the coin toss example is explained:\n\n\n\n\nYou may have question marks in your head, especially regarding where the probabilities in the Expectation step come from. Please have a look at the explanations on this maths stack exchange \npage\n.\n\n\nLook at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:   \n\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n## E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* ##\n\ndef get_binomial_log_likelihood(obs,probs):\n    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n    # Binomial Distribution Log PDF\n    # ln (pdf)      = Binomial Coeff * product of probabilities\n    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)\n\n    N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik\n\n# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n# 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45\n\n# represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50\n\n# E-M begins!\ndelta = 0.001  \nj = 0 # iteration counter\nimprovement = float('inf')\nwhile (improvement>delta):\n    expectation_A = np.zeros((len(experiments),2), dtype=float) \n    expectation_B = np.zeros((len(experiments),2), dtype=float)\n    for i in range(0,len(experiments)):\n        e = experiments[i] # i'th experiment\n          # loglikelihood of e given coin A:\n        ll_A = get_binomial_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) \n          # loglikelihood of e given coin B\n        ll_B = get_binomial_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]]))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72800/402101 \n # corresponding weight of A proportional to likelihood of A \n        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n          # corresponding weight of B proportional to likelihood of B\n        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n        expectation_A[i] = np.dot(weightA, e) \n        expectation_B[i] = np.dot(weightB, e)\n\n    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n\n    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - \n                    np.array([pA_heads[j],pB_heads[j]]) )) )\n    j = j+1\n\nplt.figure();\nplt.plot(range(0,j),pA_heads[0:j], 'r--')\nplt.plot(range(0,j),pB_heads[0:j])\nplt.show()\n\nRead this short \nEM tutorial paper\n by Do and Batzoglou. This is the schema where the coin toss example is explained:\n\n\n\nYou may have question marks in your head, especially regarding where the probabilities in the Expectation step come from. Please have a look at the explanations on this maths stack exchange \npage\n.\n\nLook at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n## E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* ##\n\ndef get_binomial_log_likelihood(obs,probs):\n    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n    # Binomial Distribution Log PDF\n    # ln (pdf)      = Binomial Coeff * product of probabilities\n    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)\n\n    N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik\n\n# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n# 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45\n\n# represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72800/402101 \n # represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50\n\n# E-M begins!\ndelta = 0.001  \nj = 0 # iteration counter\nimprovement = float('inf')\nwhile (improvement>delta):\n    expectation_A = np.zeros((len(experiments),2), dtype=float) \n    expectation_B = np.zeros((len(experiments),2), dtype=float)\n    for i in range(0,len(experiments)):\n        e = experiments[i] # i'th experiment\n          # loglikelihood of e given coin A:\n        ll_A = get_binomial_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) \n          # loglikelihood of e given coin B\n        ll_B = get_binomial_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) \n\n          # corresponding weight of A proportional to likelihood of A \n        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n          # corresponding weight of B proportional to likelihood of B\n        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n        expectation_A[i] = np.dot(weightA, e) \n        expectation_B[i] = np.dot(weightB, e)\n\n    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n\n    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - \n                    np.array([pA_heads[j],pB_heads[j]]) )) )\n    j = j+1\n\nplt.figure();\nplt.plot(range(0,j),pA_heads[0:j], 'r--')\nplt.plot(range(0,j),pB_heads[0:j])\nplt.show()\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n## E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* ##\n\ndef get_binomial_log_likelihood(obs,probs):\n    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n    # Binomial Distribution Log PDF\n    # ln (pdf)      = Binomial Coeff * product of probabilities\n    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)\n\n    N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72800/402101 \n N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik\n\n# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n# 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45\n\n# represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50\n\n# E-M begins!\ndelta = 0.001  \nj = 0 # iteration counter\nimprovement = float('inf')\nwhile (improvement>delta):\n    expectation_A = np.zeros((len(experiments),2), dtype=float) \n    expectation_B = np.zeros((len(experiments),2), dtype=float)\n    for i in range(0,len(experiments)):\n        e = experiments[i] # i'th experiment\n          # loglikelihood of e given coin A:\n        ll_A = get_binomial_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) \n          # loglikelihood of e given coin B\n        ll_B = get_binomial_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) \n\n          # corresponding weight of A proportional to likelihood of A \n        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n          # corresponding weight of B proportional to likelihood of B\n        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n        expectation_A[i] = np.dot(weightA, e) \n        expectation_B[i] = np.dot(weightB, e)\n\n    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n\n    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - \n                    np.array([pA_heads[j],pB_heads[j]]) )) )\n    j = j+1\n\nplt.figure();\nplt.plot(range(0,j),pA_heads[0:j], 'r--')\nplt.plot(range(0,j),pB_heads[0:j])\nplt.show()",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/650036/402101 \n This is impossible. Suppose \n$X$\n is the distribution, and let \n$a,b,c$\n be its first three moments, \n$a=E[X], b=E[X^2], c=E[X^3]$\n.\n\nWe derive three relationships for these variables, and show that they can not be satisfied simulateously.\n\nFirst, by the \nCauchy-Schwarz\n inequality:\n\n\\begin{align}\nE[\\sqrt{X^\\phantom{1}}\\!\\! \\sqrt{X^3}] &\\le \\sqrt{E[X]}\\sqrt{E[X^3]}\\\\\n(E[X^2])^2 &\\le E[X]E[X^3]\\\\\nb^2 &\\le ac \\\\\n\\end{align}\n\nwhere \n$\\sqrt{X}$\n and \n$\\sqrt{X^3}$\n are positive by the positivity of \n$X$\n.\n\nSecond, since skewness is 0,\n\n\\begin{align}\n\\phantom{skewness = skewness = = = }0 &= E[(X-a)^3]\\\\\n&= E[X^3]-3aE[X^2]+3a^2E[X]-a^3\\\\\n&= c-3ab+3a^3-a^3\\\\\n&= c-3ab+2a^3\n\\end{align}\n\nThird, since the standard deviation is greater than the mean,\n\n\\begin{align}\n\\text{variance} &> \\text{mean}^2 \\phantom{+mean} \\\\\nb - a^2 &> a^2\\\\\nb &> 2a^2\\\\\n\\end{align}\n\nand, since \n$a^2$\n is positive, also \n$b>a^2$\n.\n\nTogether these yield\n\n$$0\\ge b^2-ac=(b-2a^2)(b-a^2)>0$$\n\nwhich is impossible.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163450/402101 \n Let us imagine that you want to infer some parameter \n$\\beta$\n from some observed input-output pairs \n$(x_1,y_1)\\dots,(x_N,y_N)$\n. Let us assume that the outputs are linearly related to the inputs via \n$\\beta$\n and that the data are corrupted by some noise \n$\\epsilon$\n:\n\n$$y_n = \\beta x_n + \\epsilon,$$\n\nwhere \n$\\epsilon$\n is Gaussian noise with mean \n$0$\n and variance \n$\\sigma^2$\n.\nThis gives rise to a Gaussian likelihood:\n\n$$\\prod_{n=1}^N \\mathcal{N}(y_n|\\beta x_n,\\sigma^2).$$\n\nLet us regularise parameter \n$\\beta$\n by imposing the Gaussian prior \n$\\mathcal{N}(\\beta|0,\\lambda^{-1}),$\n where \n$\\lambda$\n is a strictly positive scalar (\n$\\lambda$\n quantifies of by how much we believe that \n$\\beta$\n should be close to zero, i.e. it controls the strength of the regularisation).\nHence, combining the likelihood and the prior we simply have:\n\n$$\\prod_{n=1}^N \\mathcal{N}(y_n|\\beta x_n,\\sigma^2) \\mathcal{N}(\\beta|0,\\lambda^{-1}).$$\n\nLet us take the logarithm of the above expression. Dropping some constants we get:\n\n$$\\sum_{n=1}^N -\\frac{1}{\\sigma^2}(y_n-\\beta x_n)^2 - \\lambda \\beta^2 + \\mbox{const}.$$\n\nIf we maximise the above expression with respect to \n$\\beta$\n, we get the so called maximum a-posteriori estimate for \n$\\beta$\n, or MAP estimate for short. In this expression it becomes apparent why the Gaussian prior can be interpreted as a L2 regularisation term.\n\nThe relationship between the L1 norm and the Laplace prior can be understood in the same fashion. Instead of a Gaussian prior, multiply your likelihood with a Laplace prior and then take the logarithm.\n\nA good reference (perhaps slightly advanced) detailing both issues is the paper \"Adaptive Sparseness for Supervised Learning\", which currently does not seem easy to find online. Alternatively look at \n\"Adaptive Sparseness using Jeffreys Prior\"\n. Another good reference is \n\"On Bayesian classification with Laplace priors\"\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/165008/402101 \n $\\hat\\beta_n=(XX^T+\u03bbI)^{\u22121} \\sum\\limits_{i=0}^{n-1} x_iy_i$\n\nLet \n$M_n^{-1} = (XX^T+\u03bbI)^{\u22121}$\n, then\n\n$\\hat\\beta_{n+1}=M_{n+1}^{\u22121} (\\sum\\limits_{i=0}^{n-1} x_iy_i + x_ny_n)$\n , and\n\n$M_{n+1} - M_n = x_nx_n^T$\n,  we can get\n\n$\\hat\\beta_{n+1}=\\hat\\beta_{n}+M_{n+1}^{\u22121} x_n(y_n - x_n^T\\hat\\beta_{n})$\n\nAccording to \nWoodbury formula\n, we have\n\n$M_{n+1}^{-1} = M_{n}^{-1} - \\frac{M_{n}^{-1}x_nx_n^TM_{n}^{-1}}{(1+x_n^TM_n^{-1}x_n)}$\n\nAs a result,\n\n$\\hat\\beta_{n+1}=\\hat\\beta_{n}+\\frac{M_{n}^{\u22121}}{1 + x_n^TM_n^{-1}x_n} x_n(y_n - x_n^T\\hat\\beta_{n})$\n\nPolyak averaging\n indicates you can use \n$\\eta_n = n^{-\\alpha}$\n\nto approximate \n$\\frac{M_{n}^{\u22121}}{1 + x_n^TM_n^{-1}x_n}$\n with \n$\\alpha$\n ranges from \n$0.5$\n to \n$1$\n. You may try in your case to select the best \n$\\alpha$\n for your recursion.\n\nI think it also works if you apply a batch gradient algorithm:\n\n$\\hat\\beta_{n+1}=\\hat\\beta_{n}+\\frac{\\eta_n}{n} \\sum\\limits_{i=0}^{n-1}x_i(y_i - x_i^T\\hat\\beta_{n})$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/97534/402101 \n The point is that sometimes, different models (for the same data) can lead to likelihood functions which differ by a multiplicative constant, but the information content must clearly be the same. An example:\n\nWe model \n$n$\n independent Bernoulli experiments, leading to data \n$X_1, \\dots, X_n$\n, each with a Bernoulli distribution with (probability) parameter \n$p$\n. This leads to the likelihood function\n\n$$\n   \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}\n$$\n\nOr we can summarize the data by the binomially distributed variable \n$Y=X_1+X_2+\\dotsm+X_n$\n, which has a binomial distribution, leading to the likelihood function\n\n$$\n   \\binom{n}{y} p^y (1-p)^{n-y}\n$$\n\nwhich, as a function of the unknown parameter \n$p$\n, is proportional to the former likelihood function.  The two likelihood functions clearly contains the same information, and should lead to the same inferences!\n\nAnd indeed, by definition, they are considered the same likelihood function.\n\nAnother viewpoint:  observe that when the likelihood functions are used in Bayes theorem, as needed for bayesian analysis, such multiplicative constants simply cancel!  so they are clearly irrelevant to bayesian inference.  Likewise, it will cancel when calculating likelihood ratios, as used in optimal hypothesis tests (Neyman-Pearson lemma.)  And it will have no influence on the value of maximum likelihood estimators. So we can see that in much of frequentist inference it cannot play a role.\n\nWe can argue from still another viewpoint.  The Bernoulli probability function (hereafter we use the term \"density\") above is really a density with respect to counting measure, that is, the measure on the non-negative integers with mass one for each non-negative integer.  But we could have defined a density with respect to some other dominating measure. In this example this will seem (and is) artificial, but in larger spaces (function spaces) it is really fundamental! Let us, for the purpose of illustration, use the specific geometric distribution, written \n$\\lambda$\n, with \n$\\lambda(0)=1/2$\n, \n$\\lambda(1)=1/4$\n, \n$\\lambda(2)=1/8$\n and so on. Then the density of the Bernoulli distribution \nwith respect to \n$\\lambda$\n is given by\n\n$$\n   f_{\\lambda}(x) = p^x (1-p)^{1-x}\\cdot 2^{x+1}\n$$\n\nmeaning that \n$$\n   P(X=x)= f_\\lambda(x) \\cdot \\lambda(x)\n$$\n\nWith this new, dominating, measure, the likelihood function becomes (with notation from above) \n\n$$\n   \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} 2^{x_i+1} = p^y (1-p)^{n-y} 2^{y+n}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/97534/402101 \n $$\n   f_{\\lambda}(x) = p^x (1-p)^{1-x}\\cdot 2^{x+1}\n$$\n\nmeaning that \n$$\n   P(X=x)= f_\\lambda(x) \\cdot \\lambda(x)\n$$\n\nWith this new, dominating, measure, the likelihood function becomes (with notation from above) \n\n$$\n   \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} 2^{x_i+1} = p^y (1-p)^{n-y} 2^{y+n}\n$$\n\nnote the extra factor \n$2^{y+n}$\n.  So when changing the dominating measure used in the definition of the likelihood function, there arises a new multiplicative constant, which does not depend on the unknown parameter \n$p$\n, and is clearly irrelevant. That is another way to see how multiplicative constants must be irrelevant.  This argument can be generalized using Radon-Nikodym derivatives (as the argument above is an example of.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/69578/402101 \n In an a sample $x$ of $n$ independent values from a distribution $F$ with pdf $f$, the pdf of the joint distribution of the extremes $\\min(x)=x_{[1]}$ and $\\max(x)=x_{[n]}$ is proportional to\n\n$$f(x_{[1]})\\left(F(x_{[n]})-F(x_{[1]})\\right)^{n-2}f(x_{[n]})dx_{[1]}dx_{[n]} = H_F(x_{[1]}, x_{[n]})dx_{[1]}dx_{[n]}.$$\n\n(The constant of proportionality is the reciprocal of the multinomial coefficient $\\binom{n}{1,n-2,1} = n(n-1)$.  Intuitively, this joint PDF expresses the chance of finding the smallest value in the range $[x_{[1]},x_{[1]}+dx_{[1]})$, the largest value in the range $[x_{[n]},x_{[n]}+dx_{[n]})$, and the middle $n-2$ values between them within the range $[x_{[1]}+dx_{[1]}, x_{[n]})$.  When $F$ is continuous, we may replace that middle range by $(x_{[1]}, x_{[n]}]$, thereby neglecting only an \"infinitesimal\" amount of probability. The associated probabilities, to first order in the differentials, are $f(x_{[1]})dx_{[1]},$  $f(x_{[n]})dx_{[n]},$ and $F(x_{[n]})-F(x_{[1]}),$respectively, now making it obvious where the formula comes from.)\n\nTaking the expectation of the range $x_{[n]} - x_{[1]}$ gives $2.53441\\ \\sigma$ for any Normal distribution with standard deviation $\\sigma$ and $n=6$. The expected range as a multiple of $\\sigma$ depends on the sample size $n$:\n\n\n\nThese values were computed by numerically integrating $\\binom{n}{1,n-2,1}\\left(y-x\\right)H_F(x,y)dxdy$ over $\\{(x,y)\\in\\mathbb{R}^2|x\\le y\\}$, with $F$ set to the standard Normal CDF, and dividing by the standard deviation of $F$ (which is just $1$).\n\nA similar multiplicative relationship between the expected range and the standard deviation will hold for any location-scale family of distributions, because it is a property of the \nshape\n of the distribution alone.  For instance, here is a comparable plot for uniform distributions:\n\n\n\nand exponential distributions:\n\n\n\nThe values in the preceding two plots were obtained by exact--not numerical--integration, which is possible due to the relatively simple algebraic forms of $f$ and $F$ in each case.  For the uniform distributions they equal $\\frac{n-1}{(n+1)}\\sqrt{12}$ and for the exponential distributions they are $\\gamma + \\psi(n) = \\gamma + \\frac{\\Gamma'(n)}{\\Gamma(n)}$ where $\\gamma$ is Euler's constant and $\\psi$ is the \"polygamma\" function, the logarithmic derivative of Euler's Gamma function.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/69578/402101 \n The values in the preceding two plots were obtained by exact--not numerical--integration, which is possible due to the relatively simple algebraic forms of $f$ and $F$ in each case.  For the uniform distributions they equal $\\frac{n-1}{(n+1)}\\sqrt{12}$ and for the exponential distributions they are $\\gamma + \\psi(n) = \\gamma + \\frac{\\Gamma'(n)}{\\Gamma(n)}$ where $\\gamma$ is Euler's constant and $\\psi$ is the \"polygamma\" function, the logarithmic derivative of Euler's Gamma function.\n\nAlthough they differ (because these distributions display a wide range of shapes), the three roughly agree around $n=6$, showing that the multiplier $2.5$ does not depend heavily on the shape and therefore can serve as an omnibus, robust assessment of the standard deviation when ranges of small subsamples are known.  (Indeed, the very heavy-tailed Student $t$ distribution with three degrees of freedom still has a multiplier around $2.3$ for $n=6$, not far at all from $2.5$.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/127420/402101 \n Logistic regression is emphatically \nnot\n a classification algorithm on its own. It is only a classification algorithm \nin combination with\n a decision rule that makes dichotomous the predicted probabilities of the outcome. Logistic regression \nis\n a regression model because it estimates the probability of class membership as a (transformation of a) multilinear function of the features.\n\nFrank Harrell\n has posted a number of answers on this website enumerating the pitfalls of regarding logistic regression as a classification algorithm. Among them:\n\nClassification is a decision\n. To make an optimal decision, you need to asses a utility function, which implies that you need to account for the uncertainty in the outcome, i.e. a probability.\n\n\nThe costs of misclassification are not uniform across all units.\n\n\nDon't use cutoffs.\n\n\nUse proper scoring rules.\n\n\nThe problem is actually risk estimation, not classification.\n\nIf I recall correctly, he once pointed me to his book on regression strategies for more elaboration on these (and more!) points, but I can't seem to find that particular post.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/191781/402101 \n The saddlepoint approximation to a probability density function (it works likewise for mass functions, but I will only talk here in terms of densities)\nis a surprisingly well working approximation, that can be seen as a refinement on the central limit theorem. So, it will only work in settings where there is a central limit theorem, but it needs stronger assumptions.\n\nWe start with the assumption that the moment generating function exists and is twice differentiable. This implies in particular that all moments exist. Let \n$X$\n be a random variable with moment generating function (mgf)\n\n$$ \\DeclareMathOperator{\\E}{\\mathbb{E}}\n   M(t) = \\E  e^{t X}\n$$\n\nand cgf (cumulant generating function) \n$K(t)=\\log M(t)$\n (where \n$\\log $\n denotes the natural logarithm). In the development I will follow closely Ronald W Butler: \"Saddlepoint Approximations with Applications\" (CUP). We will develop the saddlepoint approximation using the Laplace approximation to a certain integral.  Write\n\n$$\ne^{K(t)} = \\int_{-\\infty}^\\infty e^{t x} f(x) \\; dx =\\int_{-\\infty}^\\infty \\exp(tx+\\log f(x) ) \\; dx \\\\\n    = \\int_{-\\infty}^\\infty \\exp(-h(t,x)) \\; dx\n$$\n\nwhere\n\n$h(t,x) = -tx - \\log f(x) $\n. Now we will Taylor expand \n$h(t,x)$\n in \n$x$\n considering \n$t$\n as a constant. This gives\n\n$$\n  h(t,x)=h(t,x_0) + h'(t,x_0)(x-x_0) +\\frac12 h''(t,x_0) (x-x_0)^2 +\\dotsm \n$$\n\nwhere \n$'$\n denotes differentiation with respect to \n$x$\n. Note that\n\n$$\nh'(t,x)=-t-\\frac{\\partial}{\\partial x}\\log f(x) \\\\\nh''(t,x)= -\\frac{\\partial^2}{\\partial x^2} \\log f(x) > 0\n$$\n\n(the last inequality by assumption as it is needed for the approximation to work). Let \n$x_t$\n be the solution to \n$h'(t,x_t)=0$\n. We will assume that this gives a minimum for  \n$h(t,x)$\n as a function of \n$x$\n.  Using this expansion in the integral and forgetting about the \n$\\dotsm$\n part, gives\n\n$$\ne^{K(t)} \\approx \\int_{-\\infty}^\\infty \\exp(-h(t,x_t)-\\frac12 h''(t,x_t) (x-x_t)^2 ) \\; dx \\\\\n= e^{-h(t,x_t)} \\int_{-\\infty}^\\infty e^{-\\frac12 h''(t,x_t) (x-x_t)^2} \\; dx\n$$\n\nwhich is a Gaussian integral, giving\n\n$$\ne^{K(t)} \\approx e^{-h(t,x_t)} \\sqrt{\\frac{2\\pi}{h''(t,x_t)}}. \n$$\n\nThis gives (a first version) of the saddlepoint approximation as\n\n$$ \nf(x_t) \\approx \\sqrt{\\frac{h''(t,x_t)}{2\\pi}} \\exp(K(t) -t x_t) \\\\\n     \\tag{*} \\label{*}\n$$\n\nNote that the approximation has the form of an exponential family.\n\nNow we need to do some work to get this in a more useful form.\n\nFrom \n$h'(t,x_t)=0$\n we get",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/191781/402101 \n which is a Gaussian integral, giving\n\n$$\ne^{K(t)} \\approx e^{-h(t,x_t)} \\sqrt{\\frac{2\\pi}{h''(t,x_t)}}. \n$$\n\nThis gives (a first version) of the saddlepoint approximation as\n\n$$ \nf(x_t) \\approx \\sqrt{\\frac{h''(t,x_t)}{2\\pi}} \\exp(K(t) -t x_t) \\\\\n     \\tag{*} \\label{*}\n$$\n\nNote that the approximation has the form of an exponential family.\n\nNow we need to do some work to get this in a more useful form.\n\nFrom \n$h'(t,x_t)=0$\n we get\n\n$$\n    t = -\\frac{\\partial}{\\partial x_t} \\log f(x_t).\n$$\n\nDifferentiating this with respect to \n$x_t$\n gives\n\n$$\n \\frac{\\partial t}{\\partial x_t} = -\\frac{\\partial^2}{\\partial x_t^2} \\log f(x_t) > 0$$\n\n(by our assumptions), so the relationship between \n$t$\n and \n$x_t$\n is monotone, so \n$x_t$\n is well defined. We need an approximation to \n$\\frac{\\partial}{\\partial x_t} \\log f(x_t)$\n. To that end, we get by solving from \\eqref{*}\n\n$$\n\\log f(x_t) = K(t) -t x_t -\\frac12 \\log \\frac{2\\pi}{-\\frac{\\partial^2}{\\partial x_t^2} \\log f(x_t)}.   \\tag{**}  \\label{**}\n$$\n\nAssuming the last term above only depends weakly on \n$x_t$\n, so its derivative with respect to \n$x_t$\n is approximately zero (we will come back to comment on this), we get\n\n$$\n\\frac{\\partial \\log f(x_t)}{\\partial x_t} \\approx \n  (K'(t)-x_t) \\frac{\\partial t}{\\partial x_t} - t\n$$\n\nUp to this approximation we then have that\n\n$$\n0 = t + \\frac{\\partial \\log f(x_t)}{\\partial x_t} \\approx (K'(t)-x_t) \\frac{\\partial t}{\\partial x_t}\n$$\n\nso that \n$t$\n and \n$x_t$\n must be related through the equation\n\n$$\nK'(t) - x_t=0, \\\\\n     \\tag{\u00a7} \\label{\u00a7}\n$$\n\nwhich is called the saddlepoint equation.\n\nWhat we miss now in determining \\eqref{*} is\n\n$$\n  h''(t,x_t) = -\\frac{\\partial^2 \\log f(x_t)}{\\partial x_t^2} \\\\\n = -\\frac{\\partial}{\\partial x_t} \\left(\\frac{\\partial \\log f(x_t)}{\\partial x_t}    \\right)\n  \\\\\n= -\\frac{\\partial}{\\partial x_t}(-t)= \\left(\\frac{\\partial x_t}{\\partial t}\\right)^{-1} \n$$\n\nand that we can find by implicit differentiation of the saddlepoint equation \n$K'(t)=x_t$\n:\n\n$$\n\\frac{\\partial x_t}{\\partial t} = K''(t).\n$$\n\nThe result is that (up to our approximation)\n\n$$\nh''(t,x_t) = \\frac1{K''(t)}\n$$\n\nPutting everything together, we have the final saddlepoint approximation of the density \n$f(x)$\n as\n\n$$\n   f(x_t) \\approx e^{K(t)- t x_t} \\sqrt{\\frac1{2\\pi K''(t)}}. \n$$\n\nNow, to use this practically, to approximate the density at a specific point \n$x_t$\n, we solve the saddlepoint equation for that \n$x_t$\n to find \n$t$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/191781/402101 \n $$\n\\frac{\\partial x_t}{\\partial t} = K''(t).\n$$\n\nThe result is that (up to our approximation)\n\n$$\nh''(t,x_t) = \\frac1{K''(t)}\n$$\n\nPutting everything together, we have the final saddlepoint approximation of the density \n$f(x)$\n as\n\n$$\n   f(x_t) \\approx e^{K(t)- t x_t} \\sqrt{\\frac1{2\\pi K''(t)}}. \n$$\n\nNow, to use this practically, to approximate the density at a specific point \n$x_t$\n, we solve the saddlepoint equation for that \n$x_t$\n to find \n$t$\n.\n\nThe saddlepoint approximation is often stated as an approximation to the density of the mean based on \n$n$\n iid observations \n$X_1, X_2, \\dotsc, X_n$\n.\nThe cumulant generating function of the mean is simply \n$n K(t)$\n, so the saddlepoint approximation for the mean becomes\n\n$$\nf(\\bar{x}_t) = e^{nK(t) - n t \\bar{x}_t} \\sqrt{\\frac{n}{2\\pi K''(t)}}\n$$\n\nLet us look at a first example. What does we get if we try to approximate the standard normal density\n\n$$\nf(x)=\\frac1{\\sqrt{2\\pi}} e^{-\\frac12 x^2}\n$$\n\nThe mgf is \n$M(t)=\\exp(\\frac12 t^2)$\n so\n\n$$\n   K(t)=\\frac12 t^2 \\\\\n   K'(t)=t  \\\\\n   K''(t)=1\n$$\n\nso the saddlepoint equation is \n$t=x_t$\n and the saddlepoint approximation gives\n\n$$\n  f(x_t) \\approx e^{\\frac12 t^2 -t x_t} \\sqrt{\\frac1{2\\pi \\cdot 1}}\n    = \\frac1{\\sqrt{2\\pi}} e^{-\\frac12 x_t^2} \n$$\n\nso in this case the approximation is exact.\n\nLet us look at a very different application: Bootstrap in the transform domain, we can do bootstrapping analytically using the saddlepoint approximation to the bootstrap distribution of the mean!\n\nAssume we have \n$X_1, X_2, \\dotsc, X_n$\n iid distributed from some density \n$f$\n (in the simulated example we will use a unit exponential distribution). From the sample we calculate the empirical moment generating function\n\n$$\n  \\hat{M}(t)= \\frac1{n} \\sum_{i=1}^n e^{t x_i}\n$$\n\nand then the empirical cgf \n$\\hat{K}(t) = \\log \\hat{M}(t)$\n. We need the empirical mgf for the mean which is \n$\\log ( \\hat{M}(t/n)^n )$\n and the empirical cgf for the mean\n\n$$\n  \\hat{K}_{\\bar{X}}(t) = n \\log \\hat{M}(t/n) \n$$\n\nwhich we use to construct a saddlepoint approximation. In the following some R code (R version 3.2.3):\n\nset.seed(1234)\nx  <-  rexp(10)\n\nrequire(Deriv)   ### From CRAN\ndrule[[\"sexpmean\"]]   <-  alist(t=sexpmean1(t))  # adding diff rules to \n                                                 # Deriv\ndrule[[\"sexpmean1\"]]  <-  alist(t=sexpmean2(t))\n\n###",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/191781/402101 \n $$\n  \\hat{K}_{\\bar{X}}(t) = n \\log \\hat{M}(t/n) \n$$\n\nwhich we use to construct a saddlepoint approximation. In the following some R code (R version 3.2.3):\n\nset.seed(1234)\nx  <-  rexp(10)\n\nrequire(Deriv)   ### From CRAN\ndrule[[\"sexpmean\"]]   <-  alist(t=sexpmean1(t))  # adding diff rules to \n                                                 # Deriv\ndrule[[\"sexpmean1\"]]  <-  alist(t=sexpmean2(t))\n\n###\n\nmake_ecgf_mean  <-   function(x)   {\n    n  <-  length(x)\n    sexpmean  <-  function(t) mean(exp(t*x))\n    sexpmean1 <-  function(t) mean(x*exp(t*x))\n    sexpmean2 <-  function(t) mean(x*x*exp(t*x))\n    emgf  <-  function(t) sexpmean(t)\n    ecgf  <-   function(t)  n * log( emgf(t/n) )\n    ecgf1 <-   Deriv(ecgf)\n    ecgf2 <-   Deriv(ecgf1)\n    return( list(ecgf=Vectorize(ecgf),\n                 ecgf1=Vectorize(ecgf1),\n                 ecgf2 =Vectorize(ecgf2) )    )\n}\n\n### Now we need a function solving the saddlepoint equation and constructing\n### the approximation:\n###\n\nmake_spa <-  function(cumgenfun_list) {\n    K  <- cumgenfun_list[[1]]\n    K1 <- cumgenfun_list[[2]]\n    K2 <- cumgenfun_list[[3]]\n    # local function for solving the speq:\n    solve_speq  <-  function(x) {\n          # Returns saddle point!\n          uniroot(function(s) K1(s)-x,lower=-100,\n                  upper = 100, \n                  extendInt = \"yes\")$root\n}\n    # Function finding fhat for one specific x:\n    fhat0  <- function(x) {\n        # Solve saddlepoint equation:\n        s  <-  solve_speq(x)\n        # Calculating saddlepoint density value:\n        (1/sqrt(2*pi*K2(s)))*exp(K(s)-s*x)\n    }\n    # Returning a vectorized version:\n    return(Vectorize(fhat0))\n} #end make_fhat\n\nset.seed(1234)\nx  <-  rexp(10)\n\nrequire(Deriv)   ### From CRAN\ndrule[[\"sexpmean\"]]   <-  alist(t=sexpmean1(t))  # adding diff rules to \n                                                 # Deriv\ndrule[[\"sexpmean1\"]]  <-  alist(t=sexpmean2(t))\n\n###\n\nmake_ecgf_mean  <-   function(x)   {\n    n  <-  length(x)\n    sexpmean  <-  function(t) mean(exp(t*x))\n    sexpmean1 <-  function(t) mean(x*exp(t*x))\n    sexpmean2 <-  function(t) mean(x*x*exp(t*x))\n    emgf  <-  function(t) sexpmean(t)\n    ecgf  <-   function(t)  n * log( emgf(t/n) )\n    ecgf1 <-   Deriv(ecgf)\n    ecgf2 <-   Deriv(ecgf1)\n    return( list(ecgf=Vectorize(ecgf),\n                 ecgf1=Vectorize(ecgf1),\n                 ecgf2 =Vectorize(ecgf2) )    )\n}\n\n### Now we need a function solving the saddlepoint equation and constructing\n### the approximation:\n###",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/191781/402101 \n ### Now we need a function solving the saddlepoint equation and constructing\n### the approximation:\n###\n\nmake_spa <-  function(cumgenfun_list) {\n    K  <- cumgenfun_list[[1]]\n    K1 <- cumgenfun_list[[2]]\n    K2 <- cumgenfun_list[[3]]\n    # local function for solving the speq:\n    solve_speq  <-  function(x) {\n          # Returns saddle point!\n          uniroot(function(s) K1(s)-x,lower=-100,\n                  upper = 100, \n                  extendInt = \"yes\")$root\n}\n    # Function finding fhat for one specific x:\n    fhat0  <- function(x) {\n        # Solve saddlepoint equation:\n        s  <-  solve_speq(x)\n        # Calculating saddlepoint density value:\n        (1/sqrt(2*pi*K2(s)))*exp(K(s)-s*x)\n    }\n    # Returning a vectorized version:\n    return(Vectorize(fhat0))\n} #end make_fhat\n\n( I have tried to write this as general code which can be modified easily for other cgfs, but the code is still not very robust ...)\n\nThen we use this for a sample of ten independent observations from a unit exponential distribution. We do the usual nonparametric bootstrapping \"by hand\", plot the resulting bootstrap histogram for the mean, and overplot the saddlepoint approximation:\n\n> ECGF  <- make_ecgf_mean(x)\n> fhat  <-  make_spa(ECGF)\n> fhat\nfunction (x) \n{\n    args <- lapply(as.list(match.call())[-1L], eval, parent.frame())\n    names <- if (is.null(names(args))) \n        character(length(args))\n    else names(args)\n    dovec <- names %in% vectorize.args\n    do.call(\"mapply\", c(FUN = FUN, args[dovec], MoreArgs = list(args[!dovec]), \n        SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES))\n}\n<environment: 0x4e5a598>\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> hist(boots, prob=TRUE)\n> plot(fhat, from=0.001, to=2, col=\"red\", add=TRUE)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/191781/402101 \n > ECGF  <- make_ecgf_mean(x)\n> fhat  <-  make_spa(ECGF)\n> fhat\nfunction (x) \n{\n    args <- lapply(as.list(match.call())[-1L], eval, parent.frame())\n    names <- if (is.null(names(args))) \n        character(length(args))\n    else names(args)\n    dovec <- names %in% vectorize.args\n    do.call(\"mapply\", c(FUN = FUN, args[dovec], MoreArgs = list(args[!dovec]), \n        SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES))\n}\n<environment: 0x4e5a598>\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> hist(boots, prob=TRUE)\n> plot(fhat, from=0.001, to=2, col=\"red\", add=TRUE)\n\nGiving the resulting plot:\n\n\n\nThe approximation seems to be rather good!\n\nWe could get an even better approximation by integrating the saddlepoint approximation and rescaling:\n\n> integrate(fhat, lower=0.1, upper=2)\n1.026476 with absolute error < 9.7e-07\n\n> integrate(fhat, lower=0.1, upper=2)\n1.026476 with absolute error < 9.7e-07\n\nNow the cumulative distribution function based on this approximation could be found by numerical integration, but it is also possible to make a direct saddlepoint approximation for that. But that is for another post, this is long enough.\n\nFinally, some comments left out of the development above. In \\eqref{**} we did an approximation essentially ignoring the third term. Why can we do that?  One observation is that for the normal density function,  the left-out term contributes nothing, so that approximation is exact.  So, since the saddlepoint-approximation is a refinement on the central limit theorem, so we are somewhat close to the normal, so this should work well. One can also look at specific examples. Looking at the saddlepoint approximation to the Poisson distribution, looking at that left-out third term, in this case that becomes a trigamma function, which indeed is rather flat when the argument is not to close to zero.\n\nFinally, why the name? The name come from an alternative derivation, using complex-analysis techniques.  Later we can look into that, but in another post!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/11521/402101 \n It looks like you have a standard 2x2 table and you want to condition on the margins and compare the conditional proportion to the marginal proportion.  This is not simple theoretically since they are not going to be independent.  Someone has probably solved this problem sometime, probably in a thesis somewhere, but I have no idea where to look.\n\nOn the other hand, you con easily simulate to estimate the distribution and answer many questions using that.  If you want to condition on the margins and test the null that p1/p2=1 then this is just a simple permutation test:\n\nCompute p1/p2, then randomly permute the A/B status of individual points and compute p1/p2 again, repeate a bunch of times and compare your original to the distribution.\n\nIf you want to see how your estimates of p1/p2 are distributed based on values of p1 and p2 with them unequal, then just simulate a bunch of times from binomials with the assumed p1 and p2 and do the computations.\n\nCombining both of the above would show the power of the permutation test for a given alternative.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/241063/402101 \n As essentially discussed in the comments, unbiasedness is a finite sample property, and if it held it would be expressed as\n\n$$E (\\hat \\beta ) = \\beta$$\n\n(where the expected value is the first moment of the finite-sample distribution)\n\nwhile consistency is an asymptotic property expressed as\n\n$$\\text{plim} \\hat \\beta = \\beta$$\n\nThe OP shows that even though OLS in this context is biased, it is still consistent.\n\n$$E (\\hat \\beta ) \\neq \\beta\\;\\;\\; \\text{but}\\;\\;\\; \\text{plim} \\hat \\beta = \\beta$$\n\nNo contradiction here.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/363696/402101 \n is there a way to retrieve those joint probabilities using only what\n  information is contained in the tables presented above?\n\n\n\n\nYou can \nnot\n obtain the joint probabilities from the marginal probabilities. See the following table:\n\n\n$$\\begin{array}{cc|c}\n    x_{11}-y    & x_{12}+y  &   R_1 \\\\ \n    x_{21}+y    & x_{22}-y   &  R_2  \\\\  \\hline\n    C_1    &  C_2    & N \\\\\n\\end{array}$$\n\n\nYou could derive some values for the inner cells like: \n\n\n$$x_{ij} = \\frac{R_iC_j}{N}$$\n\n\nhowever multiple \ndifferent joint probabilities\n (obtained by varying the value of $y$) can give the \nsame marginals\n. \n\n\nThus you can not go in the opposite direction and derive the joint probabilities based on the marginal probabilities because there are multiple options.\n\n\n\n\nAre there plausible assumptions/restrictions that would assist or\n  facilitate the calculations for finding the joint probabilities\n\n\n\n\nPossibly. For instance in the 2 x 2 table this can be some measure for the degree of dependence. However in the case of larger tables this becomes more and more complicated. There will be $(n_R-1)(n_C-1)$ parameters that you need to fix in order to be able to derive the joint probabilities. And for every extra dimension there will be more factors.\n\nis there a way to retrieve those joint probabilities using only what\n  information is contained in the tables presented above?\n\nYou can \nnot\n obtain the joint probabilities from the marginal probabilities. See the following table:\n\n$$\\begin{array}{cc|c}\n    x_{11}-y    & x_{12}+y  &   R_1 \\\\ \n    x_{21}+y    & x_{22}-y   &  R_2  \\\\  \\hline\n    C_1    &  C_2    & N \\\\\n\\end{array}$$\n\nYou could derive some values for the inner cells like:\n\n$$x_{ij} = \\frac{R_iC_j}{N}$$\n\nhowever multiple \ndifferent joint probabilities\n (obtained by varying the value of $y$) can give the \nsame marginals\n.\n\nThus you can not go in the opposite direction and derive the joint probabilities based on the marginal probabilities because there are multiple options.\n\nAre there plausible assumptions/restrictions that would assist or\n  facilitate the calculations for finding the joint probabilities\n\nPossibly. For instance in the 2 x 2 table this can be some measure for the degree of dependence. However in the case of larger tables this becomes more and more complicated. There will be $(n_R-1)(n_C-1)$ parameters that you need to fix in order to be able to derive the joint probabilities. And for every extra dimension there will be more factors.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n Causal Inference is an important topic in statistics generally, for both observational research and controlled experiments such as clinical trials.\n\nA \nDAG\n is a \nD\nirected \nA\ncyclic \nG\nraph.\n\nA \u201c\nG\nraph\u201d is a structure with nodes (which are usually variables in statistics) and arcs (lines) connecting nodes to other nodes. \u201c\nD\nirected\u201d means that all the arcs have a direction, where one end of the arc has an arrow head, and the other does not, which usually refers to causation. \u201c\nA\ncyclic\u201d means that the graph is not cyclic \u2013 that means there can be no path from any node that leads back to the same node.  In statistics a DAG is a very powerful tool to aid in causal inference \u2013 to estimate the causal effect of one variable (often called the main exposure) on another (often called the outcome) in the presence of other variables which may be competing exposures, confounders or mediators.  The DAG can be used to identify a minimal sufficient set of variables to be used in a multivariable regression model for the estimation of said causal effect.  For example it is usually a very bad idea to condition on a mediator (a variable that lies on the causal path between the main exposure and the outcome), while it is usually a very good idea to condition on a confounder (a variable that is a cause, or a proxy for a cause, of both the main exposure and the outcome). It is also a bad idea to condition on a collider (to be defined below).\n\nBut first, what is the problem we want to overcome?  This is what a multivariable regression model looks like to your statistical software:\n\n\n\nThe software does not \u201cknow\u201d which variables are our main exposure, competing exposures, confounders or mediators. It treats them all the same. In the real world it is far more common for the variables to be inter-related. For example, knowledge of the particular area of research may indicate a structure such as:\n\n\n\nIt is the responsibility of the researchers to determine the causal pathways by utilising their expertise in the relevant subject matter.   DAGs are graphical representations of a collection of causal ideas that are typically abstracted and relevant to certain causal relationships. It is not unusual for one researcher's DAG to differ from another researcher's DAG. Similarly, a researcher may develop several DAGs for the same study design. Employing DAGs in a systematic manner, as I will try to show below, serves as a means to acquire knowledge or substantiate a certain theory.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n Let\u2019s suppose that our primary interest is in the causal effect of \n$X7$\n on \n$Y$\n. What are we to do? A very naive approach is simply to put all the variables into a regression model, and take the estimated coefficient for \n$X7$\n as our \u201canswer\u201d. This would be a big mistake. It turns out that the \nonly\n variable that should be adjusted for in this DAG is \n$X3$\n, because it is a confounder.  But what if our interest was in the effect of \n$X3$\n, not \n$X7$\n ? Do we simply use the same model (also containing \n$X7$\n) and just take the estimate of \n$X3$\n as our \u201canswer\u201d? No! In this case, we do not adjust for \n$X7$\n because it is a mediator. No adjustment is needed at all. In both cases, we may also adjust for \n$X1$\n because this is a competing exposure and will improve the precision of our casual inferences in both models. In both models we should not adjust for \n$X2$\n, \n$X4$\n, \n$X5$\n and \n$X6$\n because all of them are mediators for the effect of \n$X7$\n on \n$Y$\n.\n\nSo, getting back to the question, how do DAGs actually enable us to do this?  First we need to establish a few ground truths.\n\nA collider is a variable which has more than 1 cause \u2013 that is, at least 2 arrows are pointing at it (hence the incoming arrows \u201ccollide\u201d). \n$X5$\n in the above DAG is a collider\n\n\n\n\nIf there are no variables being conditioned on, a path is blocked if and only if it contains a collider. The path \n$X4 \\rightarrow X5 \\leftarrow X6$\n is blocked by the collider \n$X5$\n.\n\n\nNote: when we talk about \"conditioning\" on a variable this could refer to a few things, for example stratifying, but perhaps more commonly including the variable as a covariate in a multivariable regression model. Other synonymous terms are \"controlling for\" and \"adjusting for\".\n\n\n\n\nAny path that contains a non-collider that has been conditioned on is blocked. The path \n$Y \\leftarrow X3 \\rightarrow X7$\n will be blocked if we condition on \n$X3$\n.\n\n\n\n\nA collider (or a descendant of a collider) that has been conditioned on does not block a path. If we condition on \n$X5$\n we will open the path \n$X4 \\rightarrow X5 \\leftarrow X6$\n\n\n\n\nA backdoor path is a non-causal path between an outcome and a cause. It is non-causal because it contains an arrow pointing at both the cause and the outcome. For example the path \n$Y \\leftarrow X3 \\rightarrow X7$\n is a backdoor path from \n$Y$\n to \n$X3$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n A collider (or a descendant of a collider) that has been conditioned on does not block a path. If we condition on \n$X5$\n we will open the path \n$X4 \\rightarrow X5 \\leftarrow X6$\n\n\n\n\nA backdoor path is a non-causal path between an outcome and a cause. It is non-causal because it contains an arrow pointing at both the cause and the outcome. For example the path \n$Y \\leftarrow X3 \\rightarrow X7$\n is a backdoor path from \n$Y$\n to \n$X3$\n.\n\n\n\n\nConfounding of a causal path occurs where a common cause for both variables is present. In other words confounding occurs where an unblocked backdoor path is present. Again, \n$Y \\leftarrow X3 \\rightarrow X7$\n is such a path.\n\nA collider is a variable which has more than 1 cause \u2013 that is, at least 2 arrows are pointing at it (hence the incoming arrows \u201ccollide\u201d). \n$X5$\n in the above DAG is a collider\n\nIf there are no variables being conditioned on, a path is blocked if and only if it contains a collider. The path \n$X4 \\rightarrow X5 \\leftarrow X6$\n is blocked by the collider \n$X5$\n.\n\nNote: when we talk about \"conditioning\" on a variable this could refer to a few things, for example stratifying, but perhaps more commonly including the variable as a covariate in a multivariable regression model. Other synonymous terms are \"controlling for\" and \"adjusting for\".\n\nAny path that contains a non-collider that has been conditioned on is blocked. The path \n$Y \\leftarrow X3 \\rightarrow X7$\n will be blocked if we condition on \n$X3$\n.\n\nA collider (or a descendant of a collider) that has been conditioned on does not block a path. If we condition on \n$X5$\n we will open the path \n$X4 \\rightarrow X5 \\leftarrow X6$\n\nA backdoor path is a non-causal path between an outcome and a cause. It is non-causal because it contains an arrow pointing at both the cause and the outcome. For example the path \n$Y \\leftarrow X3 \\rightarrow X7$\n is a backdoor path from \n$Y$\n to \n$X3$\n.\n\nConfounding of a causal path occurs where a common cause for both variables is present. In other words confounding occurs where an unblocked backdoor path is present. Again, \n$Y \\leftarrow X3 \\rightarrow X7$\n is such a path.\n\nSo, armed with this knowledge, let\u2019s see how DAGs help us with removing bias:\n\nConfounding\n\nThe definition of confounding is 6 above.  If we apply 4 and condition on the confounder we will block the backdoor path from the outcome to the cause, thereby removing confounding bias.  The example is the association of carrying a lighter and lung cancer:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n So, armed with this knowledge, let\u2019s see how DAGs help us with removing bias:\n\nConfounding\n\nThe definition of confounding is 6 above.  If we apply 4 and condition on the confounder we will block the backdoor path from the outcome to the cause, thereby removing confounding bias.  The example is the association of carrying a lighter and lung cancer:\n\n\n\nCarrying a lighter has no causal effect on lung cancer, however, they share a common cause - smoking - so applying rule 5 above, a backdoor path from Lung cancer to carrying a lighter is present which induces an association between carrying a lighter and Lung cancer. Conditioning on Smoking will remove this association, which can be demonstrate with a simple simulation where I use continuous variables for simplicity:\n\n> set.seed(15)\n> N       <- 100\n> Smoking <- rnorm(N, 10, 2)\n> Cancer  <- Smoking + rnorm(N)\n> Lighter <- Smoking + rnorm(N)\n\n> summary(lm(Cancer ~ Lighter)) \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.66263    0.76079   0.871    0.386    \nLighter      0.91076    0.07217  12.620   <2e-16 ***\n\n> set.seed(15)\n> N       <- 100\n> Smoking <- rnorm(N, 10, 2)\n> Cancer  <- Smoking + rnorm(N)\n> Lighter <- Smoking + rnorm(N)\n\n> summary(lm(Cancer ~ Lighter)) \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.66263    0.76079   0.871    0.386    \nLighter      0.91076    0.07217  12.620   <2e-16 ***\n\nwhich shows the spurious association between Lighter and Cancer, but now when we condition on Smoking:\n\n> summary(lm(Cancer ~ Lighter + Smoking))  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.42978    0.60363  -0.712    0.478    \nLighter      0.07781    0.11627   0.669    0.505    \nSmoking      0.95215    0.11658   8.168 1.18e-12 ***\n\n> summary(lm(Cancer ~ Lighter + Smoking))  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.42978    0.60363  -0.712    0.478    \nLighter      0.07781    0.11627   0.669    0.505    \nSmoking      0.95215    0.11658   8.168 1.18e-12 ***\n\n...the bias is removed.\n\nMediation",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n > summary(lm(Cancer ~ Lighter + Smoking))  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.42978    0.60363  -0.712    0.478    \nLighter      0.07781    0.11627   0.669    0.505    \nSmoking      0.95215    0.11658   8.168 1.18e-12 ***\n\n...the bias is removed.\n\nMediation\n\nA mediator is a variable that lies on the causal path between the cause and the outcome.  This means that the outcome is a collider. Therefore, applying rule 3 means that we should not condition on the mediator otherwise the indirect effect of the cause on the outcome (i.e., that mediated by the mediator) will be blocked. A good example example is the grades of a student and their happiness. A mediating variable is self-esteem:\n\n\n\nHere, Grades has a direct effect on Happiness, but it also has an indirect effect mediated by self-esteem. We want to estimate the total causal effect of Grades on Happiness. Rule 3 says that a path that contains a non-collider that has been conditioned on is blocked. Since we want the total effect (i.e., including the indirect effect) we should not condition on self-esteem otherwise the mediated path will be blocked, as we can see in the following simulation:\n\n> set.seed(15)\n> N          <- 100\n> Grades     <- rnorm(N, 10, 2)\n> SelfEsteem <- Grades + rnorm(N)\n> Happiness  <- Grades + SelfEsteem + rnorm(N)\n\n> set.seed(15)\n> N          <- 100\n> Grades     <- rnorm(N, 10, 2)\n> SelfEsteem <- Grades + rnorm(N)\n> Happiness  <- Grades + SelfEsteem + rnorm(N)\n\nSo the total effect should be 2:\n\n> summary(m0 <- lm(Happiness ~ Grades)) # happy times\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.05650    0.79509   1.329    0.187    \nGrades       1.90003    0.07649  24.840   <2e-16 ***\n\n> summary(m0 <- lm(Happiness ~ Grades)) # happy times\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.05650    0.79509   1.329    0.187    \nGrades       1.90003    0.07649  24.840   <2e-16 ***\n\nwhich is what we do find. But if we now condition on self esteem:\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.39804    0.50783   2.753  0.00705 ** \nGrades       0.81917    0.10244   7.997 2.73e-12 ***\nSelfEsteem   1.05907    0.08826  11.999  < 2e-16 ***\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n which is what we do find. But if we now condition on self esteem:\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.39804    0.50783   2.753  0.00705 ** \nGrades       0.81917    0.10244   7.997 2.73e-12 ***\nSelfEsteem   1.05907    0.08826  11.999  < 2e-16 ***\n\n> summary(m0 <- lm(Happiness ~ Grades + SelfEsteem\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.39804    0.50783   2.753  0.00705 ** \nGrades       0.81917    0.10244   7.997 2.73e-12 ***\nSelfEsteem   1.05907    0.08826  11.999  < 2e-16 ***\n\nonly the direct effect for grades is estimated, due to blocking the indirect effect by conditioning on the mediator \nSelfEsteem\n.\n\nSelfEsteem\n\nCollider bias\n\nThis is probably the most difficult one to understand, but with the aid of a very simple DAG we can easily see the problem:\n\n\n\nHere, there is no causal path between X and Y. However, both cause C, the collider. If we condition on C, then applying rule 4 above we will invoke collider bias by opening up the (non causal) path between X, and Y. This may be a little hard to grasp at first, but it should become apparent by thinking in terms of equations. We have X + Y = C.  Let X and Y be binary variables taking the values 1 or zero. Hence, C can only take the values of 0, 1 or 2. Now, when we condition on C we fix its value. Say we fix it at 1. This immediately means that if X is zero then Y must be 1, and if Y is zero then X must be one. That is, X = -Y, so they are perfectly (negatively) correlated, conditional on C= 1. We can also see this in action with the following simulation:\n\n> set.seed(16)\n> N <- 100\n> X <- rnorm(N, 10, 2)\n> Y <- rnorm(N, 15, 3)\n> C <- X + Y + rnorm(N)\n\n> set.seed(16)\n> N <- 100\n> X <- rnorm(N, 10, 2)\n> Y <- rnorm(N, 15, 3)\n> C <- X + Y + rnorm(N)\n\nSo, X and Y are independent so we should find no association:\n\n> summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\n> summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\nand indeed no association is found. But now condition on C\n\n> summary(m1 <- lm(Y ~ X + C))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n > summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\n> summary(m0 <- lm(Y ~ X))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.18496    1.54838   9.161 8.01e-15 ***\nX            0.08604    0.15009   0.573    0.568\n\nand indeed no association is found. But now condition on C\n\n> summary(m1 <- lm(Y ~ X + C))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.10461    0.61206   1.805   0.0742 .  \nX           -0.92633    0.05435 -17.043   <2e-16 ***\nC            0.92454    0.02881  32.092   <2e-16 ***\n\n> summary(m1 <- lm(Y ~ X + C))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.10461    0.61206   1.805   0.0742 .  \nX           -0.92633    0.05435 -17.043   <2e-16 ***\nC            0.92454    0.02881  32.092   <2e-16 ***\n\nand now we have a spurious association between X and Y.\n\nNow let\u2019s consider a slightly more complex situation:\n\n\n\nHere we are interested in the causal effect of Activity on Cervical Cancer. Hypochondria is an unmeasured variable which is a psychological condition that is characterized by fears of minor and sometimes non-existent medical symptoms being an indication of major illness. Lesion is also an unobserved variable that indicates the presence of a pre-cancerous lesion. Test is a diagnostic test for early stage cervical cancer. Here we hypothesise that both the unmeasured variables affect Test, obviously in the case of Lesion, and by making frequent visits to the doctor in the case of Hypochondria. Lesion also (obviously causes Cancer) and Hypochondria causes more physical activity (because persons with hypochondria are worried about a sedentary lifestyle leading to disease in later life.\n\nFirst notice that if the collider, Test, was removed and replace with an arc either from Lesion to Hypochondria or vice versa, then our causal path of interest, Activity to Cancer, would be confounded, but due to rule 2 above, the collider blocks the backdoor path \n$\\text{Cancer}\\leftarrow \\text{Lesion} \\rightarrow \\text{Test} \\leftarrow \\text{Hypochondria} \\rightarrow \\text{Activity}$\n, as we can see with a simple simulation:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n First notice that if the collider, Test, was removed and replace with an arc either from Lesion to Hypochondria or vice versa, then our causal path of interest, Activity to Cancer, would be confounded, but due to rule 2 above, the collider blocks the backdoor path \n$\\text{Cancer}\\leftarrow \\text{Lesion} \\rightarrow \\text{Test} \\leftarrow \\text{Hypochondria} \\rightarrow \\text{Activity}$\n, as we can see with a simple simulation:\n\n> set.seed(16)\n> N            <- 100\n> Lesion       <- rnorm(N, 10, 2)\n> Hypochondria <- rnorm(N, 10, 2)\n> Test         <- Lesion + Hypochondria + rnorm(N)\n> Activity     <- Hypochondria + rnorm(N)\n> Cancer       <- Lesion + 0.25 * Activity + rnorm(N)\n\n> set.seed(16)\n> N            <- 100\n> Lesion       <- rnorm(N, 10, 2)\n> Hypochondria <- rnorm(N, 10, 2)\n> Test         <- Lesion + Hypochondria + rnorm(N)\n> Activity     <- Hypochondria + rnorm(N)\n> Cancer       <- Lesion + 0.25 * Activity + rnorm(N)\n\nwhere we hypothesize a much smaller effect of Activity on Cancer than Lesion on Cancer\n\n> summary(lm(Cancer ~ Activity))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.47570    1.01150  10.357   <2e-16 ***\nActivity     0.21103    0.09667   2.183   0.0314 *\n\n> summary(lm(Cancer ~ Activity))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 10.47570    1.01150  10.357   <2e-16 ***\nActivity     0.21103    0.09667   2.183   0.0314 *\n\nAnd indeed we obtain a reasonable estimate.\n\nNow, also observe the association of Activity and Cancer with Test (due to their common, but unmeasured causes:\n\n> cor(Test, Activity); cor(Test, Cancer)\n[1] 0.6245565\n[1] 0.7200811\n\n> cor(Test, Activity); cor(Test, Cancer)\n[1] 0.6245565\n[1] 0.7200811\n\nThe traditional definition of confounding is that a confounder is variable that is \nassociated\n with both the exposure and the outcome. So, we might mistakenly think that Test is a confounder and condition on it. However, we then open up the backdoor path \n$\\text{Cancer}\\leftarrow \\text{Lesion} \\rightarrow \\text{Test} \\leftarrow \\text{Hypochondria} \\rightarrow \\text{Activity}$\n, and introduce confounding which would otherwise not be present, as we can see from:\n\n> summary(lm(Cancer ~ Activity + Test))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.77204    0.98383   1.801   0.0748 .  \nActivity    -0.37663    0.07971  -4.725 7.78e-06 ***\nTest         0.72716    0.06160  11.804  < 2e-16 ***",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/445590/402101 \n > summary(lm(Cancer ~ Activity + Test))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.77204    0.98383   1.801   0.0748 .  \nActivity    -0.37663    0.07971  -4.725 7.78e-06 ***\nTest         0.72716    0.06160  11.804  < 2e-16 ***\n\n> summary(lm(Cancer ~ Activity + Test))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.77204    0.98383   1.801   0.0748 .  \nActivity    -0.37663    0.07971  -4.725 7.78e-06 ***\nTest         0.72716    0.06160  11.804  < 2e-16 ***\n\nNow not only is the estimate for Activity biased, but it is of larger magnitude and of the opposite sign!\n\nSelection bias\n\nThe preceding example can also be used to demonstrate selection bias. A researcher may identify Test as a potential confounder, and then only conduct the analysis on those that have tested negative (or positive).\n\n> dtPos <- data.frame(Lesion, Hypochondria, Test, Activity, Cancer)\n> dtNeg <- dtPos[dtPos\n$Test <  22, ]\n> dtPos <- dtPos[dtPos$\nTest >= 22, ]\n> summary(lm(Cancer ~ Activity, data = dtPos))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.15915    3.07604   4.278 0.000242 ***\nActivity     0.08662    0.25074   0.345 0.732637\n\n> dtPos <- data.frame(Lesion, Hypochondria, Test, Activity, Cancer)\n> dtNeg <- dtPos[dtPos\n$Test <  22, ]\n> dtPos <- dtPos[dtPos$\nTest >= 22, ]\n> summary(lm(Cancer ~ Activity, data = dtPos))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.15915    3.07604   4.278 0.000242 ***\nActivity     0.08662    0.25074   0.345 0.732637\n\nSo for those that test positive we obtain a very small positive effect, that is not statistically significant at the 5% level\n\n> summary(lm(Cancer ~ Activity, data = dtNeg))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.18865    1.12071  10.876   <2e-16 ***\nActivity    -0.01553    0.11541  -0.135    0.893\n\n> summary(lm(Cancer ~ Activity, data = dtNeg))\n\n    Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.18865    1.12071  10.876   <2e-16 ***\nActivity    -0.01553    0.11541  -0.135    0.893\n\nAnd for those that test negative we obtain a very small negative association which is also not significant.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/268646/402101 \n In a parametric model, the number of parameters is fixed with respect to the sample size.  In a nonparametric model, the (effective) number of parameters can grow with the sample size.\n\nIn an OLS regression, the number of parameters will always be the length of $\\beta$, plus one for the variance.\n\nA neural net with fixed architecture and no weight decay would be a parametric model.\n\nBut if you have weight decay, then the value of the decay parameter selected by cross-validation will generally get smaller with more data.  This can be interpreted as an increase in the effective number of parameters with increasing sample size.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/133435/402101 \n AUC = Area Under the Curve.\n\n\nAUROC = \nArea Under the Receiver Operating Characteristic curve\n.\n\nAUC is used most of the time to mean AUROC, which is a bad practice since as Marc Claesen pointed out AUC is ambiguous (could be any curve) while AUROC is not.\n\nThe AUROC has \nseveral equivalent interpretations\n:\n\nThe expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\n\n\nThe expected proportion of positives ranked before a uniformly drawn random negative.\n\n\nThe expected true positive rate if the ranking is split just before a uniformly drawn random negative.\n\n\nThe expected proportion of negatives ranked after a uniformly drawn random positive.\n\n\nThe expected false positive rate if the ranking is split just after a uniformly drawn random positive.\n\nGoing further: \nHow to derive the probabilistic interpretation of the AUROC?\n\nAssume we have a probabilistic, binary classifier such as logistic regression.\n\nBefore presenting the ROC curve (= Receiver Operating Characteristic curve), the concept of \nconfusion matrix\n must be understood. When we make a binary prediction, there can be 4 types of outcomes:\n\nWe predict 0 while the true class is actually 0: this is called a \nTrue Negative\n, i.e. we correctly predict that the class is negative (0). For example, an antivirus did not detect a harmless file as a virus .\n\n\nWe predict 0 while the true class is actually 1: this is called a \nFalse Negative\n, i.e. we incorrectly predict that the class is negative (0). For example, an antivirus failed to detect a virus.\n\n\nWe predict 1 while the true class is actually 0: this is called a \nFalse Positive\n, i.e. we incorrectly predict that the class is positive (1). For example, an antivirus considered a harmless file to be a virus.\n\n\nWe predict 1 while the true class is actually 1: this is called a \nTrue Positive\n, i.e. we correctly predict that the class is positive (1). For example, an antivirus rightfully detected a virus.\n\nTo get the confusion matrix, we go over all the predictions made by the model, and count how many times each of those 4 types of outcomes occur:\n\n\n\nIn this example of a confusion matrix, among the 50 data points that are classified, 45 are correctly classified and the 5 are misclassified.\n\nSince to compare two different models it is often more convenient to have a single metric rather than several ones, we compute two metrics from the confusion matrix, which we will later combine into one:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/133435/402101 \n To get the confusion matrix, we go over all the predictions made by the model, and count how many times each of those 4 types of outcomes occur:\n\n\n\nIn this example of a confusion matrix, among the 50 data points that are classified, 45 are correctly classified and the 5 are misclassified.\n\nSince to compare two different models it is often more convenient to have a single metric rather than several ones, we compute two metrics from the confusion matrix, which we will later combine into one:\n\nTrue positive rate\n (\nTPR\n), aka. sensitivity, \nhit rate\n, and \nrecall\n, which is defined as \n$ \\frac{TP}{TP+FN}$\n. Intuitively this metric corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points. In other words, the higher TPR, the fewer positive data points we will miss.\n\n\nFalse positive rate\n (\nFPR\n), aka. \nfall-out\n, which is defined as \n$ \\frac{FP}{FP+TN}$\n. Intuitively this metric corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.  In other words, the higher FPR, the more negative data points will be missclassified.\n\nTo combine the FPR and the TPR into one single metric, we first compute the two former metrics with many different threshold (for example \n$0.00; 0.01, 0.02, \\dots, 1.00$\n) for the logistic regression, then plot them on a single graph, with the FPR values on the abscissa and the TPR values on the ordinate. The resulting curve is called ROC curve, and the metric we consider is the AUC of this curve, which we call AUROC.\n\nThe following figure shows the AUROC graphically:\n\n\n\nIn this figure, the blue area corresponds to the Area Under the curve of the Receiver Operating Characteristic (AUROC). The dashed line in the diagonal we present the ROC curve of a random predictor: it has an AUROC of 0.5. The random predictor is commonly used as a baseline to see whether the model is useful.\n\nIf you want to get some first-hand experience:\n\nPython: \nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n\n\nMATLAB: \nhttp://www.mathworks.com/help/stats/perfcurve.html",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/576/402101 \n What is a difference in differences estimator\n\nDifference in differences (DiD) is a tool to estimate treatment effects comparing the pre- and post-treatment differences in the outcome of a treatment and a control group. In general, we are interested in estimating the effect of a treatment $D_i$ (e.g. union status, medication, etc.) on an outcome $Y_i$ (e.g. wages, health, etc.) as in\n$$Y_{it} = \\alpha_i + \\lambda_t + \\rho D_{it} + X'_{it}\\beta + \\epsilon_{it}$$\nwhere $\\alpha_i$ are individual fixed effects (characteristics of individuals that do not change over time), $\\lambda_t$ are time fixed effects, $X_{it}$ are time-varying covariates like individuals' age, and $\\epsilon_{it}$ is an error term. Individuals and time are indexed by $i$ and $t$, respectively. If there is a correlation between the fixed effects and $D_{it}$ then estimating this regression via OLS will be biased given that the fixed effects are not controlled for. This is the typical \nomitted variable bias\n.\n\nTo see the effect of a treatment we would like to know the difference between a person in a world in which she received the treatment and one in which she does not. Of course, only one of these is ever observable in practice. Therefore we look for people with the same pre-treatment trends in the outcome. Suppose we have two periods $t = 1, 2$ and two groups $s = A,B$. Then, under the assumption that the trends in the treatment and control groups would have continued the same way as before in the absence of treatment, we can estimate the treatment effect as\n$$\\rho = (E[Y_{ist}|s=A,t=2] - E[Y_{ist}|s=A,t=1]) - (E[Y_{ist}|s=B,t=2] - E[Y_{ist}|s=B,t=1])$$\n\nGraphically this would look something like this:\n\nYou can simply calculate these means by hand, i.e. obtain the mean outcome of group $A$ in both periods and take their difference. Then obtain the mean outcome of group $B$ in both periods and take their difference. Then take the difference in the differences and that's the treatment effect. However, it is more convenient to do this in a regression framework because this allows you\n\nto control for covariates\n\n\nto obtain standard errors for the treatment effect to see if it is significant",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/576/402101 \n You can simply calculate these means by hand, i.e. obtain the mean outcome of group $A$ in both periods and take their difference. Then obtain the mean outcome of group $B$ in both periods and take their difference. Then take the difference in the differences and that's the treatment effect. However, it is more convenient to do this in a regression framework because this allows you\n\nto control for covariates\n\n\nto obtain standard errors for the treatment effect to see if it is significant\n\nTo do this, you can follow either of two equivalent strategies. Generate a control group dummy $\\text{treat}_i$ which is equal to 1 if a person is in group $A$ and 0 otherwise, generate a time dummy $\\text{time}_t$ which is equal to 1 if $t=2$ and 0 otherwise, and then regress\n$$Y_{it} = \\beta_1 + \\beta_2 (\\text{treat}_i) + \\beta_3 (\\text{time}_t) + \\rho (\\text{treat}_i \\cdot \\text{time}_t) + \\epsilon_{it}$$\n\nOr you simply generate a dummy $T_{it}$ which equals one if a person is in the treatment group AND the time period is the post-treatment period and is zero otherwise. Then you would regress\n$$Y_{it} = \\beta_1 \\gamma_s + \\beta_2 \\lambda_t + \\rho T_{it} + \\epsilon_{it}$$\n\nwhere $\\gamma_s$ is again a dummy for the control group and $\\lambda_t$ are time dummies. The two regressions give you the same results for two periods and two groups. The second equation is more general though as it easily extends to multiple groups and time periods. In either case, this is how you can estimate the difference in differences parameter in a way such that you can include control variables (I left those out from the above equations to not clutter them up but you can simply include them) and obtain standard errors for inference.\n\nWhy is the difference in differences estimator useful?\n\nAs stated before, DiD is a method to estimate treatment effects with non-experimental data. That's the most useful feature. DiD is also a version of fixed effects estimation. Whereas the fixed effects model assumes $E(Y_{0it}|i,t) = \\alpha_i + \\lambda_t$, DiD makes a similar assumption but at the group level, $E(Y_{0it}|s,t) = \\gamma_s + \\lambda_t$. So the expected value of the outcome here is the sum of a group and a time effect. So what's the difference? For DiD you don't necessarily need panel data as long as your repeated cross sections are drawn from the same aggregate unit $s$. This makes DiD applicable to a wider array of data than the standard fixed effects models that require panel data.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/576/402101 \n Can we trust difference in differences?\n\nThe most important assumption in DiD is the parallel trends assumption (see the figure above). Never trust a study that does not graphically show these trends! Papers in the 1990s might have gotten away with this but nowadays our understanding of DiD is much better. If there is no convincing graph that shows the parallel trends in the pre-treatment outcomes for the treatment and control groups, be cautious. If the parallel trends assumption holds and we can credibly rule out any other time-variant changes that may confound the treatment, then DiD is a trustworthy method.\n\nAnother word of caution should be applied when it comes to the treatment of standard errors. With many years of data you need to adjust the standard errors for autocorrelation. In the past, this has been neglected but since \nBertrand et al. (2004) \"How Much Should We Trust Differences-In-Differences Estimates?\"\n we know that this is an issue. In the paper they provide several remedies for dealing with autocorrelation. The easiest is to cluster on the individual panel identifier which allows for arbitrary correlation of the residuals among individual time series. This corrects for both autocorrelation and heteroscedasticity.\n\nFor further references see these lecture notes by \nWaldinger\n and \nPischke\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/162435/402101 \n Pearl's theory of causality is completely \nnon-parametric\n. Interactions are not made explicit because of that, neither in the graph nor in the structural equations it represents. However, causal effects can vary (wildly) by assumption.\n\nIf an effect is identified and you estimate it from data non-parametrically, you obtain a complete distribution of causal effects (instead of, say, a single parameter). Accordingly, you can evaluate the causal effect of tobacco exposure conditional on asbestos exposure non-parametrically to see whether it changes, without committing to any functional form.\n\nLet's have a look at the structural equations in your case, which correspond to your \"DAG\" stripped of the red arrow:\n\nMesothelioma = $f_{1}$(Tobacco, Asbestos, $\\epsilon_{m}$)\n\nTobacco = $f_{2}$($\\epsilon_{t}$)\n\nAsbestos = $f_{3}$($\\epsilon_{a}$)\n\nwhere the $\\epsilon$ are assumed to be independent because of missing dashed arrows between them.\n\nWe have left the respective functions f() and the distributions of the errors unspecified, except for saying that the latter are independent. Nonetheless, we can apply Pearl's theory and immediately state that the causal effects of both tobacco and asbestos exposure on mesothelioma are \nidentified\n. This means that if we had infinitely many observations from this process, we could exactly measure the effect of \nsetting\n the exposures to different levels by simply \nseeing\n the incidences of mesothelioma in individuals with different levels of exposure. So we could infer causality without doing an actual experiment. This is because there exist no back-door paths from the exposure variables to the outcome variable.\n\nSo you would get\n\nP(mesothelioma | do(Tobacco = t)) = P(mesothelioma | Tobacco = t)\n\nThe same logic holds for the causal effect of asbestos, which allows you to simply evaluate:\n\nP(mesothelioma | Tobacco = t, Asbestos = a) - P(mesothelioma | Tobacco = t', Asbestos = a)\n\nin comparison to\n\nP(mesothelioma | Tobacco = t, Asbestos = a') - P(mesothelioma | Tobacco = t', Asbestos = a')\n\nfor all relevant values of t and a in order to estimate the interaction effects.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/162435/402101 \n So you would get\n\nP(mesothelioma | do(Tobacco = t)) = P(mesothelioma | Tobacco = t)\n\nThe same logic holds for the causal effect of asbestos, which allows you to simply evaluate:\n\nP(mesothelioma | Tobacco = t, Asbestos = a) - P(mesothelioma | Tobacco = t', Asbestos = a)\n\nin comparison to\n\nP(mesothelioma | Tobacco = t, Asbestos = a') - P(mesothelioma | Tobacco = t', Asbestos = a')\n\nfor all relevant values of t and a in order to estimate the interaction effects.\n\nIn your concrete example, let's assume that the outcome variable is a Bernoulli variable - you can either have mesothelioma or not - and that a person has been exposed to a very high asbestos level a. Then, it is very likely that he will suffer from mesothelioma; accordingly, the effect of increasing tobacco exposure will be very low. On the other hand, if asbestos levels a' are very low, increasing tobacco exposure will have a greater effect. This would constitute an interaction between the effects of tobacco and asbestos.\n\nOf course, non-parametric estimation can be extremely demanding and noisy with finite data and lots of different t and a values, so you might think about assuming some structure in f(). But basically you can do it without that.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/310919/402101 \n This is a broad question, but given the Box, Hunter and Hunter quote is true I think what it comes down to is\n\nThe quality of the experimental design:\n\n\n\n\nrandomization, sample sizes, control of confounders,...\n\n\n\n\nThe quality of the implementation of the design:\n\n\n\n\nadherance to protocol, measurement error, data handling, ...\n\n\n\n\nThe quality of the model to accurately reflect the design:\n\n\n\n\nblocking structures are accurately represented, proper degrees of freedom are associated with effects, estimators are unbiased, ...\n\nThe quality of the experimental design:\n\nrandomization, sample sizes, control of confounders,...\n\nThe quality of the implementation of the design:\n\nadherance to protocol, measurement error, data handling, ...\n\nThe quality of the model to accurately reflect the design:\n\nblocking structures are accurately represented, proper degrees of freedom are associated with effects, estimators are unbiased, ...\n\nAt the risk of stating the obvious I'll try to hit on the key points of each:\n\nis a large sub-field of statistics, but in it's most basic form I think it comes down to the fact that when making causal inference we ideally start with identical units that are monitored in identical environments other than being assigned to a treatment.  Any systematic differences between groups after assigment are then logically attributable to the treatment (we can infer cause).  But, the world isn't that nice and units differ prior to treatment and evironments during experiments are not perfectly controlled.  So we \"control what we can and randomize what we can't\", which helps to insure that there won't be systematic bias due to the confounders that we controlled or randomized.  One problem is that experiments tend to be difficult (to impossible) and expensive and a large variety of designs have been developed to efficiently extract as much information as possible in as carefully controlled a setting as possible, given the costs.  Some of these are quite rigorous (e.g. in medicine the double-blind, randomized, placebo-controlled trial) and others less so (e.g. various forms of 'quasi-experiments').",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/310919/402101 \n is also a big issue and one that statisticians generally don't think about...though we should.  In applied statistical work I can recall incidences where 'effects' found in the data were spurious results of inconsistency of data collection or handling.  I also wonder how often information on true causal effects of interest is lost due to these issues (I believe students in the applied sciences generally have little-to-no training about ways that data can become corrupted - but I'm getting off topic here...)\n\n\nis another large technical subject, and another necessary step in objective causal inference.  To a certain degree this is taken care of because the design crowd develop designs and models together (since inference from a model is the goal, the attributes of the estimators drive design). But this only gets us so far because in the 'real world' we end up analysing experimental data from non-textbook designs and then we have to think hard about things like the appropriate controls and how they should enter the model and what associated degrees of freedom should be and whether assumptions are met if if not how to adjust of violations and how robust the estimators are to any remaining violations and...\n\nis a large sub-field of statistics, but in it's most basic form I think it comes down to the fact that when making causal inference we ideally start with identical units that are monitored in identical environments other than being assigned to a treatment.  Any systematic differences between groups after assigment are then logically attributable to the treatment (we can infer cause).  But, the world isn't that nice and units differ prior to treatment and evironments during experiments are not perfectly controlled.  So we \"control what we can and randomize what we can't\", which helps to insure that there won't be systematic bias due to the confounders that we controlled or randomized.  One problem is that experiments tend to be difficult (to impossible) and expensive and a large variety of designs have been developed to efficiently extract as much information as possible in as carefully controlled a setting as possible, given the costs.  Some of these are quite rigorous (e.g. in medicine the double-blind, randomized, placebo-controlled trial) and others less so (e.g. various forms of 'quasi-experiments').",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/310919/402101 \n is also a big issue and one that statisticians generally don't think about...though we should.  In applied statistical work I can recall incidences where 'effects' found in the data were spurious results of inconsistency of data collection or handling.  I also wonder how often information on true causal effects of interest is lost due to these issues (I believe students in the applied sciences generally have little-to-no training about ways that data can become corrupted - but I'm getting off topic here...)\n\nis another large technical subject, and another necessary step in objective causal inference.  To a certain degree this is taken care of because the design crowd develop designs and models together (since inference from a model is the goal, the attributes of the estimators drive design). But this only gets us so far because in the 'real world' we end up analysing experimental data from non-textbook designs and then we have to think hard about things like the appropriate controls and how they should enter the model and what associated degrees of freedom should be and whether assumptions are met if if not how to adjust of violations and how robust the estimators are to any remaining violations and...\n\nAnyway, hopefully some of the above helps in thinking about considerations in making causal inference from a model.  Did I forget anything big?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6336/402101 \n Personally, I'd favour instead showing the \nfit\n of the theoretical to the empirical distribution using a set of \nP-P plots\n or \nQ-Q plots\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6348/402101 \n Some caveats before to proceed. As I often suggest to my students, use \nauto.arima()\n things only as a first approximation to your final result or if you want to have parsimonious model when you check that your rival theory-based model do better.\n\nauto.arima()\n\nData\n\nYou have clearly to start from the description of time series data you are working with. In macro-econometrics you usually work with aggregated data, and geometric means (surprisingly) have more empirical evidence for macro time series data, probably because most of them decomposable into \nexponentially growing trend\n.\n\nBy the way Rob's suggestion \"visually\" works for time series with \nclear seasonal part\n, as slowly varying annual data is less clear for the increases in variation. Luckily exponentially growing trend is usually seen (if it seems to be linear, than no need for logs).\n\nModel\n\nIf your analysis is based on some theory that states that some \nweighted geometric mean\n $Y(t) = X_1^{\\alpha_1}(t)...X_k^{\\alpha_k}(t)\\varepsilon(t)$ more known as the \nmultiplicative regression model\n is the one you have to work with. Then you usually move to a \nlog-log regression model\n, that is linear in parameters and most of your variables, but some growth rates, are transformed.\n\nIn financial econometrics logs are a common thing due to the popularity of log-returns, because...\n\nLog transformations have nice properties\n\nIn log-log regression model it is the interpretation of estimated parameter, say $\\alpha_i$ as the \nelasticity\n of $Y(t)$ on $X_i(t)$.\n\nIn error-correction models we have an empirically stronger assumption that \nproportions are more stable\n (\nstationary\n) than the absolute differences.\n\nIn financial econometrics it is \neasy to aggregate the log-returns over time\n.\n\nThere are many other reasons not mentioned here.\n\nFinally\n\nNote that log-transformation is usually applied to non-negative (level) variables. If you observe the differences of two time series (net export, for instance) it is not even possible to take the log, you have either to search for original data in levels or assume the form of common trend that was subtracted.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6348/402101 \n In financial econometrics it is \neasy to aggregate the log-returns over time\n.\n\nThere are many other reasons not mentioned here.\n\nFinally\n\nNote that log-transformation is usually applied to non-negative (level) variables. If you observe the differences of two time series (net export, for instance) it is not even possible to take the log, you have either to search for original data in levels or assume the form of common trend that was subtracted.\n\n[\naddition after edit\n] If you still want a \nstatistical criterion\n for when to do log transformation a simple solution would be any test for heteroscedasticity. In the case of increasing variance I would recommend  \nGoldfeld-Quandt Test\n or similar to it. In R it is located in \nlibrary(lmtest)\n and is denoted by \ngqtest(y~1)\n function. Simply regress on intercept term if you don't have any regression model, \ny\n is your dependent variable.\n\nlibrary(lmtest)\n\ngqtest(y~1)\n\ny",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/208683/402101 \n Here is a guide to solving this problem (and others like it).\n  I use simulated values to illustrate, so let's begin by simulating a large number of independent realizations from the distribution with density $f$.  (All the code in this answer is written in \nR\n.)\n\nR\n\nn <- 4e4 # Number of trials in the simulation\nx <- matrix(pmax(runif(n*3), runif(n*3)), nrow=3)\n\n# Plot the data\npar(mfrow=c(1,3))\nfor (i in 1:3) {\n  hist(x[i, ], freq=FALSE, main=paste(\"i =\", i))\n  curve(f(x), add=TRUE, col=\"Red\", lwd=2)\n}\n\nn <- 4e4 # Number of trials in the simulation\nx <- matrix(pmax(runif(n*3), runif(n*3)), nrow=3)\n\n# Plot the data\npar(mfrow=c(1,3))\nfor (i in 1:3) {\n  hist(x[i, ], freq=FALSE, main=paste(\"i =\", i))\n  curve(f(x), add=TRUE, col=\"Red\", lwd=2)\n}\n\n\n\nThe histograms show $40,000$ independent realizations of the first, second, and third elements of the datasets.  The red curves graph $f$.  That they coincide with the histograms confirms the simulation is working as intended.\n\nYou need to work out the joint density of $(Y_1, Y_2, Y_3)$.\n Since you're studying order statistics, this should be routine--but the code gives some clues, because it plots their distributions for reference.\n\ny <- apply(x, 2, sort)\n\n# Plot the order statistics.\nf <- function(x) 2*x\nff <- function(x) x^2\nfor (i in 1:3) {\n  hist(y[i, ], freq=FALSE, main=paste(\"i =\", i))\n  k <- factorial(3) / (factorial(3-i)*factorial(1)*factorial(i-1))\n  curve(k * (1-ff(x))^(3-i) * f(x) * ff(x)^(i-1), add=TRUE, col=\"Red\", lwd=2)\n}\n\ny <- apply(x, 2, sort)\n\n# Plot the order statistics.\nf <- function(x) 2*x\nff <- function(x) x^2\nfor (i in 1:3) {\n  hist(y[i, ], freq=FALSE, main=paste(\"i =\", i))\n  k <- factorial(3) / (factorial(3-i)*factorial(1)*factorial(i-1))\n  curve(k * (1-ff(x))^(3-i) * f(x) * ff(x)^(i-1), add=TRUE, col=\"Red\", lwd=2)\n}\n\n\n\nThe same data have been reordered within each of the $40,000$ datasets.  On the left is the histogram of their minima $Y_1$, on the right their maxima $Y_3$, and in the middle their medians $Y_2$.\n\nNext, compute the joint distribution of $(U_1, U_2)$ directly.\n  By definition this is\n\n$$F(u_1, u_2) = \\Pr(U_1 \\le u_1, U_2 \\le u_2) = \\Pr(Y_1 \\le u_1 Y_2, Y_2 \\le u_2 Y_3).$$\n\nSince you have computed the joint density of $(Y_1, Y_2, Y_3)$, this is a routine matter of doing the (triple) integral expressed by the right-hand probability.  The region of integration must be $$0 \\le Y_1 \\le u_1 Y_2,\\ 0 \\le Y_2 \\le u_2 Y_3,\\ 0 \\le Y_3 \\le 1.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/208683/402101 \n Next, compute the joint distribution of $(U_1, U_2)$ directly.\n  By definition this is\n\n$$F(u_1, u_2) = \\Pr(U_1 \\le u_1, U_2 \\le u_2) = \\Pr(Y_1 \\le u_1 Y_2, Y_2 \\le u_2 Y_3).$$\n\nSince you have computed the joint density of $(Y_1, Y_2, Y_3)$, this is a routine matter of doing the (triple) integral expressed by the right-hand probability.  The region of integration must be $$0 \\le Y_1 \\le u_1 Y_2,\\ 0 \\le Y_2 \\le u_2 Y_3,\\ 0 \\le Y_3 \\le 1.$$\n\nThe simulation can give us an inkling of how $(U_1, U_2)$ are distributed: here is a scatterplot of the realized values of $(U_1, U_2)$.  Your theoretical answer should describe this density.\n\npar(mfrow=c(1,1))\nu <- cbind(y[1, ]/y[2, ], y[2, ]/y[3, ])\nplot(u, pch=16, cex=1/2, col=\"#00000008\", asp=1)\n\npar(mfrow=c(1,1))\nu <- cbind(y[1, ]/y[2, ], y[2, ]/y[3, ])\nplot(u, pch=16, cex=1/2, col=\"#00000008\", asp=1)\n\n\n\nAs a check, we may look at the marginal distributions and compare them to the theoretical solutions.  The marginal densities, shown as red curves, are obtained as $\\partial F(u_1, 1)/\\partial u_1$ and  $\\partial F(1, u_2)/\\partial u_2$.\n\npar(mfrow=c(1,2))\nhist(u[, 1], freq=FALSE); curve(2*x, add=TRUE, col=\"Red\", lwd=2)\nhist(u[, 2], freq=FALSE); curve(4*x^3, add=TRUE, col=\"Red\", lwd=2)\npar(mfrow=c(1,1))\n\npar(mfrow=c(1,2))\nhist(u[, 1], freq=FALSE); curve(2*x, add=TRUE, col=\"Red\", lwd=2)\nhist(u[, 2], freq=FALSE); curve(4*x^3, add=TRUE, col=\"Red\", lwd=2)\npar(mfrow=c(1,1))\n\n\n\nIt is curious that $U_1$ has the same distribution as the original $X_i$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1172/402101 \n [The following perhaps seems a little technical because of the use of equations but it builds mainly on the arrow charts to provide the intuition which only requires very basic understanding of OLS - so don't be repulsed.]\n\nSuppose you want to estimate the causal effect of $x_i$ on $y_i$ given by the estimated coefficient for $\\beta$, but for some reason there is a correlation between your explanatory variable and the error term:\n\n$$\\begin{matrix}y_i &=& \\alpha &+& \\beta x_i &+&   \\epsilon_i & \\\\ & && & & \\hspace{-1cm}\\nwarrow & \\hspace{-0.8cm} \\nearrow \\\\ &  & & & & corr &   \\end{matrix}$$\n\nThis might have happened because we forgot to include an important variable that also correlates with $x_i$. This problem is known as omitted variable bias and then your $\\widehat{\\beta}$ will not give you the causal effect (see \nhere\n for the details). This is a case when you would want to use an instrument because only then can you find the true causal effect.\n\nAn instrument is a new variable $z_i$ which is uncorrelated with $\\epsilon_i$, but that correlates well with $x_i$ and which only influences $y_i$ through $x_i$ - so our instrument is what is called \"exogenous\". It's like in this chart here:\n\n$$\\begin{matrix}\nz_i & \\rightarrow & x_i & \\rightarrow & y_i \\newline\n  &   & \\uparrow & \\nearrow & \\newline\n & & \\epsilon_i &  \n\\end{matrix}$$\n\nSo how do we use this new variable?\n\nMaybe you remember the ANOVA type idea behind regression where you split the total variation of a dependent variable into an explained and an unexplained component. For example, if you regress your $x_i$ on the instrument,\n\n$$\\underbrace{x_i}_{\\text{total variation}} = \\underbrace{a \\quad + \\quad \\pi z_i}_{\\text{explained variation}} \\quad + \\underbrace{\\eta_i}_{\\text{unexplained variation}}$$\n\nthen you know that the explained variation here is exogenous to our original equation because it depends on the exogenous variable $z_i$ only. So in this sense, we split our $x_i$ up into a part that we can claim is certainly exogenous (that's the part that depends on $z_i$) and some unexplained part $\\eta_i$ that keeps all the bad variation which correlates with $\\epsilon_i$. Now we take the exogenous part of this regression, call it $\\widehat{x_i}$,\n\n$$x_i \\quad = \\underbrace{a \\quad + \\quad \\pi z_i}_{\\text{good variation} \\: = \\: \\widehat{x}_i } \\quad + \\underbrace{\\eta_i}_{\\text{bad variation}}$$\n\nand put this into our original regression:\n$$y_i = \\alpha + \\beta \\widehat{x}_i + \\epsilon_i$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1172/402101 \n $$x_i \\quad = \\underbrace{a \\quad + \\quad \\pi z_i}_{\\text{good variation} \\: = \\: \\widehat{x}_i } \\quad + \\underbrace{\\eta_i}_{\\text{bad variation}}$$\n\nand put this into our original regression:\n$$y_i = \\alpha + \\beta \\widehat{x}_i + \\epsilon_i$$\n\nNow since $\\widehat{x}_i$ is not correlated anymore with $\\epsilon_i$ (remember, we \"filtered out\" this part from $x_i$ and left it in $\\eta_i$), we can consistently estimate our $\\beta$ because the instrument has helped us to break the correlation between the explanatory variably and the error. This was one way how you can apply instrumental variables. This method is actually called 2-stage least squares, where our regression of $x_i$ on $z_i$ is called the \"first stage\" and the last equation here is called the \"second stage\".\n\nIn terms of our original picture (I leave out the $\\epsilon_i$ to not make a mess but remember that it is there!), instead of taking the direct but flawed route between $x_i$ to $y_i$ we took an intermediate step via $\\widehat{x}_i$\n\n$$\\begin{matrix}\n& &  &     &             & \\widehat{x}_i \\newline\n& &  &     &    \\nearrow         & \\downarrow \\newline\n& z_i & \\rightarrow  & x_i & \\rightarrow & y_i \n\\end{matrix}$$\n\nThanks to this slight diversion of our road to the causal effect we were able to consistently estimate $\\beta$ by using the instrument. The cost of this diversion is that instrumental variables models are generally less precise, meaning that they tend to have larger standard errors.\n\nHow do we find instruments?\n\nThat's not an easy question because you need to make a good case as to why your $z_i$ would not be correlated with $\\epsilon_i$ - this cannot be tested formally because the true error is unobserved. The main challenge is therefore to come up with something that can be plausibly seen as exogenous such as natural disasters, policy changes, or sometimes you can even run a randomized experiment. The other answers had some very good examples for this so I won't repeat this part.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2647/402101 \n The answer depends on whether you are dealing with discrete or continuous random variables. So, I will split my answer accordingly. I will assume that you want some technical details and not necessarily an explanation in plain English.\n\nDiscrete Random Variables\n\nSuppose that you have a stochastic process that takes discrete values (e.g., outcomes of tossing a coin 10 times, number of customers who arrive at a store in 10 minutes etc). In such cases, we can calculate the probability of observing a particular set of outcomes by making suitable assumptions about the underlying stochastic process (e.g., probability of coin landing heads is \n$p$\n and that coin tosses are independent).\n\nDenote the observed outcomes by \n$O$\n and the set of parameters that describe the stochastic process as \n$\\theta$\n. Thus, when we speak of probability we want to calculate \n$P(O|\\theta)$\n. In other words, given specific values for \n$\\theta$\n, \n$P(O|\\theta)$\n is the probability that we would observe the outcomes represented by \n$O$\n.\n\nHowever, when we model a real life stochastic process, we often do not know \n$\\theta$\n. We simply observe \n$O$\n and the goal then is to arrive at an estimate for \n$\\theta$\n that would be a plausible choice given the observed outcomes \n$O$\n. We know that given a value of \n$\\theta$\n the probability of observing \n$O$\n is \n$P(O|\\theta)$\n. Thus, a 'natural' estimation process is to choose that value of \n$\\theta$\n that would maximize the probability that we would actually observe \n$O$\n. In other words, we find the parameter values \n$\\theta$\n that maximize the following function:\n\n$L(\\theta|O) = P(O|\\theta)$\n\n$L(\\theta|O)$\n is called the likelihood function. Notice that by definition the likelihood function is conditioned on the observed \n$O$\n and that it is a function of the unknown parameters \n$\\theta$\n.\n\nContinuous Random Variables\n\nIn the continuous case the situation is similar with one important difference. We can no longer talk about the probability that we observed \n$O$\n given \n$\\theta$\n because in the continuous case \n$P(O|\\theta) = 0$\n. Without getting into technicalities, the basic idea is as follows:\n\nDenote the probability density function (pdf) associated with the outcomes \n$O$\n as: \n$f(O|\\theta)$\n. Thus, in the continuous case we estimate \n$\\theta$\n given observed outcomes \n$O$\n by maximizing the following function:\n\n$L(\\theta|O) = f(O|\\theta)$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2647/402101 \n Denote the probability density function (pdf) associated with the outcomes \n$O$\n as: \n$f(O|\\theta)$\n. Thus, in the continuous case we estimate \n$\\theta$\n given observed outcomes \n$O$\n by maximizing the following function:\n\n$L(\\theta|O) = f(O|\\theta)$\n\nIn this situation, we cannot technically assert that we are finding the parameter value that maximizes the probability that we observe \n$O$\n as we maximize the PDF associated with the observed outcomes \n$O$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72810/402101 \n This is a recipe to learn EM with a practical and (in my opinion) very intuitive 'Coin-Toss' example:\n\nRead this short \nEM tutorial paper\n by Do and Batzoglou. This is the schema where the coin toss example is explained:\n\n\n\n\nYou may have question marks in your head, especially regarding where the probabilities in the Expectation step come from. Please have a look at the explanations on this maths stack exchange \npage\n.\n\n\nLook at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:   \n\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n## E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* ##\n\ndef get_binomial_log_likelihood(obs,probs):\n    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n    # Binomial Distribution Log PDF\n    # ln (pdf)      = Binomial Coeff * product of probabilities\n    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)\n\n    N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik\n\n# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n# 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45\n\n# represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50\n\n# E-M begins!\ndelta = 0.001  \nj = 0 # iteration counter\nimprovement = float('inf')\nwhile (improvement>delta):\n    expectation_A = np.zeros((len(experiments),2), dtype=float) \n    expectation_B = np.zeros((len(experiments),2), dtype=float)\n    for i in range(0,len(experiments)):\n        e = experiments[i] # i'th experiment\n          # loglikelihood of e given coin A:\n        ll_A = get_binomial_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) \n          # loglikelihood of e given coin B\n        ll_B = get_binomial_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]]))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72810/402101 \n # corresponding weight of A proportional to likelihood of A \n        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n          # corresponding weight of B proportional to likelihood of B\n        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n        expectation_A[i] = np.dot(weightA, e) \n        expectation_B[i] = np.dot(weightB, e)\n\n    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n\n    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - \n                    np.array([pA_heads[j],pB_heads[j]]) )) )\n    j = j+1\n\nplt.figure();\nplt.plot(range(0,j),pA_heads[0:j], 'r--')\nplt.plot(range(0,j),pB_heads[0:j])\nplt.show()\n\nRead this short \nEM tutorial paper\n by Do and Batzoglou. This is the schema where the coin toss example is explained:\n\n\n\nYou may have question marks in your head, especially regarding where the probabilities in the Expectation step come from. Please have a look at the explanations on this maths stack exchange \npage\n.\n\nLook at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n## E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* ##\n\ndef get_binomial_log_likelihood(obs,probs):\n    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n    # Binomial Distribution Log PDF\n    # ln (pdf)      = Binomial Coeff * product of probabilities\n    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)\n\n    N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik\n\n# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n# 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45\n\n# represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72810/402101 \n # represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50\n\n# E-M begins!\ndelta = 0.001  \nj = 0 # iteration counter\nimprovement = float('inf')\nwhile (improvement>delta):\n    expectation_A = np.zeros((len(experiments),2), dtype=float) \n    expectation_B = np.zeros((len(experiments),2), dtype=float)\n    for i in range(0,len(experiments)):\n        e = experiments[i] # i'th experiment\n          # loglikelihood of e given coin A:\n        ll_A = get_binomial_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) \n          # loglikelihood of e given coin B\n        ll_B = get_binomial_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) \n\n          # corresponding weight of A proportional to likelihood of A \n        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n          # corresponding weight of B proportional to likelihood of B\n        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n        expectation_A[i] = np.dot(weightA, e) \n        expectation_B[i] = np.dot(weightB, e)\n\n    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n\n    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - \n                    np.array([pA_heads[j],pB_heads[j]]) )) )\n    j = j+1\n\nplt.figure();\nplt.plot(range(0,j),pA_heads[0:j], 'r--')\nplt.plot(range(0,j),pB_heads[0:j])\nplt.show()\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n## E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* ##\n\ndef get_binomial_log_likelihood(obs,probs):\n    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n    # Binomial Distribution Log PDF\n    # ln (pdf)      = Binomial Coeff * product of probabilities\n    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)\n\n    N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/72810/402101 \n N = sum(obs);#number of trials  \n    k = obs[0] # number of heads\n    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n    log_lik = binomial_coeff + prod_probs\n\n    return log_lik\n\n# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n# 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45\n\n# represent the experiments\nhead_counts = np.array([5,9,8,4,7])\ntail_counts = 10-head_counts\nexperiments = zip(head_counts,tail_counts)\n\n# initialise the pA(heads) and pB(heads)\npA_heads = np.zeros(100); pA_heads[0] = 0.60\npB_heads = np.zeros(100); pB_heads[0] = 0.50\n\n# E-M begins!\ndelta = 0.001  \nj = 0 # iteration counter\nimprovement = float('inf')\nwhile (improvement>delta):\n    expectation_A = np.zeros((len(experiments),2), dtype=float) \n    expectation_B = np.zeros((len(experiments),2), dtype=float)\n    for i in range(0,len(experiments)):\n        e = experiments[i] # i'th experiment\n          # loglikelihood of e given coin A:\n        ll_A = get_binomial_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) \n          # loglikelihood of e given coin B\n        ll_B = get_binomial_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) \n\n          # corresponding weight of A proportional to likelihood of A \n        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n          # corresponding weight of B proportional to likelihood of B\n        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) \n\n        expectation_A[i] = np.dot(weightA, e) \n        expectation_B[i] = np.dot(weightB, e)\n\n    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n\n    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - \n                    np.array([pA_heads[j],pB_heads[j]]) )) )\n    j = j+1\n\nplt.figure();\nplt.plot(range(0,j),pA_heads[0:j], 'r--')\nplt.plot(range(0,j),pB_heads[0:j])\nplt.show()",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/140579/402101 \n Imagine a big family dinner where everybody starts asking you about PCA. First, you explain it to your great-grandmother; then to your grandmother; then to your mother; then to your spouse; finally, to your daughter (a mathematician). Each time the next person is less of a layman. Here is how the conversation might go.\n\nGreat-grandmother: I heard you are studying \"Pee-See-Ay\". I wonder what that is...\n\nYou:\n Ah, it's just a method of summarizing some data. Look, we have some wine bottles standing here on the table. We can describe each wine by its colour, how strong it is, how old it is, and so on.\n\n\n\nVisualization originally found\n \nhere\n.\n\nWe can compose a whole list of different characteristics of each wine in our cellar. But many of them will measure related properties and so will be redundant. If so, we should be able to summarize each wine with fewer characteristics! This is what PCA does.\n\nGrandmother: This is interesting! So this PCA thing checks what characteristics are redundant and discards them?\n\nYou:\n Excellent question, granny! No, PCA is not selecting some characteristics and discarding the others. Instead, it constructs some \nnew\n characteristics that turn out to summarize our list of wines well. Of course, these new characteristics are constructed using the old ones; for example, a new characteristic might be computed as wine age minus wine acidity level or some other combination (we call them \nlinear combinations\n).\n\nIn fact, PCA finds the best possible characteristics, the ones that summarize the list of wines as well as only possible (among all conceivable linear combinations). This is why it is so useful.\n\nMother: Hmmm, this certainly sounds good, but I am not sure I understand. What do you actually mean when you say that these new PCA characteristics \"summarize\" the list of wines?\n\nYou:\n I guess I can give two different answers to this question. The first answer is that you are looking for some wine properties (characteristics) that strongly differ across wines. Indeed, imagine that you come up with a property that is the same for most of the wines - like the stillness of wine after being poured. This would not be very useful, would it? Wines are very different, but your new property makes them all look the same! This would certainly be a bad summary. Instead, PCA looks for properties that show as much variation across wines as possible.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/140579/402101 \n The second answer is that you look for the properties that would allow you to predict, or \"reconstruct\", the original wine characteristics. Again, imagine that you come up with a property that has no relation to the original characteristics - like the shape of a wine bottle; if you use only this new property, there is no way you could reconstruct the original ones! This, again, would be a bad summary. So PCA looks for properties that allow reconstructing the original characteristics as well as possible.\n\nSurprisingly, it turns out that these two aims are equivalent and so PCA can kill two birds with one stone.\n\nSpouse: But darling, these two \"goals\" of PCA sound so different! Why would they be equivalent?\n\nYou:\n Hmmm. Perhaps I should make a little drawing \n(takes a napkin and starts scribbling)\n. Let us pick two wine characteristics, perhaps wine darkness and alcohol content -- I don't know if they are correlated, but let's imagine that they are. Here is what a scatter plot of different wines could look like:\n\n\n\nEach dot in this \"wine cloud\" shows one particular wine. You see that the two properties (\n$x$\n and \n$y$\n on this figure) are correlated. A new property can be constructed by drawing a line through the centre of this wine cloud and projecting all points onto this line. This new property will be given by a linear combination \n$w_1 x + w_2 y$\n, where each line corresponds to some particular values of \n$w_1$\n and \n$w_2$\n.\n\nNow, look here very carefully -- here is what these projections look like for different lines (red dots are projections of the blue dots):\n\n\n\nAs I said before, PCA will find the \"best\" line according to two different criteria of what is the \"best\". First, the variation of values along this line should be maximal. Pay attention to how the \"spread\" (we call it \"variance\") of the red dots changes while the line rotates; can you see when it reaches maximum? Second, if we reconstruct the original two characteristics (position of a blue dot) from the new one (position of a red dot), the reconstruction error will be given by the length of the connecting red line. Observe how the length of these red lines changes while the line rotates; can you see when the total length reaches minimum?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/140579/402101 \n If you stare at this animation for some time, you will notice that \"the maximum variance\" and \"the minimum error\" are reached at the same time, namely when the line points to the magenta ticks I marked on both sides of the wine cloud. This line corresponds to the new wine property that will be constructed by PCA.\n\nBy the way, PCA stands for \"principal component analysis\", and this new property is called \"first principal component\". And instead of saying \"property\" or \"characteristic\", we usually say \"feature\" or \"variable\".\n\nDaughter: Very nice, papa! I think I can see why the two goals yield the same result: it is essentially because of the Pythagoras theorem, isn't it? Anyway, I heard that PCA is somehow related to eigenvectors and eigenvalues; where are they in this picture?\n\nYou:\n Brilliant observation. Mathematically, the spread of the red dots is measured as the average squared distance from the centre of the wine cloud to each red dot; as you know, it is called the \nvariance\n. On the other hand, the total reconstruction error is measured as the average squared length of the corresponding red lines. But as the angle between red lines and the black line is always \n$90^\\circ$\n, the sum of these two quantities is equal to the average squared distance between the centre of the wine cloud and each blue dot; this is precisely Pythagoras theorem. Of course, this average distance does not depend on the orientation of the black line, so the higher the variance, the lower the error (because their sum is constant). This hand-wavy argument can be made precise (\nsee here\n).\n\nBy the way, you can imagine that the black line is a solid rod, and each red line is a spring. The energy of the spring is proportional to its squared length (this is known in physics as Hooke's law), so the rod will orient itself such as to minimize the sum of these squared distances. I made a simulation of what it will look like in the presence of some viscous friction:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/140579/402101 \n By the way, you can imagine that the black line is a solid rod, and each red line is a spring. The energy of the spring is proportional to its squared length (this is known in physics as Hooke's law), so the rod will orient itself such as to minimize the sum of these squared distances. I made a simulation of what it will look like in the presence of some viscous friction:\n\n\n\nRegarding eigenvectors and eigenvalues. You know what a \ncovariance matrix\n is; in my example it is a \n$2\\times 2$\n matrix that is given by \n$$\\begin{pmatrix}1.07 &0.63\\\\0.63 & 0.64\\end{pmatrix}.$$\n What this means is that the variance of the \n$x$\n variable is \n$1.07$\n, the variance of the \n$y$\n variable is \n$0.64$\n, and the covariance between them is \n$0.63$\n. As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors (incidentally, this is called \nspectral theorem\n); corresponding eigenvalues will then be located on the diagonal. In this new coordinate system, the covariance matrix is diagonal and looks like that: \n$$\\begin{pmatrix}1.52 &0\\\\0 & 0.19\\end{pmatrix},$$\n meaning that the correlation between points is now zero. It becomes clear that the variance of any projection will be given by a weighted average of the eigenvalues (I am only sketching the intuition here). Consequently, the maximum possible variance (\n$1.52$\n) will be achieved if we simply take the projection on the first coordinate axis. It follows that the direction of the first principal component is given by the first eigenvector of the covariance matrix. (\nMore details here.\n)\n\nYou can see this on the rotating figure as well: there is a gray line there orthogonal to the black one; together, they form a rotating coordinate frame. Try to notice when the blue dots become uncorrelated in this rotating frame. The answer, again, is that it happens precisely when the black line points at the magenta ticks. Now I can tell you how I found them (the magenta ticks): they mark the direction of the first eigenvector of the covariance matrix, which in this case is equal to \n$(0.81, 0.58)$\n.\n\nPer popular request, I shared \nthe Matlab code to produce the above animations\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/212728/402101 \n Two things to consider\n\nThe Gini is scale independent whereas the SD is in the original units\n\nSuppose we have a measure bounded above and below. SD takes on its maximum value if half measurements are at each bound whereas Gini takes on the maximum is one is at one bound and all the rest at the other.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/20874/402101 \n I think this approach is mistaken, but perhaps it will be more helpful if I explain why.  Wanting to know the best model given some information about a large number of variables is quite understandable.  Moreover, it is a situation in which people seem to find themselves regularly.  In addition, many textbooks (and courses) on regression cover stepwise selection methods, which implies that they must be legitimate.  Unfortunately, however, they are not, and the pairing of this situation and goal is quite difficult to successfully navigate.  The following is a list of problems with automated stepwise model selection procedures (attributed to Frank Harrell, and copied from \nhere\n):\n\nIt yields R-squared values that are badly biased to be high.\n\n\n\n\nThe F and chi-squared tests quoted next to each variable on the  printout do not have the claimed distribution.\n\n\n\n\nThe method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\n\n\n\n\nIt yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\n\n\n\n\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani\n[1996]).\n\n\n\n\nIt has severe problems in the presence of collinearity.\n\n\n\n\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\n\n\n\n\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\n\n\n\n\nIt allows us to not think about the problem.\n\n\n\n\nIt uses a lot of paper.\n\nIt yields R-squared values that are badly biased to be high.\n\nThe F and chi-squared tests quoted next to each variable on the  printout do not have the claimed distribution.\n\nThe method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\n\nIt yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\n\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani\n[1996]).\n\nIt has severe problems in the presence of collinearity.\n\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\n\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\n\nIt allows us to not think about the problem.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/20874/402101 \n It gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani\n[1996]).\n\nIt has severe problems in the presence of collinearity.\n\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\n\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\n\nIt allows us to not think about the problem.\n\nIt uses a lot of paper.\n\nThe question is, what's so bad about these procedures / why do these problems occur?  Most people who have taken a basic regression course are familiar with the concept of \nregression to the mean\n, so this is what I use to explain these issues.  (Although this may seem off-topic at first, bear with me, I promise it's relevant.)\n\nImagine a high school track coach on the first day of tryouts.  Thirty kids show up.  These kids have some underlying level of intrinsic ability to which neither the coach nor anyone else, has direct access.  As a result, the coach does the only thing he can do, which is have them all run a 100m dash.  The times are presumably a measure of their intrinsic ability and are taken as such.  However, they are probabilistic; some proportion of how well someone does is based on their actual ability, and some proportion is random.  Imagine that the true situation is the following:\n\nset.seed(59)\nintrinsic_ability = runif(30, min=9, max=10)\ntime = 31 - 2*intrinsic_ability + rnorm(30, mean=0, sd=.5)\n\nset.seed(59)\nintrinsic_ability = runif(30, min=9, max=10)\ntime = 31 - 2*intrinsic_ability + rnorm(30, mean=0, sd=.5)\n\nThe results of the first race are displayed in the following figure along with the coach's comments to the kids.\n\n\n\nNote that partitioning the kids by their race times leaves overlaps on their intrinsic ability--this fact is crucial.  After praising some, and yelling at some others (as coaches tend to do), he has them run again.  Here are the results of the second race with the coach's reactions (simulated from the same model above):",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/20874/402101 \n The results of the first race are displayed in the following figure along with the coach's comments to the kids.\n\n\n\nNote that partitioning the kids by their race times leaves overlaps on their intrinsic ability--this fact is crucial.  After praising some, and yelling at some others (as coaches tend to do), he has them run again.  Here are the results of the second race with the coach's reactions (simulated from the same model above):\n\n\n\nNotice that their intrinsic ability is identical, but the times bounced around relative to the first race.  From the coach's point of view, those he yelled at tended to improve, and those he praised tended to do worse (I adapted this concrete example from the Kahneman quote listed on the wiki page), although actually regression to the mean is a simple mathematical consequence of the fact that the coach is selecting athletes for the team based on a measurement that is partly random.\n\nNow, what does this have to do with automated (e.g., stepwise) model selection techniques?  Developing and confirming a model based on the same dataset is sometimes called \ndata dredging\n.  Although there is some underlying relationship amongst the variables, and stronger relationships are expected to yield stronger scores (e.g., higher t-statistics), these are random variables, and the realized values contain error.  Thus, when you select variables based on having higher (or lower) realized values, they may be such because of their underlying true value, error, or both.  If you proceed in this manner, you will be as surprised as the coach was after the second race.  This is true whether you select variables based on having high t-statistics, or low intercorrelations.  True, using the AIC is better than using p-values, because it penalizes the model for complexity, but the AIC is itself a random variable (if you run a study several times and fit the same model, the AIC will bounce around just like everything else).  Unfortunately, this is just a problem intrinsic to the epistemic nature of reality itself.\n\nI hope this is helpful.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/474165/402101 \n The result is correct, but the reasoning is somewhat inaccurate. You need to keep track of the property that the density is zero outside \n$[0,\\theta]$\n. This implies that the likelihood is zero to the left of the sample maximum, and jumps to \n$\\theta^n$\n in the maximum. It indeed decreases afterwards, so that the maximum is the MLE.\n\nThis also entails that the likelihood is not differentiable in this point, so that finding the MLE via the \"canonical\" route of the score function is not the way to go here.\n\nA more detailed formal derivation is, e.g., given \nhere",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/265430/402101 \n I'd like to provide a straightforward answer.\n\nWhat is the main difference between maximum likelihood estimation\n(MLE) vs. least squares estimation (LSE) ?\n\nAs @TrynnaDoStat commented, minimizing squared error is equivalent to maximizing the likelihood in this case. As said in \nWikipedia\n,\n\nIn a linear model, if the errors belong to a normal distribution the\nleast squares estimators are also the maximum likelihood estimators.\n\nthey can be viewed as almost the same in your case since the conditions of the least square methods are \nthese four\n: 1) linearity; 2) linear normal residuals; 3) constant variability/homoscedasticity; 4) independence.\n\nLet me detail it a bit. Since we know that the response variable \n$y$\n.\n\n$$y=w^T X +\\epsilon \\quad\\text{ where }\\epsilon\\thicksim N(0,\\sigma^2)$$\n\nfollows a normal distribution (normal residuals),\n\n\n$$P(y|w, X)=\\mathcal{N}(y|w^TX, \\sigma^2I)$$\n\nthen the likelihood function (independence) is,\n\n\\begin{align}\nL(y^{(1)},\\dots,y^{(N)};w, X^{(1)},\\dots,X^{(N)}) &= \\prod_{i=1}^N \\mathcal{N}(y^{(i)}|w^TX^{(i)}, \\sigma^2I) \\\\ &= \n\\frac{1}{(2\\pi)^{\\frac{N}{2}}\\sigma^N}\\exp\\left(\\frac{-1}{2\\sigma^2}\\left(\\sum_{i=1}^N(y^{(i)}-w^TX^{(i)})^2\\right)\\right).\n\\end{align}\n\nMaximizing L is equivalent to minimizing (since other stuff are all constants, homoscedasticity)\n\n$$\\sum_{i=1}^n(y^{(i)}-w^TX^{(i)})^2.$$\n\nThat's the least-squares method, the difference between the expected \n$\\hat{Y_i}$\n and the actual \n$Y_i$\n.\n\nWhy can't we use MLE for predicting \n$y$\n values in linear regression\nand vice versa?\n\nAs explained above we're actually (more precisely equivalently) using the MLE for predicting \n$y$\n values. And if the response variable has arbitrary distributions rather than the normal distribution, like\nBernoulli distribution or anyone from the \nexponential family\n we map the linear predictor to the response variable distribution using a \nlink function\n (according to the response distribution), then the likelihood function becomes the product of all the outcomes (probabilities between 0 and 1) after the transformation. We can treat the link function in the linear regression as the identity function (since the response is already a probability).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3555/402101 \n As Rob mentions, this occurs when you have highly correlated variables. The standard example I use is predicting weight from shoe size. You can predict weight equally well with the right or left shoe size. But together it doesn't work out.\n\nBrief simulation example\n\nRSS = 3:10 #Right shoe size\nLSS = rnorm(RSS, RSS, 0.1) #Left shoe size - similar to RSS\ncor(LSS, RSS) #correlation ~ 0.99\n\nweights = 120 + rnorm(RSS, 10*RSS, 10)\n\n##Fit a joint model\nm = lm(weights ~ LSS + RSS)\n\n##F-value is very small, but neither LSS or RSS are significant\nsummary(m)\n\n##Fitting RSS or LSS separately gives a significant result. \nsummary(lm(weights ~ LSS))\n\nRSS = 3:10 #Right shoe size\nLSS = rnorm(RSS, RSS, 0.1) #Left shoe size - similar to RSS\ncor(LSS, RSS) #correlation ~ 0.99\n\nweights = 120 + rnorm(RSS, 10*RSS, 10)\n\n##Fit a joint model\nm = lm(weights ~ LSS + RSS)\n\n##F-value is very small, but neither LSS or RSS are significant\nsummary(m)\n\n##Fitting RSS or LSS separately gives a significant result. \nsummary(lm(weights ~ LSS))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/144608/402101 \n With generalized linear models, there are three different types of statistical tests that can be run.  These are: Wald tests, likelihood ratio tests, and score tests.  The excellent UCLA statistics help site has a discussion of them \nhere\n.  The following figure (copied from their site) helps to illustrate them:\n\n\n\nThe \nWald test\n assumes that the likelihood is normally distributed, and on that basis, uses the degree of curvature to estimate the standard error.  Then, the parameter estimate divided by the SE yields a \n$z$\n-score.  This holds under large \n$N$\n, but isn't quite true with smaller \n$N$\ns.  It is hard to say when your \n$N$\n is large enough for this property to hold, so this test can be slightly risky.  \n\n\nLikelihood ratio tests\n look at the ratio of the likelihoods (or difference in log likelihoods) at its maximum and at the null.  This is often considered the best test.  \n\n\nThe \nscore test\n is based on the slope of the likelihood at the null value.  This is typically less powerful, but there are times when the full likelihood cannot be computed and so this is a nice fallback option.\n\nThe tests that come with \nsummary.glm()\n are Wald tests.  You don't say how you got your confidence intervals, but I assume you used \nconfint()\n, which in turn calls \nprofile()\n.  More specifically, those confidence intervals are calculated by profiling the likelihood (which is a better approach than multiplying the SE by \n$1.96$\n).  That is, they are analogous to the likelihood ratio test, not the Wald test.  The \n$\\chi^2$\n-test, in turn, is a score test.\n\nsummary.glm()\n\nconfint()\n\nprofile()\n\nAs your \n$N$\n becomes indefinitely large, the three different \n$p$\n's should converge on the same value, but they can differ slightly when you don't have infinite data.  It is worth noting that the (Wald) \n$p$\n-value in your initial output is just barely significant and there is little real difference between just over and just under \n$\\alpha=.05$\n (\nquote\n).  That line isn't 'magic'.  Given that the two more reliable tests are just over \n$.05$\n, I would say that your data are not quite 'significant' by conventional criteria.\n\nBelow I profile the coefficients on the scale of the linear predictor and run the likelihood ratio test explicitly (via \nanova.glm()\n).  I get the same results as you:\n\nanova.glm()",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/144608/402101 \n Below I profile the coefficients on the scale of the linear predictor and run the likelihood ratio test explicitly (via \nanova.glm()\n).  I get the same results as you:\n\nanova.glm()\n\nlibrary(MASS)\nx = matrix(c(343-268,268,73-49,49), nrow=2, byrow=T);  x\n#      [,1] [,2]\n# [1,]   75  268\n# [2,]   24   49\nD = factor(c(\"N\",\"Diabetes\"), levels=c(\"N\",\"Diabetes\"))\nm = glm(x~D, family=binomial)\nsummary(m)\n# ...\n# Coefficients:\n#             Estimate Std. Error z value Pr(>|z|)    \n# (Intercept)  -1.2735     0.1306  -9.749   <2e-16 ***\n# DDiabetes     0.5597     0.2813   1.990   0.0466 *  \n# ...\nconfint(m)\n# Waiting for profiling to be done...\n#                    2.5 %    97.5 %\n# (Intercept) -1.536085360 -1.023243\n# DDiabetes   -0.003161693  1.103671\nanova(m, test=\"LRT\")\n# ...\n#      Df Deviance Resid. Df Resid. Dev Pr(>Chi)  \n# NULL                     1     3.7997           \n# D     1   3.7997         0     0.0000  0.05126 .\nchisq.test(x)\n#         Pearson's Chi-squared test with Yates' continuity correction\n# \n# X-squared = 3.4397, df = 1, p-value = 0.06365\n\nlibrary(MASS)\nx = matrix(c(343-268,268,73-49,49), nrow=2, byrow=T);  x\n#      [,1] [,2]\n# [1,]   75  268\n# [2,]   24   49\nD = factor(c(\"N\",\"Diabetes\"), levels=c(\"N\",\"Diabetes\"))\nm = glm(x~D, family=binomial)\nsummary(m)\n# ...\n# Coefficients:\n#             Estimate Std. Error z value Pr(>|z|)    \n# (Intercept)  -1.2735     0.1306  -9.749   <2e-16 ***\n# DDiabetes     0.5597     0.2813   1.990   0.0466 *  \n# ...\nconfint(m)\n# Waiting for profiling to be done...\n#                    2.5 %    97.5 %\n# (Intercept) -1.536085360 -1.023243\n# DDiabetes   -0.003161693  1.103671\nanova(m, test=\"LRT\")\n# ...\n#      Df Deviance Resid. Df Resid. Dev Pr(>Chi)  \n# NULL                     1     3.7997           \n# D     1   3.7997         0     0.0000  0.05126 .\nchisq.test(x)\n#         Pearson's Chi-squared test with Yates' continuity correction\n# \n# X-squared = 3.4397, df = 1, p-value = 0.06365\n\nAs @JWilliman pointed out in a comment (now deleted), in \nR\n, you can also get a score-based p-value using \nanova.glm(model, test=\"Rao\")\n.  In the example below, note that the p-value isn't quite the same as in the chi-squared test above, because by default, \nR\n's \nchisq.test()\n applies a continuity correction.  If we change that setting, the p-values match:\n\nR\n\nanova.glm(model, test=\"Rao\")\n\nR\n\nchisq.test()",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/144608/402101 \n As @JWilliman pointed out in a comment (now deleted), in \nR\n, you can also get a score-based p-value using \nanova.glm(model, test=\"Rao\")\n.  In the example below, note that the p-value isn't quite the same as in the chi-squared test above, because by default, \nR\n's \nchisq.test()\n applies a continuity correction.  If we change that setting, the p-values match:\n\nR\n\nanova.glm(model, test=\"Rao\")\n\nR\n\nchisq.test()\n\nanova(m, test=\"Rao\")\n# ...\n#      Df Deviance Resid. Df Resid. Dev   Rao Pr(>Chi)  \n# NULL                     1     3.7997                 \n# D     1   3.7997         0     0.0000 4.024  0.04486 *\nchisq.test(x, correct=FALSE)\n#   Pearson's Chi-squared test\n# \n# data:  x\n# X-squared = 4.024, df = 1, p-value = 0.04486\n\nanova(m, test=\"Rao\")\n# ...\n#      Df Deviance Resid. Df Resid. Dev   Rao Pr(>Chi)  \n# NULL                     1     3.7997                 \n# D     1   3.7997         0     0.0000 4.024  0.04486 *\nchisq.test(x, correct=FALSE)\n#   Pearson's Chi-squared test\n# \n# data:  x\n# X-squared = 4.024, df = 1, p-value = 0.04486",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2498/402101 \n It's not an argument. It is a (a bit strongly stated) fact that formal normality tests always reject on the huge sample sizes we work with today. It's even easy to prove that when n gets large, even the smallest deviation from perfect normality will lead to a significant result. And as every dataset has some degree of randomness, no single dataset will be a perfectly normally distributed sample. But in applied statistics the question is not whether the data/residuals ... are perfectly normal, but normal enough for the assumptions to hold.\n\nLet me illustrate with \nthe Shapiro-Wilk test\n. The code below constructs a set of distributions that approach normality but aren't completely normal. Next, we test with \nshapiro.test\n whether a sample from these almost-normal distributions deviate from normality. In R:\n\nshapiro.test\n\nx <- replicate(100, { # generates 100 different tests on each distribution\n                     c(shapiro.test(rnorm(10)+c(1,0,2,0,1))$p.value,   #$\n                       shapiro.test(rnorm(100)+c(1,0,2,0,1))$p.value,  #$\n                       shapiro.test(rnorm(1000)+c(1,0,2,0,1))$p.value, #$\n                       shapiro.test(rnorm(5000)+c(1,0,2,0,1))$p.value) #$\n                    } # rnorm gives a random draw from the normal distribution\n               )\nrownames(x) <- c(\"n10\",\"n100\",\"n1000\",\"n5000\")\n\nrowMeans(x<0.05) # the proportion of significant deviations\n  n10  n100 n1000 n5000 \n 0.04  0.04  0.20  0.87\n\nx <- replicate(100, { # generates 100 different tests on each distribution\n                     c(shapiro.test(rnorm(10)+c(1,0,2,0,1))$p.value,   #$\n                       shapiro.test(rnorm(100)+c(1,0,2,0,1))$p.value,  #$\n                       shapiro.test(rnorm(1000)+c(1,0,2,0,1))$p.value, #$\n                       shapiro.test(rnorm(5000)+c(1,0,2,0,1))$p.value) #$\n                    } # rnorm gives a random draw from the normal distribution\n               )\nrownames(x) <- c(\"n10\",\"n100\",\"n1000\",\"n5000\")\n\nrowMeans(x<0.05) # the proportion of significant deviations\n  n10  n100 n1000 n5000 \n 0.04  0.04  0.20  0.87\n\nThe last line checks which fraction of the simulations for every sample size deviate significantly from normality. So in 87% of the cases, a sample of 5000 observations deviates significantly from normality according to Shapiro-Wilks. Yet, if you see the qq plots, you would never ever decide on a deviation from normality. Below you see as an example the qq-plots for one set of random samples\n\n\n\nwith p-values",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2498/402101 \n The last line checks which fraction of the simulations for every sample size deviate significantly from normality. So in 87% of the cases, a sample of 5000 observations deviates significantly from normality according to Shapiro-Wilks. Yet, if you see the qq plots, you would never ever decide on a deviation from normality. Below you see as an example the qq-plots for one set of random samples\n\n\n\nwith p-values\n\nn10  n100 n1000 n5000 \n0.760 0.681 0.164 0.007\n\nn10  n100 n1000 n5000 \n0.760 0.681 0.164 0.007",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45089/402101 \n What does ARIMA(1, 0, 12) mean?\n\nSpecifically for your model, ARIMA(1, 0, 12) means that it you are describing some response variable (Y) by combining a 1st order Auto-Regressive model and a 12th order Moving Average model. A good way to think about it is (AR, I, MA). This makes your model look the following, in simple terms:\n\nY = (Auto-Regressive Parameters) + (Moving Average Parameters)\n\nThe 0 between the 1 and the 12 represents the 'I' part of the model (the Integrative part) and it signifies a model where you're taking the difference between response variable data - this can be done with non-stationary data and it doesn't seem like you're dealing with that, so you can just ignore it.\n\nThe link that DanTheMan posted shows a nice mix of models that could help you understand yours by comparing it to those.\n\nWhat values can be assigned to p, d, q?\n\nLots of different whole numbers. There are diagnostic tests you can do to try to find the best values of p,d,q (see part 3).\n\nWhat is the process to find the values of p, d, q?\n\nThere are a number of ways, and I don't intend this to be exhaustive:\n\nlook at an autocorrelation graph of the data (will help if Moving Average (MA) model is appropriate)\n\n\nlook at a partial autocorrelation graph of the data (will help if AutoRegressive (AR) model is appropriate)\n\n\nlook at extended autocorrelation chart of the data (will help if a combination of AR and MA are needed)\n\n\ntry Akaike's Information Criterion (AIC) on a set of models and investigate the models with the lowest AIC values\n\n\ntry the Schwartz Bayesian Information Criterion (BIC) and investigate the models with the lowest BIC values\n\nWithout knowing how much more you need to know, I can't go too much farther, but if you have more questions, feel free to ask and maybe I, or someone else, can help.\n\n* \nEdit\n: All of the ways to find p, d, q that I listed here can be found in the R package TSA if you are familiar with R.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7516/402101 \n I think what you're looking for is a copula. You've got two marginal distributions (specified by either parametric or empirical cdfs) and now you want to specify the dependence between the two. For the bivariate case there are all kinds of choices, but the basic recipe is the same. I'll use a Gaussian copula for ease of interpretation.\n\nTo draw from the Gaussian copula with correlation matrix $C$\n\nDraw $(Z=(Z_1, Z_2)\\sim N(0, C)$\n\n\nSet $U_i = \\Phi(Z_i)$ for $i=1, 2$ (with $\\Phi$ the standard normal cdf). Now $U_1, U_2\\sim U[0,1]$, but they're dependent.\n\n\nSet $Y_i = F_i^{-1}(U_i)$ where $F_i^{-1}$ is the (pseudo) inverse of the marginal cdf for variable $i$. This implies that $Y_i$ follow the desired distribution (this step is just inverse transform sampling).\n\nDraw $(Z=(Z_1, Z_2)\\sim N(0, C)$\n\nSet $U_i = \\Phi(Z_i)$ for $i=1, 2$ (with $\\Phi$ the standard normal cdf). Now $U_1, U_2\\sim U[0,1]$, but they're dependent.\n\nSet $Y_i = F_i^{-1}(U_i)$ where $F_i^{-1}$ is the (pseudo) inverse of the marginal cdf for variable $i$. This implies that $Y_i$ follow the desired distribution (this step is just inverse transform sampling).\n\nVoila! Try it for some simple cases, and look at marginal histograms and scatterpolots, it's fun.\n\nNo guarantee that this is appropriate for your particular application though (in particular, you might need to replace the Gaussian copula with a t copula) but this should get you started. A good reference on copula modeling is Nelsen (1999), \nAn Introduction to Copulas\n, but there are some pretty good introductions online too.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/110401/402101 \n I'll try to give an intuitive explanation.\n\nThe t-statistic* has a numerator and a denominator. For example, the statistic in the one sample t-test is\n\n$$\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}$$\n\n*(there are several, but this discussion should hopefully be general enough to cover the ones you are asking about)\n\nUnder the assumptions, the numerator has a normal distribution with mean 0 and some unknown standard deviation.\n\nUnder the same set of assumptions, the denominator is an estimate of the standard deviation of the distribution of the numerator (the standard error of the statistic on the numerator). It is independent of the numerator. Its square is a chi-square random variable divided by its degrees of freedom (which is also the d.f. of the t-distribution) times the square of \n$\\sigma_\\text{numerator}$\n.\n\nWhen the degrees of freedom are small, the denominator tends to be fairly right-skew. It has a high chance of being less than its mean, and a relatively good chance of being quite small. At the same time, it also has some chance of being much, much larger than its mean.\n\nUnder the assumption of normality, the numerator and denominator are independent. So if we draw randomly from the distribution of this t-statistic we have a normal random number divided by a second randomly* chosen value from a right-skew distribution that's on average around 1.\n\n* without regard to the normal term\n\nBecause it's on the denominator, the small values in the distribution of the denominator produce very large t-values. The right-skew in the denominator make the t-statistic heavy-tailed. The right tail of the distribution, when on the denominator makes the t-distribution more sharply peaked than a normal with the same standard deviation as the \nt\n.\n\nHowever, as the degrees of freedom become large, the distribution becomes much more normal-looking and much more \"tight\" around its mean.\n\n\n\nAs such, the effect of dividing by the denominator on the shape of the distribution of the numerator reduces as the degrees of freedom increase.\n\nEventually - as Slutsky's theorem might suggest to us could happen  - the effect of the denominator becomes more like dividing by a constant and the distribution of the t-statistic is very close to normal.\n\nwhuber suggested in comments that it might be more illuminating to look at the reciprocal of the denominator. That is, we could write our t-statistics as numerator (normal) times reciprocal-of-denominator (right-skew).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/110401/402101 \n Eventually - as Slutsky's theorem might suggest to us could happen  - the effect of the denominator becomes more like dividing by a constant and the distribution of the t-statistic is very close to normal.\n\nwhuber suggested in comments that it might be more illuminating to look at the reciprocal of the denominator. That is, we could write our t-statistics as numerator (normal) times reciprocal-of-denominator (right-skew).\n\nFor example, our one-sample-t statistic above would become:\n\n$${\\sqrt{n}(\\bar{x}-\\mu_0)}\\cdot{1/s}$$\n\nNow consider the population standard deviation of the original \n$X_i$\n, \n$\\sigma_x$\n. We can multiply and divide by it, like so:\n\n$${\\sqrt{n}(\\bar{x}-\\mu_0)/\\sigma_x}\\cdot{\\sigma_x/s}$$\n\nThe first term is standard normal. The second term (the square root of a scaled inverse-chi-squared random variable) then scales that standard normal by values that are either larger or smaller than 1, \"spreading it out\".\n\nUnder the assumption of normality, the two terms in the product are independent. So if we draw randomly from the distribution of this t-statistic we have a normal random number (the first term in the product) times a second randomly-chosen value (without regard to the normal term) from a right-skew distribution that's 'typically' around 1.\n\nWhen the d.f. are large, the value tends to be very close to 1, but when the df are small, it's quite skew and the spread is large, with the big right tail of this scaling factor making the tail quite fat:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/366656/402101 \n See this \nsecond post\n for McDonald's feedback on my answer where the notion of risk consistency is related to stability.\n\nYour question is difficult to answer because it mentions two very different topics: \nuniqueness\n and \nstability\n.\n\nIntuitively, a solution is \nunique\n if given a fixed data set, the algorithm always produces the same results. Martin's answer cover's this point in great detail. \n\n\nStability\n on the other hand can be intuitively understood as one for which the prediction does not change much when the training data is modified slightly.\n\nIntuitively, a solution is \nunique\n if given a fixed data set, the algorithm always produces the same results. Martin's answer cover's this point in great detail.\n\nStability\n on the other hand can be intuitively understood as one for which the prediction does not change much when the training data is modified slightly.\n\nStability applies to your question because Lasso feature selection is (often) performed via Cross Validation, hence the Lasso algorithm is performed on different folds of data and may yield different results each time.\n\nUsing the definition from \nhere\n if we define \nUniform stability\n as:\n\nAn algorithm has uniform stability $\\beta$ with respect to the loss\n  function $V$ if the following holds:\n\n$$\\forall S \\in Z^m \\ \\ \\forall i \\in \\{ 1,...,m\\}, \\ \\ \\sup |\n> V(f_s,z) - V(f_{S^{|i},z})  |\\  \\ \\leq \\beta$$\n\nConsidered as a function of $m$, the term $\\beta$ can be written as\n  $\\beta_m$. We say the algorithm is stable when $\\beta_m$ decreases as\n  $\\frac{1}{m}$.\n\nthen the \n\"No Free Lunch Theorem, Xu and Caramis (2012)\"\n states that\n\nIf an algorithm is \nsparse\n, in the sense that it identifies redundant features, then that algorithm is \nnot stable\n (and the uniform stability bound $\\beta$ does not go to zero). [...] If an algorithm is stable, then there is no hope that it will be sparse. (pages 3 and 4)\n\nFor instance, $L_2$ regularized regression is stable and does not identify redundant features, while $L_1$ regularized regression (Lasso) is unstable.\n\nI think 'lasso favors a sparse solution' is not an answer to why use lasso for feature selection\n\nI disagree, the reason Lasso is used for feature selection is that it yields a \nsparse\n solution and can be shown to have the IRF property, i.e. Identifies Redundant Features.\n\nWhat is the most crucial reason that causes this instability\n\nThe No Free Lunch Theorem",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/366656/402101 \n I think 'lasso favors a sparse solution' is not an answer to why use lasso for feature selection\n\nI disagree, the reason Lasso is used for feature selection is that it yields a \nsparse\n solution and can be shown to have the IRF property, i.e. Identifies Redundant Features.\n\nWhat is the most crucial reason that causes this instability\n\nThe No Free Lunch Theorem\n\nThis is not to say that the combination of Cross Validation and Lasso doesn't work... in fact it has been shown experimentally (and with much supporting theory) to work very well under various conditions. The main keywords here are \nconsistency\n, risk, oracle inequalities etc..\n\nThe following slides and paper by McDonald and Homrighausen (2013) describe some conditions under which Lasso feature selection works well: \nslides\n and paper: \n\"The lasso, persistence, and cross-validation, McDonald and Homrighausen (2013)\"\n. Tibshirani himself also posted an great set of notes on \nsparcity\n, \nlinear regression\n\nThe various conditions for consistency and their impact on Lasso is an active topic of research and is definitely not a trivial question. I can point you towards some research papers which are relevant:\n\nVideo lectures on the No free lunch theorem, by Xu\n\n\nH.M. B\u00f8velstad et all, A comparison of feature selection approaches for gene selection, (2007)\n \n\n\nThe lasso, persistence, and cross-validation, McDonald and Homrighausen (2013)\n \n\n\nHuang and Bowick, Summary and discussion of: \u201cStability Selection\u201d\n\n\nLim and Yu, Estimation Stability with Cross Validation, (2015)\n \n\n\nA talk by Peter Buhlmann: \nStability Selection for High-Dimensional Data, (2008)\n and the accompanying \npaper\n\n\nWang, Nan et all, Random Lasso, (2011)\n\n\nStackexchange post: \nModel stability when dealing with large $p$, small $n$ problem\n\n\nRoberts, Nowakm Stabilizing the lasso against cross-validation variability, (2014)\n which argue that \"percentile-lasso, can result in large reductions in both model-selection instability and model-selection error, compared to the lasso\"\n\n\nAn awesome set of notes by Tibshirani and Wasserman on \nsparcity\n, \nlinear regression",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/514259/402101 \n The threshold is chosen such that it ensures convergence of the \nhypergeometric distribution\n ($\\sqrt{\\frac{N-n}{N-1}}$ is its SD), instead of a binomial distribution (for sampling with replacement), to a normal distribution (this is the Central Limit Theorem, see e.g., \nThe Normal Curve, the Central Limit Theorem, and Markov's and Chebychev's Inequalities for Random Variables\n). In other words, when $n/N\\leq 0.05$ (i.e., $n$ is not 'too large' compared to $N$), the FPC can safely be ignored; it is easy to see how the correction factor evolves with varying $n$ for a fixed $N$: with $N=10,000$, we have $\\text{FPC}=.9995$ when $n=10$ while $\\text{FPC}=.3162$ when $n=9,000$. When $N\\to\\infty$, the FPC approaches 1 and we are close to the situation of sampling with replacement (i.e., like with an infinite population).\n\nTo understand this results, a good starting point is to read some online tutorials on sampling theory where sampling is done without replacement (\nsimple random sampling\n). This online tutorial on \nNonparametric statistics\n has an illustration on computing the expectation and variance for a total.\n\nYou will notice that some authors use $N$ instead of $N-1$ in the denominator of the FPC; in fact, it depends on whether you work with the sample or population statistic: for the variance, it will be $N$ instead of $N-1$ if you are interested in $S^2$ rather than $\\sigma^2$.\n\nAs for online references, I can suggest you\n\nEstimation and statistical inference\n\n\nA new look at inference for the Hypergeometric Distribution\n\n\nFinite Population Sampling with Application to the Hypergeometric Distribution\n\n\nSimple random sampling",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24466/402101 \n To understand what can go on, it is instructive to generate (and analyze) data that behave in the manner described.\n\nFor simplicity, let's forget about that sixth independent variable.  So, the question describes regressions of one dependent variable $y$ against five independent variables $x_1, x_2, x_3, x_4, x_5$, in which\n\nEach ordinary regression $y \\sim x_i$ is significant at levels from $0.01$ to less than $0.001$.\n\n\nThe multiple regression $y \\sim x_1 + \\cdots + x_5$ yields significant coefficients only for $x_1$ and $x_2$.\n\n\nAll variance inflation factors (VIFs) are low, indicating good conditioning in the design matrix (that is, \nlack\n of collinearity among the $x_i$).\n\nEach ordinary regression $y \\sim x_i$ is significant at levels from $0.01$ to less than $0.001$.\n\nThe multiple regression $y \\sim x_1 + \\cdots + x_5$ yields significant coefficients only for $x_1$ and $x_2$.\n\nAll variance inflation factors (VIFs) are low, indicating good conditioning in the design matrix (that is, \nlack\n of collinearity among the $x_i$).\n\nLet's make this happen as follows:\n\nGenerate $n$ normally distributed values for $x_1$ and $x_2$.  (We will choose $n$ later.)\n\n\nLet $y = x_1 + x_2 + \\varepsilon$ where $\\varepsilon$ is independent normal error of mean $0$.  Some trial and error is needed to find a suitable standard deviation for $\\varepsilon$; $1/100$ works fine (and is rather dramatic: $y$ is \nextremely\n well correlated with $x_1$ and $x_2$, even though it is only moderately correlated with $x_1$ and $x_2$ individually).\n\n\nLet $x_j$ = $x_1/5 + \\delta$, $j=3,4,5$, where $\\delta$ is independent standard normal error.  This makes $x_3,x_4,x_5$ only \nslightly\n dependent on $x_1$.  However, via the tight correlation between $x_1$ and $y$, this induces a \ntiny\n correlation between $y$ and these $x_j$.\n\nGenerate $n$ normally distributed values for $x_1$ and $x_2$.  (We will choose $n$ later.)\n\nLet $y = x_1 + x_2 + \\varepsilon$ where $\\varepsilon$ is independent normal error of mean $0$.  Some trial and error is needed to find a suitable standard deviation for $\\varepsilon$; $1/100$ works fine (and is rather dramatic: $y$ is \nextremely\n well correlated with $x_1$ and $x_2$, even though it is only moderately correlated with $x_1$ and $x_2$ individually).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24466/402101 \n Generate $n$ normally distributed values for $x_1$ and $x_2$.  (We will choose $n$ later.)\n\nLet $y = x_1 + x_2 + \\varepsilon$ where $\\varepsilon$ is independent normal error of mean $0$.  Some trial and error is needed to find a suitable standard deviation for $\\varepsilon$; $1/100$ works fine (and is rather dramatic: $y$ is \nextremely\n well correlated with $x_1$ and $x_2$, even though it is only moderately correlated with $x_1$ and $x_2$ individually).\n\nLet $x_j$ = $x_1/5 + \\delta$, $j=3,4,5$, where $\\delta$ is independent standard normal error.  This makes $x_3,x_4,x_5$ only \nslightly\n dependent on $x_1$.  However, via the tight correlation between $x_1$ and $y$, this induces a \ntiny\n correlation between $y$ and these $x_j$.\n\nHere's the rub: if we make $n$ large enough, these \nslight\n correlations will result in significant coefficients, even though $y$ is almost entirely \"explained\" by only the first two variables.\n\nI found that $n=500$ works just fine for reproducing the reported p-values.  Here's a scatterplot matrix of all six variables:\n\n\n\nBy inspecting the right column (or the bottom row) you can \nsee\n that $y$ has a good (positive) correlation with $x_1$ and $x_2$ but little apparent correlation with the other variables.  By inspecting the rest of this matrix, you can see that the independent variables $x_1, \\ldots, x_5$ appear to be mutually \nuncorrelated\n (the random $\\delta$ mask the tiny dependencies we know are there.)  There are no exceptional data--nothing terribly outlying or with high leverage.  The histograms show that all six variables are approximately normally distributed, by the way: these data are as ordinary and \"plain vanilla\" as one could possibly want.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24466/402101 \n In the regression of $y$ against $x_1$ and $x_2$, the p-values are essentially 0.  In the individual regressions of $y$ against $x_3$, then $y$ against $x_4$, and $y$ against $x_5$, the p-values are 0.0024, 0.0083, and 0.00064, respectively: that is, they are \"highly significant.\"  But in the full multiple regression, the corresponding p-values inflate to .46, .36, and .52, respectively: not significant at all.  The reason for this is that once $y$ has been regressed against $x_1$ and $x_2$, the only stuff left to \"explain\" is the tiny amount of error in the residuals, which will approximate $\\varepsilon$, and this error is almost completely unrelated to the remaining $x_i$.  (\"Almost\" is correct: there is a really tiny relationship induced from the fact that the residuals were computed in part from the values of $x_1$ and $x_2$ and the $x_i$, $i=3,4,5$, do have some weak relationship to $x_1$ and $x_2$.  This residual relationship is practically undetectable, though, as we saw.)\n\nThe conditioning number of the design matrix is only 2.17: that's very low, showing \nno indication of high multicollinearity whatsoever.\n  (Perfect lack of collinearity would be reflected in a conditioning number of 1, but in practice this is seen only with artificial data and designed experiments.  Conditioning numbers in the range 1-6 (or even higher, with more variables) are unremarkable.)  This completes the simulation: it has successfully reproduced every aspect of the problem.\n\nThe important insights this analysis offers include\n\np-values don't tell us anything directly about collinearity.\n  They depend strongly on the amount of data.\n\n\nRelationships among p-values in multiple regressions and p-values in related regressions (involving subsets of the independent variable) are complex and usually unpredictable.\n\np-values don't tell us anything directly about collinearity.\n  They depend strongly on the amount of data.\n\nRelationships among p-values in multiple regressions and p-values in related regressions (involving subsets of the independent variable) are complex and usually unpredictable.\n\nConsequently, as others have argued, p-values should not be your sole guide (or even your principal guide) to model selection.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24466/402101 \n p-values don't tell us anything directly about collinearity.\n  They depend strongly on the amount of data.\n\nRelationships among p-values in multiple regressions and p-values in related regressions (involving subsets of the independent variable) are complex and usually unpredictable.\n\nConsequently, as others have argued, p-values should not be your sole guide (or even your principal guide) to model selection.\n\nIt is not necessary for $n$ to be as large as $500$ for these phenomena to appear.\n  Inspired by additional information in the question, the following is a dataset constructed in a similar fashion with $n=24$ (in this case $x_j = 0.4 x_1 + 0.4 x_2 + \\delta$ for $j=3,4,5$).  This creates correlations of 0.38 to 0.73 between $x_{1-2}$ and $x_{3-5}$.  The condition number of the design matrix is 9.05: a little high, but not terrible.  (Some \nrules of thumb\n say that condition numbers as high as 10 are ok.)  The p-values of the individual regressions against $x_3, x_4, x_5$ are 0.002, 0.015, and 0.008: significant to highly significant.  Thus, some multicollinearity is involved, but it's not so large that one would work to change it.  \nThe basic insight remains the same\n: significance and multicollinearity are different things; only mild mathematical constraints hold among them; and it is possible for the inclusion or exclusion of even a single variable to have profound effects on all p-values even without severe multicollinearity being an issue.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24466/402101 \n x1 x2 x3 x4 x5 y\n-1.78256    -0.334959   -1.22672    -1.11643    0.233048    -2.12772\n0.796957    -0.282075   1.11182 0.773499    0.954179    0.511363\n0.956733    0.925203    1.65832 0.25006 -0.273526   1.89336\n0.346049    0.0111112   1.57815 0.767076    1.48114 0.365872\n-0.73198    -1.56574    -1.06783    -0.914841   -1.68338    -2.30272\n0.221718    -0.175337   -0.0922871  1.25869 -1.05304    0.0268453\n1.71033 0.0487565   -0.435238   -0.239226   1.08944 1.76248\n0.936259    1.00507 1.56755 0.715845    1.50658 1.93177\n-0.664651   0.531793    -0.150516   -0.577719   2.57178 -0.121927\n-0.0847412  -1.14022    0.577469    0.694189    -1.02427    -1.2199\n-1.30773    1.40016 -1.5949 0.506035    0.539175    0.0955259\n-0.55336    1.93245 1.34462 1.15979 2.25317 1.38259\n1.6934  0.192212    0.965777    0.283766    3.63855 1.86975\n-0.715726   0.259011    -0.674307   0.864498    0.504759    -0.478025\n-0.800315   -0.655506   0.0899015   -2.19869    -0.941662   -1.46332\n-0.169604   -1.08992    -1.80457    -0.350718   0.818985    -1.2727\n0.365721    1.10428 0.33128 -0.0163167  0.295945    1.48115\n0.215779    2.233   0.33428 1.07424 0.815481    2.4511\n1.07042 0.0490205   -0.195314   0.101451    -0.721812   1.11711\n-0.478905   -0.438893   -1.54429    0.798461    -0.774219   -0.90456\n1.2487  1.03267 0.958559    1.26925 1.31709 2.26846\n-0.124634   -0.616711   0.334179    0.404281    0.531215    -0.747697\n-1.82317    1.11467 0.407822    -0.937689   -1.90806    -0.723693\n-1.34046    1.16957 0.271146    1.71505 0.910682    -0.176185",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24466/402101 \n x1 x2 x3 x4 x5 y\n-1.78256    -0.334959   -1.22672    -1.11643    0.233048    -2.12772\n0.796957    -0.282075   1.11182 0.773499    0.954179    0.511363\n0.956733    0.925203    1.65832 0.25006 -0.273526   1.89336\n0.346049    0.0111112   1.57815 0.767076    1.48114 0.365872\n-0.73198    -1.56574    -1.06783    -0.914841   -1.68338    -2.30272\n0.221718    -0.175337   -0.0922871  1.25869 -1.05304    0.0268453\n1.71033 0.0487565   -0.435238   -0.239226   1.08944 1.76248\n0.936259    1.00507 1.56755 0.715845    1.50658 1.93177\n-0.664651   0.531793    -0.150516   -0.577719   2.57178 -0.121927\n-0.0847412  -1.14022    0.577469    0.694189    -1.02427    -1.2199\n-1.30773    1.40016 -1.5949 0.506035    0.539175    0.0955259\n-0.55336    1.93245 1.34462 1.15979 2.25317 1.38259\n1.6934  0.192212    0.965777    0.283766    3.63855 1.86975\n-0.715726   0.259011    -0.674307   0.864498    0.504759    -0.478025\n-0.800315   -0.655506   0.0899015   -2.19869    -0.941662   -1.46332\n-0.169604   -1.08992    -1.80457    -0.350718   0.818985    -1.2727\n0.365721    1.10428 0.33128 -0.0163167  0.295945    1.48115\n0.215779    2.233   0.33428 1.07424 0.815481    2.4511\n1.07042 0.0490205   -0.195314   0.101451    -0.721812   1.11711\n-0.478905   -0.438893   -1.54429    0.798461    -0.774219   -0.90456\n1.2487  1.03267 0.958559    1.26925 1.31709 2.26846\n-0.124634   -0.616711   0.334179    0.404281    0.531215    -0.747697\n-1.82317    1.11467 0.407822    -0.937689   -1.90806    -0.723693\n-1.34046    1.16957 0.271146    1.71505 0.910682    -0.176185",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/141429/402101 \n Suppose you want to find a linear combination of $X_1$ and $X_2$ such that\n\n$$\n\\text{corr}(\\alpha X_1 + \\beta X_2, X_1) = \\rho\n$$\n\nNotice that if you multiply both $\\alpha$ and $\\beta$ by the same (non-zero) constant, the correlation will not change. Thus, we're going to add a condition to preserve variance: $\\text{var}(\\alpha X_1 + \\beta X_2) = \\text{var}(X_1)$\n\nThis is equivalent to\n\n$$\n\\rho\n= \\frac{\\text{cov}(\\alpha X_1 + \\beta X_2, X_1)}{\\sqrt{\\text{var}(\\alpha X_1 + \\beta X_2) \\text{var}(X_1)}}\n= \\frac{\\alpha \\overbrace{\\text{cov}(X_1, X_1)}^{=\\text{var}(X_1)} + \\overbrace{\\beta \\text{cov}(X_2, X_1)}^{=0}}{\\sqrt{\\text{var}(\\alpha X_1 + \\beta X_2) \\text{var}(X_1)}} = \\alpha \\sqrt{\\frac{\\text{var}(X_1)}{\\alpha^2 \\text{var}(X_1) + \\beta^2 \\text{var}(X_2)}}\n$$\n\nAssuming \nboth random variables have the same variance\n (this is a crucial assumption!) ($\\text{var}(X_1) = \\text{var}(X_2)$), we get\n\n$$\n\\rho \\sqrt{\\alpha^2 + \\beta^2} = \\alpha\n$$\n\nThere are many solutions to this equation, so it's time to recall variance-preserving condition:\n\n$$\n\\text{var}(X_1)\n = \\text{var}(\\alpha X_1 + \\beta X_2)\n = \\alpha^2 \\text{var}(X_1) + \\beta^2 \\text{var}(X_2)\n\\Rightarrow \\alpha^2 + \\beta^2 = 1\n$$\n\nAnd this leads us to\n\n$$\n\\alpha = \\rho \\\\\n\\beta = \\pm \\sqrt{1-\\rho^2}\n$$\n\nUPD\n. Regarding the second question: yes, this is known as \nwhitening\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/34016/402101 \n Causal theory offers another explanation for how two variables could be unconditionally independent yet conditionally dependent. I am not an expert on causal theory and am grateful for any criticism that will correct any misguidance below.\n\nTo illustrate, I will use \ndirected acyclic graphs\n (DAG). In these graphs, edges (\n$-$\n) between variables represent direct causal relationships. Arrowheads (\n$\\leftarrow$\n or \n$\\rightarrow$\n) indicate the direction of causal relationships. Thus \n$A \\rightarrow B$\n infers that \n$A$\n directly causes \n$B$\n, and \n$A \\leftarrow B$\n infers that \n$A$\n is directly caused by \n$B$\n. \n$A \\rightarrow B \\rightarrow C$\n is a causal path that infers that \n$A$\n indirectly causes \n$C$\n through \n$B$\n. For simplicity, assume all causal relationships are linear.\n\nFirst, consider a simple example of \nconfounder bias\n:\n\n\n\nHere, a simple bivariable regression will suggest a dependence between \n$X$\n and \n$Y$\n. However, there is no direct causal relationship between \n$X$\n and \n$Y$\n. Instead, both are directly caused by \n$Z$\n, and in the simple bivariable regression, observing \n$Z$\n induces a dependency between \n$X$\n and \n$Y$\n, resulting in bias by confounding. However, a multivariable regression conditioning on \n$Z$\n will remove the bias and suggest no dependence between \n$X$\n and \n$Y$\n.\n\nSecond, consider an example of \ncollider bias\n (also known as Berkson's bias or berksonian bias, of which selection bias is a special type):\n\n\n\nHere, a simple bivariable regression will suggest no dependence between \n$X$\n and \n$Y$\n. This agrees with the DAG, which infers no direct causal relationship between \n$X$\n and \n$Y$\n. However, a multivariable regression conditioning on \n$Z$\n will induce a dependence between \n$X$\n and \n$Y$\n, suggesting that a direct causal relationship between the two variables may exist when in fact, none exist. The inclusion of \n$Z$\n in the multivariable regression results in collider bias.\n\nThird, consider an example of incidental cancellation:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/34016/402101 \n Third, consider an example of incidental cancellation:\n\n\n\nLet us assume that \n$\\alpha$\n, \n$\\beta$\n, and \n$\\gamma$\n are path coefficients and that \n$\\beta = -\\alpha\\gamma$\n. A simple bivariable regression will suggest no dependence between \n$X$\n and \n$Y$\n. Although \n$X$\n is, in fact, a direct cause of \n$Y$\n, the confounding effect of \n$Z$\n on \n$X$\n and \n$Y$\n incidentally cancels out the effect of \n$X$\n on \n$Y$\n. A multivariable regression conditioning on \n$Z$\n will remove the confounding effect of \n$Z$\n on \n$X$\n and \n$Y$\n, allowing for the estimation of the direct effect of \n$X$\n on \n$Y$\n, assuming the DAG of the causal model is correct.\n\nTo summarize:\n\nConfounder example:\n \n$X$\n and \n$Y$\n are dependent in bivariable regression and independent in multivariable regression conditioning on confounder \n$Z$\n.\n\nCollider example:\n \n$X$\n and \n$Y$\n are independent in bivariable regression and dependent in multivariable regression conditioning on collider \n$Z$\n.\n\nIncidental cancellation example:\n \n$X$\n and \n$Y$\n are independent in bivariable regression and dependent in multivariable regression conditioning on confounder \n$Z$\n.\n\nDiscussion:\n\nThe results of your analysis are not compatible with the confounder example but are compatible with both the collider example and the incidental cancellation example. Thus, a potential explanation is that you have incorrectly conditioned on a collider variable in your multivariable regression and have induced an association between \n$X$\n and \n$Y$\n even though \n$X$\n is not a cause of \n$Y$\n and \n$Y$\n is not a cause of \n$X$\n. Alternatively, you might have correctly conditioned on a confounder in your multivariable regression that was incidentally cancelling out the true effect of \n$X$\n on \n$Y$\n in your bivariable regression.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/34016/402101 \n I find using background knowledge to construct causal models helpful when considering which variables to include in statistical models. For example, if previous high-quality randomized studies concluded that \n$X$\n causes \n$Z$\n and \n$Y$\n causes \n$Z$\n, I could make a strong assumption that \n$Z$\n is a collider of \n$X$\n and \n$Y$\n and not condition upon it in a statistical model. However, if I merely had an intuition that \n$X$\n causes \n$Z$\n, and \n$Y$\n causes \n$Z$\n, but no strong scientific evidence to support my intuition, I could only make a weak assumption that \n$Z$\n is a collider of \n$X$\n and \n$Y$\n, as human intuition has a history of being misguided. Subsequently, I would be skeptical of inferring causal relationships between \n$X$\n and \n$Y$\n without further investigating their causal relationships with \n$Z$\n. In lieu of or in addition to background knowledge, there are also algorithms designed to infer causal models from the data using a series of tests of association (e.g. PC algorithm and FCI algorithm, see \nTETRAD\n for Java implementation, \nPCalg\n for R implementation). These algorithms are very interesting, but I would not recommend relying on them without a strong understanding of the power and limitations of causal calculus and causal models in causal theory.\n\nConclusion:\n\nContemplation of causal models does not excuse the investigator from addressing the statistical considerations discussed in other answers here. However, I feel that causal models can nevertheless provide a helpful framework when thinking of potential explanations for observed statistical dependence and independence in statistical models, especially when visualizing potential confounders and colliders.\n\nFurther reading:\n\nGelman, Andrew. 2011. \"\nCausality and Statistical Learning\n.\" Am. J. Sociology 117 (3) (November): 955\u2013966.\n\nGreenland, S, J Pearl, and J M Robins. 1999. \u201c\nCausal Diagrams for Epidemiologic Research\n.\u201d Epidemiology (Cambridge, Mass.) 10 (1) (January): 37\u201348.\n\nGreenland, Sander. 2003. \u201c\nQuantifying Biases in Causal Models: Classical Confounding Vs Collider-Stratification Bias\n.\u201d Epidemiology 14 (3) (May 1): 300\u2013306.\n\nPearl, Judea. 1998. \nWhy There Is No Statistical Test For Confounding, Why Many Think There Is, And Why They Are Almost Right\n.\n\nPearl, Judea. 2009. \nCausality: Models, Reasoning and Inference\n. 2nd ed. Cambridge University Press.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/34016/402101 \n Greenland, Sander. 2003. \u201c\nQuantifying Biases in Causal Models: Classical Confounding Vs Collider-Stratification Bias\n.\u201d Epidemiology 14 (3) (May 1): 300\u2013306.\n\nPearl, Judea. 1998. \nWhy There Is No Statistical Test For Confounding, Why Many Think There Is, And Why They Are Almost Right\n.\n\nPearl, Judea. 2009. \nCausality: Models, Reasoning and Inference\n. 2nd ed. Cambridge University Press.\n\nSpirtes, Peter, Clark Glymour, and Richard Scheines. 2001. \nCausation, Prediction, and Search\n, Second Edition. A Bradford Book.\n\nUpdate:\n Judea Pearl discusses the theory of causal inference and the need to incorporate causal inference into introductory statistics courses in the \nNovember 2012 edition of Amstat News\n. His \nTuring Award Lecture\n, entitled \"The mechanization of causal inference: A 'mini' Turing Test and beyond\" is also of interest.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/442339/402101 \n Least squares is indeed maximum likelihood if the errors are iid normal, but if they aren't iid normal, least squares is not maximum likelihood. For example if my errors were logistic, least squares wouldn't be a terrible idea but it wouldn't be maximum likelihood.\n\nLots of estimators are not maximum likelihood estimators; while maximum likelihood estimators typically have a number of useful and attractive properties they're not the only game in town (and indeed not even always a great idea).\n\nA few examples of other estimation methods would include\n\nmethod of moments\n (this involves equating enough sample and population moments to solve for parameter estimates; sometimes this turns out to be maximum likelihood but usually it doesn't)\n\n\nFor example, equating first and second moments to estimate the parameters of a gamma distribution or a uniform distribution; not maximum likelihood in either case.\n\n\n\n\nmethod of quantiles (equating sufficient sample and population quantiles to solve for parameter estimates; occasionally this is maximum likelihood but usually it isn't),\n\n\n\n\nminimizing some other measure of lack of fit than \n$-\\log\\mathcal{L}$\n (e.g. minimum chi-square, minimum K-S distance).\n\nmethod of moments\n (this involves equating enough sample and population moments to solve for parameter estimates; sometimes this turns out to be maximum likelihood but usually it doesn't)\n\nFor example, equating first and second moments to estimate the parameters of a gamma distribution or a uniform distribution; not maximum likelihood in either case.\n\nmethod of quantiles (equating sufficient sample and population quantiles to solve for parameter estimates; occasionally this is maximum likelihood but usually it isn't),\n\nminimizing some other measure of lack of fit than \n$-\\log\\mathcal{L}$\n (e.g. minimum chi-square, minimum K-S distance).\n\nWith fitting linear regression type models, you could for example look at robust regression (some of which do correspond to ML methods for some particular error distribution but many of which do not).\n\nIn the case of \nsimple\n linear regression, I show an example of two methods of fitting lines that are not maximum likelihood \nhere\n - there estimating slope by setting to 0 some other measure of correlation (i.e. other than the usual Pearson) between residuals and the predictor.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/442339/402101 \n With fitting linear regression type models, you could for example look at robust regression (some of which do correspond to ML methods for some particular error distribution but many of which do not).\n\nIn the case of \nsimple\n linear regression, I show an example of two methods of fitting lines that are not maximum likelihood \nhere\n - there estimating slope by setting to 0 some other measure of correlation (i.e. other than the usual Pearson) between residuals and the predictor.\n\nAnother example would be the Tukey's resistant line/Tukey's three group line (e.g. see \n?line\n in R). There are many other possibilities, though many of them don't generalize readily to the multiple regression situation.\n\n?line",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78829/402101 \n Controlling\n for something and \nignoring\n something are not the same thing.  Let's consider a universe in which only 3 variables exist: $Y$, $X_1$, and $X_2$.  We want to build a regression model that predicts $Y$, and we are especially interested in its relationship with $X_1$.  There are two basic possibilities.\n\nWe could assess the relationship between $X_1$ and $Y$ while \ncontrolling\n for $X_2$:\n\n$$\n Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2\n $$\nor,\n\n\nwe could assess the relationship between $X_1$ and $Y$ while \nignoring\n $X_2$:  \n\n\n$$\n Y = \\beta_0 + \\beta_1X_1\n $$\n\nwe could assess the relationship between $X_1$ and $Y$ while \nignoring\n $X_2$:\n\n$$\n Y = \\beta_0 + \\beta_1X_1\n $$\n\nGranted, these are very simple models, but they constitute different ways of looking at how the relationship between $X_1$ and $Y$ manifests.  Often, the estimated $\\hat\\beta_1$s might be similar in both models, but they can be quite different.  What is most important in determining how different they are is the relationship (or lack thereof) between $X_1$ and $X_2$.  Consider this figure:\n\n\n\nIn this scenario, $X_1$ is correlated with $X_2$.  Since the plot is two-dimensional, it sort of ignores $X_2$ (perhaps ironically), so I have indicated the values of $X_2$ for each point with distinct symbols and colors (the pseudo-3D plot below provides another way to try to display the structure of the data).  If we fit a regression model that \nignored\n $X_2$, we would get the solid black regression line.  If we fit a model that \ncontrolled\n for $X_2$, we would get a regression plane, which is again hard to plot, so I have plotted three slices through that plane where $X_2=1$, $X_2=2$, and $X_2=3$.  Thus, we have the lines that show the relationship between $X_1$ and $Y$ that hold when we \ncontrol\n for $X_2$.  Of note, we see that \ncontrolling for\n $X_2$ does not yield a single line, but a set of lines.\n\n\n\nAnother way to think about the distinction between \nignoring\n and \ncontrolling\n for another variable, is to consider the distinction between a \nmarginal distribution\n and a \nconditional distribution\n.  Consider this figure:\n\n\n\n(\nThis is taken from my answer here: \nWhat is the intuition behind conditional Gaussian distributions?\n)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78829/402101 \n Another way to think about the distinction between \nignoring\n and \ncontrolling\n for another variable, is to consider the distinction between a \nmarginal distribution\n and a \nconditional distribution\n.  Consider this figure:\n\n\n\n(\nThis is taken from my answer here: \nWhat is the intuition behind conditional Gaussian distributions?\n)\n\nIf you look at the normal curve drawn to the left of the main figure, that is the \nmarginal\n distribution of $Y$.  It is the distribution of $Y$ if we \nignore\n its relationship with $X$.  Within the main figure, there are two normal curves representing \nconditional\n distributions of $Y$ when $X_1 = 25$ and $X_1 = 45$.  The conditional distributions \ncontrol\n for the level of $X_1$, whereas the marginal distribution \nignores\n it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18200/402101 \n Sometimes we can \"augment knowledge\" with an unusual or different approach.  I would like this reply to be accessible to kindergartners and also have some fun, so \neverybody get out your crayons!\n\nGiven paired \n$(x,y)$\n data, draw their scatterplot.  (The younger students may need a teacher to produce this for them. :-)  Each pair of points \n$(x_i,y_i)$\n, \n$(x_j,y_j)$\n in that plot determines a rectangle: it's the smallest rectangle, whose sides are parallel to the axes, containing those points.  Thus the points are either at the upper right and lower left corners (a \"positive\" relationship) or they are at the upper left and lower right corners (a \"negative\" relationship).\n\nDraw all possible such rectangles.\n Color them transparently, making the positive rectangles red (say) and the negative rectangles \"anti-red\" (blue).  In this fashion, wherever rectangles overlap, their colors are either enhanced when they are the same (blue and blue or red and red) or cancel out when they are different.\n\n\n\n(\nIn this illustration of a positive (red) and negative (blue) rectangle, the overlap ought to be white; unfortunately, this software does not have a true \"anti-red\" color.   The overlap is gray, so it will darken the plot, but on the whole the\n net \namount of red is correct.\n)\n\nNow we're ready for the explanation of covariance.\n\nThe covariance is the net amount of red in the plot\n (treating blue as negative values).\n\nHere are some examples with 32 binormal points drawn from distributions with the given covariances, ordered from most negative (bluest) to most positive (reddest).\n\n\n\nThey are drawn on common axes to make them comparable.  The rectangles are lightly outlined to help you see them.  This is an updated (2019) version of the original: it uses software that properly cancels the red and cyan colors in overlapping rectangles.\n\nLet's deduce some properties of covariance.\n  Understanding of these properties will be accessible to anyone who has actually drawn a few of the rectangles. :-)\n\nBilinearity.\n Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.\n\n\n\n\nCorrelation.\n Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18200/402101 \n Bilinearity.\n Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.\n\n\n\n\nCorrelation.\n Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.\n\n\n\n\nRelationship to linear associations.\n Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances.  Linear associations can be fully interpreted by means of the preceding two characterizations.\n\n\n\n\nSensitivity to outliers.\n A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points.  It alone can create a net positive or negative amount of red in the overall picture.\n\nBilinearity.\n Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.\n\nCorrelation.\n Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.\n\nRelationship to linear associations.\n Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances.  Linear associations can be fully interpreted by means of the preceding two characterizations.\n\nSensitivity to outliers.\n A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points.  It alone can create a net positive or negative amount of red in the overall picture.\n\nIncidentally, this definition of covariance differs from the usual one only by a constant of proportionality.  The mathematically inclined will have no trouble performing the algebraic demonstration that the formula given here is always twice the usual covariance.  For a full explanation, see the follow-up thread at \nhttps://stats.stackexchange.com/a/222091/919\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/109838/402101 \n From \nSklar's Theorem\n, it follows that you can construct the joint distribution using a copula:\n\n$$H(x,y) = C(F(x),G(y)).$$\n\nSo, you need two ingredients: the marginal distributions $(F,G)$, and the copula $C$. You mentioned that you know the marginals, so this ingredient is done. Now, you need information to construct the copula. So, if you cannot come up with enough information to select/estimate/guess/divine the copula, then you cannot construct the joint distribution.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/57756/402101 \n A fitted regression model uses the parameters to generate point estimate predictions which are the means of observed responses if you were to replicate the study with the same $X$ values an infinite number of times (and when the linear model is true). The difference between these predicted values and the ones used to fit the model are called \"residuals\" which, when replicating the data collection process, have properties of random variables with 0 means.\n\nThe observed residuals are then used to subsequently estimate the variability in these values and to estimate the sampling distribution of the parameters. When the residual standard error is exactly 0 then the model fits the data perfectly (likely due to overfitting). If the residual standard error can not be shown to be significantly different from the variability in the unconditional response, then there is little evidence to suggest the linear model has any predictive ability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7008/402101 \n For i.i.d. random variables $X_1, \\dotsc, X_n$, the unbiased estimator for the variance $s^2$ (the one with denominator $n-1$) has variance:\n\n$$\\mathrm{Var}(s^2) = \\sigma^4 \\left(\\frac{2}{n-1} + \\frac{\\kappa}{n}\\right)$$\n\nwhere $\\kappa$ is the excess kurtosis of the distribution (reference: \nWikipedia\n). So now you need to estimate the kurtosis of your distribution as well. You can use a quantity sometimes described as $\\gamma_2$ (also from \nWikipedia\n):\n\n$$\\gamma_2 = \\frac{\\mu_4}{\\sigma_4} - 3$$\n\nI would assume that if you use $s$ as an estimate for $\\sigma$ and $\\gamma_2$ as an estimate for $\\kappa$, that you get a reasonable estimate for $\\mathrm{Var}(s^2)$, although I don't see a guarantee that it is unbiased. See if it matches with the variance among the subsets of your 500 data points reasonably, and if it does don't worry about it anymore :)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78830/402101 \n Controlling\n for something and \nignoring\n something are not the same thing.  Let's consider a universe in which only 3 variables exist: $Y$, $X_1$, and $X_2$.  We want to build a regression model that predicts $Y$, and we are especially interested in its relationship with $X_1$.  There are two basic possibilities.\n\nWe could assess the relationship between $X_1$ and $Y$ while \ncontrolling\n for $X_2$:\n\n$$\n Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2\n $$\nor,\n\n\nwe could assess the relationship between $X_1$ and $Y$ while \nignoring\n $X_2$:  \n\n\n$$\n Y = \\beta_0 + \\beta_1X_1\n $$\n\nwe could assess the relationship between $X_1$ and $Y$ while \nignoring\n $X_2$:\n\n$$\n Y = \\beta_0 + \\beta_1X_1\n $$\n\nGranted, these are very simple models, but they constitute different ways of looking at how the relationship between $X_1$ and $Y$ manifests.  Often, the estimated $\\hat\\beta_1$s might be similar in both models, but they can be quite different.  What is most important in determining how different they are is the relationship (or lack thereof) between $X_1$ and $X_2$.  Consider this figure:\n\n\n\nIn this scenario, $X_1$ is correlated with $X_2$.  Since the plot is two-dimensional, it sort of ignores $X_2$ (perhaps ironically), so I have indicated the values of $X_2$ for each point with distinct symbols and colors (the pseudo-3D plot below provides another way to try to display the structure of the data).  If we fit a regression model that \nignored\n $X_2$, we would get the solid black regression line.  If we fit a model that \ncontrolled\n for $X_2$, we would get a regression plane, which is again hard to plot, so I have plotted three slices through that plane where $X_2=1$, $X_2=2$, and $X_2=3$.  Thus, we have the lines that show the relationship between $X_1$ and $Y$ that hold when we \ncontrol\n for $X_2$.  Of note, we see that \ncontrolling for\n $X_2$ does not yield a single line, but a set of lines.\n\n\n\nAnother way to think about the distinction between \nignoring\n and \ncontrolling\n for another variable, is to consider the distinction between a \nmarginal distribution\n and a \nconditional distribution\n.  Consider this figure:\n\n\n\n(\nThis is taken from my answer here: \nWhat is the intuition behind conditional Gaussian distributions?\n)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78830/402101 \n Another way to think about the distinction between \nignoring\n and \ncontrolling\n for another variable, is to consider the distinction between a \nmarginal distribution\n and a \nconditional distribution\n.  Consider this figure:\n\n\n\n(\nThis is taken from my answer here: \nWhat is the intuition behind conditional Gaussian distributions?\n)\n\nIf you look at the normal curve drawn to the left of the main figure, that is the \nmarginal\n distribution of $Y$.  It is the distribution of $Y$ if we \nignore\n its relationship with $X$.  Within the main figure, there are two normal curves representing \nconditional\n distributions of $Y$ when $X_1 = 25$ and $X_1 = 45$.  The conditional distributions \ncontrol\n for the level of $X_1$, whereas the marginal distribution \nignores\n it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/5417/402101 \n This is a good question.\n\nHere are some common pitfalls:\n\nUsing standard likelihood theory, we may derive a test to compare two nested\nhypotheses, $H_0$ and $H_1$, by computing the likelihood ratio test statistic. The null distribution of this test statistic is approximately chi-squared with degrees of freedom equal to difference in the dimensions of the two parameters spaces. Unfortunately, this test is only approximate and requires several assumptions. One crucial assumption is that the parameters under the null are not on the boundary of the parameter space. Since we are often interested in testing hypotheses about the random effects that take the form:\n$$H_0: \\sigma^2=0$$\r\n\nThis a real concern.\n The way to get around this problem is using REML. But still, the p-values will tend to be larger than they should be. This means that if you observe a significant effect using the \u03c72 approximation, you can be fairly confident that it is actually significant. Small, but not significant, p-values might spur one to use more accurate, but time-consuming, bootstrap methods.\n\n\nComparing fixed effects: If you plan to use the likelihood ratio test to compare two nested models that differ only in their fixed effects, you cannot use the REML estimation method. The reason is that REML estimates the random effects by considering linear combinations of the data that remove the fixed effects. If these fixed effects are changed, the likelihoods of the two models will not be directly comparable.\n\n\nP-values: The p-values generated by the likelihood ratio test for fixed effects are approximate and unfortunately tend to be too small, thereby sometimes overstating the importance of some effects. We may use \nnonparametric\n bootstrap methods to find more accurate p-values for the likelihood ratio test.\n\n\nThere are other concerns about p-values for the fixed effects test which are highlighted by Dr. Doug Bates [\nhere\n].",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/5417/402101 \n P-values: The p-values generated by the likelihood ratio test for fixed effects are approximate and unfortunately tend to be too small, thereby sometimes overstating the importance of some effects. We may use \nnonparametric\n bootstrap methods to find more accurate p-values for the likelihood ratio test.\n\n\nThere are other concerns about p-values for the fixed effects test which are highlighted by Dr. Doug Bates [\nhere\n].\n\nUsing standard likelihood theory, we may derive a test to compare two nested\nhypotheses, $H_0$ and $H_1$, by computing the likelihood ratio test statistic. The null distribution of this test statistic is approximately chi-squared with degrees of freedom equal to difference in the dimensions of the two parameters spaces. Unfortunately, this test is only approximate and requires several assumptions. One crucial assumption is that the parameters under the null are not on the boundary of the parameter space. Since we are often interested in testing hypotheses about the random effects that take the form:\n$$H_0: \\sigma^2=0$$\r\n\nThis a real concern.\n The way to get around this problem is using REML. But still, the p-values will tend to be larger than they should be. This means that if you observe a significant effect using the \u03c72 approximation, you can be fairly confident that it is actually significant. Small, but not significant, p-values might spur one to use more accurate, but time-consuming, bootstrap methods.\n\nComparing fixed effects: If you plan to use the likelihood ratio test to compare two nested models that differ only in their fixed effects, you cannot use the REML estimation method. The reason is that REML estimates the random effects by considering linear combinations of the data that remove the fixed effects. If these fixed effects are changed, the likelihoods of the two models will not be directly comparable.\n\nP-values: The p-values generated by the likelihood ratio test for fixed effects are approximate and unfortunately tend to be too small, thereby sometimes overstating the importance of some effects. We may use \nnonparametric\n bootstrap methods to find more accurate p-values for the likelihood ratio test.\n\nThere are other concerns about p-values for the fixed effects test which are highlighted by Dr. Doug Bates [\nhere\n].\n\nI am sure other members of the forum will have better answers.\n\nSource: Extending linear models with R -- Dr. Julain Faraway.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73651/402101 \n (Answer partially updated in 2023.)\n\nThis is a very hard problem in general, though your variables are apparently only 1d so that helps. Of course, the first step (when possible) should be to plot the data and see if anything pops out at you; you're in 2d so this should be easy.\n\nHere are a few approaches that work in \n$\\mathbb{R}^d$\n or even more general settings, to match the general title of the question.\n\nOne general category is, related to the suggestion here, to estimate the mutual information:\n\nEstimate mutual information via entropies, as mentioned. In low dimensions with sufficient samples, histograms / KDE / nearest-neighbour estimators should work okay, but expect them to behave very poorly as the dimension increases. In particular, the following simple estimator has finite-sample bounds (compared to most approaches' asymptotic-only properties):\n\nSricharan, Raich, and Hero. \nEmpirical estimation of entropy functionals with confidence.\n arXiv:1012.4188 [math.ST]\n\nSimilar direct estimators of mutual information, e.g. the following based on nearest neighbours:\n\nP\u00e1l, P\u00f3czos, and Svepes\u00e1ri. \nEstimation of R\u00e9nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs\n, NeurIPS 2010.\n\nVariational estimators of mutual information, based on optimizing some function parameterized typically as a neural network; this is probably the \"default\" modern approach in high dimensions. The following paper gives a nice overview of the relationship between various estimators. Be aware, however, that these approaches are highly dependent on the neural network class and optimization scheme, and can have \nparticularly surprising behaviour in their bias/variance tradeoffs\n.\n\nPoole, Ozair, van den Oord, Alemi, and Tucker. \nOn Variational Bounds of Mutual Information\n, ICML 2019.\n\nThere are also other approaches, based on measures other than the mutual information.\n\nThe Schweizer-Wolff approach is a classic one based on copula transformations, and so is invariant to monotone increasing transformations. I'm not very familiar with this one, but I think it's computationally simpler but also maybe less powerful than most of the other approaches here. (I vaguely expect it can be framed as a special case of some of the other approaches but haven't really thought about it.)\n\nSchweizer and Wolff, \nOn Nonparametric Measures of Dependence for Random Variables\n, Annals of Statistics 1981.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73651/402101 \n Schweizer and Wolff, \nOn Nonparametric Measures of Dependence for Random Variables\n, Annals of Statistics 1981.\n\nThe Hilbert-Schmidt independence criterion (HSIC): a kernel (in the sense of RKHS, not KDE)-based approach, based on measuring the norm of \n$\\operatorname{Cov}(\\phi(X), \\psi(Y))$\n for kernel features \n$\\phi$\n and \n$\\psi$\n.  In fact, the HSIC with kernels defined by a deep network is related to one of the more common variational estimators, InfoNCE; see \ndiscussion here\n.\n\nGretton, Bousqet, Smola, and Sch\u00f6lkopf, \nMeasuring Statistical Independence with Hilbert-Schmidt Norms\n, Algorithmic Learning Theory 2005.\n\nStatisticians are probably more familiar with the distance covariance/correlation as mentioned here previously; this is in fact \na special case\n of the HSIC with a particular choice of kernel, but that choice is maybe often a better kernel choice than the default Gaussian kernel typically used for HSIC.\n\nSz\u00e9kely, Rizzo, and Bakirov, \nMeasuring and testing dependence by correlation of distances\n, Annals of Statistics 2007.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238227/402101 \n Summary:\n the \"random-effects model\" in econometrics and a \"random intercept mixed model\" are indeed the same models, but they are estimated in different ways. The econometrics way is to use FGLS, and the mixed model way is to use ML. There are different algorithms of doing FGLS, and some of them (on this dataset) produce results that are very close to ML.\n\nplm\n\nI will answer with my testing on \nplm(..., model = \"random\")\n and \nlmer()\n, using the data generated by @ChristophHanck.\n\nplm(..., model = \"random\")\n\nlmer()\n\nAccording to \nthe plm package manual\n, there are four options for \nrandom.method\n: the method of estimation for the variance components in the random effects model. @amoeba used the default one \nswar\n (Swamy and Arora, 1972).\n\nrandom.method\n\nswar\n\nFor random effects models, four estimators of the transformation\n  parameter are available by setting random.method to one of \"swar\"\n  (Swamy and Arora (1972)) (default), \"amemiya\" (Amemiya (1971)),\n  \"walhus\" (Wallace and Hussain (1969)), or \"nerlove\" (Nerlove (1971)).\n\nI tested all the four options using the same data, \ngetting an error for \namemiya\n, and three totally different coefficient estimates for the variable \nstackX\n. The ones from using \nrandom.method='nerlove'\n and 'amemiya' are nearly equivalent to that from \nlmer()\n, -1.029 and -1.025 vs -1.026. They are also not very different from that obtained in the \"fixed-effects\" model, -1.045.\n\namemiya\n\nstackX\n\nrandom.method='nerlove'\n\nlmer()\n\n# \"amemiya\" only works using the most recent version:\n# install.packages(\"plm\", repos=\"http://R-Forge.R-project.org\")\n\nre0 <- plm(stackY~stackX, data = paneldata, model = \"random\") #random.method='swar'\nre1 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='amemiya')\nre2 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='walhus')\nre3 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='nerlove')\nl2  <- lmer(stackY~stackX+(1|as.factor(unit)), data = paneldata)\n\ncoef(re0)     #    (Intercept)   stackX    18.3458553   0.7703073 \ncoef(re1)     #    (Intercept)   stackX    30.217721   -1.025186 \ncoef(re2)     #    (Intercept)   stackX    -1.15584     3.71973 \ncoef(re3)     #    (Intercept)   stackX    30.243678   -1.029111 \nfixef(l2)     #    (Intercept)   stackX    30.226295   -1.026482\n\n# \"amemiya\" only works using the most recent version:\n# install.packages(\"plm\", repos=\"http://R-Forge.R-project.org\")",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238227/402101 \n coef(re0)     #    (Intercept)   stackX    18.3458553   0.7703073 \ncoef(re1)     #    (Intercept)   stackX    30.217721   -1.025186 \ncoef(re2)     #    (Intercept)   stackX    -1.15584     3.71973 \ncoef(re3)     #    (Intercept)   stackX    30.243678   -1.029111 \nfixef(l2)     #    (Intercept)   stackX    30.226295   -1.026482\n\n# \"amemiya\" only works using the most recent version:\n# install.packages(\"plm\", repos=\"http://R-Forge.R-project.org\")\n\nre0 <- plm(stackY~stackX, data = paneldata, model = \"random\") #random.method='swar'\nre1 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='amemiya')\nre2 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='walhus')\nre3 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='nerlove')\nl2  <- lmer(stackY~stackX+(1|as.factor(unit)), data = paneldata)\n\ncoef(re0)     #    (Intercept)   stackX    18.3458553   0.7703073 \ncoef(re1)     #    (Intercept)   stackX    30.217721   -1.025186 \ncoef(re2)     #    (Intercept)   stackX    -1.15584     3.71973 \ncoef(re3)     #    (Intercept)   stackX    30.243678   -1.029111 \nfixef(l2)     #    (Intercept)   stackX    30.226295   -1.026482\n\nUnfortunately I do not have time right now, but interested readers can find the four references, to check their estimation procedures. It would be very helpful to figure out why they make such a difference. I expect that for some cases, the \nplm\n estimation procedure using the \nlm()\n on transformed data should be equivalent to the maximum likelihood procedure utilized in \nlmer()\n.\n\nplm\n\nlm()\n\nlmer()\n\nThe authors of \nplm\n package did compare the two in Section 7 of their paper: \nYves Croissant and Giovanni Millo, 2008, Panel Data Econometrics in R: The plm package\n.\n\nplm\n\nEconometrics deal mostly with non-experimental data. Great emphasis is put on specification procedures and misspecification testing. Model specifications tend therefore to be very simple, while great attention is put on the issues of endogeneity of the regressors, dependence\n  structures in the errors and robustness of the estimators under deviations from normality.\n  The preferred approach is often semi- or non-parametric, and heteroskedasticity-consistent\n  techniques are becoming standard practice both in estimation and testing.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238227/402101 \n For all these reasons, [...] panel model estimation in econometrics is mostly\n  accomplished in the generalized least squares framework based on Aitken\u2019s Theorem [...]. On the contrary, longitudinal data\n  models in \nnlme\n and \nlme4\n are estimated by (restricted or unrestricted) maximum likelihood. [...]\n\nnlme\n\nlme4\n\nThe econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on\n  the machine, the expressions for the estimators are usually rather simple. ML estimation of\n  longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence\n  criteria.\n\nI appreciate that @ChristophHanck provided a thorough introduction about the four \nrandom.method\n used in \nplm\n and explained why their estimates are so different. As requested by @amoeba, I will add some thoughts on the mixed models (likelihood-based) and its connection with GLS.\n\nrandom.method\n\nplm\n\nThe likelihood-based method usually assumes a distribution for both the random effect and the error term. A normal distribution assumption is commonly used, but there are also some studies assuming a non-normal distribution. I will follow @ChristophHanck's notations for a random intercept model, and allow unbalanced data, i.e., let $T=n_i$.\n\nThe model is\n\\begin{equation}\ny_{it}= \\boldsymbol x_{it}^{'}\\boldsymbol\\beta + \\eta_i + \\epsilon_{it}\\qquad i=1,\\ldots,m,\\quad t=1,\\ldots,n_i\n\\end{equation}\nwith $\\eta_i \\sim N(0,\\sigma^2_\\eta), \\epsilon_{it} \\sim N(0,\\sigma^2_\\epsilon)$.\n\nFor each $i$, $$\\boldsymbol y_i \\sim N(\\boldsymbol X_{i}\\boldsymbol\\beta, \\boldsymbol\\Sigma_i), \\qquad\\boldsymbol\\Sigma_i = \\sigma^2_\\eta \\boldsymbol 1_{n_i} \\boldsymbol 1_{n_i}^{'} + \\sigma^2_\\epsilon \\boldsymbol I_{n_i}.$$\nSo the log-likelihood function is $$const -\\frac{1}{2} \\sum_i\\mathrm{log}|\\boldsymbol\\Sigma_i| - \\frac{1}{2} \\sum_i(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta)^{'}\\boldsymbol\\Sigma_i^{-1}(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta).$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238227/402101 \n For each $i$, $$\\boldsymbol y_i \\sim N(\\boldsymbol X_{i}\\boldsymbol\\beta, \\boldsymbol\\Sigma_i), \\qquad\\boldsymbol\\Sigma_i = \\sigma^2_\\eta \\boldsymbol 1_{n_i} \\boldsymbol 1_{n_i}^{'} + \\sigma^2_\\epsilon \\boldsymbol I_{n_i}.$$\nSo the log-likelihood function is $$const -\\frac{1}{2} \\sum_i\\mathrm{log}|\\boldsymbol\\Sigma_i| - \\frac{1}{2} \\sum_i(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta)^{'}\\boldsymbol\\Sigma_i^{-1}(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta).$$\n\nWhen all the variances are known, as shown in Laird and Ware (1982), the MLE is\n$$\\hat{\\boldsymbol\\beta} = \\left(\\sum_i\\boldsymbol X_i^{'} \\boldsymbol\\Sigma_i^{-1} \\boldsymbol X_i \\right)^{-1} \\left(\\sum_i \\boldsymbol X_i^{'} \\boldsymbol\\Sigma_i^{-1} \\boldsymbol y_i \\right),$$\nwhich is equivalent to the GLS $\\hat\\beta_{RE}$ derived by @ChristophHanck. So the key difference is in the estimation for the variances. Given that there is no closed-form solution, there are several approaches:\n\ndirectly maximization of the log-likelihood function using optimization algorithms;\n\n\nExpectation-Maximization (EM) algorithm: closed-form solutions exist, but the estimator for $\\boldsymbol \\beta$ involves empirical Bayesian estimates of the random intercept;\n\n\na combination of the above two, Expectation/Conditional Maximization Either (ECME) algorithm (Schafer, 1998; R package \nlmm\n). With a different parameterization, closed-form solutions for $\\boldsymbol \\beta$ (as above) and $\\sigma^2_\\epsilon$ exist. The solution for $\\sigma^2_\\epsilon$ can be written as $$\\sigma^2_\\epsilon = \\frac{1}{\\sum_i n_i}\\sum_i(\\boldsymbol y_i - \\boldsymbol X_{i} \\hat{\\boldsymbol\\beta})^{'}(\\hat\\xi \\boldsymbol 1_{n_i} \\boldsymbol 1_{n_i}^{'} + \\boldsymbol I_{n_i})^{-1}(\\boldsymbol y_i - \\boldsymbol X_{i} \\hat{\\boldsymbol\\beta}),$$ where $\\xi$ is defined as $\\sigma^2_\\eta/\\sigma^2_\\epsilon$ and can be estimated in an EM framework.\n\nlmm\n\nIn summary, MLE has distribution assumptions, and it is estimated in an iterative algorithm. The key difference between MLE and GLS is in the estimation for the variances.\n\nCroissant and Millo (2008) pointed out that\n\nWhile under normality, homoskedasticity and no serial correlation of the errors OLS are also the maximum likelihood estimator, in all the other cases there are important differences.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238227/402101 \n lmm\n\nIn summary, MLE has distribution assumptions, and it is estimated in an iterative algorithm. The key difference between MLE and GLS is in the estimation for the variances.\n\nCroissant and Millo (2008) pointed out that\n\nWhile under normality, homoskedasticity and no serial correlation of the errors OLS are also the maximum likelihood estimator, in all the other cases there are important differences.\n\nIn my opinion, for the distribution assumption, just as the difference between parametric and non-parametric approaches, MLE would be more efficient when the assumption holds, while GLS would be more robust.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/23837/402101 \n I think A and E aren't a good combination, because A says you should pick Mercy and E says you should pick Hope.\n\nA and D have the virtue of advocating the same choice.  But, lets examine the line of reasoning in D in further detail, since that seems to be the confusion.  The probability of success for the surgeries follows the same ordering at both hospitals, with the A type being most likely to be successful and the E type being the least likely.  If we collapse over (i.e., ignore) the hospitals, we can see that the marginal probability of success for the surgeries is:\n\nType     A     B     C     D     E     All  \nProb   .81   .78   .56   .21   .08     .52\n\nType     A     B     C     D     E     All  \nProb   .81   .78   .56   .21   .08     .52\n\nBecause E is \nmuch\n less likely to be successful, it is reasonable to imagine that it is more difficult (although in the real world, other possibilities exist as well).  We can extend that line of thinking to the other four types also.  Now lets look at what proportion of each hospital's total surgeries are of each type:\n\nType     A     B     C     D     E  \nMercy  .08   .39   .06   .44   .03  \nHope   .09   .54   .23   .09   .05\n\nType     A     B     C     D     E  \nMercy  .08   .39   .06   .44   .03  \nHope   .09   .54   .23   .09   .05\n\nWhat we notice here is that Hope tends to do more of the easier surgeries A-C (and especially B & C), and fewer of the harder surgeries like D.  E is pretty uncommon in both hospitals, but, for what it's worth, Hope actually does a higher percentage.  Nonetheless, the Simpson's Paradox effect is going to mostly be driven by B-D here (not actually column E as answer choice D suggested).\n\nSimpson's Paradox occurs because the surgeries vary in difficulty (in general) \nand also\n because the N's differ.  It is the differing base rates of the different types of surgeries that makes this counter-intuitive.  What is happening would be easy to see if both hospitals did exactly the same number of each type of surgery.  We can do that by simply calculating the success probabilities and multiplying by 100; this adjusts for the different frequencies:\n\nType     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225\n\nType     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/23837/402101 \n Type     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225\n\nType     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225\n\nNow, because both hospitals did 100 of each surgery (500 total), the answer is obvious: Mercy is the better hospital.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/244019/402101 \n 1) My understanding is that users have a choice of functions to use for $\\phi$, and that the Gaussian function is a very common choice.\n\n2) The density at $x$ is the mean of the different values of $\\phi_h(x_i - x)$ at $x$. For example, you might have $x_1=1$, $x_2 = 2$, and a Gaussian distribution with $\\sigma=1$ for $\\phi_h$. In this case, the density at $x$ would be $\\frac{\\mathcal{N}_{1, 1}(x) + \\mathcal{N}_{2, 1}(x)}{2}$.\n\n3) You can plug in any density function you like as your window function.\n\n4) $h$ determines the width of your chosen window function.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4273/402101 \n Short answer: Whenever you are facing one of these situations:\n\nlarge number of variables or low ratio of no. observations to no. variables (including the $n\\ll p$ case), \n\n\nhigh collinearity,\n\n\nseeking for a sparse solution (i.e., embed feature selection when estimating model parameters), or \n\n\naccounting for variables grouping in high-dimensional data set.\n\nRidge regression generally yields better predictions than OLS solution, through a better compromise between bias and variance. Its main drawback is that all predictors are kept in the model, so it is not very interesting if you seek a parsimonious model or want to apply some kind of feature selection.\n\nTo achieve sparsity, the lasso is more appropriate but it will not necessarily yield good results in presence of high collinearity (it has been observed that if predictors are highly correlated, the prediction performance of the lasso is dominated by ridge regression). The second problem with L1 penalty is that the lasso solution is not uniquely determined when the number of variables is greater than the number of subjects (this is not the case of ridge regression). The last drawback of lasso is that it tends to select only one variable among a group of predictors with high pairwise correlations. In this case, there are alternative solutions like the \ngroup\n (i.e., achieve shrinkage on block of covariates, that is some blocks of regression coefficients are exactly zero) or \nfused\n lasso. The \nGraphical Lasso\n also offers promising features for GGMs (see the R \nglasso\n package).\n\nBut, definitely, the \nelasticnet\n criteria, which is a combination of L1 and L2 penalties achieve both shrinkage and automatic variable selection, and it allows to keep $m>p$ variables in the case where $n\\ll p$. Following Zou and Hastie (2005), it is defined as the argument that minimizes (over $\\beta$)\n\n$$\nL(\\lambda_1,\\lambda_2,\\mathbf{\\beta}) = \\|Y-X\\beta\\|^2 + \\lambda_2\\|\\beta\\|^2 + \\lambda_1\\|\\beta\\|_1\n$$\n\nwhere $\\|\\beta\\|^2=\\sum_{j=1}^p\\beta_j^2$ and $\\|\\beta\\|^1=\\sum_{j=1}^p|\\beta_j |$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4273/402101 \n But, definitely, the \nelasticnet\n criteria, which is a combination of L1 and L2 penalties achieve both shrinkage and automatic variable selection, and it allows to keep $m>p$ variables in the case where $n\\ll p$. Following Zou and Hastie (2005), it is defined as the argument that minimizes (over $\\beta$)\n\n$$\nL(\\lambda_1,\\lambda_2,\\mathbf{\\beta}) = \\|Y-X\\beta\\|^2 + \\lambda_2\\|\\beta\\|^2 + \\lambda_1\\|\\beta\\|_1\n$$\n\nwhere $\\|\\beta\\|^2=\\sum_{j=1}^p\\beta_j^2$ and $\\|\\beta\\|^1=\\sum_{j=1}^p|\\beta_j |$.\n\nThe lasso can be computed with an algorithm based on coordinate descent as described in the recent paper by Friedman and coll., \nRegularization Paths for Generalized Linear Models via Coordinate Descent\n (JSS, 2010) or the LARS algorithm. In R, the \npenalized\n, \nlars\n or \nbiglars\n, and \nglmnet\n packages are useful packages; in Python, there's the \nscikit.learn\n toolkit, with extensive \ndocumentation\n on the algorithms used to apply all three kind of regularization schemes.\n\nAs for general references, the \nLasso page\n contains most of what is needed to get started with lasso regression and technical details about L1-penalty, and this related question features essential references, \nWhen should I use lasso vs ridge?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/41514/402101 \n You are conflating the features of a process with its representation. Consider the (return) process $(Y_t)_{t=0}^\\infty$.\n\nAn ARMA(p,q) model specifies the \nconditional mean\n of the process as\n\n$$\n\\begin{align}\n\\mathbb{E}(Y_t \\mid \\mathcal{I}_t) &= \\alpha_0 + \\sum_{j=1}^p \\alpha_j Y_{t-j}+ \\sum_{k=1}^q \\beta_k\\epsilon_{t-k}\\\\\n\\end{align}\n$$\nHere, $\\mathcal{I}_t$ is the information set at time $t$, which is the $\\sigma$-algebra generated by the lagged values of the outcome process $(Y_t)$.\n\nThe GARCH(r,s) model specifies the \nconditional variance\n of the process\n$$\n\\begin{alignat}{2}\n& \\mathbb{V}(Y_t \\mid \\mathcal{I}_t) &{}={}& \\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t) \\\\\n\\equiv \\,& \\sigma^2_t&{}={}& \\delta_0 + \\sum_{l=1}^r \\delta_j \\sigma^2_{t-l} + \\sum_{m=1}^s \\gamma_k \\epsilon^2_{t-m}\n\\end{alignat}\n$$\n\nNote in particular the first equivalence $ \\mathbb{V}(Y_t \\mid \\mathcal{I}_t)= \\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t)$.\n\nAside\n: Based on this representation, you can write\n$$\n\\epsilon_t \\equiv \\sigma_t Z_t\n$$\nwhere $Z_t$ is a strong white noise process, but this follows from the way the process is defined.\n\nThe two models (for the conditional mean and the variance) are perfectly compatible with each other, in that the mean of the process can be modeled as ARMA, and the variances as GARCH. This leads to the complete specification of an ARMA(p,q)-GARCH(r,s) model for the  process as in the following representation\n$$\n\\begin{align}\nY_t &= \\alpha_0 + \\sum_{j=1}^p \\alpha_j Y_{t-j} + \\sum_{k=1}^q \\beta_k\\epsilon_{t-k} +\\epsilon_t\\\\\n \\mathbb{E}(\\epsilon_t\\mid \\mathcal{I}_t) &=0,\\, \\forall t \\\\\n\\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t) &= \\delta_0 + \\sum_{l=1}^r \\delta_l \\sigma^2_{t-l} + \\sum_{m=1}^s \\gamma_m \\epsilon^2_{t-m}\\, \\forall t\n\\end{align}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/38553/402101 \n In this case, you can consider an \nABC\n approximation of the likelihood (and consequently of the \nMLE\n) under the following assumption/restriction:\n\nAssumption.\n The original sample size \n$n$\n is known.\n\nThis is not a wild assumption given that the quality, in terms of convergence, of frequentist estimators depends on the sample size, therefore one cannot obtain arbitrarily good estimators without knowing the original sample size.\n\nThe idea is to generate a sample from the posterior distribution of \n$\\theta$\n and, \nin order to produce an approximation of the MLE\n, you can use an importance sampling technique as in \n[1]\n or to consider a uniform prior on \n$\\theta$\n with support on a suitable set as in \n[2]\n.\n\nI am going to describe the method in [2]. First of all, let me describe the ABC sampler.\n\nABC Sampler\n\nLet \n$f(\\cdot\\vert\\theta)$\n be the model that generates the sample where \n$\\theta \\in \\Theta$\n is a parameter (to be estimated), \n$T$\n be a statistic (a function of the sample) and \n$T_0$\n be the observed statistic, in the ABC jargon this is called a \nsummary statistic\n, \n$\\rho$\n be a metric, \n$\\pi(\\theta)$\n a prior distribution on \n$\\theta$\n and \n$\\epsilon>0$\n a tolerance. Then, the ABC-rejection sampler can be implemented as follows.\n\nSample \n$\\theta^*$\n from \n$\\pi(\\cdot)$\n.\n\n\nGenerate a sample \n$\\bf{x}$\n of size \n$n$\n from the model \n$f(\\cdot\\vert\\theta^*)$\n.\n\n\nCompute \n$T^*=T({\\bf x})$\n.\n\n\nIf \n$\\rho(T^*,T_0)<\\epsilon$\n, accept \n$\\theta^*$\n as a simulation from the posterior of \n$\\theta$\n.\n\nThis algorithm generates an approximate sample from the posterior distribution of \n$\\theta$\n given \n$T({\\bf x})=T_0$\n. Therefore, the best scenario is when the statistic \n$T$\n is sufficient but other statistics can be used. For a more detailed description of this see \nthis paper\n.\n\nNow, in a general framework, if one uses a uniform prior that contains the MLE in its support, then the Maximum \na posteriori\n (MAP) coincides with Maximum Likelihood Estimator (MLE). Therefore, if you consider an appropriate uniform prior in the ABC Sampler, then you can generate an approximate sample of a posterior distribution whose MAP coincides with the MLE. The remaining step consists of estimating this mode. This problem has been discussed in CV, for instance in \n\"Computationally efficient estimation of multivariate mode\"\n.\n\nA toy example",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/38553/402101 \n A toy example\n\nLet \n$(x_1,...,x_n)$\n be a sample from a \n$N(\\mu,1)$\n and suppose that the only information available from this sample is \n$\\bar{x}=\\dfrac{1}{n}\\sum_{j=1}^n x_j$\n. Let \n$\\rho$\n be the Euclidean metric in \n${\\mathbb R}$\n and \n$\\epsilon=0.001$\n. The following R code shows how to obtain an approximate MLE using the methods described above using a simulated sample with \n$n=100$\n and \n$\\mu=0$\n, a sample of the posterior distribution of size \n$1000$\n, a uniform prior for \n$\\mu$\n on \n$(-0.3,0.3)$\n, and a kernel density estimator for the estimation of the mode of the posterior sample (MAP=MLE).\n\n# rm(list=ls())\n\n# Simulated data\nset.seed(1)\nx = rnorm(100)\n\n# Observed statistic\nT0 = mean(x)\n\n# ABC Sampler using a uniform prior \n\nN=1000\neps = 0.001\nABCsamp = rep(0,N)\ni=1\n\nwhile(i < N+1){\n  u = runif(1,-0.3,0.3)\n  t.samp = rnorm(100,u,1)\n  Ts = mean(t.samp)\n  if(abs(Ts-T0)<eps){\n    ABCsamp[i]=u\n    i=i+1\n    print(i)\n  }\n}\n\n# Approximation of the MLE\nkd = density(ABCsamp)\nkd\n$x[which(kd$\ny==max(kd$y))]\n\n# rm(list=ls())\n\n# Simulated data\nset.seed(1)\nx = rnorm(100)\n\n# Observed statistic\nT0 = mean(x)\n\n# ABC Sampler using a uniform prior \n\nN=1000\neps = 0.001\nABCsamp = rep(0,N)\ni=1\n\nwhile(i < N+1){\n  u = runif(1,-0.3,0.3)\n  t.samp = rnorm(100,u,1)\n  Ts = mean(t.samp)\n  if(abs(Ts-T0)<eps){\n    ABCsamp[i]=u\n    i=i+1\n    print(i)\n  }\n}\n\n# Approximation of the MLE\nkd = density(ABCsamp)\nkd\n$x[which(kd$\ny==max(kd$y))]\n\nAs you can see, using a small tolerance we get a very good approximation of the MLE (which in this trivial example can be calculated from the statistic given that it is sufficient). It is important to notice that the choice of the summary statistic is crucial. Quantiles are typically a good choice for the summary statistic, but not all the choices produce a good approximation. It may be the case that the summary statistic is not very informative and then the quality of the approximation might be poor, which is well-known in the ABC community.\n\nUpdate:\n A similar approach was recently published in \nFan et al. (2012)\n. See \nthis entry\n for a discussion on the paper.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6303/402101 \n Personally, I'd favour instead showing the \nfit\n of the theoretical to the empirical distribution using a set of \nP-P plots\n or \nQ-Q plots\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/195391/402101 \n Standardization isn't required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization. For example, if you use Newton-Raphson to maximize the likelihood, standardizing the features makes the convergence faster. Otherwise, you can run your logistic regression without any standardization treatment on the features.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/11132/402101 \n Briefly stated, this is because base-R's \nmanova(lm())\n uses sequential model comparisons for so-called Type I sum of squares, whereas \ncar\n's \nManova()\n by default uses model comparisons for Type II sum of squares.\n\nmanova(lm())\n\ncar\n\nManova()\n\nI assume you're familiar with the model-comparison approach to ANOVA or regression analysis. This approach defines these tests by comparing a restricted model (corresponding to a null hypothesis) to an unrestricted model (corresponding to the alternative hypothesis). If you're not familiar with this idea, I recommend Maxwell & Delaney's excellent \"Designing experiments and analyzing data\" (2004).\n\nFor type I SS, the restricted model in a regression analysis for your first predictor \nc\n is the null-model which only uses the absolute term: \nlm(Y ~ 1)\n, where \nY\n in your case would be the multivariate DV defined by \ncbind(A, B)\n. The unrestricted model then adds predictor \nc\n, i.e. \nlm(Y ~ c + 1)\n.\n\nc\n\nlm(Y ~ 1)\n\nY\n\ncbind(A, B)\n\nc\n\nlm(Y ~ c + 1)\n\nFor type II SS, the unrestricted model in a regression analysis for your first predictor \nc\n is the full model which includes all predictors except for their interactions, i.e., \nlm(Y ~ c + d + e + f + g + H + I)\n. The restricted model removes predictor \nc\n from the unrestricted model, i.e., \nlm(Y ~ d + e + f + g + H + I)\n.\n\nc\n\nlm(Y ~ c + d + e + f + g + H + I)\n\nc\n\nlm(Y ~ d + e + f + g + H + I)\n\nSince both functions rely on different model comparisons, they lead to different results. The question which one is preferable is hard to answer - it really depends on your hypotheses.\n\nWhat follows assumes you're familiar with how multivariate test statistics like the Pillai-Bartlett Trace are calculated based on the null-model, the full model, and the pair of restricted-unrestricted models. For brevity, I only consider predictors \nc\n and \nH\n, and only test for \nc\n.\n\nc\n\nH\n\nc",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/11132/402101 \n Since both functions rely on different model comparisons, they lead to different results. The question which one is preferable is hard to answer - it really depends on your hypotheses.\n\nWhat follows assumes you're familiar with how multivariate test statistics like the Pillai-Bartlett Trace are calculated based on the null-model, the full model, and the pair of restricted-unrestricted models. For brevity, I only consider predictors \nc\n and \nH\n, and only test for \nc\n.\n\nc\n\nH\n\nc\n\nN <- 100                             # generate some data: number of subjects\nc <- rbinom(N, 1, 0.2)               # dichotomous predictor c\nH <- rnorm(N, -10, 2)                # metric predictor H\nA <- -1.4*c + 0.6*H + rnorm(N, 0, 3) # DV A\nB <-  1.4*c - 0.6*H + rnorm(N, 0, 3) # DV B\nY <- cbind(A, B)                     # DV matrix\nmy.model <- lm(Y ~ c + H)            # the multivariate model\nsummary(manova(my.model))            # from base-R: SS type I\n#           Df  Pillai approx F num Df den Df  Pr(>F)    \n# c          1 0.06835   3.5213      2     96 0.03344 *  \n# H          1 0.32664  23.2842      2     96 5.7e-09 ***\n# Residuals 97\n\nN <- 100                             # generate some data: number of subjects\nc <- rbinom(N, 1, 0.2)               # dichotomous predictor c\nH <- rnorm(N, -10, 2)                # metric predictor H\nA <- -1.4*c + 0.6*H + rnorm(N, 0, 3) # DV A\nB <-  1.4*c - 0.6*H + rnorm(N, 0, 3) # DV B\nY <- cbind(A, B)                     # DV matrix\nmy.model <- lm(Y ~ c + H)            # the multivariate model\nsummary(manova(my.model))            # from base-R: SS type I\n#           Df  Pillai approx F num Df den Df  Pr(>F)    \n# c          1 0.06835   3.5213      2     96 0.03344 *  \n# H          1 0.32664  23.2842      2     96 5.7e-09 ***\n# Residuals 97\n\nFor comparison, the result from \ncar\n's \nManova()\n function using SS type II.\n\ncar\n\nManova()\n\nlibrary(car)                           # for Manova()\nManova(my.model, type=\"II\")\n# Type II MANOVA Tests: Pillai test statistic\n#   Df test stat approx F num Df den Df  Pr(>F)    \n# c  1   0.05904   3.0119      2     96 0.05387 .  \n# H  1   0.32664  23.2842      2     96 5.7e-09 ***\n\nlibrary(car)                           # for Manova()\nManova(my.model, type=\"II\")\n# Type II MANOVA Tests: Pillai test statistic\n#   Df test stat approx F num Df den Df  Pr(>F)    \n# c  1   0.05904   3.0119      2     96 0.05387 .  \n# H  1   0.32664  23.2842      2     96 5.7e-09 ***",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/11132/402101 \n library(car)                           # for Manova()\nManova(my.model, type=\"II\")\n# Type II MANOVA Tests: Pillai test statistic\n#   Df test stat approx F num Df den Df  Pr(>F)    \n# c  1   0.05904   3.0119      2     96 0.05387 .  \n# H  1   0.32664  23.2842      2     96 5.7e-09 ***\n\nNow manually verify both results. Build the design matrix $X$ first and compare to R's design matrix.\n\nX  <- cbind(1, c, H)\nXR <- model.matrix(~ c + H)\nall.equal(X, XR, check.attributes=FALSE)\n# [1] TRUE\n\nX  <- cbind(1, c, H)\nXR <- model.matrix(~ c + H)\nall.equal(X, XR, check.attributes=FALSE)\n# [1] TRUE\n\nNow define the orthogonal projection for the full model ($P_{f} = X (X'X)^{-1} X'$, using all predictors). This gives us the matrix $W = Y' (I-P_{f}) Y$.\n\nPf  <- X %*% solve(t(X) %*% X) %*% t(X)\nId  <- diag(N)\nWW  <- t(Y) %*% (Id - Pf) %*% Y\n\nPf  <- X %*% solve(t(X) %*% X) %*% t(X)\nId  <- diag(N)\nWW  <- t(Y) %*% (Id - Pf) %*% Y\n\nRestricted and unrestricted models for SS type I plus their projections $P_{rI}$ and $P_{uI}$, leading to matrix $B_{I} = Y' (P_{uI} - P_{PrI}) Y$.\n\nXrI <- X[ , 1]\nPrI <- XrI %*% solve(t(XrI) %*% XrI) %*% t(XrI)\nXuI <- X[ , c(1, 2)]\nPuI <- XuI %*% solve(t(XuI) %*% XuI) %*% t(XuI)\nBi  <- t(Y) %*% (PuI - PrI) %*% Y\n\nXrI <- X[ , 1]\nPrI <- XrI %*% solve(t(XrI) %*% XrI) %*% t(XrI)\nXuI <- X[ , c(1, 2)]\nPuI <- XuI %*% solve(t(XuI) %*% XuI) %*% t(XuI)\nBi  <- t(Y) %*% (PuI - PrI) %*% Y\n\nRestricted and unrestricted models for SS type II plus their projections $P_{rI}$ and $P_{uII}$, leading to matrix $B_{II} = Y' (P_{uII} - P_{PrII}) Y$.\n\nXrII <- X[ , -2]\nPrII <- XrII %*% solve(t(XrII) %*% XrII) %*% t(XrII)\nPuII <- Pf\nBii  <- t(Y) %*% (PuII - PrII) %*% Y\n\nXrII <- X[ , -2]\nPrII <- XrII %*% solve(t(XrII) %*% XrII) %*% t(XrII)\nPuII <- Pf\nBii  <- t(Y) %*% (PuII - PrII) %*% Y\n\nPillai-Bartlett trace for both types of SS: trace of $(B + W)^{-1} B$.\n\n(PBTi  <- sum(diag(solve(Bi  + WW) %*% Bi)))   # SS type I\n# [1] 0.0683467\n\n(PBTii <- sum(diag(solve(Bii + WW) %*% Bii)))  # SS type II\n# [1] 0.05904288\n\n(PBTi  <- sum(diag(solve(Bi  + WW) %*% Bi)))   # SS type I\n# [1] 0.0683467\n\n(PBTii <- sum(diag(solve(Bii + WW) %*% Bii)))  # SS type II\n# [1] 0.05904288\n\nNote that the calculations for the orthogonal projections mimic the mathematical formula, but are a bad idea numerically. One should really use QR-decompositions or SVD in combination with \ncrossprod()\n instead.\n\ncrossprod()",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2700/402101 \n Imagine a big family dinner where everybody starts asking you about PCA. First, you explain it to your great-grandmother; then to your grandmother; then to your mother; then to your spouse; finally, to your daughter (a mathematician). Each time the next person is less of a layman. Here is how the conversation might go.\n\nGreat-grandmother: I heard you are studying \"Pee-See-Ay\". I wonder what that is...\n\nYou:\n Ah, it's just a method of summarizing some data. Look, we have some wine bottles standing here on the table. We can describe each wine by its colour, how strong it is, how old it is, and so on.\n\n\n\nVisualization originally found\n \nhere\n.\n\nWe can compose a whole list of different characteristics of each wine in our cellar. But many of them will measure related properties and so will be redundant. If so, we should be able to summarize each wine with fewer characteristics! This is what PCA does.\n\nGrandmother: This is interesting! So this PCA thing checks what characteristics are redundant and discards them?\n\nYou:\n Excellent question, granny! No, PCA is not selecting some characteristics and discarding the others. Instead, it constructs some \nnew\n characteristics that turn out to summarize our list of wines well. Of course, these new characteristics are constructed using the old ones; for example, a new characteristic might be computed as wine age minus wine acidity level or some other combination (we call them \nlinear combinations\n).\n\nIn fact, PCA finds the best possible characteristics, the ones that summarize the list of wines as well as only possible (among all conceivable linear combinations). This is why it is so useful.\n\nMother: Hmmm, this certainly sounds good, but I am not sure I understand. What do you actually mean when you say that these new PCA characteristics \"summarize\" the list of wines?\n\nYou:\n I guess I can give two different answers to this question. The first answer is that you are looking for some wine properties (characteristics) that strongly differ across wines. Indeed, imagine that you come up with a property that is the same for most of the wines - like the stillness of wine after being poured. This would not be very useful, would it? Wines are very different, but your new property makes them all look the same! This would certainly be a bad summary. Instead, PCA looks for properties that show as much variation across wines as possible.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2700/402101 \n The second answer is that you look for the properties that would allow you to predict, or \"reconstruct\", the original wine characteristics. Again, imagine that you come up with a property that has no relation to the original characteristics - like the shape of a wine bottle; if you use only this new property, there is no way you could reconstruct the original ones! This, again, would be a bad summary. So PCA looks for properties that allow reconstructing the original characteristics as well as possible.\n\nSurprisingly, it turns out that these two aims are equivalent and so PCA can kill two birds with one stone.\n\nSpouse: But darling, these two \"goals\" of PCA sound so different! Why would they be equivalent?\n\nYou:\n Hmmm. Perhaps I should make a little drawing \n(takes a napkin and starts scribbling)\n. Let us pick two wine characteristics, perhaps wine darkness and alcohol content -- I don't know if they are correlated, but let's imagine that they are. Here is what a scatter plot of different wines could look like:\n\n\n\nEach dot in this \"wine cloud\" shows one particular wine. You see that the two properties (\n$x$\n and \n$y$\n on this figure) are correlated. A new property can be constructed by drawing a line through the centre of this wine cloud and projecting all points onto this line. This new property will be given by a linear combination \n$w_1 x + w_2 y$\n, where each line corresponds to some particular values of \n$w_1$\n and \n$w_2$\n.\n\nNow, look here very carefully -- here is what these projections look like for different lines (red dots are projections of the blue dots):\n\n\n\nAs I said before, PCA will find the \"best\" line according to two different criteria of what is the \"best\". First, the variation of values along this line should be maximal. Pay attention to how the \"spread\" (we call it \"variance\") of the red dots changes while the line rotates; can you see when it reaches maximum? Second, if we reconstruct the original two characteristics (position of a blue dot) from the new one (position of a red dot), the reconstruction error will be given by the length of the connecting red line. Observe how the length of these red lines changes while the line rotates; can you see when the total length reaches minimum?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2700/402101 \n If you stare at this animation for some time, you will notice that \"the maximum variance\" and \"the minimum error\" are reached at the same time, namely when the line points to the magenta ticks I marked on both sides of the wine cloud. This line corresponds to the new wine property that will be constructed by PCA.\n\nBy the way, PCA stands for \"principal component analysis\", and this new property is called \"first principal component\". And instead of saying \"property\" or \"characteristic\", we usually say \"feature\" or \"variable\".\n\nDaughter: Very nice, papa! I think I can see why the two goals yield the same result: it is essentially because of the Pythagoras theorem, isn't it? Anyway, I heard that PCA is somehow related to eigenvectors and eigenvalues; where are they in this picture?\n\nYou:\n Brilliant observation. Mathematically, the spread of the red dots is measured as the average squared distance from the centre of the wine cloud to each red dot; as you know, it is called the \nvariance\n. On the other hand, the total reconstruction error is measured as the average squared length of the corresponding red lines. But as the angle between red lines and the black line is always \n$90^\\circ$\n, the sum of these two quantities is equal to the average squared distance between the centre of the wine cloud and each blue dot; this is precisely Pythagoras theorem. Of course, this average distance does not depend on the orientation of the black line, so the higher the variance, the lower the error (because their sum is constant). This hand-wavy argument can be made precise (\nsee here\n).\n\nBy the way, you can imagine that the black line is a solid rod, and each red line is a spring. The energy of the spring is proportional to its squared length (this is known in physics as Hooke's law), so the rod will orient itself such as to minimize the sum of these squared distances. I made a simulation of what it will look like in the presence of some viscous friction:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2700/402101 \n By the way, you can imagine that the black line is a solid rod, and each red line is a spring. The energy of the spring is proportional to its squared length (this is known in physics as Hooke's law), so the rod will orient itself such as to minimize the sum of these squared distances. I made a simulation of what it will look like in the presence of some viscous friction:\n\n\n\nRegarding eigenvectors and eigenvalues. You know what a \ncovariance matrix\n is; in my example it is a \n$2\\times 2$\n matrix that is given by \n$$\\begin{pmatrix}1.07 &0.63\\\\0.63 & 0.64\\end{pmatrix}.$$\n What this means is that the variance of the \n$x$\n variable is \n$1.07$\n, the variance of the \n$y$\n variable is \n$0.64$\n, and the covariance between them is \n$0.63$\n. As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors (incidentally, this is called \nspectral theorem\n); corresponding eigenvalues will then be located on the diagonal. In this new coordinate system, the covariance matrix is diagonal and looks like that: \n$$\\begin{pmatrix}1.52 &0\\\\0 & 0.19\\end{pmatrix},$$\n meaning that the correlation between points is now zero. It becomes clear that the variance of any projection will be given by a weighted average of the eigenvalues (I am only sketching the intuition here). Consequently, the maximum possible variance (\n$1.52$\n) will be achieved if we simply take the projection on the first coordinate axis. It follows that the direction of the first principal component is given by the first eigenvector of the covariance matrix. (\nMore details here.\n)\n\nYou can see this on the rotating figure as well: there is a gray line there orthogonal to the black one; together, they form a rotating coordinate frame. Try to notice when the blue dots become uncorrelated in this rotating frame. The answer, again, is that it happens precisely when the black line points at the magenta ticks. Now I can tell you how I found them (the magenta ticks): they mark the direction of the first eigenvector of the covariance matrix, which in this case is equal to \n$(0.81, 0.58)$\n.\n\nPer popular request, I shared \nthe Matlab code to produce the above animations\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/421748/402101 \n Yes, you can consider \n$X$\n and \n$Z$\n to be arbitrary vectors of variables. The identification problem of expressions of the type \n$E[Y|do(X)]$\n and \n$E[Y|do(X), Z]$\n for arbitrary vectors of variables \n$X$\n and \n$Z$\n \nhas been solved for nonparametric models using the do-calculus (via the ID-algorithm).\n\nFor instance, in the model below, suppose you are interested in identifying \n$E[Y|do(X_1, X_2)]$\n:\n\n\n\nThis is given by (here you can just use the truncated factorization formula):\n\n$$\nE[Y|do(X_1, X_2)] = \\sum_{Z_1, Z_2} P(Y|X_1, X_2, Z_2) P(Z_2|X_1,Z_1) P(Z_1)\n$$\n\nOr equivalently, using inverse probability weights:\n\n$$\nE[Y|do(X_1, X_2)] = \\sum_{Z_1, Z_2} \\frac{P(Y, X_1, X_2, Z_1, Z_2)}{P(X_2|X_1, Z_1, Z_2)P(X_1|Z_1)} \n$$\n\nThe \nR package causaleffect\n has several of the existing identification algorithms implemented.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214493/402101 \n Every probability distribution on (a subset of) $\\mathbb R^n$ has a \ncumulative distribution function\n, and it uniquely defines the distribution.  So, in this sense, the CDF is indeed as fundamental as the distribution itself.\n\nA \nprobability density function\n, however, exists only for \n(absolutely) continuous probability distributions\n.  The simplest example of a distribution lacking a PDF is any \ndiscrete probability distribution\n, such as the distribution of a random variable that only takes integer values.\n\nOf course, such discrete probability distributions can be characterized by a \nprobability mass function\n instead, but there are also distributions that have \nneither\n and PDF or a PMF, such as any mixture of a continuous and a discrete distribution:\n\n(Diagram shamelessly stolen from \nGlen_b's answer\n to a related question.)\n\nThere are even \nsingular probability distributions\n, such as the \nCantor distribution\n, which cannot be described even by a \ncombination\n of a PDF and a PMF.  Such distributions still have a well defined CDF, though.  For example, here is the CDF of the Cantor distribution, also sometimes called the \"Devil's staircase\":\n\n(\nImage\n from \nWikimedia Commons\n by users \nTheon\n and \nAmirki\n, used under the \nCC-By-SA 3.0\n license.)\n\nThe CDF, known as the \nCantor function\n, is continuous but not absolutely continuous.  In fact, it is constant everywhere except on a \nCantor set\n of zero Lebesgue measure, but which still contains infinitely many points.  Thus, the entire probability mass of the Cantor distribution is concentrated on this vanishingly small subset of the real number line, but every point in the set still individually has zero probability.\n\nThere are also probability distributions that do not have a \nmoment-generating function\n.  Probably the best known example is the \nCauchy distribution\n, a \nfat-tailed distribution\n which has no well-defined moments of order 1 or higher (thus, in particular, having no well-defined mean or variance!).\n\nAll probability distributions on $\\mathbb R^n$ do, however, have a (possibly complex-valued) \ncharacteristic function\n), whose definition differs from that of the MGF only by a multiplication with the \nimaginary unit\n.  Thus, the characteristic function may be regarded as being as fundamental as the CDF.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/5164/402101 \n The threshold is chosen such that it ensures convergence of the \nhypergeometric distribution\n ($\\sqrt{\\frac{N-n}{N-1}}$ is its SD), instead of a binomial distribution (for sampling with replacement), to a normal distribution (this is the Central Limit Theorem, see e.g., \nThe Normal Curve, the Central Limit Theorem, and Markov's and Chebychev's Inequalities for Random Variables\n). In other words, when $n/N\\leq 0.05$ (i.e., $n$ is not 'too large' compared to $N$), the FPC can safely be ignored; it is easy to see how the correction factor evolves with varying $n$ for a fixed $N$: with $N=10,000$, we have $\\text{FPC}=.9995$ when $n=10$ while $\\text{FPC}=.3162$ when $n=9,000$. When $N\\to\\infty$, the FPC approaches 1 and we are close to the situation of sampling with replacement (i.e., like with an infinite population).\n\nTo understand this results, a good starting point is to read some online tutorials on sampling theory where sampling is done without replacement (\nsimple random sampling\n). This online tutorial on \nNonparametric statistics\n has an illustration on computing the expectation and variance for a total.\n\nYou will notice that some authors use $N$ instead of $N-1$ in the denominator of the FPC; in fact, it depends on whether you work with the sample or population statistic: for the variance, it will be $N$ instead of $N-1$ if you are interested in $S^2$ rather than $\\sigma^2$.\n\nAs for online references, I can suggest you\n\nEstimation and statistical inference\n\n\nA new look at inference for the Hypergeometric Distribution\n\n\nFinite Population Sampling with Application to the Hypergeometric Distribution\n\n\nSimple random sampling",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/46858/402101 \n (Technically, the P-value is the probability of observing data \nat least as extreme\n as that actually observed, given the null hypothesis.)\n\nQ1. A decision to reject the null hypothesis on the basis of a small P-value typically depends on 'Fisher's disjunction': Either a rare event has happened or the null hypothesis is false. In effect, it is rarity of the event is what the P-value tells you rather than the probability that the null is false.\n\nThe probability that the null is false can be obtained from the experimental data only by way of Bayes' theorem, which requires specification of the 'prior' probability of the null hypothesis (presumably what Gill is referring to as \"marginal distributions\").\n\nQ2. This part of your question is much harder than it might seem. There is a great deal of confusion regarding P-values and error rates which is, presumably, what Gill is referring to with \"but is typically treated as such.\" The combination of Fisherian P-values with Neyman-Pearsonian error rates has been called an incoherent mishmash, and it is unfortunately very widespread. No short answer is going to be completely adequate here, but I can point you to a couple of good papers (yes, one is mine). Both will help you make sense of the Gill paper.\n\nHurlbert, S., & Lombardi, C. (2009). Final collapse of the Neyman-Pearson decision theoretic framework and rise of the neoFisherian. Annales Zoologici Fennici, 46(5), 311\u2013349. \n(Link to paper)\n\nLew, M. J. (2012). Bad statistical practice in pharmacology (and other basic biomedical disciplines): you probably don't know P. British Journal of Pharmacology, 166(5), 1559\u20131567. doi:10.1111/j.1476-5381.2012.01931.x \n(Link to paper)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/494464/402101 \n In a parametric model, the number of parameters is fixed with respect to the sample size.  In a nonparametric model, the (effective) number of parameters can grow with the sample size.\n\nIn an OLS regression, the number of parameters will always be the length of $\\beta$, plus one for the variance.\n\nA neural net with fixed architecture and no weight decay would be a parametric model.\n\nBut if you have weight decay, then the value of the decay parameter selected by cross-validation will generally get smaller with more data.  This can be interpreted as an increase in the effective number of parameters with increasing sample size.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/20856/402101 \n I think this approach is mistaken, but perhaps it will be more helpful if I explain why.  Wanting to know the best model given some information about a large number of variables is quite understandable.  Moreover, it is a situation in which people seem to find themselves regularly.  In addition, many textbooks (and courses) on regression cover stepwise selection methods, which implies that they must be legitimate.  Unfortunately, however, they are not, and the pairing of this situation and goal is quite difficult to successfully navigate.  The following is a list of problems with automated stepwise model selection procedures (attributed to Frank Harrell, and copied from \nhere\n):\n\nIt yields R-squared values that are badly biased to be high.\n\n\n\n\nThe F and chi-squared tests quoted next to each variable on the  printout do not have the claimed distribution.\n\n\n\n\nThe method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\n\n\n\n\nIt yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\n\n\n\n\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani\n[1996]).\n\n\n\n\nIt has severe problems in the presence of collinearity.\n\n\n\n\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\n\n\n\n\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\n\n\n\n\nIt allows us to not think about the problem.\n\n\n\n\nIt uses a lot of paper.\n\nIt yields R-squared values that are badly biased to be high.\n\nThe F and chi-squared tests quoted next to each variable on the  printout do not have the claimed distribution.\n\nThe method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\n\nIt yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\n\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani\n[1996]).\n\nIt has severe problems in the presence of collinearity.\n\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\n\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\n\nIt allows us to not think about the problem.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/20856/402101 \n It gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani\n[1996]).\n\nIt has severe problems in the presence of collinearity.\n\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\n\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\n\nIt allows us to not think about the problem.\n\nIt uses a lot of paper.\n\nThe question is, what's so bad about these procedures / why do these problems occur?  Most people who have taken a basic regression course are familiar with the concept of \nregression to the mean\n, so this is what I use to explain these issues.  (Although this may seem off-topic at first, bear with me, I promise it's relevant.)\n\nImagine a high school track coach on the first day of tryouts.  Thirty kids show up.  These kids have some underlying level of intrinsic ability to which neither the coach nor anyone else, has direct access.  As a result, the coach does the only thing he can do, which is have them all run a 100m dash.  The times are presumably a measure of their intrinsic ability and are taken as such.  However, they are probabilistic; some proportion of how well someone does is based on their actual ability, and some proportion is random.  Imagine that the true situation is the following:\n\nset.seed(59)\nintrinsic_ability = runif(30, min=9, max=10)\ntime = 31 - 2*intrinsic_ability + rnorm(30, mean=0, sd=.5)\n\nset.seed(59)\nintrinsic_ability = runif(30, min=9, max=10)\ntime = 31 - 2*intrinsic_ability + rnorm(30, mean=0, sd=.5)\n\nThe results of the first race are displayed in the following figure along with the coach's comments to the kids.\n\n\n\nNote that partitioning the kids by their race times leaves overlaps on their intrinsic ability--this fact is crucial.  After praising some, and yelling at some others (as coaches tend to do), he has them run again.  Here are the results of the second race with the coach's reactions (simulated from the same model above):",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/20856/402101 \n The results of the first race are displayed in the following figure along with the coach's comments to the kids.\n\n\n\nNote that partitioning the kids by their race times leaves overlaps on their intrinsic ability--this fact is crucial.  After praising some, and yelling at some others (as coaches tend to do), he has them run again.  Here are the results of the second race with the coach's reactions (simulated from the same model above):\n\n\n\nNotice that their intrinsic ability is identical, but the times bounced around relative to the first race.  From the coach's point of view, those he yelled at tended to improve, and those he praised tended to do worse (I adapted this concrete example from the Kahneman quote listed on the wiki page), although actually regression to the mean is a simple mathematical consequence of the fact that the coach is selecting athletes for the team based on a measurement that is partly random.\n\nNow, what does this have to do with automated (e.g., stepwise) model selection techniques?  Developing and confirming a model based on the same dataset is sometimes called \ndata dredging\n.  Although there is some underlying relationship amongst the variables, and stronger relationships are expected to yield stronger scores (e.g., higher t-statistics), these are random variables, and the realized values contain error.  Thus, when you select variables based on having higher (or lower) realized values, they may be such because of their underlying true value, error, or both.  If you proceed in this manner, you will be as surprised as the coach was after the second race.  This is true whether you select variables based on having high t-statistics, or low intercorrelations.  True, using the AIC is better than using p-values, because it penalizes the model for complexity, but the AIC is itself a random variable (if you run a study several times and fit the same model, the AIC will bounce around just like everything else).  Unfortunately, this is just a problem intrinsic to the epistemic nature of reality itself.\n\nI hope this is helpful.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/142632/402101 \n The typical way to estimate a difference in differences model with more than two time periods is your proposed solution b). Keeping your notation you would regress\n$$Y_{ist} = \\alpha +\\gamma_s (\\text{Treatment}_s) + \\lambda (\\text{year dummy}_t) + \\delta D_{st} + \\epsilon_{ist}$$\nwhere $D_t \\equiv \\text{Treatment}_s\\cdot d_t$ is a dummy variable which equals one for treatment units $s$ in the post-treatment period ($d_t = 1$) and is zero otherwise. Note that this is a more general formulation of the difference in differences regression which allows for different timings of the treatment for different treated units.\n\nAs was pointed out correctly in the comments your proposed solution c) does not work out due to collinearity with the time dummies and the dummy for the post-treatment period. However, a slight variant of this turns out to be a robustness check. Let $\\gamma_{s0}$ and $\\gamma_{s1}$ be two sets of dummy variables for each control unit $s0$ and each treated unit $s1$, respectively, then interacting the dummies for the treated units with the time variable $t$ and regressing\n$$Y_{ist} = \\gamma_{s0} + \\gamma_{s1}t + \\lambda (\\text{year dummy}_t) + \\delta D_{st} + \\epsilon_{ist}$$\nincludes a unit specific time trend $\\gamma_{s1}t$. When you include these unit specific time trends and the difference in differences coefficient $\\delta$ does not change significantly you can be more confident about your results. Otherwise you might wonder whether your treatment effect has absorbed differences between treated units due to an underlying time trend (can happen when policies kick in at different points in time).\n\nAn example cited in Angrist and Pischke (2009) Mostly Harmless Econometrics is a labor market policy study by \nBesley and Burgess (2004)\n. In their paper it happens that the inclusion of state-specific time trends kills the estimated treatment effect. Note though that for this robustness check you need more than 3 time periods.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/442429/402101 \n Least squares is indeed maximum likelihood if the errors are iid normal, but if they aren't iid normal, least squares is not maximum likelihood. For example if my errors were logistic, least squares wouldn't be a terrible idea but it wouldn't be maximum likelihood.\n\nLots of estimators are not maximum likelihood estimators; while maximum likelihood estimators typically have a number of useful and attractive properties they're not the only game in town (and indeed not even always a great idea).\n\nA few examples of other estimation methods would include\n\nmethod of moments\n (this involves equating enough sample and population moments to solve for parameter estimates; sometimes this turns out to be maximum likelihood but usually it doesn't)\n\n\nFor example, equating first and second moments to estimate the parameters of a gamma distribution or a uniform distribution; not maximum likelihood in either case.\n\n\n\n\nmethod of quantiles (equating sufficient sample and population quantiles to solve for parameter estimates; occasionally this is maximum likelihood but usually it isn't),\n\n\n\n\nminimizing some other measure of lack of fit than \n$-\\log\\mathcal{L}$\n (e.g. minimum chi-square, minimum K-S distance).\n\nmethod of moments\n (this involves equating enough sample and population moments to solve for parameter estimates; sometimes this turns out to be maximum likelihood but usually it doesn't)\n\nFor example, equating first and second moments to estimate the parameters of a gamma distribution or a uniform distribution; not maximum likelihood in either case.\n\nmethod of quantiles (equating sufficient sample and population quantiles to solve for parameter estimates; occasionally this is maximum likelihood but usually it isn't),\n\nminimizing some other measure of lack of fit than \n$-\\log\\mathcal{L}$\n (e.g. minimum chi-square, minimum K-S distance).\n\nWith fitting linear regression type models, you could for example look at robust regression (some of which do correspond to ML methods for some particular error distribution but many of which do not).\n\nIn the case of \nsimple\n linear regression, I show an example of two methods of fitting lines that are not maximum likelihood \nhere\n - there estimating slope by setting to 0 some other measure of correlation (i.e. other than the usual Pearson) between residuals and the predictor.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/442429/402101 \n With fitting linear regression type models, you could for example look at robust regression (some of which do correspond to ML methods for some particular error distribution but many of which do not).\n\nIn the case of \nsimple\n linear regression, I show an example of two methods of fitting lines that are not maximum likelihood \nhere\n - there estimating slope by setting to 0 some other measure of correlation (i.e. other than the usual Pearson) between residuals and the predictor.\n\nAnother example would be the Tukey's resistant line/Tukey's three group line (e.g. see \n?line\n in R). There are many other possibilities, though many of them don't generalize readily to the multiple regression situation.\n\n?line",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/307/402101 \n A saturated model is one in which there are as many estimated parameters as data points. By definition, this will lead to a perfect fit, but will be of little use statistically, as you have no data left to estimate variance.\n\nFor example, if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7536/402101 \n Some caveats before to proceed. As I often suggest to my students, use \nauto.arima()\n things only as a first approximation to your final result or if you want to have parsimonious model when you check that your rival theory-based model do better.\n\nauto.arima()\n\nData\n\nYou have clearly to start from the description of time series data you are working with. In macro-econometrics you usually work with aggregated data, and geometric means (surprisingly) have more empirical evidence for macro time series data, probably because most of them decomposable into \nexponentially growing trend\n.\n\nBy the way Rob's suggestion \"visually\" works for time series with \nclear seasonal part\n, as slowly varying annual data is less clear for the increases in variation. Luckily exponentially growing trend is usually seen (if it seems to be linear, than no need for logs).\n\nModel\n\nIf your analysis is based on some theory that states that some \nweighted geometric mean\n $Y(t) = X_1^{\\alpha_1}(t)...X_k^{\\alpha_k}(t)\\varepsilon(t)$ more known as the \nmultiplicative regression model\n is the one you have to work with. Then you usually move to a \nlog-log regression model\n, that is linear in parameters and most of your variables, but some growth rates, are transformed.\n\nIn financial econometrics logs are a common thing due to the popularity of log-returns, because...\n\nLog transformations have nice properties\n\nIn log-log regression model it is the interpretation of estimated parameter, say $\\alpha_i$ as the \nelasticity\n of $Y(t)$ on $X_i(t)$.\n\nIn error-correction models we have an empirically stronger assumption that \nproportions are more stable\n (\nstationary\n) than the absolute differences.\n\nIn financial econometrics it is \neasy to aggregate the log-returns over time\n.\n\nThere are many other reasons not mentioned here.\n\nFinally\n\nNote that log-transformation is usually applied to non-negative (level) variables. If you observe the differences of two time series (net export, for instance) it is not even possible to take the log, you have either to search for original data in levels or assume the form of common trend that was subtracted.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7536/402101 \n In financial econometrics it is \neasy to aggregate the log-returns over time\n.\n\nThere are many other reasons not mentioned here.\n\nFinally\n\nNote that log-transformation is usually applied to non-negative (level) variables. If you observe the differences of two time series (net export, for instance) it is not even possible to take the log, you have either to search for original data in levels or assume the form of common trend that was subtracted.\n\n[\naddition after edit\n] If you still want a \nstatistical criterion\n for when to do log transformation a simple solution would be any test for heteroscedasticity. In the case of increasing variance I would recommend  \nGoldfeld-Quandt Test\n or similar to it. In R it is located in \nlibrary(lmtest)\n and is denoted by \ngqtest(y~1)\n function. Simply regress on intercept term if you don't have any regression model, \ny\n is your dependent variable.\n\nlibrary(lmtest)\n\ngqtest(y~1)\n\ny",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/525342/402101 \n In general, the \njoint distribution\n of random variables \n$X$\n and \n$Y$\n, defined on a common probability space \n$(\\Omega, \\mathcal{A}, \\mathbb{P})$\n and taking values in measurable spaces \n$(\\mathcal{X}, \\mathcal{B})$\n and \n$(\\mathcal{Y}, \\mathcal{C})$\n, respectively, is the probability measure defined on \n$(\\mathcal{X} \\times \\mathcal{Y}, \\mathcal{B} \\otimes \\mathcal{C})$\n by\n\n$$\n\\mathbb{P}_{X, Y}(E) = \\mathbb{P}((X, Y) \\in E)\n$$\n\nfor all \n$E \\in \\mathcal{B} \\otimes \\mathcal{C}$\n.\n\nThis is the same as the ordinary distribution of \n$(X, Y) : \\Omega \\to \\mathcal{X} \\times \\mathcal{Y}$\n when viewed as a single random variable defined on \n$\\Omega$\n.\n\nAlso, \n$X$\n and \n$Y$\n are said to be \nindependent\n if it holds that\n\n$$\n\\mathbb{P}(X \\in B, Y \\in C) = \\mathbb{P}(X \\in B) \\mathbb{P}(Y \\in C)\n$$\n\nfor all \n$B \\in \\mathcal{B}$\n and \n$C \\in \\mathcal{C}$\n.\n\nThe independence condition can be rephrased in terms of the joint distribution of \n$X$\n and \n$Y$\n: \n$X$\n and \n$Y$\n are independent if and only if\n\n$$\n\\mathbb{P}_{X,Y}(B \\times C) = \\mathbb{P}_X(B) \\mathbb{P}_Y(C)\n$$\n\nfor all \n$B \\in \\mathcal{B}$\n and \n$C \\in \\mathcal{C}$\n. That is, if and only if\n\n$$\n\\mathbb{P}_{X, Y} = \\mathbb{P}_X \\otimes \\mathbb{P}_Y.\n$$\n\nThus, the joint distribution of \n$X$\n and \n$Y$\n is the product measure of the (marginal) distributions of \n$X$\n and \n$Y$\n precisely in the case that \n$X$\n and \n$Y$\n are independent.\nIf \n$X$\n and \n$Y$\n are dependent, then their joint distribution is not the product measure of the marginal distributions.\n\nIf \n$X$\n and \n$Y$\n, as above, are independent and \n$f : \\mathcal{X} \\times \\mathcal{Y} \\to \\mathbb{R}$\n is a measurable function (satisfying either non-negativity or integrability with respect to \n$\\mathbb{P}_{X,Y}$\n), then Fubini's theorem allows you to compute\n\n$$\n\\begin{aligned}\n\\mathbb{E}[f(X, Y)]\n&= \\int_\\Omega f(X(\\omega), Y(\\omega)) \\, \\mathbb{P}(d\\omega) &&\n\\text{(def. of expectation)}\n\\\\\n&= \\int_{\\mathcal{X} \\times \\mathcal{Y}} f(x, y) \\, \\mathbb{P}_{X, Y}(d(x, y)) &&\n\\text{(change of variables)}\n\\\\\n&= \\int_{\\mathcal{X} \\times \\mathcal{Y}} f(x, y) \\, \\mathbb{P}_X\\otimes\\mathbb{P}_Y(d(x, y)) &&\\text{(independence)}\n\\\\\n&= \\int_{\\mathcal{Y}} \\left(\\int_{\\mathcal{X}} f(x, y) \\, \\mathbb{P}_X(dx)\\right) \\, \\mathbb{P}_Y(dy) &&\\text{(Fubini's theorem)}\n\\end{aligned}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/525342/402101 \n However, if \n$X$\n are \n$Y$\n are not independent, then this argument won't work. Instead, if you want to break an expectation of \n$f(X,Y)$\n into an integral over \n$\\mathcal{X}$\n followed by an integral over \n$\\mathcal{Y}$\n, as we did above, you need to know something about the \nconditional distribution\n of \n$X$\n given \n$Y$\n.\n\nFor what follows, suppose \n$(\\mathcal{X},\\mathcal{B})$\n and \n$(\\mathcal{Y}, \\mathcal{C})$\n are \"sufficiently nice\" measurable spaces, meaning that they admit conditional distributions (this will happen for most spaces in practice; a sufficient condition is being standard Borel).\n\nThen if \n$\\mathbb{P}_{X\\mid Y} : \\mathcal{B} \\times \\mathcal{Y} \\to [0, 1]$\n is a version of the conditional distribution of \n$X$\n given \n$Y$\n, then we can proceed similarly to the calculations above:\n\n$$\n\\begin{aligned}\n\\mathbb{E}[f(X, Y)]\n&= \\int_{\\mathcal{X} \\times \\mathcal{Y}} f(x, y) \\, \\mathbb{P}_{X, Y}(d(x, y))\n\\\\\n&= \\int_{\\mathcal{Y}} \\left(\\int_{\\mathcal{X}} f(x, y) \\, \\mathbb{P}_{X\\mid Y}(dx, y)\\right) \\, \\mathbb{P}_Y(dy) \\\\\n&= \\mathbb{E}[\\mathbb{E}[f(X, Y) \\mid Y]]\n\\end{aligned}\n$$\n\n(in fact, the formula \n$\\mathbb{E}[f(X, Y)] = \\mathbb{E}[\\mathbb{E}[f(X, Y) \\mid Y]]$\n holds even without considering conditional distributions (\nproof\n), but it's arguably harder to compute in that case).\n\nIf \n$X$\n and \n$Y$\n \nare\n independent, then it happens that \n$\\mathbb{P}_{X\\mid Y}(B, y) = \\mathbb{P}_X(B)$\n for every \n$B \\in \\mathcal{B}$\n and \n$\\mathbb{P}_Y$\n-almost every \n$y \\in Y$\n.\nIn this case, the calculation reduces to the first computation above.\n\nIn practice, the conditional distribution \n$\\mathbb{P}_{X\\mid Y}$\n will usually be given by a conditional density \n$p_{X\\mid Y} : \\mathcal{X} \\times \\mathcal{Y} \\to [0, \\infty)$\n of \n$X$\n given \n$Y$\n with respect to some dominating measure \n$\\mu$\n on \n$(\\mathcal{X}, \\mathcal{B})$\n, yielding\n\n$$\nE[f(X, Y)]\n= \\int_{\\mathcal{Y}} \\left(\\int_{\\mathcal{X}} f(x, y) p_{X \\mid Y}(x, y) \\, \\mu(dx)\\right) \\, \\mathbb{P}_Y(d y).\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/85908/402101 \n Traditionally, the \nnull hypothesis\n is a point value.  (It is typically $0$, but can in fact be any point value.)  The alternative hypothesis is that the true value is \nany value other than the null value\n.  Because a continuous variable (such as a mean difference) can take on a value which is indefinitely close to the null value but still not quite equal and thus make the null hypothesis false, a traditional point null hypothesis cannot be proven.\n\nImagine your null hypothesis is $0$, and the mean difference you observe is $0.01$.  Is it reasonable to assume the null hypothesis is true?  You don't know yet; it would be helpful to know what our \nconfidence interval\n looks like.  Let's say that your 95% confidence interval is $(-4.99,\\ 5.01)$.  Now, should we conclude that the true value is $0$?  I would not feel comfortable saying that, because the CI is very wide, and there are many, large non-zero values that we might reasonably suspect are consistent with our data.  So let's say we gather much, much more data, and now our observed mean difference is $0.01$, but the 95% CI is $(0.005,\\ 0.015)$.  The observed mean difference has stayed the same (which would be amazing if it really happened), but the confidence interval now excludes the null value.  Of course, this is just a thought experiment, but it should make the basic ideas clear.  We can never prove that the true value is any particular point value; we can only (possibly) disprove that it is some point value.  In statistical hypothesis testing, the fact that the p-value is > 0.05 (and that the 95% CI includes zero) means that \nwe are not sure if the null hypothesis is true\n.\n\nAs for your concrete case, you cannot construct a test where the alternative hypothesis is that the mean difference is $0$ and the null hypothesis is anything other than zero.  This violates the logic of hypothesis testing.  It is perfectly reasonable that it is your substantive, scientific hypothesis, but it cannot be your alternative hypothesis in a hypothesis testing situation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/85908/402101 \n As for your concrete case, you cannot construct a test where the alternative hypothesis is that the mean difference is $0$ and the null hypothesis is anything other than zero.  This violates the logic of hypothesis testing.  It is perfectly reasonable that it is your substantive, scientific hypothesis, but it cannot be your alternative hypothesis in a hypothesis testing situation.\n\nSo what can you do?  In this situation, you use equivalence testing.  (You might want to read through some of our threads on this topic by clicking on the \nequivalence\n tag.)  The typical strategy is to use the two one sided tests approach.  Very briefly, you select an interval within which you would consider that the true mean difference might as well be $0$ for all you could care, then you perform a one-sided test to determine if the observed value is less than the upper bound of that interval, and another one-sided test to see if it is greater than the lower bound.  If both of these tests are significant, then you have rejected the hypothesis that the true value is outside the interval you care about.  If one (or both) are non-significant, you fail to reject the hypothesis that the true value is outside the interval.\n\nFor example, suppose anything within the interval $(-0.02,\\ 0.02)$ is so close to zero that you think it is essentially the same as zero for your purposes, so you use that as your substantive hypothesis.  Now imagine that you get the first result described above.  Although $0.01$ falls within that interval, you would not be able to reject the null hypothesis on either one-sided t-test, so you would fail to reject the null hypothesis.  On the other hand, imagine that you got the second result described above.  Now you find that the observed value falls within the designated interval, and it can be shown to be both less than the upper bound and greater than the lower bound, so you can reject the null.  (It is worth noting that you can reject \nboth\n the hypothesis that the true value is $0$, \nand\n the hypothesis that the true value lies outside of the interval $(-0.02,\\ 0.02)$, which may seem perplexing at first, but is fully consistent with the logic of hypothesis testing.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/36052/402101 \n In a word, \nyes\n. I believe there are still clear situations where sampling is appropriate, within and without the \"big data\" world, but the nature of big data will certainly change our approach to sampling, and we will use more datasets that are nearly complete representations of the underlying population.\n\nOn sampling:\n Depending on the circumstances it will almost always be clear if sampling is an appropriate thing to do. Sampling is not an inherently beneficial activity; it is just what we do because we need to make tradeoffs on the cost of implementing data collection. We are trying to characterize populations and need to select the appropriate method for gathering and analyzing data about the population. Sampling makes sense when the marginal cost of a method of data collection or data processing is high. Trying to reach 100% of the population is not a good use of resources in that case, because you are often better off addressing things like non-response bias than making tiny improvements in the random sampling error.\n\nHow is big data different?\n \"Big data\" addresses many of the same questions we've had for ages, but what's \"new\" is that the data collection happens off an existing, computer-mediated process, so the marginal cost of collecting data is essentially zero. This dramatically reduces our need for sampling.\n\nWhen will we still use sampling?\n If your \"big data\" population is the right population for the problem, then you will only employ sampling in a few cases: the need to run separate experimental groups, or if the sheer volume of data is too large to capture and process (many of us can handle millions of rows of data with ease nowadays, so the boundary here is getting further and further out). If it seems like I'm dismissing your question, it's probably because I've rarely encountered situations where the volume of the data was a concern in either the collection or processing stages, although I know many have",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/36052/402101 \n The situation that seems hard to me is when your \"big data\" population doesn't perfectly represent your target population, so the tradeoffs are more apples to oranges. Say you are a regional transportation planner, and Google has offered to give you access to its Android GPS navigation logs to help you. While the dataset would no doubt be interesting to use, the population would probably be systematically biased against the low-income, the public-transportation users, and the elderly. In such a situation, traditional travel diaries sent to a random household sample, although costlier and smaller in number, could still be the superior method of data collection. But, this is not simply a question of \"sampling vs. big data\", it's a question of which population combined with the relevant data collection and analysis methods you can apply to that population will best meet your needs.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/19334/402101 \n As others have stated, you need to have a common frequency of measurement (i.e. the time between observations). With that in place I would identify a common model that would reasonably describe each series separately. This might be an ARIMA model or a multiply-trended  Regression Model with possible Level Shifts or a composite model integrating both memory (ARIMA) and dummy variables. This common model could be estimated globally and separately for each of the two series and then one could construct an F test to test the hypothesis of a common set of parameters.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78845/402101 \n Here is some preliminary list of disadvantages I was able to extract from your comments. Criticism and additions are very welcome!\n\nOverall - compared to ARIMA, state-space models allow you to model more complex processes, have interpretable structure and easily handle data irregularities; but for this you pay with increased complexity of a model, harder calibration, less community knowledge.\n\nARIMA is a universal approximator - you don't care what is the true model behind your data and you use universal ARIMA diagnostic and fitting tools to \napproximate\n this model. It is like a polynomial curve fitting - you don't care what is the true function, you always can approximate it with a polynomial of some degree. \n\n\nState-space models naturally require you to write-down \nsome\n reasonable model for your process (which is good - you use your prior knowledge of your process to improve estimates). Of course, if you don't have any idea of your process, you always can use some \nuniversal\n state-space model also - e.g. represent ARIMA in a state-space form. But then ARIMA in its original form has more parsimonious formulation - without introducing unnecessary hidden states.\n\n\nBecause there is such a great variety of state-space models formulations (much richer than class of ARIMA models), behavior of all these potential models is not well studied and if the model you formulated is complicated - it's hard to say how it will behave under different circumstances. Of course, if your state-space model is simple or composed of interpretable components, there is no such problem. But ARIMA is always the same well studied ARIMA so it should be easier to anticipate its behavior even if you use it to approximate some complex process.\n\n\nBecause state-space allows you directly and exactly model complex/nonlinear models, then for these complex/nonlinear models you may have problems with stability of filtering/prediction (EKF/UKF divergence, particle filter degradation). You may also have problems with calibrating complicated-model's parameters - it's a computationally-hard optimization problem. ARIMA is simple, has less parameters (1 noise source instead of 2 noise sources, no hidden variables) so its calibration is simpler. \n\n\nFor state-space there is less community knowledge and software in statistical community than for ARIMA.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/31241/402101 \n We'll start with two definitions:\n\nA \nprobability density function (pdf)\n is a non-negative function that integrates to $1$. \n\n\nThe likelihood is defined as the joint density of the observed data as a function of the parameter. But, as pointed out by the reference to Lehmann made by @whuber in a comment below, \nthe likelihood function is a function of the parameter only, with the data held as a fixed constant.\n So the fact that it is a density as a function of the data is irrelevant.\n\nA \nprobability density function (pdf)\n is a non-negative function that integrates to $1$.\n\nThe likelihood is defined as the joint density of the observed data as a function of the parameter. But, as pointed out by the reference to Lehmann made by @whuber in a comment below, \nthe likelihood function is a function of the parameter only, with the data held as a fixed constant.\n So the fact that it is a density as a function of the data is irrelevant.\n\nTherefore, the likelihood function is not a pdf because \nits integral with respect to the parameter does not necessarily equal 1\n (and may not be integrable at all, actually, as pointed out by another comment from @whuber).\n\nTo see this, we'll use a simple example. Suppose you have a single observation, $x$, from a ${\\rm Bernoulli}(\\theta)$ distribution. Then the likelihood function is\n\n$$ L(\\theta) = \\theta^{x} (1 - \\theta)^{1-x} $$\n\nIt is a fact that $\\int_{0}^{1} L(\\theta) d \\theta = 1/2$. Specifically, if $x = 1$, then $L(\\theta) = \\theta$, so $$\\int_{0}^{1} L(\\theta) d \\theta = \\int_{0}^{1} \\theta \\  d \\theta = 1/2$$\n\nand a similar calculation applies when $x = 0$. Therefore, $L(\\theta)$ cannot be a density function.\n\nPerhaps even more important than this technical example showing why the likelihood isn't a probability density is to point out that \nthe likelihood is \nnot\n the probability of the parameter value being correct\n or anything like that - \nit is the probability (density) \nof the data given the parameter value\n, which is a completely different thing. Therefore one should not expect the likelihood function to behave like a probability density.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/187492/402101 \n It means \"Independent and identically distributed\".\n\nA good example is a succession of throws of a fair coin: The coin has no memory, so all the throws are \"independent\".\n\nAnd every throw is 50:50 (heads:tails), so the coin is and stays fair - the distribution from which every throw is drawn, so to speak, is and stays the same: \"identically distributed\".\n\nA good starting point would be the \nWikipedia page\n.\n\n::EDIT::\n\nFollow this \nlink\n to further explore the concept.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/164546/402101 \n It suffices to modify the loss function by adding the penalty. In matrix terms, the initial quadratic loss function becomes\n$$ (Y - X\\beta)^{T}(Y-X\\beta) + \\lambda \\beta^T\\beta.$$\nDeriving with respect to $\\beta$ leads to the normal equation\n$$ X^{T}Y = \\left(X^{T}X + \\lambda I\\right)\\beta $$\nwhich leads to the Ridge estimator.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214514/402101 \n Every probability distribution on (a subset of) $\\mathbb R^n$ has a \ncumulative distribution function\n, and it uniquely defines the distribution.  So, in this sense, the CDF is indeed as fundamental as the distribution itself.\n\nA \nprobability density function\n, however, exists only for \n(absolutely) continuous probability distributions\n.  The simplest example of a distribution lacking a PDF is any \ndiscrete probability distribution\n, such as the distribution of a random variable that only takes integer values.\n\nOf course, such discrete probability distributions can be characterized by a \nprobability mass function\n instead, but there are also distributions that have \nneither\n and PDF or a PMF, such as any mixture of a continuous and a discrete distribution:\n\n(Diagram shamelessly stolen from \nGlen_b's answer\n to a related question.)\n\nThere are even \nsingular probability distributions\n, such as the \nCantor distribution\n, which cannot be described even by a \ncombination\n of a PDF and a PMF.  Such distributions still have a well defined CDF, though.  For example, here is the CDF of the Cantor distribution, also sometimes called the \"Devil's staircase\":\n\n(\nImage\n from \nWikimedia Commons\n by users \nTheon\n and \nAmirki\n, used under the \nCC-By-SA 3.0\n license.)\n\nThe CDF, known as the \nCantor function\n, is continuous but not absolutely continuous.  In fact, it is constant everywhere except on a \nCantor set\n of zero Lebesgue measure, but which still contains infinitely many points.  Thus, the entire probability mass of the Cantor distribution is concentrated on this vanishingly small subset of the real number line, but every point in the set still individually has zero probability.\n\nThere are also probability distributions that do not have a \nmoment-generating function\n.  Probably the best known example is the \nCauchy distribution\n, a \nfat-tailed distribution\n which has no well-defined moments of order 1 or higher (thus, in particular, having no well-defined mean or variance!).\n\nAll probability distributions on $\\mathbb R^n$ do, however, have a (possibly complex-valued) \ncharacteristic function\n), whose definition differs from that of the MGF only by a multiplication with the \nimaginary unit\n.  Thus, the characteristic function may be regarded as being as fundamental as the CDF.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/63885/402101 \n A concise introduction is \nT. Schmidt 2008 - Copulas and dependent measurement\n.\nAlso noteworthy is \nEmbrechts 2009 - Copulas - A personal view\n.\n\nFor Schmidt I could not provide a better summary than the section titles. It provides basic definitions, intuition and examples. Discussion of sampling is bare-bone, and a brief literature review covers the must-have. As for Embrechts apart from the obligatory definitions, properties and examples the discussion is interesting since it touches drawbacks and some critical remarks made to copula modeling over the years. The bibliography is here more extensive and covers most works that one shall read",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2659/402101 \n The answer depends on whether you are dealing with discrete or continuous random variables. So, I will split my answer accordingly. I will assume that you want some technical details and not necessarily an explanation in plain English.\n\nDiscrete Random Variables\n\nSuppose that you have a stochastic process that takes discrete values (e.g., outcomes of tossing a coin 10 times, number of customers who arrive at a store in 10 minutes etc). In such cases, we can calculate the probability of observing a particular set of outcomes by making suitable assumptions about the underlying stochastic process (e.g., probability of coin landing heads is \n$p$\n and that coin tosses are independent).\n\nDenote the observed outcomes by \n$O$\n and the set of parameters that describe the stochastic process as \n$\\theta$\n. Thus, when we speak of probability we want to calculate \n$P(O|\\theta)$\n. In other words, given specific values for \n$\\theta$\n, \n$P(O|\\theta)$\n is the probability that we would observe the outcomes represented by \n$O$\n.\n\nHowever, when we model a real life stochastic process, we often do not know \n$\\theta$\n. We simply observe \n$O$\n and the goal then is to arrive at an estimate for \n$\\theta$\n that would be a plausible choice given the observed outcomes \n$O$\n. We know that given a value of \n$\\theta$\n the probability of observing \n$O$\n is \n$P(O|\\theta)$\n. Thus, a 'natural' estimation process is to choose that value of \n$\\theta$\n that would maximize the probability that we would actually observe \n$O$\n. In other words, we find the parameter values \n$\\theta$\n that maximize the following function:\n\n$L(\\theta|O) = P(O|\\theta)$\n\n$L(\\theta|O)$\n is called the likelihood function. Notice that by definition the likelihood function is conditioned on the observed \n$O$\n and that it is a function of the unknown parameters \n$\\theta$\n.\n\nContinuous Random Variables\n\nIn the continuous case the situation is similar with one important difference. We can no longer talk about the probability that we observed \n$O$\n given \n$\\theta$\n because in the continuous case \n$P(O|\\theta) = 0$\n. Without getting into technicalities, the basic idea is as follows:\n\nDenote the probability density function (pdf) associated with the outcomes \n$O$\n as: \n$f(O|\\theta)$\n. Thus, in the continuous case we estimate \n$\\theta$\n given observed outcomes \n$O$\n by maximizing the following function:\n\n$L(\\theta|O) = f(O|\\theta)$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2659/402101 \n Denote the probability density function (pdf) associated with the outcomes \n$O$\n as: \n$f(O|\\theta)$\n. Thus, in the continuous case we estimate \n$\\theta$\n given observed outcomes \n$O$\n by maximizing the following function:\n\n$L(\\theta|O) = f(O|\\theta)$\n\nIn this situation, we cannot technically assert that we are finding the parameter value that maximizes the probability that we observe \n$O$\n as we maximize the PDF associated with the observed outcomes \n$O$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/339349/402101 \n Another way to think about this is that the statistical experiment is the protocol we follow to generate data and the statistical model is the protocol we use to analyze these data.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/159379/402101 \n Consider the vector \n$\\vec{x}=(1,\\varepsilon)\\in\\mathbb{R}^2$\n where \n$\\varepsilon>0$\n is small. The \n$l_1$\n and \n$l_2$\n norms of \n$\\vec{x}$\n, respectively, are given by\n\n$$\\Vert \\vec{x}\\Vert_1 = 1+\\varepsilon,\\ \\ \\Vert\\vec{x}\\Vert_2^2 = 1+\\varepsilon^2$$\n\nNow say that, as part of some regularization procedure, we are going to reduce the magnitude of one of the elements of \n$\\vec{x}$\n by \n$\\delta\\leq\\varepsilon$\n. If we change \n$x_1$\n to \n$1-\\delta$\n, the resulting norms are\n\n$$\\Vert\\vec{x}-(\\delta,0)\\Vert_1 = 1-\\delta+\\varepsilon,\\ \\ \\Vert\\vec{x}-(\\delta,0)\\Vert_2^2 = 1-2\\delta+\\delta^2+\\varepsilon^2$$\n\nOn the other hand, reducing \n$x_2$\n by \n$\\delta$\n gives norms\n\n$$\\Vert\\vec{x}-(0,\\delta)\\Vert_1 = 1-\\delta+\\varepsilon,\\ \\ \\Vert\\vec{x}-(0,\\delta)\\Vert_2^2 = 1-2\\varepsilon\\delta+\\delta^2+\\varepsilon^2$$\n\nThe thing to notice here is that, for an \n$l_2$\n penalty, regularizing the larger term \n$x_1$\n results in a much greater reduction in norm than doing so to the smaller term \n$x_2\\approx 0$\n. For the \n$l_1$\n penalty, however, the reduction is the same. Thus, when penalizing a model using the \n$l_2$\n norm, it is highly unlikely that anything will ever be set to zero, since the reduction in \n$l_2$\n norm going from \n$\\varepsilon$\n to \n$0$\n is almost nonexistent when \n$\\varepsilon$\n is small. On the other hand, the reduction in \n$l_1$\n norm is always equal to \n$\\delta$\n, regardless of the quantity being penalized.\n\nAnother way to think of it: it's not so much that \n$l_1$\n penalties encourage sparsity, but that \n$l_2$\n penalties in some sense \ndiscourage\n sparsity by yielding diminishing returns as elements are moved closer to zero.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197432/402101 \n I will try to add to the other answer. First, completeness is a technical condition which is justified mainly by the theorems that use it. So let us start with some related concepts and theorems where they occur.\n\nLet \n$X=(X_1,X_2,\\dotsc,X_n)$\n represent a vector of iid data, which we model as having a distribution \n$f(x;\\theta), \\theta \\in \\Theta$\n where the parameter \n$\\theta$\n governing the data is unknown. \n$T=T(X)$\n is \nsufficient\n if the conditional distribution of \n$X \\mid T$\n does not depend on the parameter \n$\\theta$\n.  \n$V=V(X)$\n is \nancillary\n if the distribution of \n$V$\n does not depend on \n$\\theta$\n (within the family \n$f(x;\\theta)$\n). \n$U=U(X)$\n is an \nunbiased estimator of zero\n if its expectation is zero, irrespective of \n$\\theta$\n. \n$S=S(X)$\n is a \ncomplete statistic\n if any unbiased estimator of zero based on \n$S$\n is identically zero, that is, if \n$\\DeclareMathOperator{\\E}{\\mathbb{E}} \\E g(S)=0 (\\text{for all $\\theta$})$\n then \n$g(S)=0$\n a.e. (for all \n$\\theta$\n).\n\nNow, suppose you have two different unbiased estimators of \n$\\theta$\n based on the sufficient statistic \n$T$\n, \n$g_1(T), g_2(T)$\n. That is, in symbols\n\n$$\n    \\E g_1(T)=\\theta ,\\\\\n     \\E g_2(T)=\\theta\n$$\n\nand \n$\\DeclareMathOperator{\\P}{\\mathbb{P}} \\P(g_1(T) \\not= g_2(T) ) > 0$\n (for all \n$\\theta$\n). Then \n$g_1(T)-g_2(T)$\n is an unbiased estimator of zero, which is not identically zero, proving  that \n$T$\n is not complete. So, completeness of an sufficient statistic \n$T$\n gives us that there exists only one unique unbiased estimator of \n$\\theta$\n based on \n$T$\n. That is already very close to the Lehmann\u2013Scheff\u00e9 theorem.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197432/402101 \n $$\n    \\E g_1(T)=\\theta ,\\\\\n     \\E g_2(T)=\\theta\n$$\n\nand \n$\\DeclareMathOperator{\\P}{\\mathbb{P}} \\P(g_1(T) \\not= g_2(T) ) > 0$\n (for all \n$\\theta$\n). Then \n$g_1(T)-g_2(T)$\n is an unbiased estimator of zero, which is not identically zero, proving  that \n$T$\n is not complete. So, completeness of an sufficient statistic \n$T$\n gives us that there exists only one unique unbiased estimator of \n$\\theta$\n based on \n$T$\n. That is already very close to the Lehmann\u2013Scheff\u00e9 theorem.\n\nLet us look at some examples. Suppose \n$X_1, \\dotsc, X_n$\n now are iid uniform on the interval \n$(\\theta, \\theta+1)$\n. We can show that (\n$X_{(1)} < X_{(2)} < \\dotsm < X_{(n)}$\n are the order statistics) the pair \n$(X_{(1)}, X_{(n)})$\n is sufficient, but it is not complete, because the difference \n$X_{(n)}-X_{(1)}$\n is ancillary; we can compute its expectation, let it be \n$c$\n (which is a function of \n$n$\n only), and then \n$X_{(n)}-X_{(1)} -c$\n will be an unbiased estimator of zero which is not identically zero. So our sufficient statistic, in this case, is not complete and sufficient. And we can see what that means: there exist functions of the sufficient statistic which are not informative about \n$\\theta$\n (in the context of the model). This cannot happen with a complete sufficient statistic; it is in a sense maximally informative, in that no functions of it are uninformative. On the other hand, if there is some function of the minimally sufficient statistic that has expectation zero, that could be seen as a \nnoise term\n; disturbance/noise terms in models have expectation zero. So we could say that non-complete sufficient statistics \ndo contain some noise\n.\n\nLook again at the range \n$R=X_{(n)}-X_{(1)}$\n in this example. Since its distribution does not depend on \n$\\theta$\n, it doesn't \nby itself alone\n contain any information about \n$\\theta$\n. But, together with the sufficient statistic, it does! How? Look at the case where \n$R=1$\n is observed.Then, in the context of our (known to be true) model, we have perfect knowledge of \n$\\theta$\n! Namely, we can say with certainty that \n$\\theta = X_{(1)}$\n. You can check that any other value for \n$\\theta$\n then leads to either \n$X_{(1)}$\n or \n$X_{(n)}$\n being an impossible observation, under the assumed model. On the other hand, if we observe \n$R=0.1$\n, then the range of possible values for \n$\\theta$\n is rather large (exercise ...).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197432/402101 \n In this sense, the ancillary statistic \n$R$\n does contain some information about the precision with which we can estimate \n$\\theta$\n based on this data and model. In this example, and others, the ancillary statistic \n$R$\n \"takes over the role of the sample size\". Usually, confidence intervals and such need the sample size \n$n$\n, but in this example, we can make a \nconditional confidence interval\n this is computed using only \n$R$\n, not \n$n$\n (exercise.)\nThis was an idea of Fisher, that inference should be conditional on some ancillary statistic.\n\nNow, Basu's theorem: If \n$T$\n is complete sufficient, then it is independent of any ancillary statistic. That is, inference based on a complete sufficient statistic is simpler, in that we do not need to consider conditional inference.\nConditioning on a statistic which is independent of \n$T$\n does not change anything, of course.\n\nThen, a last example to give some more intuition. Change our uniform distribution example to a uniform distribution on the interval \n$(\\theta_1, \\theta_2)$\n (with \n$\\theta_1<\\theta_2$\n).  In this case the statistic \n$(X_{(1)}, X_{(n)})$\n \nis\n complete and sufficient. What changed? We can see that completeness is really a property of the \nmodel\n. In the former case, we had a restricted parameter space. This restriction destroyed completeness by introducing relationships on the order statistics. By removing this restriction we got completeness! So, in a sense, lack of completeness means that the parameter space is not big enough, and by enlarging it we can hope to restore completeness (and thus, easier inference).\n\nSome other examples where lack of completeness is caused by restrictions on the parameter space,\n\nsee my answer to:  \nWhat kind of information is Fisher information?\n\n\n\n\nLet \n$X_1, \\dotsc, X_n$\n be iid \n$\\mathcal{Cauchy}(\\theta,\\sigma)$\n (a location-scale model). Then the order statistics are sufficient but not complete. But now enlarge this model to a fully nonparametric model, still iid but from some completely unspecified distribution \n$F$\n. Then the order statistics are sufficient and complete.\n\n\n\n\nFor exponential families with canonical parameter space (that is, as large as possible) the minimal sufficient statistic is also complete. But in many cases, introducing restrictions on the parameter space, as with \ncurved exponential families\n, destroys completeness.\n\nsee my answer to:  \nWhat kind of information is Fisher information?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/197432/402101 \n For exponential families with canonical parameter space (that is, as large as possible) the minimal sufficient statistic is also complete. But in many cases, introducing restrictions on the parameter space, as with \ncurved exponential families\n, destroys completeness.\n\nsee my answer to:  \nWhat kind of information is Fisher information?\n\nLet \n$X_1, \\dotsc, X_n$\n be iid \n$\\mathcal{Cauchy}(\\theta,\\sigma)$\n (a location-scale model). Then the order statistics are sufficient but not complete. But now enlarge this model to a fully nonparametric model, still iid but from some completely unspecified distribution \n$F$\n. Then the order statistics are sufficient and complete.\n\nFor exponential families with canonical parameter space (that is, as large as possible) the minimal sufficient statistic is also complete. But in many cases, introducing restrictions on the parameter space, as with \ncurved exponential families\n, destroys completeness.\n\nA very relevant paper is \nLehmann (1981), \nJ. Am. Stat. Assoc.\n, \n76\n, 374, \"An Interpretation of Completeness and\nBasu's Theorem\".",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/52855/402101 \n As mentioned by @Nick this is a consequence of \nWilks' theorem\n. But note that the test statistic is \nasymptotically\n $\\chi^2$-distributed, not $\\chi^2$-distributed.\n\nI am very impressed by this theorem because it holds in a very wide context. Consider a statistical model with likelihood $l(\\theta \\mid y)$ where $y$ is the vector observations of  $n$ independent replicated observations from a distribution with parameter $\\theta$ belonging to a submanifold $B_1$ of $\\mathbb{R}^d$ with dimension $\\dim(B_1)=s$. Let $B_0 \\subset B_1$ be a submanifold with dimension $\\dim(B_0)=m$. Imagine you are interested in testing $H_0\\colon\\{\\theta \\in B_0\\}$.\n\nThe \nlikelihood ratio\n is \n$$lr(y) = \\frac{\\sup_{\\theta \\in B_1}l(\\theta \\mid y)}{\\sup_{\\theta \\in B_0}l(\\theta \\mid y)}. $$\nDefine the \ndeviance\n $d(y)=2 \\log \\big(lr(y)\\big)$. Then \nWilks' theorem\n says that, under usual regularity assumptions, $d(y)$ is asymptotically $\\chi^2$-distributed with $s-m$ degrees of freedom when $H_0$ holds true.\n\nIt is proven in \nWilk's original paper\n mentioned by @Nick. I think this paper is not easy to read. Wilks published a book later, perhaps with an easiest presentation of his theorem. A short heuristic proof is given in \nWilliams' excellent book\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18062/402101 \n This is a fundamental results from linear algebra on orthogonal projections. A relatively simple approach is as follows. If $u_1, \\ldots, u_m$ are orthonormal vectors spanning an $m$-dimensional subspace $A$, and $\\mathbf{U}$ is the $n \\times p$ matrix with the $u_i$'s as the columns, then \n$$\\mathbf{P} = \\mathbf{U}\\mathbf{U}^T.$$\nThis follows directly from the fact that the orthogonal projection of $x$ onto $A$ can be computed in terms of the orthonormal basis of $A$ as\n$$\\sum_{i=1}^m u_i u_i^T x.$$\nIt follows directly from the formula above that $\\mathbf{P}^2 = \\mathbf{P}$ and that $\\mathbf{P}^T = \\mathbf{P}.$\n\nIt is also possible to give a different argument. If $\\mathbf{P}$ is a projection matrix for an orthogonal projection, then, by definition, for all $x,y \\in \\mathbb{R}^n$ \n$$\\mathbf{P}x \\perp y-\\mathbf{P}y.$$\nConsequently,\n\n$$0 = (\\mathbf{P} x)^T (y - \\mathbf{P}y) = x^T \\mathbf{P}^T (I - \\mathbf{P}) y = x^T (\\mathbf{P}^T - \\mathbf{P}^T \\mathbf{P}) y $$\nfor all $x, y \\in \\mathbb{R}^n$. This shows that $\\mathbf{P}^T = \\mathbf{P}^T \\mathbf{P}$, whence \n$$\\mathbf{P} = (\\mathbf{P}^T)^T = (\\mathbf{P}^T \\mathbf{P})^T = \\mathbf{P}^T \\mathbf{P} = \\mathbf{P}^T.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32329/402101 \n To complete the answer to the question, Ocram nicely addressed standard error but did not contrast it to standard deviation and did not mention the dependence on sample size.  As a special case for the estimator consider the sample mean.  The standard error for the mean is $\\sigma \\, / \\, \\sqrt{n}$ where $\\sigma$ is the population standard deviation.  So in this example we see explicitly how the standard error decreases with increasing sample size. The standard deviation is most often used to refer to the individual observations. So standard deviation describes the variability of the individual observations while standard error shows the variability of the estimator. Good estimators are consistent which means that they converge to the true parameter value. When their standard error decreases to 0 as the sample size increases the estimators are consistent which in most cases happens because the standard error goes to 0 as we see explicitly with the sample mean.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/37734/402101 \n In this case, you can consider an \nABC\n approximation of the likelihood (and consequently of the \nMLE\n) under the following assumption/restriction:\n\nAssumption.\n The original sample size \n$n$\n is known.\n\nThis is not a wild assumption given that the quality, in terms of convergence, of frequentist estimators depends on the sample size, therefore one cannot obtain arbitrarily good estimators without knowing the original sample size.\n\nThe idea is to generate a sample from the posterior distribution of \n$\\theta$\n and, \nin order to produce an approximation of the MLE\n, you can use an importance sampling technique as in \n[1]\n or to consider a uniform prior on \n$\\theta$\n with support on a suitable set as in \n[2]\n.\n\nI am going to describe the method in [2]. First of all, let me describe the ABC sampler.\n\nABC Sampler\n\nLet \n$f(\\cdot\\vert\\theta)$\n be the model that generates the sample where \n$\\theta \\in \\Theta$\n is a parameter (to be estimated), \n$T$\n be a statistic (a function of the sample) and \n$T_0$\n be the observed statistic, in the ABC jargon this is called a \nsummary statistic\n, \n$\\rho$\n be a metric, \n$\\pi(\\theta)$\n a prior distribution on \n$\\theta$\n and \n$\\epsilon>0$\n a tolerance. Then, the ABC-rejection sampler can be implemented as follows.\n\nSample \n$\\theta^*$\n from \n$\\pi(\\cdot)$\n.\n\n\nGenerate a sample \n$\\bf{x}$\n of size \n$n$\n from the model \n$f(\\cdot\\vert\\theta^*)$\n.\n\n\nCompute \n$T^*=T({\\bf x})$\n.\n\n\nIf \n$\\rho(T^*,T_0)<\\epsilon$\n, accept \n$\\theta^*$\n as a simulation from the posterior of \n$\\theta$\n.\n\nThis algorithm generates an approximate sample from the posterior distribution of \n$\\theta$\n given \n$T({\\bf x})=T_0$\n. Therefore, the best scenario is when the statistic \n$T$\n is sufficient but other statistics can be used. For a more detailed description of this see \nthis paper\n.\n\nNow, in a general framework, if one uses a uniform prior that contains the MLE in its support, then the Maximum \na posteriori\n (MAP) coincides with Maximum Likelihood Estimator (MLE). Therefore, if you consider an appropriate uniform prior in the ABC Sampler, then you can generate an approximate sample of a posterior distribution whose MAP coincides with the MLE. The remaining step consists of estimating this mode. This problem has been discussed in CV, for instance in \n\"Computationally efficient estimation of multivariate mode\"\n.\n\nA toy example",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/37734/402101 \n A toy example\n\nLet \n$(x_1,...,x_n)$\n be a sample from a \n$N(\\mu,1)$\n and suppose that the only information available from this sample is \n$\\bar{x}=\\dfrac{1}{n}\\sum_{j=1}^n x_j$\n. Let \n$\\rho$\n be the Euclidean metric in \n${\\mathbb R}$\n and \n$\\epsilon=0.001$\n. The following R code shows how to obtain an approximate MLE using the methods described above using a simulated sample with \n$n=100$\n and \n$\\mu=0$\n, a sample of the posterior distribution of size \n$1000$\n, a uniform prior for \n$\\mu$\n on \n$(-0.3,0.3)$\n, and a kernel density estimator for the estimation of the mode of the posterior sample (MAP=MLE).\n\n# rm(list=ls())\n\n# Simulated data\nset.seed(1)\nx = rnorm(100)\n\n# Observed statistic\nT0 = mean(x)\n\n# ABC Sampler using a uniform prior \n\nN=1000\neps = 0.001\nABCsamp = rep(0,N)\ni=1\n\nwhile(i < N+1){\n  u = runif(1,-0.3,0.3)\n  t.samp = rnorm(100,u,1)\n  Ts = mean(t.samp)\n  if(abs(Ts-T0)<eps){\n    ABCsamp[i]=u\n    i=i+1\n    print(i)\n  }\n}\n\n# Approximation of the MLE\nkd = density(ABCsamp)\nkd\n$x[which(kd$\ny==max(kd$y))]\n\n# rm(list=ls())\n\n# Simulated data\nset.seed(1)\nx = rnorm(100)\n\n# Observed statistic\nT0 = mean(x)\n\n# ABC Sampler using a uniform prior \n\nN=1000\neps = 0.001\nABCsamp = rep(0,N)\ni=1\n\nwhile(i < N+1){\n  u = runif(1,-0.3,0.3)\n  t.samp = rnorm(100,u,1)\n  Ts = mean(t.samp)\n  if(abs(Ts-T0)<eps){\n    ABCsamp[i]=u\n    i=i+1\n    print(i)\n  }\n}\n\n# Approximation of the MLE\nkd = density(ABCsamp)\nkd\n$x[which(kd$\ny==max(kd$y))]\n\nAs you can see, using a small tolerance we get a very good approximation of the MLE (which in this trivial example can be calculated from the statistic given that it is sufficient). It is important to notice that the choice of the summary statistic is crucial. Quantiles are typically a good choice for the summary statistic, but not all the choices produce a good approximation. It may be the case that the summary statistic is not very informative and then the quality of the approximation might be poor, which is well-known in the ABC community.\n\nUpdate:\n A similar approach was recently published in \nFan et al. (2012)\n. See \nthis entry\n for a discussion on the paper.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27301/402101 \n The basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (\nloadings\n). You may recall that PCA seeks to replace $p$ (more or less correlated) variables by $k<p$ uncorrelated linear combinations (projections) of the original variables. Let us ignore how to choose an optimal $k$ for the problem at hand. Those $k$ \nprincipal components\n are ranked by importance through their explained variance, and each variable contributes with varying degree to each component. Using the largest variance criteria would be akin to \nfeature extraction\n, where principal component are used as new features, instead of the original variables. However, we can decide to keep only the first component and select the $j<p$ variables that have the highest absolute coefficient; the number $j$ might be based on the proportion of the number of variables (e.g., keep only the top 10% of the $p$ variables), or a fixed cutoff (e.g., considering a threshold on the normalized coefficients). This approach bears some resemblance with the \nLasso\n operator in penalized regression (or \nPLS\n regression). Neither the value of $j$, nor the number of components to retain are obvious choices, though.\n\nThe problem with using PCA is that (1) measurements from all of the original variables are used in the projection to the lower dimensional space, (2) only linear relationships are considered, and (3) PCA or SVD-based methods, as well as univariate screening methods (t-test, correlation, etc.), do not take into account the potential multivariate nature of the data structure (e.g., higher order interaction between variables).\n\nAbout point 1, some more elaborate screening methods have been proposed, for example \nprincipal feature analysis\n or stepwise method, like the one used for '\ngene shaving\n' in gene expression studies. Also, \nsparse PCA\n might be used to perform dimension reduction and variable selection based on the resulting variable loadings. About point 2, it is possible to use kernel PCA (using the \nkernel trick\n) if one needs to embed nonlinear relationships into a lower dimensional space. \nDecision trees\n, or better the \nrandom forest\n algorithm, are probably better able to solve Point 3. The latter allows to derive Gini- or permutation-based measures of \nvariable importance\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27301/402101 \n A last point: If you intend to perform feature selection before applying a classification or regression model, be sure to cross-validate the whole process (see \u00a77.10.2 of the \nElements of Statistical Learning\n, or \nAmbroise and McLachlan, 2002\n).\n\nAs you seem to be interested in R solution, I would recommend taking a look at the \ncaret\n package which includes a lot of handy functions for data preprocessing and variable selection in a classification or regression context.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18059/402101 \n This is a fundamental results from linear algebra on orthogonal projections. A relatively simple approach is as follows. If $u_1, \\ldots, u_m$ are orthonormal vectors spanning an $m$-dimensional subspace $A$, and $\\mathbf{U}$ is the $n \\times p$ matrix with the $u_i$'s as the columns, then \n$$\\mathbf{P} = \\mathbf{U}\\mathbf{U}^T.$$\nThis follows directly from the fact that the orthogonal projection of $x$ onto $A$ can be computed in terms of the orthonormal basis of $A$ as\n$$\\sum_{i=1}^m u_i u_i^T x.$$\nIt follows directly from the formula above that $\\mathbf{P}^2 = \\mathbf{P}$ and that $\\mathbf{P}^T = \\mathbf{P}.$\n\nIt is also possible to give a different argument. If $\\mathbf{P}$ is a projection matrix for an orthogonal projection, then, by definition, for all $x,y \\in \\mathbb{R}^n$ \n$$\\mathbf{P}x \\perp y-\\mathbf{P}y.$$\nConsequently,\n\n$$0 = (\\mathbf{P} x)^T (y - \\mathbf{P}y) = x^T \\mathbf{P}^T (I - \\mathbf{P}) y = x^T (\\mathbf{P}^T - \\mathbf{P}^T \\mathbf{P}) y $$\nfor all $x, y \\in \\mathbb{R}^n$. This shows that $\\mathbf{P}^T = \\mathbf{P}^T \\mathbf{P}$, whence \n$$\\mathbf{P} = (\\mathbf{P}^T)^T = (\\mathbf{P}^T \\mathbf{P})^T = \\mathbf{P}^T \\mathbf{P} = \\mathbf{P}^T.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/35124/402101 \n The standard deviation is the square root of the variance.\n\nThe standard deviation is expressed in the same units as the mean is, whereas the variance is expressed in squared units, but for looking at a distribution, you can use either just so long as you are clear about what you are using. For example, a Normal distribution with mean = 10 and sd = 3 is exactly the same thing as a Normal distribution with mean = 10 and variance = 9.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3934/402101 \n The standard deviation calculated with a divisor of \n$n-1$\n is a standard deviation calculated from the sample as an estimate of the standard deviation of the population from which the sample was drawn. Because the observed values fall, on average, closer to the sample mean than to the population mean, the standard deviation which is calculated using deviations from the sample mean underestimates the desired standard deviation of the population. Using \n$n-1$\n instead of \n$n$\n as the divisor corrects for that by making the result a little bit bigger.\n\nNote that the correction has a larger proportional effect when \n$n$\n is small than when it is large, which is what we want because when n is larger the sample mean is likely to be a good estimator of the population mean.\n\nWhen the sample is the whole population we use the standard deviation with \n$n$\n as the divisor because the sample mean \nis\n population mean.\n\n(I note parenthetically that nothing that starts with \"second moment recentered around a known, definite mean\" is going to fulfil the questioner's request for an intuitive explanation.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/220324/402101 \n The question appears to ask for a demonstration that Ridge Regression shrinks coefficient estimates towards zero, using a spectral decomposition.  The spectral decomposition can be understood as an easy consequence of the \nSingular Value Decomposition\n (SVD).  Therefore, this post starts with SVD.  It explains it in simple terms and then illustrates it with important applications.  Then it provides the requested (algebraic) demonstration.  (The algebra, of course, is identical to the geometric demonstration; it merely is couched in a different language.)\n\n##What the SVD is\n\nAny \n$n\\times p$\n matrix \n$X$\n, with \n$p \\le n$\n, can be written \n$$X = UDV^\\prime$$\n where\n\n$U$\n is an \n$n\\times p$\n matrix.\n\n\n\n\nThe columns of \n$U$\n have length \n$1$\n.\n\n\nThe columns of \n$U$\n are mutually orthogonal.\n\n\nThey are called the \nprincipal components\n of \n$X$\n.\n\n\n\n\n\n\n$V$\n is a \n$p \\times p$\n matrix.\n\n\n\n\nThe columns of \n$V$\n have length \n$1$\n.\n\n\nThe columns of \n$V$\n are mutually orthogonal.\n\n\nThis makes \n$V$\n a \nrotation\n of \n$\\mathbb{R}^p$\n.\n\n\n\n\n\n\n$D$\n is a \ndiagonal\n \n$p \\times p$\n matrix.\n\n\n\n\nThe diagonal elements \n$d_{11}, d_{22}, \\ldots, d_{pp}$\n are not negative.  These are the \nsingular values\n of \n$X$\n.\n\n\nIf we wish, we may order them from largest to smallest.\n\n$U$\n is an \n$n\\times p$\n matrix.\n\nThe columns of \n$U$\n have length \n$1$\n.\n\n\nThe columns of \n$U$\n are mutually orthogonal.\n\n\nThey are called the \nprincipal components\n of \n$X$\n.\n\n$V$\n is a \n$p \\times p$\n matrix.\n\nThe columns of \n$V$\n have length \n$1$\n.\n\n\nThe columns of \n$V$\n are mutually orthogonal.\n\n\nThis makes \n$V$\n a \nrotation\n of \n$\\mathbb{R}^p$\n.\n\n$D$\n is a \ndiagonal\n \n$p \\times p$\n matrix.\n\nThe diagonal elements \n$d_{11}, d_{22}, \\ldots, d_{pp}$\n are not negative.  These are the \nsingular values\n of \n$X$\n.\n\n\nIf we wish, we may order them from largest to smallest.\n\nCriteria (1) and (2) assert that both \n$U$\n and \n$V$\n are \northonormal\n matrices.  They can be neatly summarized by the conditions\n\n$$U^\\prime U = 1_p,\\ V^\\prime V = 1_p.$$\n\nAs a consequence (that \n$V$\n represents a rotation), \n$VV^\\prime = 1_p$\n also.  This will be used in the Ridge Regression derivation below.\n\n##What it does for us\n\nIt can simplify formulas.\n  This works both algebraically and conceptually.  Here are some examples.\n\n###The Normal Equations",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/220324/402101 \n Criteria (1) and (2) assert that both \n$U$\n and \n$V$\n are \northonormal\n matrices.  They can be neatly summarized by the conditions\n\n$$U^\\prime U = 1_p,\\ V^\\prime V = 1_p.$$\n\nAs a consequence (that \n$V$\n represents a rotation), \n$VV^\\prime = 1_p$\n also.  This will be used in the Ridge Regression derivation below.\n\n##What it does for us\n\nIt can simplify formulas.\n  This works both algebraically and conceptually.  Here are some examples.\n\n###The Normal Equations\n\nConsider the regression \n$y = X\\beta + \\varepsilon$\n where, as usual, the \n$\\varepsilon$\n are independent and identically distributed according to a law that has zero expectation and finite variance \n$\\sigma^2$\n.  The least squares solution via the Normal Equations is \n$$\\hat\\beta = (X^\\prime X)^{-1} X^\\prime y.$$\n  Applying the SVD and simplifying the resulting algebraic mess (which is easy) provides a nice insight:\n\n$$(X^\\prime X)^{-1} X^\\prime = ((UDV^\\prime)^\\prime (UDV^\\prime))^{-1} (UDV^\\prime)^\\prime \\\\= (VDU^\\prime U D V^\\prime)^{-1} (VDU^\\prime) = VD^{-2}V^\\prime VDU^\\prime = VD^{-1}U^\\prime.$$\n\nThe only difference between this and \n$X^\\prime = VDU^\\prime$\n is that the reciprocals of the elements of \n$D$\n are used!  In other words, the \"equation\" \n$y=X\\beta$\n is solved by \"inverting\" \n$X$\n: this pseudo-inversion undoes the rotations \n$U$\n and \n$V^\\prime$\n (merely by transposing them) and undoes the multiplication (represented by \n$D$\n) separately in each principal direction.\n\nFor future reference, notice that \"rotated\" estimates \n$V^\\prime \\hat\\beta $\n are linear combinations of \"rotated\" responses \n$U^\\prime y$\n.  The coefficients are inverses of the (positive) diagonal elements of \n$D$\n, equal to \n$d_{ii}^{-1}$\n.\n\n###Covariance of the coefficient estimates\n\nRecall that the covariance of the estimates is \n$$\\text{Cov}(\\hat\\beta) = \\sigma^2(X^\\prime X)^{-1}.$$\n  Using the SVD, this becomes \n$$\\sigma^2(V D^2 V^\\prime)^{-1} = \\sigma^2 V D^{-2} V^\\prime.$$\n  In other words, the covariance acts like that of \n$k$\n \northogonal\n variables, each with variances \n$d^2_{ii}$\n, that have been rotated in \n$\\mathbb{R}^k$\n.\n\n###The Hat matrix\n\nThe hat matrix is \n$$H = X(X^\\prime X)^{-1} X^\\prime.$$\n By means of the preceding result we may rewrite it as \n$$H = (UDV^\\prime)(VD^{-1}U^\\prime) = UU^\\prime.$$\n Simple!\n\n###Eigenanalysis (spectral decomposition)\n\nSince \n$$X^\\prime X = VDU^\\prime U D V^\\prime = VD^2V^\\prime$$\n and \n$$XX^\\prime = UDV^\\prime VDU^\\prime = UD^2U^\\prime,$$\n it is immediate that",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/220324/402101 \n ###The Hat matrix\n\nThe hat matrix is \n$$H = X(X^\\prime X)^{-1} X^\\prime.$$\n By means of the preceding result we may rewrite it as \n$$H = (UDV^\\prime)(VD^{-1}U^\\prime) = UU^\\prime.$$\n Simple!\n\n###Eigenanalysis (spectral decomposition)\n\nSince \n$$X^\\prime X = VDU^\\prime U D V^\\prime = VD^2V^\\prime$$\n and \n$$XX^\\prime = UDV^\\prime VDU^\\prime = UD^2U^\\prime,$$\n it is immediate that\n\nThe eigenvalues of \n$X^\\prime X$\n and \n$XX^\\prime$\n are the squares of the singular values.\n\n\nThe columns of \n$V$\n are the eigenvectors of \n$X^\\prime X$\n.\n\n\nThe columns of \n$U$\n are some of the eigenvectors of \n$X X^\\prime$\n.  (Other eigenvectors exist but correspond to zero eigenvalues.)\n\nSVD can diagnose and solve collinearity problems.\n\n###Approximating the regressors\n\nWhen you replace the smallest singular values with zeros, you will change the product \n$UDV^\\prime$\n only slightly.  Now, however, the zeros eliminate the corresponding columns of \n$U$\n, \neffectively reducing the number of variables.\n  Provided those eliminated columns have little correlation with \n$y$\n, \nthis can work effectively as a variable-reduction technique.\n\n##Ridge Regression\n\nLet the columns of \n$X$\n be standardized, as well as \n$y$\n itself.  (This means we no longer need a constant column in \n$X$\n.)  For \n$\\lambda \\gt 0$\n the ridge estimator is  \n$$\\begin{aligned}\\hat\\beta_R &= (X^\\prime X + \\lambda)^{-1}X^\\prime y \\\\\n&= (VD^2V^\\prime + \\lambda\\,1_p)^{-1}VDU^\\prime y \\\\\n&=  (VD^2V^\\prime + \\lambda V V^\\prime)^{-1}VDU^\\prime y \\\\\n&= (V(D^2 + \\lambda)V^\\prime)^{-1} VDU^\\prime y \\\\\n&= V(D^2+\\lambda)^{-1}V^\\prime V DU^\\prime y \\\\\n&= V(D^2 + \\lambda)^{-1} D U^\\prime y.\\end{aligned}$$\n\nThe difference between this and \n$\\hat\\beta$\n is the replacement of \n$D^{-1} = D^{-2}D$\n by \n$(D^2+\\lambda)^{-1}D$\n.\n In effect, this multiplies the original by the fraction \n$D^2/(D^2+\\lambda)$\n.  Because (when \n$\\lambda \\gt 0$\n) the denominator is obviously greater than the numerator, \nthe parameter estimates \"shrink towards zero.\"",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/220324/402101 \n The difference between this and \n$\\hat\\beta$\n is the replacement of \n$D^{-1} = D^{-2}D$\n by \n$(D^2+\\lambda)^{-1}D$\n.\n In effect, this multiplies the original by the fraction \n$D^2/(D^2+\\lambda)$\n.  Because (when \n$\\lambda \\gt 0$\n) the denominator is obviously greater than the numerator, \nthe parameter estimates \"shrink towards zero.\"\n\nThis result has to be understood in the somewhat subtle sense alluded to previously: the \nrotated\n estimates \n$V^\\prime\\hat\\beta_R$\n are still linear combinations of the vectors \n$U^\\prime y$\n, but each coefficient--which used to be \n$d_{ii}^{-1}$\n--has been multiplied by a factor of \n$d_{ii}^2/(d_{ii}^2 + \\lambda)$\n.  As such, the \nrotated\n coefficients must shrink, but it is possible, when \n$\\lambda$\n is sufficiently small, for some of the \n$\\hat\\beta_R$\n themselves actually to increase in size.\n\nTo avoid distractions, the case of one of more zero singular values was excluded in this discussion.  In such circumstances, \nif we conventionally take \"\n$d_{ii}^{-1}$\n\" to be zero,\n then everything still works.  This is what is going on when \ngeneralized inverses\n are used to solve the Normal equations.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/186974/402101 \n What am I missing here?\n\nI don't think you're really missing anything!\n\nAnother observation is that a sum of subsequent linear regression models can be represented as a single regression model as well (adding all intercepts and corresponding coefficients) so I cannot imagine how that could ever improve the model. The last observation is that a linear regression (the most typical approach) is using sum of squared residuals as a loss function - the same one that GB is using.\n\nSeems to me that you nailed it right there, and gave a short sketch of a proof that linear regression just beats boosting linear regressions in this setting.\n\nTo be pedantic, both methods are attempting to solve the following optimization problem\n\n$$ \\hat \\beta = \\text{argmin}_\\beta (y - X \\beta)^t (y - X \\beta) $$\n\nLinear regression just observes that you can solve it directly, by finding the solution to the linear equation\n\n$$ X^t X \\beta = X^t y $$\n\nThis automatically gives you the best possible value of $\\beta$ out of all possibilities.\n\nBoosting, whether your weak classifier is a one variable or multi variable regression, gives you a sequence of coefficient vectors $\\beta_1, \\beta_2, \\ldots$.  The final model prediction is, as you observe, a sum, and has the same functional form as the full linear regressor\n\n$$ X \\beta_1 + X \\beta_2 + \\cdots + X \\beta_n = X (\\beta_1 + \\beta_2 + \\cdots + \\beta_n) $$\n\nEach of these steps is chosen to further decrease the sum of squared errors.  But we could have found the \nminimum possible\n sum of square errors within this functional form by just performing a full linear regression to begin with.\n\nA possible defense of boosting in this situation could be the implicit regularization it provides.  Possibly (I haven't played with this) you could use the early stopping feature of a gradient booster, along with a cross validation, to stop short of the full linear regression. This would provide a regularization to your regression, and possibly help with overfitting.  This is not particularly practical, as one has very efficient and well understood options like ridge regression and the elastic net in this setting.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/186974/402101 \n Boosting shines when there is no terse functional form around.  Boosting decision trees lets the functional form of the regressor/classifier evolve slowly to fit the data, often resulting in complex shapes one could not have dreamed up by hand and eye.  When a simple functional form \nis\n desired, boosting is not going to help you find it (or at least is probably a rather inefficient way to find it).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/538/402101 \n Correlation is not sufficient for causation. One can get around the Wikipedia example by imagining that those twins always cheated in their tests by having a device that gives them the answers. The twin that goes to the amusement park loses the device, hence the low grade.\n\nA good way to get this stuff straight is to think of the structure of Bayesian network that may be generating the measured quantities, as done by Pearl in his book \nCausality\n. His basic point is to look for hidden variables. If there is a hidden variable that happens not to vary in the measured sample, then the correlation would not imply causation. Expose all hidden variables and you have causation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163040/402101 \n Logistic regression can be described as a linear combination\n\n$$ \\eta = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k $$\n\nthat is passed through the link function $g$:\n\n$$ g(E(Y)) = \\eta $$\n\nwhere the link function is a \nlogit\n function\n\n$$ E(Y|X,\\beta) = p = \\text{logit}^{-1}( \\eta ) $$\n\nwhere $Y$ take only values in $\\{0,1\\}$ and inverse logit functions transforms linear combination $\\eta$ to this range. This is where classical logistic regression ends.\n\nHowever if you recall that $E(Y) = P(Y = 1)$ for variables that take only values in $\\{0,1\\}$, than $E(Y | X,\\beta)$ can be considered as $P(Y = 1 | X,\\beta)$. In this case, the logit function output could be thought as conditional probability of \"success\", i.e. $P(Y=1|X,\\beta)$. \nBernoulli distribution\n is a distribution that describes probability of observing binary outcome, with some $p$ parameter, so we can describe $Y$ as\n\n$$ y_i \\sim \\text{Bernoulli}(p) $$\n\nSo with logistic regression we look for some parameters $\\beta$ that togeder with independent variables $X$ form a linear combination $\\eta$. In classical regression $E(Y|X,\\beta) = \\eta$ (we assume link function to be identity function), however to model $Y$ that takes values in $\\{0,1\\}$ we need to transform $\\eta$ so to fit in $[0,1]$ range.\n\nNow, to estimate logistic regression in Bayesian way you pick up some priors for $\\beta_i$ parameters as with linear regression (see \nKruschke et al, 2012\n), then use logit function to transform the linear combination $\\eta$, so to use its output as a $p$ parameter of Bernoulli distribution that describes your $Y$ variable. So, yes, you actually use the equation and the logit link function the same way as in frequentionist case, and the rest works (e.g. choosing priors) like with estimating linear regression the Bayesian way.\n\nThe simple approach for choosing priors is to choose Normal distributions (but you can also use other distributions, e.g. $t$- or Laplace distribution for more robust model) for $\\beta_i$'s with parameters $\\mu_i$ and $\\sigma_i^2$ that are preset or taken from \nhierarchical priors\n. Now, having the model definition you can use software such as \nJAGS\n to perform \nMarkov Chain Monte Carlo\n simulation for you to estimate the model. Below I post JAGS code for simple logistic model (check \nhere\n for more examples).\n\nmodel {\n   # setting up priors\n   a ~ dnorm(0, .0001)\n   b ~ dnorm(0, .0001)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163040/402101 \n model {\n   # setting up priors\n   a ~ dnorm(0, .0001)\n   b ~ dnorm(0, .0001)\n\n   for (i in 1:N) {\n      # passing the linear combination through logit function\n      logit(p[i]) <- a + b * x[i]\n\n      # likelihood function\n      y[i] ~ dbern(p[i])\n   }\n}\n\nmodel {\n   # setting up priors\n   a ~ dnorm(0, .0001)\n   b ~ dnorm(0, .0001)\n\n   for (i in 1:N) {\n      # passing the linear combination through logit function\n      logit(p[i]) <- a + b * x[i]\n\n      # likelihood function\n      y[i] ~ dbern(p[i])\n   }\n}\n\nAs you can see, the code directly translates to model definition. What the software does is it draws some values from Normal priors for \na\n and \nb\n, then it uses those values to estimate \np\n and finally, uses likelihood function to assess how likely is your data given those parameters (this is when you use Bayes theorem, see \nhere\n for more detailed description).\n\na\n\nb\n\np\n\nThe basic logistic regression model can be extended to model the dependency between the predictors using a hierarchical model (including \nhyperpriors\n). In this case you can draw $\\beta_i$'s from \nMultivariate Normal distribution\n that enables us to include information about covariance $\\boldsymbol{\\Sigma}$ between independent variables\n\n$$ \\begin{pmatrix} \\beta_0  \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k  \\end{pmatrix} \\sim \\mathrm{MVN} \\left(\n\\begin{bmatrix} \\mu_0 \\\\ \\mu_1 \\\\ \\vdots \\\\ \\mu_k \\end{bmatrix},\n\\begin{bmatrix} \\sigma^2_0 & \\sigma_{0,1} & \\ldots & \\sigma_{0,k} \\\\\n               \\sigma_{1,0} & \\sigma^2_1 & \\ldots &\\sigma_{1,k} \\\\\n               \\vdots & \\vdots & \\ddots & \\vdots \\\\\n               \\sigma_{k,0} & \\sigma_{k,1} & \\ldots & \\sigma^2_k\n\\end{bmatrix}\n\\right)$$\n\n...but this is going into details, so let's stop right here.\n\nThe \"Bayesian\" part in here is choosing priors, using Bayes theorem and defining model in probabilistic terms. See here for \ndefinition of \"Bayesian model\"\n and here for some \ngeneral intuition on Bayesian approach\n. What you can also notice is that defining models is pretty straightforward and flexible with this approach.\n\nKruschke, J. K., Aguinis, H., & Joo, H. (2012). \nThe time has come: Bayesian methods for data analysis in the organizational sciences.\n \nOrganizational Research Methods, 15\n(4), 722-752.\n\nGelman, A., Jakulin, A., Pittau, G.M., and Su, Y.-S. (2008). \nA weakly informative default prior distribution for logistic and other regression models.\n \nThe Annals of Applied Statistics, 2\n(4), 1360\u20131383.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/35922/402101 \n ...the relationship is nonlinear yet there is a clear relation between x and y, how can I test the association and label its nature?\n\nOne way of doing this would be to fit $y$ as a semi-parametrically estimated function of $x$ using, for example, a \ngeneralized additive model\n and testing whether or not that functional estimate is constant, which would indicate no relationship between $y$ and $x$. This approach frees you from having to do polynomial regression and making sometimes arbitrary decisions about the order of the polynomial, etc.\n\nSpecifically, if you have observations, $(Y_i, X_i)$, you could fit the model:\n\n$$ E(Y_i | X_i) = \\alpha + f(X_i) + \\varepsilon_i $$\n\nand test the hypothesis $H_{0} : f(x) = 0, \\ \\forall x$. In \nR\n, you can do this using the \ngam()\n function. If \ny\n is your outcome and \nx\n is your predictor, you could type:\n\nR\n\ngam()\n\ny\n\nx\n\nlibrary(mgcv) \ng <- gam(y ~ s(x))\n\nlibrary(mgcv) \ng <- gam(y ~ s(x))\n\nTyping \nsummary(g)\n will give you the result of the hypothesis test above. As far as characterizing the nature of the relationship, this would be best done with a plot. One way to do this in \nR\n (assuming the code above has already been entered)\n\nsummary(g)\n\nR\n\nplot(g,scheme=2)\n\nplot(g,scheme=2)\n\nIf your response variable is discrete (e.g. binary), you can accommodate that within this framework by fitting a logistic GAM (in \nR\n, you'd add \nfamily=binomial\n to your call to \ngam\n). Also, if you have multiple predictors, you can include multiple additive terms (or ordinary linear terms), or fit multivariable functions, e.g. $f(x,z)$ if you had predictors \nx, z\n. The complexity of the relationship is automatically selected by cross validation if you use the default methods, although there is a lot of flexibility here - see the \ngam\n help file\n if interested.\n\nR\n\nfamily=binomial\n\ngam\n\nx, z\n\ngam",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/19140/402101 \n Imagine some 2D data--let's say height versus weight for students at a high school--plotted on a pair of axes.\n\nNow suppose you fit a straight line through it. This line, which of course represents a set of predicted values, has zero statistical variance. But the bias is (probably) high--i.e., it doesn't fit the data very well.\n\nNext, suppose you model the data with a high-degree polynomial spline. You're not satisfied with the fit, so you increase the polynomial degree until the fit improves (and it will, to arbitrary precision, in fact). Now you have a situation with bias that tends to zero, but the variance is very high.\n\nNote that the bias-variance trade-off doesn't describe a proportional relationship--i.e., if you plot bias versus variance you won't necessarily see a straight line through the origin with slope -1. In the polynomial spline example above, reducing the degree almost certainly increases the variance much less than it decreases the bias.\n\nThe bias-variance tradeoff is also embedded in the sum-of-squares error function. Below, I have rewritten (but not altered) the usual form of this equation to emphasize this:\n\n$$\nE\\left(\\left(y - \\dot{f}(x)\\right)^2\\right) = \\sigma^2 + \\left[f(x) - \\frac{1}{\\kappa}\\sum_{i=0}^nf(x_n)\\right]^2+\\frac{\\sigma^2}{\\kappa}\n$$\n\nOn the right-hand side, there are three terms: the first of these is just the irreducible error (the variance in the data itself); this is beyond our control so ignore it. The \nsecond\n term is the \nsquare of the bias\n; and the \nthird\n is the \nvariance\n. It's easy to see that as one goes up the other goes down--they can't both vary together in the same direction. Put another way, you can think of least-squares regression as (implicitly) finding the optimal combination of bias and variance from among candidate models.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/268758/402101 \n Answering your question with a question: \nwhat exactly is machine learning?\n Trevor Hastie, Robert Tibshirani and Jerome Friedman in \nThe Elements of\nStatistical Learning\n, Kevin P. Murphy in \nMachine Learning A Probabilistic Perspective\n, Christopher Bishop in \nPattern Recognition and Machine Learning\n,  Ian Goodfellow, Yoshua Bengio and Aaron Courville in \nDeep Learning\n and a number of other machine learning \"bibles\" mention linear regression as one of the machine learning \"algorithms\". Machine learning is partly a buzzword for applied statistics and the distinction between statistics and machine learning is often blurry.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/357270/402101 \n So if that's the case, does statistical independence automatically\n  mean lack of causation?\n\nNo, and here's a simple counter example with a multivariate normal,\n\nset.seed(100)\nn <- 1e6\na <- 0.2\nb <- 0.1\nc <- 0.5\nz <- rnorm(n)\nx <- a*z + sqrt(1-a^2)*rnorm(n)\ny <- b*x - c*z + sqrt(1- b^2 - c^2 +2*a*b*c)*rnorm(n)\ncor(x, y)\n\nset.seed(100)\nn <- 1e6\na <- 0.2\nb <- 0.1\nc <- 0.5\nz <- rnorm(n)\nx <- a*z + sqrt(1-a^2)*rnorm(n)\ny <- b*x - c*z + sqrt(1- b^2 - c^2 +2*a*b*c)*rnorm(n)\ncor(x, y)\n\nWith corresponding graph,\n\n\n\nHere we have that $x$ and $y$ are marginally independent (in the multivariate normal case, zero correlation implies independence). This happens because the backdoor path via $z$ exactly cancels out the direct path from $x$ to $y$, that is, $cov(x,y) = b - a*c = 0.1 - 0.1 = 0$. Thus $E[Y|X =x] =E[Y] =0$. Yet, $x$ directly causes $y$, and we have that $E[Y|do(X= x)] = bx$, which is different from $E[Y]=0$.\n\nAssociations, interventions and counterfactuals\n\nI think it's important to make some clarifications here regarding associations, interventions and counterfactuals.\n\nCausal models entail statements about the behavior of the system: (i) under passive observations, (ii) under interventions, as well as (iii) counterfactuals. And independence on one level does not necessarily translate to the other.\n\nAs the example above shows, we can have no association between $X$ and $Y$, that is, $P(Y|X) = P(Y)$, and still be the case that manipulations on $X$ changes the distribution of $Y$, that is, $P(Y|do(x)) \\neq P(Y)$.\n\nNow, we can go one step further. We can have causal models where  intervening on $X$ does not change the population distribution of $Y$, but that does not mean lack of counterfactual causation! That is, even though $P(Y|do(x)) = P(Y)$, for every individual their outcome $Y$ would have been different had you changed his $X$. This is precisely the case described by user20160, as well as in my previous answer \nhere.\n\nThese three levels make a \nhierarchy of causal inference tasks\n, in terms of the information needed to answer queries on each of them.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n There exist a number of frequenly mentioned regressional effects which conceptually are different but share much in common when seen purely statistically (see e.g. \nthis paper\n \"Equivalence of the Mediation, Confounding and Suppression\nEffect\" by David MacKinnon et al., or Wikipedia articles):\n\nMediator: IV which conveys effect (totally of partly) of another IV\nto the DV.\n\n\nConfounder: IV which constitutes or precludes, totally or\npartly, effect of another IV to the DV.\n\n\nModerator: IV which, varying,\nmanages the strength of the effect of another IV on the DV.\nStatistically, it is known as interaction between the two IVs.\n\n\nSuppressor: IV (a mediator or a moderator conceptually) which inclusion\nstrengthens the effect of another IV on the DV.\n\nI'm not going to discuss to what extent some or all of them are technically similar (for that, read the paper linked above). My aim is to try to show graphically what \nsuppressor\n is. The above definition that \"suppressor is a variable which inclusion strengthens the effect of another IV on the DV\" seems to me \npotentially\n broad because it does not tell anything about mechanisms of such enhancement. Below I'm discussing one mechanism - the only one I consider to be suppression. If there are \nother\n mechanisms as well (as for right now, I haven't tried to meditate of any such other) then either the above \"broad\" definition should be considered imprecise or my definition of suppression should be considered too narrow.\n\nSuppressor is the independent variable which, when added to the model, raises observed R-square \nmostly due to its accounting for the residuals\n left by the model without it, and not due to its own association with the DV (which is comparatively weak). We know that the increase in R-square in response to adding a IV is the squared part correlation of that IV in that new model. This way, if the part correlation of the IV with the DV \nis greater\n (by absolute value) than the zero-order \n$r$\n between them, that IV is a suppressor.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n So, a suppressor mostly \"suppresses\" the error of the reduced model, being weak as a predictor itself. The error term is the complement to the prediction. The prediction is \"projected on\" or \"shared between\" the IVs (regression coefficients), and so is the error term (\"complements\" to the coefficients). The suppressor suppresses such error components unevenly: greater for some IVs, lesser for other IVs. For those IVs \"whose\" such components it suppresses greatly it lends considerable facilitating aid by actually \nraising their regression coefficients\n.\n\nNot strong suppressing effects occurs often and wildly (an \nexample\n on this site). Strong suppression is typically introduced consciously. A researcher seeks for a characteristic which must correlate with the DV as weak as possible and at the same time would correlate with something in the IV of interest which is considered irrelevant, prediction-void, in respect to the DV. He enters it to the model and gets considerable increase in that IV's predictive power. The suppressor's coefficient is typically not interpreted.\n\nI could summarize my \ndefinition\n as follows [up on @Jake's answer and @gung's comments]:\n\nFormal (statistical) definition: suppressor is IV with part\ncorrelation larger than zero-order correlation (with the dependent).\n\n\nConceptual (practical) definition: the above formal definition + the zero-order\ncorrelation is small, so that the suppressor is not a sound predictor\nitself.\n\n\"Suppessor\" is a role of a IV in a specific \nmodel\n only, not the characteristic of the separate variable. When other IVs are added or removed, the suppressor can suddenly stop suppressing or resume suppressing or change the focus of its suppressing activity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n Conceptual (practical) definition: the above formal definition + the zero-order\ncorrelation is small, so that the suppressor is not a sound predictor\nitself.\n\n\"Suppessor\" is a role of a IV in a specific \nmodel\n only, not the characteristic of the separate variable. When other IVs are added or removed, the suppressor can suddenly stop suppressing or resume suppressing or change the focus of its suppressing activity.\n\nThe first picture below shows a typical regression with two predictors (we'll speak of linear regression). The picture is copied from \nhere\n where it is explained in more details. In short, moderately correlated (= having acute angle between them) predictors \n$X_1$\n and \n$X_2$\n span 2-dimesional space \"plane X\". The dependent variable \n$Y$\n is projected onto it orthogonally, leaving the predicted variable \n$Y'$\n and the residuals with st. deviation equal to the length of \n$e$\n. R-square of the regression is the angle between \n$Y$\n and \n$Y'$\n, and the two regression coefficients are directly related to the skew coordinates \n$b_1$\n and \n$b_2$\n, respectively. This situation I've called normal or typical because both \n$X_1$\n and \n$X_2$\n correlate with \n$Y$\n (oblique angle exists between each of the independents and the dependent) and the predictors compete for the prediction because they are correlated.\n\n\n\nIt is shown on the next picture. This one is like the previous; however \n$Y$\n vector now directs somewhat away from the viewer and \n$X_2$\n changed its direction considerably. \n$X_2$\n acts as a suppressor. Note first of all that it hardly correlates with \n$Y$\n. Hence it cannot be a valuable \npredictor\n itself. Second. Imagine \n$X_2$\n is absent and you predict only by \n$X_1$\n; the prediction of this one-variable regression is depicted as \n$Y^*$\n red vector, the error as \n$e^*$\n vector, and the coefficient is given by \n$b^*$\n coordinate (which is the endpoint of \n$Y^*$\n).\n\n\n\nNow bring yourself back to the full model and notice that \n$X_2$\n is fairly correlated with \n$e^*$\n. Thus, \n$X_2$\n when introduced in the model, can explain a considerable portion of that error of the reduced model, cutting down \n$e^*$\n to \n$e$\n. This constellation: (1) \n$X_2$\n is not a rival to \n$X_1$\n as a \npredictor\n; and (2) \n$X_2$\n is a dustman to pick up \nunpredictedness\n left by \n$X_1$\n, - makes \n$X_2$\n a \nsuppressor\n. As a result of its effect, predictive strength of \n$X_1$\n has grown to some extent: \n$b_1$\n is larger than \n$b^*$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n Well, why is \n$X_2$\n called a suppressor to \n$X_1$\n and how can it reinforce it when \"suppressing\" it? Look at the next picture.\n\n\n\nIt is exactly the same as the previous. Think again of the model with the single predictor \n$X_1$\n. This predictor could of course be decomposed in two parts or components (shown in grey): the part which is \"responsible\" for prediction of \n$Y$\n (and thus coinciding with that vector) and the part which is \"responsible\" for the unpredictedness (and thus parallel to \n$e^*$\n). It is \nthis\n second part of \n$X_1$\n - the part irrelevant to \n$Y$\n - is suppressed by \n$X_2$\n when that suppressor is added to the model. The irrelevant part is suppressed and thus, given that the suppressor doesn't itself predict \n$Y$\n any much, the relevant part looks stronger. A suppressor is not a predictor but rather a facilitator for another/other predictor/s. Because it competes with what impedes them to predict.\n\nIt is the sign of the correlation between the suppressor and the error variable \n$e^*$\n left by the reduced (without-the-suppressor) model. In the depiction above, it is positive. In other settings (for example, revert the direction of \n$X_2$\n) it could be negative.\n\nExample data:\n\ny         x1         x2\n\n1.64454000  .35118800 1.06384500\n1.78520400  .20000000 -1.2031500\n-1.3635700 -.96106900 -.46651400\n .31454900  .80000000 1.17505400\n .31795500  .85859700 -.10061200\n .97009700 1.00000000 1.43890400\n .66438800  .29267000 1.20404800\n-.87025200 -1.8901800 -.99385700\n1.96219200 -.27535200 -.58754000\n1.03638100 -.24644800 -.11083400\n .00741500 1.44742200 -.06923400\n1.63435300  .46709500  .96537000\n .21981300  .34809500  .55326800\n-.28577400  .16670800  .35862100\n1.49875800 -1.1375700 -2.8797100\n1.67153800  .39603400 -.81070800\n1.46203600 1.40152200 -.05767700\n-.56326600 -.74452200  .90471600\n .29787400 -.92970900  .56189800\n-1.5489800 -.83829500 -1.2610800\n\ny         x1         x2",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n y         x1         x2\n\n1.64454000  .35118800 1.06384500\n1.78520400  .20000000 -1.2031500\n-1.3635700 -.96106900 -.46651400\n .31454900  .80000000 1.17505400\n .31795500  .85859700 -.10061200\n .97009700 1.00000000 1.43890400\n .66438800  .29267000 1.20404800\n-.87025200 -1.8901800 -.99385700\n1.96219200 -.27535200 -.58754000\n1.03638100 -.24644800 -.11083400\n .00741500 1.44742200 -.06923400\n1.63435300  .46709500  .96537000\n .21981300  .34809500  .55326800\n-.28577400  .16670800  .35862100\n1.49875800 -1.1375700 -2.8797100\n1.67153800  .39603400 -.81070800\n1.46203600 1.40152200 -.05767700\n-.56326600 -.74452200  .90471600\n .29787400 -.92970900  .56189800\n-1.5489800 -.83829500 -1.2610800\n\nLinear regression results:\n\n\n\nObserve that \n$X_2$\n served as suppressor. Its zero-order correlation with \n$Y$\n is practically zero but its part correlation is much larger by magnitude, \n$-.224$\n. It strengthened to some extent the predictive force of \n$X_1$\n (from r \n$.419$\n, a would-be beta in simple regression with it, to beta \n$.538$\n in the multiple regression).\n\nAccording to the \nformal\n definition, \n$X_1$\n appeared a suppressor too, because its part correlation is greater than its zero-order correlation. But that is because we have only two IV in the simple example. Conceptually, \n$X_1$\n isn't a suppressor because its \n$r$\n with \n$Y$\n is not about \n$0$\n.\n\nBy way, sum of squared part correlations exceeded R-square: \n.4750^2+(-.2241)^2 = .2758 > .2256\n, which would not occur in normal regressional situation (see the \nVenn diagram\n below).\n\n.4750^2+(-.2241)^2 = .2758 > .2256\n\nAdding a variable that will serve a supressor may as well as may not change the sign of some other variables' coefficients. \"Suppression\" and \"change sign\" effects are not the same thing. Moreover, I believe that a suppressor can never change sign of \nthose\n predictors whom they serve suppressor. (It would be a shocking discovery to add the suppressor on purpose to facilitate a variable and then to find it having become indeed stronger but in the opposite direction! I'd be thankful if somebody could show me it is possible.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n To cite an earlier passage: \"For those IVs \"whose\" such components [error components] it suppresses greatly the suppressor lends considerable facilitating aid by actually \nraising their regression coefficients\n\". Indeed, in our Example above, \n$X_2$\n, the suppressor, raised the coefficient for \n$X_1$\n. Such enhancement of the unique predictive power of another regressor is often the \naim\n of a suppressor to a model but it is not the \ndefinition\n of suppressor or of suppression effect. For, the aforementioned enhancement of another predictor's capacity via adding more regressors can easily occure in a normal regressional situation without those regressors being suppressors. Here is an example.\n\ny       x1       x2       x3\n\n   1        1        1        1\n   3        2        2        6\n   2        3        3        5\n   3        2        4        2\n   4        3        5        9\n   3        4        4        2\n   2        5        3        3\n   3        6        4        4\n   4        7        5        5\n   5        6        6        6\n   4        5        7        5\n   3        4        5        5\n   4        5        3        5\n   5        6        4        6\n   6        7        5        4\n   5        8        6        6\n   4        2        7        7\n   5        3        8        8\n   6        4        9        4\n   5        5        3        3\n   4        6        4        2\n   3        2        1        1\n   4        3        5        4\n   5        4        6        5\n   6        9        5        4\n   5        8        3        3\n   3        5        5        2\n   2        6        6        1\n   3        7        7        5\n   5        8        8        8\n\ny       x1       x2       x3",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n y       x1       x2       x3\n\n   1        1        1        1\n   3        2        2        6\n   2        3        3        5\n   3        2        4        2\n   4        3        5        9\n   3        4        4        2\n   2        5        3        3\n   3        6        4        4\n   4        7        5        5\n   5        6        6        6\n   4        5        7        5\n   3        4        5        5\n   4        5        3        5\n   5        6        4        6\n   6        7        5        4\n   5        8        6        6\n   4        2        7        7\n   5        3        8        8\n   6        4        9        4\n   5        5        3        3\n   4        6        4        2\n   3        2        1        1\n   4        3        5        4\n   5        4        6        5\n   6        9        5        4\n   5        8        3        3\n   3        5        5        2\n   2        6        6        1\n   3        7        7        5\n   5        8        8        8\n\nRegressions results without and with \n$X_3$\n:\n\n\n\nInclusion of \n$X_3$\n in the model raised the beta of \n$X_1$\n from \n$.381$\n to \n$.399$\n (and its corresponding partial correlation with \n$Y$\n from \n$.420$\n to \n$.451$\n). Still, we find no suppressor in the model. \n$X_3$\n's part correlation (\n$.229$\n) is not greater than its zero-order correlation (\n$.427$\n). Same is for the other regressors. \"Facilitation\" effect was there, but not due to \"suppression\" effect. Definition of a suppessor is different from just strenghtening/facilitation; and it is about picking up mostly errors, due to which the part correlation exceeds the zero-order one.\n\nNormal regressional situation is often explained with the help of Venn diagram.\n\n\n\nA+B+C+D\n = 1, all \n$Y$\n variability. \nB+C+D\n area is the variability accounted by the two IV (\n$X_1$\n and \n$X_2$\n), the R-square; the remaining area \nA\n is the error variability. \nB+C\n = \n$r_{YX_1}^2$\n; \nD+C\n = \n$r_{YX_2}^2$\n, Pearson zero-order correlations. \nB\n and \nD\n are the squared part (semipartial) correlations: \nB\n = \n$r_{Y(X_1.X_2)}^2$\n; \nD\n = \n$r_{Y(X_2.X_1)}^2$\n. \nB/(A+B)\n = \n$r_{YX_1.X_2}^2$\n and \nD/(A+D)\n = \n$r_{YX_2.X_1}^2$\n are the squared partial correlations which have the \nsame basic meaning\n as the standardized regression coefficients betas.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74353/402101 \n According to the above definition (which I stick to) that a suppressor is the IV with part correlation greater than zero-order correlation, \n$X_2$\n is the suppressor if \nD\n area > \nD+C\n area. That \ncannot\n be displayed on Venn diagram. (It would imply that \nC\n from the view of \n$X_2$\n is not \"here\" and is not the same entity than \nC\n from the view of \n$X_1$\n. One must invent perhaps something like multilayered Venn diagram to wriggle oneself to show it.)\n\nP.S.\n Upon finishing my answer I found \nthis\n answer (by @gung) with a nice simple (schematic) diagram, which seems to be in agreement with what I showed above by vectors.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/121676/402101 \n [I'll assume from the discussion in your question that you're happy to accept as fact that if \n$Z_i, i=1,2,\\ldots,k$\n are independent identically distributed \n$N(0,1)$\n random variables then \n$\\sum_{i=1}^{k}Z_i^2\\sim \\chi^2_k$\n.]\n\nFormally, the result you need follows from \nCochran's theorem\n.\n (Though it can be shown in other ways)\n\nLess formally, consider that if we knew the population mean, and estimated the variance about it (rather than about the sample mean): \n$s_0^2 = \\frac{1}{n} \\sum_{i=1}^{n}(X_i-\\mu)^2$\n, then \n$s_0^2/\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2=\\frac{1}{n} \\sum_{i=1}^{n}Z_i^2$\n,  (\n$Z_i=(X_i-\\mu)/\\sigma$\n) which will be \n$\\frac{1}{n}$\n times a \n$\\chi^2_n$\n random variable.\n\nThe fact that the sample mean is used, instead of the population mean (\n$Z_i^*=(X_i-\\bar{X})/\\sigma$\n) makes the sum of squares of deviations smaller, but in just such a way that \n$\\sum_{i=1}^{n}(Z_i^*)^2\\,\\sim\\chi^2_{n-1}$\n (about which, see Cochran's theorem). That is, rather than \n$ns_0^2/\\sigma^2\\sim \\chi^2_n$\n we now have \n$(n-1)s^2/\\sigma^2\\sim\\chi^2_{n-1}$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86272/402101 \n The topic you are asking about is \nmulticollinearity\n.  You might want to read some of the threads on CV categorized under the \nmulticollinearity\n tag.  @whuber's \nanswer linked above\n in particular is also worth your time.\n\nThe assertion that \"if two predictors are correlated and both are included in a model, one will be insignificant\", is not correct.  If there is a real effect of a variable, the probability that variable will be significant is a function of several things, such as the magnitude of the effect, the magnitude of the error variance, the variance of the variable itself, the amount of data you have, and the number of other variables in the model.  Whether the variables are correlated is also relevant, but it doesn't override these facts.  Consider the following simple demonstration in \nR\n:\n\nR\n\nlibrary(MASS)    # allows you to generate correlated data\nset.seed(4314)   # makes this example exactly replicable\n\n# generate sets of 2 correlated variables w/ means=0 & SDs=1\nX0 = mvrnorm(n=20,   mu=c(0,0), Sigma=rbind(c(1.00, 0.70),    # r=.70\n                                            c(0.70, 1.00)) )\nX1 = mvrnorm(n=100,  mu=c(0,0), Sigma=rbind(c(1.00, 0.87),    # r=.87\n                                            c(0.87, 1.00)) )\nX2 = mvrnorm(n=1000, mu=c(0,0), Sigma=rbind(c(1.00, 0.95),    # r=.95\n                                            c(0.95, 1.00)) )\ny0 = 5 + 0.6*X0[,1] + 0.4*X0[,2] + rnorm(20)    # y is a function of both\ny1 = 5 + 0.6*X1[,1] + 0.4*X1[,2] + rnorm(100)   #  but is more strongly\ny2 = 5 + 0.6*X2[,1] + 0.4*X2[,2] + rnorm(1000)  #  related to the 1st\n\n# results of fitted models (skipping a lot of output, including the intercepts)\nsummary(lm(y0~X0[,1]+X0[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X0[, 1]       0.6614     0.3612   1.831   0.0847 .     # neither variable\n# X0[, 2]       0.4215     0.3217   1.310   0.2075       #  is significant\nsummary(lm(y1~X1[,1]+X1[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X1[, 1]      0.57987    0.21074   2.752  0.00708 **    # only 1 variable\n# X1[, 2]      0.25081    0.19806   1.266  0.20841       #  is significant\nsummary(lm(y2~X2[,1]+X2[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X2[, 1]      0.60783    0.09841   6.177 9.52e-10 ***   # both variables\n# X2[, 2]      0.39632    0.09781   4.052 5.47e-05 ***   #  are significant",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86272/402101 \n library(MASS)    # allows you to generate correlated data\nset.seed(4314)   # makes this example exactly replicable\n\n# generate sets of 2 correlated variables w/ means=0 & SDs=1\nX0 = mvrnorm(n=20,   mu=c(0,0), Sigma=rbind(c(1.00, 0.70),    # r=.70\n                                            c(0.70, 1.00)) )\nX1 = mvrnorm(n=100,  mu=c(0,0), Sigma=rbind(c(1.00, 0.87),    # r=.87\n                                            c(0.87, 1.00)) )\nX2 = mvrnorm(n=1000, mu=c(0,0), Sigma=rbind(c(1.00, 0.95),    # r=.95\n                                            c(0.95, 1.00)) )\ny0 = 5 + 0.6*X0[,1] + 0.4*X0[,2] + rnorm(20)    # y is a function of both\ny1 = 5 + 0.6*X1[,1] + 0.4*X1[,2] + rnorm(100)   #  but is more strongly\ny2 = 5 + 0.6*X2[,1] + 0.4*X2[,2] + rnorm(1000)  #  related to the 1st\n\n# results of fitted models (skipping a lot of output, including the intercepts)\nsummary(lm(y0~X0[,1]+X0[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X0[, 1]       0.6614     0.3612   1.831   0.0847 .     # neither variable\n# X0[, 2]       0.4215     0.3217   1.310   0.2075       #  is significant\nsummary(lm(y1~X1[,1]+X1[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X1[, 1]      0.57987    0.21074   2.752  0.00708 **    # only 1 variable\n# X1[, 2]      0.25081    0.19806   1.266  0.20841       #  is significant\nsummary(lm(y2~X2[,1]+X2[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X2[, 1]      0.60783    0.09841   6.177 9.52e-10 ***   # both variables\n# X2[, 2]      0.39632    0.09781   4.052 5.47e-05 ***   #  are significant\n\nThe correlation between the two variables is lowest in the first example and highest in the third, yet neither variable is significant in the first example and both are in the last example.  The magnitude of the effects is identical in all three cases, and the variances of the variables and the errors should be similar (they are stochastic, but drawn from populations with the same variance).  The pattern we see here is due primarily to my manipulating the $N$s for each case.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86272/402101 \n The correlation between the two variables is lowest in the first example and highest in the third, yet neither variable is significant in the first example and both are in the last example.  The magnitude of the effects is identical in all three cases, and the variances of the variables and the errors should be similar (they are stochastic, but drawn from populations with the same variance).  The pattern we see here is due primarily to my manipulating the $N$s for each case.\n\nThe key concept to understand to resolve your questions is the \nvariance inflation factor\n (VIF).  The VIF is how much the variance of your regression coefficient is larger than it would otherwise have been if the variable had been completely uncorrelated with all the other variables in the model.  Note that the VIF is a multiplicative factor, if the variable in question is uncorrelated the VIF=1.  A simple understanding of the VIF is as follows: you could fit a model predicting a variable (say, $X_1$) from all other variables in your model (say, $X_2$), and get a multiple $R^2$.  The VIF for $X_1$ would be $1/(1-R^2)$.  Let's say the VIF for $X_1$ were $10$ (often considered a threshold for excessive multicollinearity), then the variance of the sampling distribution of the regression coefficient for $X_1$ would be $10\\times$ larger than it would have been if $X_1$ had been completely uncorrelated with all the other variables in the model.\n\nThinking about what would happen if you included both correlated variables vs. only one is similar, but slightly more complicated than the approach discussed above.  This is because not including a variable means the model uses less degrees of freedom, which changes the residual variance and everything computed from that (including the variance of the regression coefficients).  In addition, if the non-included variable really is associated with the response, the variance in the response due to that variable will be included into the residual variance, making it larger than it otherwise would be.  Thus, several things change simultaneously (the variable is correlated or not with another variable, and the residual variance), and the precise effect of dropping / including the other variable will depend on how those trade off.  The best way to think through this issue is based on the counterfactual of how the model would differ if the variables were uncorrelated instead of correlated, rather than including or excluding one of the variables.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86272/402101 \n Armed with an understanding of the VIF, here are the answers to your questions:\n\nBecause the variance of the sampling distribution of the regression coefficient would be larger (by a factor of the VIF) if it were correlated with other variables in the model, the p-values would be higher (i.e., less significant) than they otherwise would.  \n\n\nThe variances of the regression coefficients would be larger, as already discussed.  \n\n\nIn general, this is hard to know without solving for the model.  Typically, if only one of two is significant, it will be the one that had the stronger bivariate correlation with $Y$.  \n\n\nHow the predicted values and their variance would change is quite complicated.  It depends on how strongly correlated the variables are and the manner in which they appear to be associated with your response variable in your data.  Regarding this issue, it may help you to read my answer here: \nIs there a difference between 'controlling for' and 'ignoring' other variables in multiple regression?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/316171/402101 \n One possible choice is the \nbeta distribution\n, but \nre-parametrized\n in terms of mean $\\mu$ and precision $\\phi$, that is, \"for fixed $\\mu$, the larger the value of $\\phi$, the smaller the variance of $y$\" (see Ferrari, and Cribari-Neto, 2004). The probability density function is constructed by replacing the standard parameters of beta distribution with $\\alpha = \\phi\\mu$ and $\\beta = \\phi(1-\\mu)$\n\n$$\nf(y) = \\frac{1}{\\mathrm{B}(\\phi\\mu,\\; \\phi(1-\\mu))}\\; y^{\\phi\\mu-1} (1-y)^{\\phi(1-\\mu)-1}\n$$\n\nwhere $E(Y) = \\mu$ and $\\mathrm{Var}(Y) = \\frac{\\mu(1-\\mu)}{1+\\phi}$.\n\nAlternatively, \nyou can calculate\n appropriate $\\alpha$ and $\\beta$ parameters that would lead to beta distribution with pre-defined mean and variance. However, notice that there are restrictions on possible values of variance that are valid for beta distribution. For me personally, the parametrization using precision is more intuitive (think of $x\\,/\\,\\phi$ proportions in \nbinomially distributed\n $X$, with sample size $\\phi$ and the probability of success $\\mu$).\n\nKumaraswamy distribution\n is another bounded continuous distribution, but it would be harder to re-parametrize like above.\n\nAs others have noticed, it is \nnot\n normal since normal distribution has the $(-\\infty, \\infty)$ support, so at best you could use the \ntruncated normal\n as an approximation.\n\nFerrari, S., & Cribari-Neto, F. (2004). \nBeta regression for modelling rates and proportions.\n Journal of Applied Statistics, 31(7), 799-815.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238259/402101 \n Summary:\n the \"random-effects model\" in econometrics and a \"random intercept mixed model\" are indeed the same models, but they are estimated in different ways. The econometrics way is to use FGLS, and the mixed model way is to use ML. There are different algorithms of doing FGLS, and some of them (on this dataset) produce results that are very close to ML.\n\nplm\n\nI will answer with my testing on \nplm(..., model = \"random\")\n and \nlmer()\n, using the data generated by @ChristophHanck.\n\nplm(..., model = \"random\")\n\nlmer()\n\nAccording to \nthe plm package manual\n, there are four options for \nrandom.method\n: the method of estimation for the variance components in the random effects model. @amoeba used the default one \nswar\n (Swamy and Arora, 1972).\n\nrandom.method\n\nswar\n\nFor random effects models, four estimators of the transformation\n  parameter are available by setting random.method to one of \"swar\"\n  (Swamy and Arora (1972)) (default), \"amemiya\" (Amemiya (1971)),\n  \"walhus\" (Wallace and Hussain (1969)), or \"nerlove\" (Nerlove (1971)).\n\nI tested all the four options using the same data, \ngetting an error for \namemiya\n, and three totally different coefficient estimates for the variable \nstackX\n. The ones from using \nrandom.method='nerlove'\n and 'amemiya' are nearly equivalent to that from \nlmer()\n, -1.029 and -1.025 vs -1.026. They are also not very different from that obtained in the \"fixed-effects\" model, -1.045.\n\namemiya\n\nstackX\n\nrandom.method='nerlove'\n\nlmer()\n\n# \"amemiya\" only works using the most recent version:\n# install.packages(\"plm\", repos=\"http://R-Forge.R-project.org\")\n\nre0 <- plm(stackY~stackX, data = paneldata, model = \"random\") #random.method='swar'\nre1 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='amemiya')\nre2 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='walhus')\nre3 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='nerlove')\nl2  <- lmer(stackY~stackX+(1|as.factor(unit)), data = paneldata)\n\ncoef(re0)     #    (Intercept)   stackX    18.3458553   0.7703073 \ncoef(re1)     #    (Intercept)   stackX    30.217721   -1.025186 \ncoef(re2)     #    (Intercept)   stackX    -1.15584     3.71973 \ncoef(re3)     #    (Intercept)   stackX    30.243678   -1.029111 \nfixef(l2)     #    (Intercept)   stackX    30.226295   -1.026482\n\n# \"amemiya\" only works using the most recent version:\n# install.packages(\"plm\", repos=\"http://R-Forge.R-project.org\")",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238259/402101 \n coef(re0)     #    (Intercept)   stackX    18.3458553   0.7703073 \ncoef(re1)     #    (Intercept)   stackX    30.217721   -1.025186 \ncoef(re2)     #    (Intercept)   stackX    -1.15584     3.71973 \ncoef(re3)     #    (Intercept)   stackX    30.243678   -1.029111 \nfixef(l2)     #    (Intercept)   stackX    30.226295   -1.026482\n\n# \"amemiya\" only works using the most recent version:\n# install.packages(\"plm\", repos=\"http://R-Forge.R-project.org\")\n\nre0 <- plm(stackY~stackX, data = paneldata, model = \"random\") #random.method='swar'\nre1 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='amemiya')\nre2 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='walhus')\nre3 <- plm(stackY~stackX, data = paneldata, model = \"random\",  random.method='nerlove')\nl2  <- lmer(stackY~stackX+(1|as.factor(unit)), data = paneldata)\n\ncoef(re0)     #    (Intercept)   stackX    18.3458553   0.7703073 \ncoef(re1)     #    (Intercept)   stackX    30.217721   -1.025186 \ncoef(re2)     #    (Intercept)   stackX    -1.15584     3.71973 \ncoef(re3)     #    (Intercept)   stackX    30.243678   -1.029111 \nfixef(l2)     #    (Intercept)   stackX    30.226295   -1.026482\n\nUnfortunately I do not have time right now, but interested readers can find the four references, to check their estimation procedures. It would be very helpful to figure out why they make such a difference. I expect that for some cases, the \nplm\n estimation procedure using the \nlm()\n on transformed data should be equivalent to the maximum likelihood procedure utilized in \nlmer()\n.\n\nplm\n\nlm()\n\nlmer()\n\nThe authors of \nplm\n package did compare the two in Section 7 of their paper: \nYves Croissant and Giovanni Millo, 2008, Panel Data Econometrics in R: The plm package\n.\n\nplm\n\nEconometrics deal mostly with non-experimental data. Great emphasis is put on specification procedures and misspecification testing. Model specifications tend therefore to be very simple, while great attention is put on the issues of endogeneity of the regressors, dependence\n  structures in the errors and robustness of the estimators under deviations from normality.\n  The preferred approach is often semi- or non-parametric, and heteroskedasticity-consistent\n  techniques are becoming standard practice both in estimation and testing.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238259/402101 \n For all these reasons, [...] panel model estimation in econometrics is mostly\n  accomplished in the generalized least squares framework based on Aitken\u2019s Theorem [...]. On the contrary, longitudinal data\n  models in \nnlme\n and \nlme4\n are estimated by (restricted or unrestricted) maximum likelihood. [...]\n\nnlme\n\nlme4\n\nThe econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on\n  the machine, the expressions for the estimators are usually rather simple. ML estimation of\n  longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence\n  criteria.\n\nI appreciate that @ChristophHanck provided a thorough introduction about the four \nrandom.method\n used in \nplm\n and explained why their estimates are so different. As requested by @amoeba, I will add some thoughts on the mixed models (likelihood-based) and its connection with GLS.\n\nrandom.method\n\nplm\n\nThe likelihood-based method usually assumes a distribution for both the random effect and the error term. A normal distribution assumption is commonly used, but there are also some studies assuming a non-normal distribution. I will follow @ChristophHanck's notations for a random intercept model, and allow unbalanced data, i.e., let $T=n_i$.\n\nThe model is\n\\begin{equation}\ny_{it}= \\boldsymbol x_{it}^{'}\\boldsymbol\\beta + \\eta_i + \\epsilon_{it}\\qquad i=1,\\ldots,m,\\quad t=1,\\ldots,n_i\n\\end{equation}\nwith $\\eta_i \\sim N(0,\\sigma^2_\\eta), \\epsilon_{it} \\sim N(0,\\sigma^2_\\epsilon)$.\n\nFor each $i$, $$\\boldsymbol y_i \\sim N(\\boldsymbol X_{i}\\boldsymbol\\beta, \\boldsymbol\\Sigma_i), \\qquad\\boldsymbol\\Sigma_i = \\sigma^2_\\eta \\boldsymbol 1_{n_i} \\boldsymbol 1_{n_i}^{'} + \\sigma^2_\\epsilon \\boldsymbol I_{n_i}.$$\nSo the log-likelihood function is $$const -\\frac{1}{2} \\sum_i\\mathrm{log}|\\boldsymbol\\Sigma_i| - \\frac{1}{2} \\sum_i(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta)^{'}\\boldsymbol\\Sigma_i^{-1}(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta).$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238259/402101 \n For each $i$, $$\\boldsymbol y_i \\sim N(\\boldsymbol X_{i}\\boldsymbol\\beta, \\boldsymbol\\Sigma_i), \\qquad\\boldsymbol\\Sigma_i = \\sigma^2_\\eta \\boldsymbol 1_{n_i} \\boldsymbol 1_{n_i}^{'} + \\sigma^2_\\epsilon \\boldsymbol I_{n_i}.$$\nSo the log-likelihood function is $$const -\\frac{1}{2} \\sum_i\\mathrm{log}|\\boldsymbol\\Sigma_i| - \\frac{1}{2} \\sum_i(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta)^{'}\\boldsymbol\\Sigma_i^{-1}(\\boldsymbol y_i - \\boldsymbol X_{i}\\boldsymbol\\beta).$$\n\nWhen all the variances are known, as shown in Laird and Ware (1982), the MLE is\n$$\\hat{\\boldsymbol\\beta} = \\left(\\sum_i\\boldsymbol X_i^{'} \\boldsymbol\\Sigma_i^{-1} \\boldsymbol X_i \\right)^{-1} \\left(\\sum_i \\boldsymbol X_i^{'} \\boldsymbol\\Sigma_i^{-1} \\boldsymbol y_i \\right),$$\nwhich is equivalent to the GLS $\\hat\\beta_{RE}$ derived by @ChristophHanck. So the key difference is in the estimation for the variances. Given that there is no closed-form solution, there are several approaches:\n\ndirectly maximization of the log-likelihood function using optimization algorithms;\n\n\nExpectation-Maximization (EM) algorithm: closed-form solutions exist, but the estimator for $\\boldsymbol \\beta$ involves empirical Bayesian estimates of the random intercept;\n\n\na combination of the above two, Expectation/Conditional Maximization Either (ECME) algorithm (Schafer, 1998; R package \nlmm\n). With a different parameterization, closed-form solutions for $\\boldsymbol \\beta$ (as above) and $\\sigma^2_\\epsilon$ exist. The solution for $\\sigma^2_\\epsilon$ can be written as $$\\sigma^2_\\epsilon = \\frac{1}{\\sum_i n_i}\\sum_i(\\boldsymbol y_i - \\boldsymbol X_{i} \\hat{\\boldsymbol\\beta})^{'}(\\hat\\xi \\boldsymbol 1_{n_i} \\boldsymbol 1_{n_i}^{'} + \\boldsymbol I_{n_i})^{-1}(\\boldsymbol y_i - \\boldsymbol X_{i} \\hat{\\boldsymbol\\beta}),$$ where $\\xi$ is defined as $\\sigma^2_\\eta/\\sigma^2_\\epsilon$ and can be estimated in an EM framework.\n\nlmm\n\nIn summary, MLE has distribution assumptions, and it is estimated in an iterative algorithm. The key difference between MLE and GLS is in the estimation for the variances.\n\nCroissant and Millo (2008) pointed out that\n\nWhile under normality, homoskedasticity and no serial correlation of the errors OLS are also the maximum likelihood estimator, in all the other cases there are important differences.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/238259/402101 \n lmm\n\nIn summary, MLE has distribution assumptions, and it is estimated in an iterative algorithm. The key difference between MLE and GLS is in the estimation for the variances.\n\nCroissant and Millo (2008) pointed out that\n\nWhile under normality, homoskedasticity and no serial correlation of the errors OLS are also the maximum likelihood estimator, in all the other cases there are important differences.\n\nIn my opinion, for the distribution assumption, just as the difference between parametric and non-parametric approaches, MLE would be more efficient when the assumption holds, while GLS would be more robust.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/496300/402101 \n This is all philosophical (as it must be):\n\nWhen a probability is a \nfrequency\n you will have difficulty applying it to something that can't happen more than once, like Thursday's match between Liverpool and Arsenal. The problem is that a frequency is of the form \n$k$\n out of every \n$n$\n. You could conceive of a class of Thursdays sufficiently similar to this one (weather, &c.) where Liverpool plays Arsenal in a sufficiently similar way (team compositions, &c.) that you would consider them (almost) repetitions of the same event and so have an idea of how this would go \non average\n, though finding the data might be difficult. But never for a given match, which can only happen this once. Similarly, you can't assign a probability to \nthe next\n coin toss, because there is only one, but you can say something about the class of sufficiently similar coin tosses.\n\nIf a probability is a \ndegree of belief\n, you can assign it to anything that you can have an opinion on if you follow the proper rules of such assignments. It is my belief that the next coin toss is about as probable to come up heads as it is to come up tails.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/100042/402101 \n The correction is called \nBessel's correction\n and it has a mathematical proof. Personally, I was taught it the easy way: using $n-1$ is how you correct the bias of $E[\\frac{1}{n}\\sum_1^n(x_i - \\bar x)^2]$ (see \nhere\n).\n\nYou can also explain the correction based on the concept of degrees of freedom, simulation isn't strictly needed.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/88634/402101 \n To understand what that diagram could mean, we have to define some things.  Let's say that Venn diagram displays the overlapping (or shared) variance amongst 4 different variables, and that we want to predict the level of $Wiki$ by recourse to our knowledge of $Digg$, $Forum$, and $Blog$.  That is, we want to be able to reduce the uncertainty (i.e., variance) in $Wiki$ from the null variance down to the residual variance.  How well can that be done?  That is the question that a \nVenn diagram\n is answering for you.\n\nEach circle represents a set of points, and thereby, an amount of variance.  For the most part, we are interested in the variance in $Wiki$, but the figure also displays the variances in the predictors.  There are a few things to notice about our figure.  First, each variable has the same amount of variance--they are all the same size (although not everyone will use Venn diagrams quite so literally).  Also, there is the same amount of overlap, etc., etc.  A more important thing to notice is that there is a good deal of overlap amongst the predictor variables.  This means that they are correlated.  This situation is very common when dealing with secondary (i.e., archival) data, observational research, or real-world prediction scenarios.  On the other hand, if this were a designed experiment, it would probably imply poor design or execution.  To continue with this example for a little bit longer, we can see that our predictive ability will be moderate; most of the variability in $Wiki$ remains as residual variability after all the variables have been used (eyeballing the diagram, I would guess $R^2\\approx.35$).  Another thing to note is that, once $Digg$ and $Blog$ have been entered into the model, $Forum$ accounts for \nnone\n of the variability in $Wiki$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/88634/402101 \n Now, after having fit a model with multiple predictors, people often want to \ntest\n those predictors to see if they are related to the response variable (although it's not clear this is as important as people seem to believe it is).  Our problem is that to test these predictors, we must \npartition the Sum of Squares\n, and since our predictors are correlated, there are SS that could be attributed to \nmore than one\n predictor.  In fact, in the asterisked region, the SS could be attributed to \nany\n of the three predictors.  This means that there is no \nunique partition\n of the SS, and thus no unique test.  How this issue is handled depends on the \ntype of SS\n that the researcher uses and \nother judgments made by the researcher\n.  Since many software applications return type III SS by default, many people \nthrow away\n the information contained in the overlapping regions \nwithout realizing they have made a judgment call\n.  I explain these issues, the different types of SS, and go into some detail \nhere\n.\n\nThe question, as stated, specifically asks about where all of this shows up in the \nbetas\n / regression equation.  The answer is that it does not.  Some information about that is contained in my answer \nhere\n (although you'll have to read between the lines a little bit).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/110365/402101 \n I'll try to give an intuitive explanation.\n\nThe t-statistic* has a numerator and a denominator. For example, the statistic in the one sample t-test is\n\n$$\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}$$\n\n*(there are several, but this discussion should hopefully be general enough to cover the ones you are asking about)\n\nUnder the assumptions, the numerator has a normal distribution with mean 0 and some unknown standard deviation.\n\nUnder the same set of assumptions, the denominator is an estimate of the standard deviation of the distribution of the numerator (the standard error of the statistic on the numerator). It is independent of the numerator. Its square is a chi-square random variable divided by its degrees of freedom (which is also the d.f. of the t-distribution) times the square of \n$\\sigma_\\text{numerator}$\n.\n\nWhen the degrees of freedom are small, the denominator tends to be fairly right-skew. It has a high chance of being less than its mean, and a relatively good chance of being quite small. At the same time, it also has some chance of being much, much larger than its mean.\n\nUnder the assumption of normality, the numerator and denominator are independent. So if we draw randomly from the distribution of this t-statistic we have a normal random number divided by a second randomly* chosen value from a right-skew distribution that's on average around 1.\n\n* without regard to the normal term\n\nBecause it's on the denominator, the small values in the distribution of the denominator produce very large t-values. The right-skew in the denominator make the t-statistic heavy-tailed. The right tail of the distribution, when on the denominator makes the t-distribution more sharply peaked than a normal with the same standard deviation as the \nt\n.\n\nHowever, as the degrees of freedom become large, the distribution becomes much more normal-looking and much more \"tight\" around its mean.\n\n\n\nAs such, the effect of dividing by the denominator on the shape of the distribution of the numerator reduces as the degrees of freedom increase.\n\nEventually - as Slutsky's theorem might suggest to us could happen  - the effect of the denominator becomes more like dividing by a constant and the distribution of the t-statistic is very close to normal.\n\nwhuber suggested in comments that it might be more illuminating to look at the reciprocal of the denominator. That is, we could write our t-statistics as numerator (normal) times reciprocal-of-denominator (right-skew).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/110365/402101 \n Eventually - as Slutsky's theorem might suggest to us could happen  - the effect of the denominator becomes more like dividing by a constant and the distribution of the t-statistic is very close to normal.\n\nwhuber suggested in comments that it might be more illuminating to look at the reciprocal of the denominator. That is, we could write our t-statistics as numerator (normal) times reciprocal-of-denominator (right-skew).\n\nFor example, our one-sample-t statistic above would become:\n\n$${\\sqrt{n}(\\bar{x}-\\mu_0)}\\cdot{1/s}$$\n\nNow consider the population standard deviation of the original \n$X_i$\n, \n$\\sigma_x$\n. We can multiply and divide by it, like so:\n\n$${\\sqrt{n}(\\bar{x}-\\mu_0)/\\sigma_x}\\cdot{\\sigma_x/s}$$\n\nThe first term is standard normal. The second term (the square root of a scaled inverse-chi-squared random variable) then scales that standard normal by values that are either larger or smaller than 1, \"spreading it out\".\n\nUnder the assumption of normality, the two terms in the product are independent. So if we draw randomly from the distribution of this t-statistic we have a normal random number (the first term in the product) times a second randomly-chosen value (without regard to the normal term) from a right-skew distribution that's 'typically' around 1.\n\nWhen the d.f. are large, the value tends to be very close to 1, but when the df are small, it's quite skew and the spread is large, with the big right tail of this scaling factor making the tail quite fat:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/85/402101 \n A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as \"state\", and then the random variable is a function of the state.\n\nExample:\n\nSuppose we have three dice rolls ($D_{1}$,$D_{2}$,$D_{3}$).  Then the state $S=(D_{1},D_{2},D_{3})$.\n\nOne random variable $X$ is the number of 5s. This is:\n\n$$ X=(D_{1}=5?)+(D_{2}=5?)+(D_{3}=5?)$$\n\nAnother random variable $Y$ is the sum of the dice rolls. This is:\n\n$$ Y=D_{1}+D_{2}+D_{3}  $$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414350/402101 \n This question can almost certainly not be answered well for you by readers at CrossValidated. \nThere is no context-free way to decide whether model metrics such as \n$R^2$\n are good or not\n. At the extremes, it is usually possible to get a consensus from a wide variety of experts: an \n$R^2$\n of almost 1 generally indicates a good model, and of close to 0 indicates a terrible one. In between lies a range where assessments are inherently subjective. In this range, it takes more than just statistical expertise to answer whether your model metric is any good. It takes additional expertise in your area, which CrossValidated readers probably do not have.\n\nWhy is this? Let me illustrate with an example from my own experience (minor details changed).\n\nI used to do microbiology lab experiments. I would set up flasks of cells at different levels of nutrient concentration, and measure the growth in cell density (i.e. slope of cell density against time, though this detail is not important). When I then modelled this growth/nutrient relationship, it was common to achieve \n$R^2$\n values of >0.90.\n\nI am now an environmental scientist. I work with datasets containing measurements from nature. If I try to fit the exact same model described above to these \u2018field\u2019 datasets, I\u2019d be surprised if I the \n$R^2$\n was as high as 0.4.\n\nThese two cases involve exactly the same parameters, with very similar measurement methods, models written and fitted using the same procedures - and even the same person doing the fitting! But in one case, an \n$R^2$\n of 0.7 would be worryingly low, and in the other it would be suspiciously high.\n\nFurthermore, we would take some chemistry measurements alongside the biological measurements. Models for the chemistry standard curves would have \n$R^2$\n around 0.99, and a value of 0.90 would be worryingly \nlow\n.\n\nWhat leads to these big differences in expectations? Context. That vague term covers a vast area, so let me try to separate it into some more specific factors (this is likely incomplete):\n\n1. What is the payoff / consequence / application?\n\nThis is where the nature of your field are likely to be most important. However valuable I think my work is, bumping up my model \n$R^2$\ns by 0.1 or 0.2 is not going to revolutionize the world. But there are applications where that magnitude of change would be a huge deal! A much smaller improvement in a stock forecast model could mean tens of millions of dollars to the firm that develops it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414350/402101 \n 1. What is the payoff / consequence / application?\n\nThis is where the nature of your field are likely to be most important. However valuable I think my work is, bumping up my model \n$R^2$\ns by 0.1 or 0.2 is not going to revolutionize the world. But there are applications where that magnitude of change would be a huge deal! A much smaller improvement in a stock forecast model could mean tens of millions of dollars to the firm that develops it.\n\nThis is even easier to illustrate for classifiers, so I\u2019m going to switch my discussion of metrics from \n$R^2$\n to accuracy for the following example (ignoring \nthe weakness of the accuracy metric\n for the moment). Consider the strange and lucrative world of \nchicken sexing\n. After years of training, a human can rapidly tell the difference between a male and female chick when they are just 1 day old. Males and females are fed differently to optimize meat & egg production, so high accuracy saves huge amounts in misallocated investment in \nbillions\n of birds. Till a few decades ago, accuracies of about 85% were considered high in the US. Nowadays, the value of achieving the very highest accuracy, of around 99%? A salary that can apparently range as high as \n60,000\n to possibly \n180,000\n dollars per year (based on some quick googling). Since humans are still limited in the speed at which they work, machine learning algorithms that can achieve similar accuracy but allow sorting to take place faster could be worth millions.\n\n(I hope you enjoyed the example \u2013 the alternative was a depressing one about very questionable algorithmic identification of terrorists).\n\n2. How strong is the influence of unmodelled factors in your system?\n\nIn many experiments, you have the luxury of isolating the system from all other factors that may influence it (that\u2019s partly the goal of experimentation, after all). Nature is messier. To continue with the earlier microbiology example: cells grow when nutrients are available but other things affect them too \u2013 how hot it is, how many predators there are to eat them, whether there are toxins in the water. All of those covary with nutrients and with each other in complex ways. Each of those other factors drives variation in the data that is not being captured by your model. Nutrients may be unimportant in driving variation \nrelative\n to the other factors, and so if I exclude those other factors, my model of my field data will necessarily have a lower \n$R^2$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414350/402101 \n 3. How precise and accurate are your measurements?\n\nMeasuring the concentration of cells and chemicals can be \nextremely\n precise and accurate. Measuring (for example) the emotional state of a community based on trending twitter hashtags is likely to be\u2026less so. If you cannot be precise in your measurements, it is unlikely that your model can ever achieve a high \n$R^2$\n. How precise are measurements in your field? We probably do not know.\n\n4. Model complexity and generalizability\n\nIf you add more factors to your model, even random ones, you will on average increase the model \n$R^2$\n (adjusted \n$R^2$\n partly addresses this). This is \noverfitting\n. An overfit model will not generalize well to new data i.e. will have higher prediction error than expected based on the fit to the original (training) dataset. This is because it has fit the \nnoise\n in the original dataset. This is partly why models are penalized for complexity in model selection procedures, or subjected to regularization.\n\nIf overfitting is ignored or not successfully prevented, the estimated \n$R^2$\n will be biased upward i.e. higher than it ought to be. In other words, your \n$R^2$\n value can give you a misleading impression of your model\u2019s performance if it is overfit.\n\nIMO, overfitting is surprisingly common in many fields. How best to avoid this is a complex topic, and I recommend reading about \nregularization\n procedures and \nmodel selection\n on this site if you are interested in this.\n\n5. Data range and extrapolation\n\nDoes your dataset extend across a substantial portion of the range of X values you are interested in? Adding new data points outside the existing data range can have a large effect on estimated \n$R^2$\n, since it is a metric based on the variance in X and Y.\n\nAside from this, if you fit a model to a dataset and need to predict a value outside the X range of that dataset (i.e. \nextrapolate\n), you might find that its performance is lower than you expect. This is because the relationship you have estimated might well change outside the data range you fitted. In the figure below, if you took measurements only in the range indicated by the green box, you might imagine that a straight line (in red) described the data well. But if you attempted to predict a value outside that range with that red line, you would be quite incorrect.\n\n\n\n[The figure is an edited version of \nthis one\n, found via a quick google search for 'Monod curve'.]\n\n6. Metrics only give you a piece of the picture",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/414350/402101 \n [The figure is an edited version of \nthis one\n, found via a quick google search for 'Monod curve'.]\n\n6. Metrics only give you a piece of the picture\n\nThis is not really a criticism of the metrics \u2013 they are \nsummaries\n, which means that they also throw away information by design. But it does mean that any single metric leaves out information that can be crucial to its interpretation. A good analysis takes into consideration more than a single metric.\n\nSuggestions, corrections and other feedback welcome. And other answers too, of course.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n I am going to change the order of questions about.\n\nI've found textbooks and lecture notes frequently disagree, and would like a system to work through the choice that can safely be recommended as best practice, and especially a textbook or paper this can be cited to.\n\nUnfortunately, some discussions of this issue in books and so on rely on received wisdom. Sometimes that received wisdom is reasonable, sometimes it is less so (at the least in the sense that it tends to focus on a smaller issue when a larger problem is ignored); we should examine the justifications offered for the advice (if any justification is offered at all) with care.\n\nMost guides to choosing a t-test or non-parametric test focus on the normality issue.\n\nThat\u2019s true, but it\u2019s somewhat misguided for several reasons that I address in this answer.\n\nIf performing an \"unrelated samples\" or \"unpaired\" t-test, whether to use a Welch correction?\n\nThis (to use it unless you have reason to think variances should be equal) is the advice of numerous references. I point to some in this answer.\n\nSome people use a hypothesis test for equality of variances, but here it would have low power. Generally I just eyeball whether the sample SDs are \"reasonably\" close or not (which is somewhat subjective, so there must be a more principled way of doing it) but again, with low n it may well be that the population SDs are rather further apart than the sample ones.\n\nIs it safer simply to always use the Welch correction for small samples, unless there is some good reason to believe population variances are equal?\nThat\u2019s what the advice is. The properties of the tests are affected by the choice based on the assumption test.\n\nSome references on this can be seen \nhere\n and \nhere\n,  though there are more that say similar things.\n\nThe equal-variances issue has many similar characteristics to the normality issue \u2013 people want to test it, advice suggests conditioning choice of tests on the results of tests can adversely affect the results of both kinds of subsequent test \u2013 it\u2019s better simply not to assume what you can\u2019t adequately justify (by reasoning about the data, using information from other studies relating to the same variables and so on).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n The equal-variances issue has many similar characteristics to the normality issue \u2013 people want to test it, advice suggests conditioning choice of tests on the results of tests can adversely affect the results of both kinds of subsequent test \u2013 it\u2019s better simply not to assume what you can\u2019t adequately justify (by reasoning about the data, using information from other studies relating to the same variables and so on).\n\nHowever,  there are differences. One is that \u2013 at least in terms of the distribution of the test statistic under the null hypothesis (and hence, its level-robustness) - non-normality is less important in large samples (at least in respect of significance level, though power might still be an issue if you need to find small effects), while the effect of unequal variances under the equal variance assumption doesn\u2019t really go away with large sample size.\n\nWhat principled method can be recommended for choosing which is the most appropriate test when the sample size is \"small\"?\n\nWith hypothesis tests, what matters (under some set of conditions) is primarily two things:\n\nWhat is the actual type I error rate?\n\n\n\n\nWhat is the power behaviour like?\n\nWhat is the actual type I error rate?\n\nWhat is the power behaviour like?\n\nWe also need to keep in mind that if we're comparing two procedures, changing the first will change the second (that is, if they\u2019re not conducted at the same actual significance level, you would expect that higher \n$\\alpha$\n is associated with higher power).\n\n(Of course we're usually not so confident we know what distributions we're dealing with, so the sensitivity of those behaviors to changes in circumstances also matter.)\n\nWith these small-sample issues in mind, is there a good - hopefully citable - checklist to work through when deciding between t and non-parametric tests?\n\nI will consider a number of situations in which I\u2019ll make some recommendations, considering both the possibility of non-normality and unequal variances. In every case, take mention of the t-test to imply the Welch-test:\n\nn medium-large\n\nNon-normal (or unknown), likely to have near-equal variance:\n\nIf the distribution is heavy-tailed, you will generally be better with a Mann-Whitney, though if it\u2019s only slightly heavy, the t-test should do okay. With light-tails the t-test may (often) be preferred.  Permutation tests are a good option (you can even do a permutation test using a t-statistic if you're so inclined). Bootstrap tests are also suitable.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n n medium-large\n\nNon-normal (or unknown), likely to have near-equal variance:\n\nIf the distribution is heavy-tailed, you will generally be better with a Mann-Whitney, though if it\u2019s only slightly heavy, the t-test should do okay. With light-tails the t-test may (often) be preferred.  Permutation tests are a good option (you can even do a permutation test using a t-statistic if you're so inclined). Bootstrap tests are also suitable.\n\nNon-normal (or unknown), unequal variance (or variance relationship unknown):\n\nIf the distribution is heavy-tailed, you will generally be better with a Mann-Whitney\n\nif inequality of variance is only related to inequality of mean - i.e. if H0 is true the difference in spread should also be absent. GLMs are often a good option, especially if there\u2019s skewness and spread is related to the mean. A permutation test is another option, with a similar caveat as for the rank-based tests. Bootstrap tests are a good possibility here.\n\nZimmerman and Zumbo (1993)\n$^{[1]}$\n suggest a Welch-t-test on the ranks which they say performs better that the Wilcoxon-Mann-Whitney in cases where the variances are unequal.\n\nn moderately small\n\nrank tests are reasonable defaults here if you expect non-normality (again with the above caveat). If you have external information about shape or variance, you might consider GLMs . If you expect things not to be too far from normal, t-tests may be fine.\n\nn very small\n\nBecause of the problem with getting suitable significance levels, neither permutation tests nor rank tests may be suitable, and at the smallest sizes, a t-test may be the best option (there\u2019s some possibility of slightly robustifying it). However, there\u2019s a good argument for using higher type I error rates with small samples (otherwise you\u2019re letting type II error rates inflate while holding type I error rates constant).\nAlso see de Winter (2013)\n$^{[2]}$\n.\n\nThe advice must be modified somewhat when the distributions are both strongly skewed and very discrete, such as Likert scale items where most of the observations are in one of the end categories. Then the Wilcoxon-Mann-Whitney isn\u2019t necessarily a better choice than the t-test.\n\nSimulation can help guide choices further when you have some information about likely circumstances.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n The advice must be modified somewhat when the distributions are both strongly skewed and very discrete, such as Likert scale items where most of the observations are in one of the end categories. Then the Wilcoxon-Mann-Whitney isn\u2019t necessarily a better choice than the t-test.\n\nSimulation can help guide choices further when you have some information about likely circumstances.\n\nI appreciate this is something of a perennial topic, but most questions concern the questioner's particular data set, sometimes a more general discussion of power, and occasionally what to do if two tests disagree, but I would like a procedure to pick the correct test in the first place!\n\nThe main problem is how hard it is to check the normality assumption in a small data set:\n\nIt \nis\n difficult to check normality in a small data set, and to some extent that's an important issue, but I think there's another issue of importance that we need to consider. A basic problem is that trying to assess normality as the basis of choosing between tests adversely impacts the properties of the tests you're choosing between.\n\nAny formal test for normality would have low power so violations may well not be detected. (Personally I wouldn't test for this purpose, and I'm clearly not alone, but\nI've found this little use when clients demand a normality test be performed because that's what their textbook or old lecture notes or some website they found once declare should be done. This is one point where a weightier looking citation would be welcome.)\n\nHere\u2019s an example of a reference (there are others)  which is unequivocal (Fay and Proschan, 2010\n$^{[3]}$\n):\n\nThe choice between t- and WMW DRs should not be based on a test of normality.\n\nThey are similarly unequivocal about not testing for equality of variance.\n\nTo make matters worse, it is unsafe to use the Central Limit Theorem as a safety net: for small n we can't rely on the convenient asymptotic normality of the test statistic and t distribution.\n\nNor even in large samples -- asymptotic normality of the numerator doesn\u2019t imply that the t-statistic will have a t-distribution. However, that may not matter so much, since  you should still have asymptotic normality (e.g. CLT for the numerator, and Slutsky\u2019s theorem suggest that eventually the t-statistic should begin to look normal, if the conditions for both hold.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n Nor even in large samples -- asymptotic normality of the numerator doesn\u2019t imply that the t-statistic will have a t-distribution. However, that may not matter so much, since  you should still have asymptotic normality (e.g. CLT for the numerator, and Slutsky\u2019s theorem suggest that eventually the t-statistic should begin to look normal, if the conditions for both hold.)\n\nOne principled response to this is \"safety first\": as there's no way to reliably verify the normality assumption on a small sample, run an equivalent non-parametric test instead.\n\nThat\u2019s actually the advice that the references I mention (or link to mentions of) give.\n\nAnother approach I've seen but feel less comfortable with, is to perform a visual check and proceed with a t-test if nothing untowards is observed (\"no reason to reject normality\", ignoring the low power of this check). My personal inclination is to consider whether there are any grounds for assuming normality, theoretical (e.g. variable is sum of several random components and CLT applies) or empirical (e.g. previous studies with larger n suggest variable is normal).\n\nBoth those are good arguments, especially when backed up with the fact that the t-test is reasonably robust against moderate deviations from normality.\n(One should keep in mind, however, that \"moderate deviations\" is a tricky phrase; certain kinds of deviations from normality may impact the power performace of the t-test quite a bit even though those deviations are visually very small - the t-test is less robust to some deviations than others. We should keep this in mind whenever we're discussing small deviations from normality.)\n\nBeware, however, the phrasing \"suggest the variable is normal\". Being reasonably consistent with normality is not the same thing as normality. We can often reject actual normality with no need even to see the data \u2013 for example, if the data cannot be negative, the distribution cannot be normal. Fortunately, what matters is closer to what we might actually have from previous studies or reasoning about how the data are composed, which is that the deviations from normality should be small.\n\nIf so, I would use a t-test if data passed visual inspection, and otherwise stick to non-parametrics. But any theoretical or empirical grounds usually only justify assuming approximate normality, and on low degrees of freedom it's hard to judge how near normal it needs to be to avoid invalidating a t-test.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n If so, I would use a t-test if data passed visual inspection, and otherwise stick to non-parametrics. But any theoretical or empirical grounds usually only justify assuming approximate normality, and on low degrees of freedom it's hard to judge how near normal it needs to be to avoid invalidating a t-test.\n\nWell, that\u2019s something we can assess the impact of fairly readily (such as via simulations, as I mentioned earlier). From what I've seen, skewness seems to matter more than heavy tails (but on the other hand I have seen some claims of the opposite - though I don't know what that's based on).\n\nFor people who see the choice of methods as a trade-off between power and robustness, claims about the asymptotic efficiency of the non-parametric methods are unhelpful. For instance, the rule of thumb that \"Wilcoxon tests have about 95% of the power of a t-test if the data really are normal, and are often far more powerful if the data is not, so just use a Wilcoxon\" is sometimes heard, but if the 95% only applies to large n, this is flawed reasoning for smaller samples.\n\nBut we can check small-sample power quite easily! It\u2019s easy enough to simulate to obtain power curves \nas here\n.\n\n(Again, also see de Winter (2013)\n$^{[2]}$\n).\n\nHaving done such simulations under a variety of circumstances, both for the two-sample and one-sample/paired-difference cases, the small sample efficiency at the normal in both cases seems to be a little lower than the asymptotic efficiency, but the efficiency of the signed rank and Wilcoxon-Mann-Whitney tests is still very high even at very small sample sizes.\n\nAt least that's if the tests are done at the same actual significance level; you can't do a 5% test with very small samples (and least not without randomized tests for example), but if you're prepared to perhaps do (say) a 5.5% or a 3.2% test instead, then the rank tests hold up very well indeed compared with a t-test at that significance level.\n\nSmall samples may make it very difficult, or impossible, to assess whether a transformation is appropriate for the data since it's hard to tell whether the transformed data belong to a (sufficiently) normal distribution. So if a QQ plot reveals very positively skewed data, which look more reasonable after taking logs, is it safe to use a t-test on the logged data? On larger samples this would be very tempting, but with small n I'd probably hold off unless there had been grounds to expect a log-normal distribution in the first place.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n There\u2019s another alternative: make a different parametric assumption. For example, if there\u2019s skewed data, one might, for example, in some situations reasonably consider a gamma distribution, or some other skewed family as a better approximation - in moderately large samples, we might just use a GLM, but in very small samples it may be necessary to look to a small-sample test - in many cases simulation can be useful.\n\nAlternative 2: robustify the t-test (but taking care about the choice of robust procedure so as not to heavily discretize the resulting distribution of the test statistic)  -  this has some advantages over a very-small-sample nonparametric procedure such as the ability to consider tests with low type I error rate.\n\nHere I'm thinking along the lines of using say M-estimators of location (and related estimators of scale) in the t-statistic to smoothly robustify against deviations from normality. Something akin to the Welch, like:\n\n$$\\frac{\\stackrel{\\sim}{x}-\\stackrel{\\sim}{y}}{\\stackrel{\\sim}{S}_p}$$\n\nwhere \n$\\stackrel{\\sim}{S}_p^2=\\frac{\\stackrel{\\sim}{s}_x^2}{n_x}+\\frac{\\stackrel{\\sim}{s}_y^2}{n_y}$\n and \n$\\stackrel{\\sim}{x}$\n, \n$\\stackrel{\\sim}{s}_x$\n etc being robust estimates of location and scale respectively.\n\nI'd aim to reduce any tendency of the statistic to discreteness - so I'd avoid things like trimming and Winsorizing, since if the original data were discrete, trimming etc will exacerbate this; by using M-estimation type approaches with a smooth \n$\\psi$\n-function you achieve similar effects without contributing to the discreteness. Keep in mind we're trying to deal with the situation where \n$n$\n is very small indeed (around 3-5, in each sample, say), so even M-estimation potentially has its issues.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n You could, for example, use simulation at the normal to get p-values (if sample sizes are very small, I'd suggest that over bootstrapping - if sample sizes aren't so small, a carefully-implemented bootstrap may do quite well, but then we might as well go back to Wilcoxon-Mann-Whitney). There's be a scaling factor as well as a d.f. adjustment to get to what I'd imagine would then be a reasonable t-approximation. This means we should get the kind of properties we seek very close to the normal, and should have reasonable robustness in the broad vicinity of the normal. There are a number of issues that come up that would be outside the scope of the present question, but I think in very small samples the benefits should outweigh the costs and the extra effort required.\n\n[I haven't read the literature on this stuff for a very long time, so I don't have suitable references to offer on that score.]\n\nOf course if you didn't expect the distribution to be somewhat normal-like, but rather similar to some other distribution, you could undertake a suitable robustification of a different parametric test.\n\nWhat if you want to check assumptions for the non-parametrics? Some sources recommend verifying a symmetric distribution before applying a Wilcoxon test, which brings up similar problems to checking normality.\n\nIndeed. I assume you mean the signed rank test*. In the case of using it on paired data, if you are prepared to assume that the two distributions are the same shape apart from location shift you are safe, since the differences should then be symmetric. Actually, we don't even need that much; for the test to work you need symmetry under the null; it's not required under the alternative (e.g. consider a paired situation with identically-shaped right skewed continuous distributions on the positive half-line, where the scales differ under the alternative but not under the null; the signed rank test should work essentially as expected in that case). The interpretation of the test is easier if the alternative is a location shift though.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122629/402101 \n *(Wilcoxon\u2019s name is  associated with both the one and two sample rank tests \u2013 signed rank and rank sum; with their U test, Mann and Whitney generalized the situation studied by Wilcoxon, and introduced important new ideas for evaluating the null distribution, but the priority between the two sets of authors on the Wilcoxon-Mann-Whitney is clearly Wilcoxon\u2019s -- so at least if we only consider Wilcoxon vs Mann&Whitney, Wilcoxon goes first in my book. However, it seems \nStigler's Law\n beats me yet again, and Wilcoxon should perhaps share some of that priority with a number of earlier contributors, and (besides Mann and Whitney) should share credit with several discoverers of an equivalent test.[4][5] )\n\nReferences\n\n[1]: Zimmerman DW and Zumbo BN, (1993),\n\n\nRank transformations and the power of the Student t-test and Welch t\u2032-test for non-normal populations,\n\nCanadian Journal Experimental Psychology, \n47\n: 523\u201339.\n\n[2]: J.C.F. de Winter (2013),\n\n\"Using the Student\u2019s t-test with extremely small sample sizes,\"\n\n\nPractical Assessment, Research and Evaluation\n,  \n18\n:10, August, ISSN 1531-7714\n\n\nhttps://openpublishing.library.umass.edu/pare/article/id/1434/\n\n[3]: Michael P. Fay and Michael A. Proschan (2010),\n\n\"Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules,\"\n\n\nStat Surv\n; \n4\n: 1\u201339.\n\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/\n\n[4]: Berry, K.J., Mielke, P.W. and Johnston, J.E. (2012),\n\n\"The Two-sample Rank-sum Test: Early Development,\"\n\n\nElectronic Journal for History of Probability and Statistics\n, Vol.8, December\n\n\npdf\n\n[5]: Kruskal, W. H. (1957),\n\n\"Historical notes on the Wilcoxon unpaired two-sample test,\"\n\n\nJournal of the American Statistical Association\n, \n52\n, 356\u2013360.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32610/402101 \n The process is iterative,\n but there is a natural order:\n\nYou have to worry first about \nconditions that cause outright numerical errors\n. Multicollinearity is one of those, because it can produce unstable systems of equations potentially resulting in outright incorrect answers (to 16 decimal places...)  Any problem here usually means you cannot proceed until it is fixed.  Multicollinearity is usually diagnosed using Variance Inflation Factors and similar examination of the \"hat matrix.\"  Additional checks at this stage can include assessing the influence of any missing values in the dataset and verifying the identifiability of important parameters.  (Missing combinations of discrete independent variables can sometimes cause trouble here.)\n\n\nNext you need to be concerned \nwhether the output reflects most of the data\n or is sensitive to a small subset.  In the latter case, everything else you subsequently do may be misleading, so it is to be avoided.  Procedures include examination of outliers and of \nleverage\n.  (A high-leverage datum might not be an outlier but even so it may unduly influence all the results.) If a robust alternative to the regression procedure exists, this is a good time to apply it: check that it is producing similar results and use it to detect outlying values.\n\n\nFinally, having achieved a situation that is numerically stable (so you can trust the computations) and which reflects the full dataset, you turn to an \nexamination of the statistical assumptions needed for correct interpretation of the output\n.  Primarily these concerns focus--in rough order of importance--on distributions of the residuals (including heteroscedasticity, but also extending to symmetry, distributional shape, possible correlation with predicted values or other variables, and autocorrelation), goodness of fit (including the possible need for interaction terms), whether to re-express the dependent variable, and whether to re-express the independent variables.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32610/402101 \n You have to worry first about \nconditions that cause outright numerical errors\n. Multicollinearity is one of those, because it can produce unstable systems of equations potentially resulting in outright incorrect answers (to 16 decimal places...)  Any problem here usually means you cannot proceed until it is fixed.  Multicollinearity is usually diagnosed using Variance Inflation Factors and similar examination of the \"hat matrix.\"  Additional checks at this stage can include assessing the influence of any missing values in the dataset and verifying the identifiability of important parameters.  (Missing combinations of discrete independent variables can sometimes cause trouble here.)\n\nNext you need to be concerned \nwhether the output reflects most of the data\n or is sensitive to a small subset.  In the latter case, everything else you subsequently do may be misleading, so it is to be avoided.  Procedures include examination of outliers and of \nleverage\n.  (A high-leverage datum might not be an outlier but even so it may unduly influence all the results.) If a robust alternative to the regression procedure exists, this is a good time to apply it: check that it is producing similar results and use it to detect outlying values.\n\nFinally, having achieved a situation that is numerically stable (so you can trust the computations) and which reflects the full dataset, you turn to an \nexamination of the statistical assumptions needed for correct interpretation of the output\n.  Primarily these concerns focus--in rough order of importance--on distributions of the residuals (including heteroscedasticity, but also extending to symmetry, distributional shape, possible correlation with predicted values or other variables, and autocorrelation), goodness of fit (including the possible need for interaction terms), whether to re-express the dependent variable, and whether to re-express the independent variables.\n\nAt any stage, if something needs to be corrected then it's wise to return to the beginning.  Repeat as many times as necessary.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/395871/402101 \n Questions\n\nQ0: The time series looks rather right-skewed and the level shift is accompanied by a scale shift. Hence, I would analyze the time series in logs rather than levels, i.e., with multiplicative rather than additive errors. In logs, it seems that an AR(1) model works quite well in each segment. See e.g. \nacf()\n and \npacf()\n before and after the break.\n\nacf()\n\npacf()\n\npacf(log(window(myts1, end = c(2018, 136))))\npacf(log(window(myts1, start = c(2018, 137))))\n\npacf(log(window(myts1, end = c(2018, 136))))\npacf(log(window(myts1, start = c(2018, 137))))\n\nQ1: For a time series without breaks in the mean, you can simply use the squared (or absolute) residuals and run a test for level shifts again. Alternatively, you can run tests and breakpoint estimation based on a maximum likelihood model where the error variance is another model parameter in addition to the regression coefficients. This is Zeileis \net al.\n (2010, \ndoi:10.1016/j.csda.2009.12.005\n). The corresponding score-based CUSUM tests are available in \nstrucchange\n as well but the breakpoint estimation is in \nfxregime\n. Finally, in the absence of regressors when looking only for changes in mean and variance the \nchangepoint\n R package also provides dedicated functions.\n\nstrucchange\n\nfxregime\n\nchangepoint\n\nHaving said that, it seems that a least-squares approach (treating the variance as a nuisance parameter) is sufficient for the time series you posted. See below.\n\nQ2: Yes. I would simply fit separate models to each segment and analyze these \"as usual\" Bai & Perron (2003, \nJournal of Applied Econometrics\n) also argue that this is justified asymptotically due to the faster convergence of the breakpoint estimates (with rate \n$n$\n rather than \n$\\sqrt{n}$\n).\n\nQ3: I'm not fully sure what you are looking for here. If you want to run the tests sequentially to monitor incoming data, then you should adopt a formal monitoring approach. This is also discussed in Zeileis \net al.\n (2010).\n\nAnalysis code snippets:\n\nCombine log series with its lags for subsequent regression.\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nTesting with supF and score-based CUSUM tests:\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\n\n\nsc <- efp(y ~ y1, data = d, type = \"Score-CUSUM\")\nplot(sc, functional = NULL)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/395871/402101 \n Analysis code snippets:\n\nCombine log series with its lags for subsequent regression.\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nTesting with supF and score-based CUSUM tests:\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\n\n\nsc <- efp(y ~ y1, data = d, type = \"Score-CUSUM\")\nplot(sc, functional = NULL)\n\nsc <- efp(y ~ y1, data = d, type = \"Score-CUSUM\")\nplot(sc, functional = NULL)\n\n\n\nThis highlights that both intercept and autocorrelation coefficient change significantly at the time point visible in the original time series. There is also some fluctuation in the variance but this is not significant at 5% level.\n\nA BIC-based dating also clearly finds this one breakpoint:\n\nbp <- breakpoints(y ~ y1, data = d)\ncoef(bp)\n##                       (Intercept)        y1\n## 2016(123) - 2018(136)    3.926381 0.3858473\n## 2018(137) - 2019(1)      3.778685 0.2845176\n\nbp <- breakpoints(y ~ y1, data = d)\ncoef(bp)\n##                       (Intercept)        y1\n## 2016(123) - 2018(136)    3.926381 0.3858473\n## 2018(137) - 2019(1)      3.778685 0.2845176\n\nClearly, the mean drops but also the autocorrelation slightly. The fitted model in logs is then:\n\nplot(log(myts1), col = \"lightgray\", lwd = 2)\nlines(fitted(bp))\nlines(confint(bp))\n\nplot(log(myts1), col = \"lightgray\", lwd = 2)\nlines(fitted(bp))\nlines(confint(bp))\n\n\n\nRe-fitting the model to each segments can then be done via:\n\nsummary(lm(y ~ y1, data = window(d, end = c(2018, 136))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, end = c(2018, 136)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.73569 -0.18457 -0.04354  0.12042  1.89052 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.92638    0.21656   18.13   <2e-16 ***\n## y1           0.38585    0.03383   11.40   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2999 on 742 degrees of freedom\n## Multiple R-squared:  0.1491, Adjusted R-squared:  0.148 \n## F-statistic: 130.1 on 1 and 742 DF,  p-value: < 2.2e-16",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/395871/402101 \n summary(lm(y ~ y1, data = window(d, end = c(2018, 136))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, end = c(2018, 136)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.73569 -0.18457 -0.04354  0.12042  1.89052 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.92638    0.21656   18.13   <2e-16 ***\n## y1           0.38585    0.03383   11.40   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2999 on 742 degrees of freedom\n## Multiple R-squared:  0.1491, Adjusted R-squared:  0.148 \n## F-statistic: 130.1 on 1 and 742 DF,  p-value: < 2.2e-16\n\n\n\nsummary(lm(y ~ y1, data = window(d, start = c(2018, 137))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, start = c(2018, 137)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.43663 -0.13953 -0.03408  0.09028  0.99777 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.61558    0.33468   10.80  < 2e-16 ***\n## y1           0.31567    0.06327    4.99  1.2e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2195 on 227 degrees of freedom\n## Multiple R-squared:  0.09883,    Adjusted R-squared:  0.09486 \n## F-statistic:  24.9 on 1 and 227 DF,  p-value: 1.204e-06\n\nsummary(lm(y ~ y1, data = window(d, start = c(2018, 137))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, start = c(2018, 137)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.43663 -0.13953 -0.03408  0.09028  0.99777 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.61558    0.33468   10.80  < 2e-16 ***\n## y1           0.31567    0.06327    4.99  1.2e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2195 on 227 degrees of freedom\n## Multiple R-squared:  0.09883,    Adjusted R-squared:  0.09486 \n## F-statistic:  24.9 on 1 and 227 DF,  p-value: 1.204e-06",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214277/402101 \n The i.i.d. assumption about the pairs \n$(\\mathbf{X}_i, y_i)$\n, \n$i = 1, \\ldots, N$\n, is often made in statistics and in machine learning. Sometimes for a good reason, sometimes out of convenience and sometimes just because we usually make this assumption. To satisfactorily answer if the assumption is really necessary, and what the consequences are of not making this assumption, I would easily end up writing a book (if you ever easily end up doing something like that). Here I will try to give a brief overview of what I find to be the most important aspects.\n\nLet's assume that we want to learn a probability model of \n$y$\n given \n$\\mathbf{X}$\n, which we call \n$p(y \\mid \\mathbf{X})$\n. We do not make any assumptions about this model a priori, but we will make the minimal assumption that such a model exists such that\n\nthe conditional distribution of \n$y_i$\n given \n$\\mathbf{X}_i$\n is \n$p(y_i \\mid \\mathbf{X}_i)$\n.\n\nWhat is worth noting about this assumption is that the conditional distribution of \n$y_i$\n depends on \n$i$\n only through \n$\\mathbf{X}_i$\n. This is what makes the model useful, e.g. for prediction. The assumption holds as a consequence of the \nidentically distributed\n part under the i.i.d. assumption, but it is weaker because we don't make any assumptions about the \n$\\mathbf{X}_i$\n's.\n\nIn the following the focus will mostly be on the role of independence.\n\nThere are two major approaches to learning a model of \n$y$\n given \n$\\mathbf{X}$\n. One approach is known as \ndiscriminative\n modelling and the other as \ngenerative\n modelling.\n\nDiscriminative modelling\n: We model \n$p(y \\mid \\mathbf{X})$\n directly, e.g. a logistic regression model, a neural network, a tree or a random forest. The \nworking modelling assumption\n will typically be that the \n$y_i$\n's are conditionally independent given the \n$\\mathbf{X}_i$\n's, though estimation techniques relying on subsampling or bootstrapping make most sense under the i.i.d. or the weaker exchangeability assumption (see below). But generally, for discriminative modelling we don't need to make distributional assumptions about the \n$\\mathbf{X}_i$\n's.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214277/402101 \n Generative modelling\n: We model the joint distribution, \n$p(\\mathbf{X}, y)$\n, of \n$(\\mathbf{X}, y)$\n typically by modelling the conditional distribution \n$p(\\mathbf{X} \\mid y)$\n and the marginal distribution \n$p(y)$\n. Then we use Bayes's formula for computing \n$p(y \\mid \\mathbf{X})$\n. Linear discriminant analysis and naive Bayes methods are examples. The \nworking modelling assumption\n will typically be the i.i.d. assumption.\n\nFor both modelling approaches the working modelling assumption is used to derive or propose learning methods (or estimators). That could be by maximising the (penalised) log-likelihood, minimising the empirical risk or by using Bayesian methods. Even if the working modelling assumption is wrong, the resulting method can still provide a sensible fit of \n$p(y \\mid \\mathbf{X})$\n.\n\nSome techniques used together with discriminative modelling, such as bagging (bootstrap aggregation), work by fitting many models to data sampled randomly from the dataset. Without the i.i.d. assumption (or exchangeability) the resampled datasets will not have a joint distribution similar to that of the original dataset. Any dependence structure has become \"messed up\" by the resampling. I have not thought deeply about this, but I don't see why that should necessarily break the method as a method for learning \n$p(y \\mid \\mathbf{X})$\n. At least not for methods based on the working independence assumptions. I am happy to be proved wrong here.\n\nA central question for all learning methods is whether they result in models close to \n$p(y \\mid \\mathbf{X})$\n. There is a vast theoretical literature in statistics and machine learning dealing with consistency and error bounds. A main goal of this literature is to prove that the learned model is close to \n$p(y \\mid \\mathbf{X})$\n when \n$N$\n is large. Consistency is a qualitative assurance, while error bounds provide (semi-) explicit quantitative control of the closeness and give rates of convergence.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214277/402101 \n The theoretical results all rely on assumptions about the joint distribution of the observations in the dataset. Often the working modelling assumptions mentioned above are made (that is, conditional independence for discriminative modelling and i.i.d. for generative modelling). For discriminative modelling, consistency and error bounds will require that the \n$\\mathbf{X}_i$\n's fulfil certain conditions. In classical regression one such condition is that \n$\\frac{1}{N} \\mathbb{X}^T \\mathbb{X} \\to \\Sigma$\n for \n$N \\to \\infty$\n, where \n$\\mathbb{X}$\n denotes the design matrix with rows \n$\\mathbf{X}_i^T$\n. Weaker conditions may be enough for consistency. In sparse learning another such condition is the restricted eigenvalue condition, see e.g. \nOn the conditions used to prove oracle results for the Lasso\n. The i.i.d. assumption together with some technical distributional assumptions imply that some such sufficient conditions are fulfilled with large probability, and thus the i.i.d. assumption may prove to be a sufficient but not a necessary assumption to get consistency and error bounds for discriminative modelling.\n\nThe working modelling assumption of independence may be wrong for either of the modelling approaches. As a rough rule-of-thumb one can still expect consistency if the data comes from an \nergodic process\n, and one can still expect some error bounds if the process is \nsufficiently fast mixing\n. A precise mathematical definition of these concepts would take us too far away from the main question. It is enough to note that there exist dependence structures besides the i.i.d. assumption for which the learning methods can be proved to work as \n$N$\n tends to infinity.\n\nIf we have more detailed knowledge about the dependence structure, we may choose to replace the working independence assumption used for modelling with a model that captures the dependence structure as well. This is often done for time series. A better working model may result in a more efficient method.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214277/402101 \n If we have more detailed knowledge about the dependence structure, we may choose to replace the working independence assumption used for modelling with a model that captures the dependence structure as well. This is often done for time series. A better working model may result in a more efficient method.\n\nRather than proving that the learning method gives a model close to \n$p(y \\mid \\mathbf{X})$\n it is of great practical value to obtain a (relative) assessment of \"how good a learned model is\". Such assessment scores are comparable for two or more learned models, but they will not provide an absolute assessment of how close a learned model is to \n$p(y \\mid \\mathbf{X})$\n. Estimates of assessment scores are typically computed empirically based on splitting the dataset into a training and a test dataset or by using cross-validation.\n\nAs with bagging, a random splitting of the dataset will \"mess up\" any dependence structure. However, for methods based on the working independence assumptions, ergodicity assumptions weaker than i.i.d. should be sufficient for the assessment estimates to be reasonable, though standard errors on these estimates will be very difficult to come up with.\n\n[\nEdit:\n Dependence among the variables will result in a distribution of the learned model that differs from the distribution under the i.i.d. assumption. The estimate produced by cross-validation is not obviously related to the generalization error. If the dependence is strong, it will most likely be a poor estimate.]\n\nAll the above is under the assumption that there is a fixed conditional probability model, \n$p(y \\mid \\mathbf{X})$\n. Thus there cannot be trends or sudden changes in the conditional distribution not captured by \n$\\mathbf{X}$\n.\n\nWhen learning a model of \n$y$\n given \n$\\mathbf{X}$\n, independence plays a role as\n\na useful working modelling assumption that allows us to derive learning methods\n\n\na sufficient but not necessary assumption for proving consistency and providing error bounds\n\n\na sufficient but not necessary assumption for using random data splitting techniques such as bagging for learning and cross-validation for assessment.\n\nTo understand precisely what alternatives to i.i.d. that are also sufficient is non-trivial and to some extent a research subject.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/113027/402101 \n Standardization isn't required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization. For example, if you use Newton-Raphson to maximize the likelihood, standardizing the features makes the convergence faster. Otherwise, you can run your logistic regression without any standardization treatment on the features.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/36013/402101 \n In a word, \nyes\n. I believe there are still clear situations where sampling is appropriate, within and without the \"big data\" world, but the nature of big data will certainly change our approach to sampling, and we will use more datasets that are nearly complete representations of the underlying population.\n\nOn sampling:\n Depending on the circumstances it will almost always be clear if sampling is an appropriate thing to do. Sampling is not an inherently beneficial activity; it is just what we do because we need to make tradeoffs on the cost of implementing data collection. We are trying to characterize populations and need to select the appropriate method for gathering and analyzing data about the population. Sampling makes sense when the marginal cost of a method of data collection or data processing is high. Trying to reach 100% of the population is not a good use of resources in that case, because you are often better off addressing things like non-response bias than making tiny improvements in the random sampling error.\n\nHow is big data different?\n \"Big data\" addresses many of the same questions we've had for ages, but what's \"new\" is that the data collection happens off an existing, computer-mediated process, so the marginal cost of collecting data is essentially zero. This dramatically reduces our need for sampling.\n\nWhen will we still use sampling?\n If your \"big data\" population is the right population for the problem, then you will only employ sampling in a few cases: the need to run separate experimental groups, or if the sheer volume of data is too large to capture and process (many of us can handle millions of rows of data with ease nowadays, so the boundary here is getting further and further out). If it seems like I'm dismissing your question, it's probably because I've rarely encountered situations where the volume of the data was a concern in either the collection or processing stages, although I know many have",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/36013/402101 \n The situation that seems hard to me is when your \"big data\" population doesn't perfectly represent your target population, so the tradeoffs are more apples to oranges. Say you are a regional transportation planner, and Google has offered to give you access to its Android GPS navigation logs to help you. While the dataset would no doubt be interesting to use, the population would probably be systematically biased against the low-income, the public-transportation users, and the elderly. In such a situation, traditional travel diaries sent to a random household sample, although costlier and smaller in number, could still be the superior method of data collection. But, this is not simply a question of \"sampling vs. big data\", it's a question of which population combined with the relevant data collection and analysis methods you can apply to that population will best meet your needs.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/366419/402101 \n See this \nsecond post\n for McDonald's feedback on my answer where the notion of risk consistency is related to stability.\n\nYour question is difficult to answer because it mentions two very different topics: \nuniqueness\n and \nstability\n.\n\nIntuitively, a solution is \nunique\n if given a fixed data set, the algorithm always produces the same results. Martin's answer cover's this point in great detail. \n\n\nStability\n on the other hand can be intuitively understood as one for which the prediction does not change much when the training data is modified slightly.\n\nIntuitively, a solution is \nunique\n if given a fixed data set, the algorithm always produces the same results. Martin's answer cover's this point in great detail.\n\nStability\n on the other hand can be intuitively understood as one for which the prediction does not change much when the training data is modified slightly.\n\nStability applies to your question because Lasso feature selection is (often) performed via Cross Validation, hence the Lasso algorithm is performed on different folds of data and may yield different results each time.\n\nUsing the definition from \nhere\n if we define \nUniform stability\n as:\n\nAn algorithm has uniform stability $\\beta$ with respect to the loss\n  function $V$ if the following holds:\n\n$$\\forall S \\in Z^m \\ \\ \\forall i \\in \\{ 1,...,m\\}, \\ \\ \\sup |\n> V(f_s,z) - V(f_{S^{|i},z})  |\\  \\ \\leq \\beta$$\n\nConsidered as a function of $m$, the term $\\beta$ can be written as\n  $\\beta_m$. We say the algorithm is stable when $\\beta_m$ decreases as\n  $\\frac{1}{m}$.\n\nthen the \n\"No Free Lunch Theorem, Xu and Caramis (2012)\"\n states that\n\nIf an algorithm is \nsparse\n, in the sense that it identifies redundant features, then that algorithm is \nnot stable\n (and the uniform stability bound $\\beta$ does not go to zero). [...] If an algorithm is stable, then there is no hope that it will be sparse. (pages 3 and 4)\n\nFor instance, $L_2$ regularized regression is stable and does not identify redundant features, while $L_1$ regularized regression (Lasso) is unstable.\n\nI think 'lasso favors a sparse solution' is not an answer to why use lasso for feature selection\n\nI disagree, the reason Lasso is used for feature selection is that it yields a \nsparse\n solution and can be shown to have the IRF property, i.e. Identifies Redundant Features.\n\nWhat is the most crucial reason that causes this instability\n\nThe No Free Lunch Theorem",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/366419/402101 \n I think 'lasso favors a sparse solution' is not an answer to why use lasso for feature selection\n\nI disagree, the reason Lasso is used for feature selection is that it yields a \nsparse\n solution and can be shown to have the IRF property, i.e. Identifies Redundant Features.\n\nWhat is the most crucial reason that causes this instability\n\nThe No Free Lunch Theorem\n\nThis is not to say that the combination of Cross Validation and Lasso doesn't work... in fact it has been shown experimentally (and with much supporting theory) to work very well under various conditions. The main keywords here are \nconsistency\n, risk, oracle inequalities etc..\n\nThe following slides and paper by McDonald and Homrighausen (2013) describe some conditions under which Lasso feature selection works well: \nslides\n and paper: \n\"The lasso, persistence, and cross-validation, McDonald and Homrighausen (2013)\"\n. Tibshirani himself also posted an great set of notes on \nsparcity\n, \nlinear regression\n\nThe various conditions for consistency and their impact on Lasso is an active topic of research and is definitely not a trivial question. I can point you towards some research papers which are relevant:\n\nVideo lectures on the No free lunch theorem, by Xu\n\n\nH.M. B\u00f8velstad et all, A comparison of feature selection approaches for gene selection, (2007)\n \n\n\nThe lasso, persistence, and cross-validation, McDonald and Homrighausen (2013)\n \n\n\nHuang and Bowick, Summary and discussion of: \u201cStability Selection\u201d\n\n\nLim and Yu, Estimation Stability with Cross Validation, (2015)\n \n\n\nA talk by Peter Buhlmann: \nStability Selection for High-Dimensional Data, (2008)\n and the accompanying \npaper\n\n\nWang, Nan et all, Random Lasso, (2011)\n\n\nStackexchange post: \nModel stability when dealing with large $p$, small $n$ problem\n\n\nRoberts, Nowakm Stabilizing the lasso against cross-validation variability, (2014)\n which argue that \"percentile-lasso, can result in large reductions in both model-selection instability and model-selection error, compared to the lasso\"\n\n\nAn awesome set of notes by Tibshirani and Wasserman on \nsparcity\n, \nlinear regression",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/463088/402101 \n Rephrasing the opposite of a confounder: It is definitely possible that an unobserved variable yields the impression that there is no relationship, when there \nis\n one.\n\nConfounding\n usually refers to a situation where an unobserved variable yields the illusion that there exists a relationship between two variables where there is none:\n\n\n\nThis is a special case of \nomitted-variable bias\n, which more generally refers to any situation wherein an unobserved variable biases the observed relationship:\n\n\n\nIt's easy to imagine a scenario where this would have a canceling effect on the estimate instead:\n\n\n\n(I wrote \n$\\rho=0$\n for the illustration, but the unobserved relationship does not have to be linear.)\n\nYou could call this phenomenon omitted-variable bias, cancellation, or masking. Confounding usually refers to the kind of causal relationship shown in the first figure.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17606/402101 \n According to the paper \n\"First (?) Occurrence of Common Terms in Mathematical Statistics\"\n by H.A. David, the first use of the word 'moment' in this situation was in a 1893 letter to \nNature\n by Karl Pearson entitled \n\"Asymmetrical Frequency Curves\"\n.\n\nNeyman's 1938 \nBiometrika\n paper \n\"A Historical Note on Karl Pearson's Deduction of the Moments of the Binomial\"\n gives a good synopsis of the letter and Pearson's subsequent work on moments of the binomial distribution and the method of moments.  It's a really good read.  Hopefully you have access JSTOR for I don't have the time now to give a good summary of the paper (though I will this weekend).  Though I will mention one piece that may give insight as to why the term 'moment' was used.  From Neyman's paper:\n\nIt [Pearson's memoir] deals primarily with methods of approximating \n      continuous frequency curves by means of some processes involving the \n      calculation of easy formulae.  One of these formulae considered was the \n      \"point-binomial\" or the \"binomial with loaded ordinates\".  The formula\n\n      differs from what to-day we call a binomial, viz. (4), only by a factor \n      $\\alpha$, representing the area under the continuous curve which it is desired\n      to fit.\n\nThis is what eventually led to the 'method of moments.' Neyman goes over the Pearson's derivation of the binomial moments in the above paper.\n\nAnd from Pearson's letter:\n\nWe shall now proceed to find the first four moments of the system of \n      rectangles round GN. If the inertia of each rectangle might be considered \n      as concentrated along its mid vertical, we should have for the $s^{\\text{th}}$ moment\n      round NG, writing $d = c(1 + nq)$.\n\nThis hints at the fact that Pearson used the term 'moment' as an allusion to \n'moment of inertia,'\n a term common in physics.\n\nHere's a scan of most of Pearson's \nNature\n letter:\n\n\n\n\n\nYou can view the entire article on page 615 \nhere\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/19529/402101 \n Simpson's paradox is an extreme form of confounding where the apparent sign of correlation is reversed; you haven't said this is the position here.\n\nI can see at least three possibilities here: the heterogenity between the subgroups, the reduction in sample sizes in each, and poor definition of the subgroups which presuppose the results.  Ignoring the third, both of the first two can have an impact: from past experience it is often the small sample size which lead to non-significance in the smaller subgroup and heterogenity which causes the whole group to produce a significant result wile the large subgroup does not.\n\nThat was an over-generalisation - each case will have its own issues.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/17392/402101 \n It means \"Independent and identically distributed\".\n\nA good example is a succession of throws of a fair coin: The coin has no memory, so all the throws are \"independent\".\n\nAnd every throw is 50:50 (heads:tails), so the coin is and stays fair - the distribution from which every throw is drawn, so to speak, is and stays the same: \"identically distributed\".\n\nA good starting point would be the \nWikipedia page\n.\n\n::EDIT::\n\nFollow this \nlink\n to further explore the concept.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/363777/402101 \n is there a way to retrieve those joint probabilities using only what\n  information is contained in the tables presented above?\n\n\n\n\nYou can \nnot\n obtain the joint probabilities from the marginal probabilities. See the following table:\n\n\n$$\\begin{array}{cc|c}\n    x_{11}-y    & x_{12}+y  &   R_1 \\\\ \n    x_{21}+y    & x_{22}-y   &  R_2  \\\\  \\hline\n    C_1    &  C_2    & N \\\\\n\\end{array}$$\n\n\nYou could derive some values for the inner cells like: \n\n\n$$x_{ij} = \\frac{R_iC_j}{N}$$\n\n\nhowever multiple \ndifferent joint probabilities\n (obtained by varying the value of $y$) can give the \nsame marginals\n. \n\n\nThus you can not go in the opposite direction and derive the joint probabilities based on the marginal probabilities because there are multiple options.\n\n\n\n\nAre there plausible assumptions/restrictions that would assist or\n  facilitate the calculations for finding the joint probabilities\n\n\n\n\nPossibly. For instance in the 2 x 2 table this can be some measure for the degree of dependence. However in the case of larger tables this becomes more and more complicated. There will be $(n_R-1)(n_C-1)$ parameters that you need to fix in order to be able to derive the joint probabilities. And for every extra dimension there will be more factors.\n\nis there a way to retrieve those joint probabilities using only what\n  information is contained in the tables presented above?\n\nYou can \nnot\n obtain the joint probabilities from the marginal probabilities. See the following table:\n\n$$\\begin{array}{cc|c}\n    x_{11}-y    & x_{12}+y  &   R_1 \\\\ \n    x_{21}+y    & x_{22}-y   &  R_2  \\\\  \\hline\n    C_1    &  C_2    & N \\\\\n\\end{array}$$\n\nYou could derive some values for the inner cells like:\n\n$$x_{ij} = \\frac{R_iC_j}{N}$$\n\nhowever multiple \ndifferent joint probabilities\n (obtained by varying the value of $y$) can give the \nsame marginals\n.\n\nThus you can not go in the opposite direction and derive the joint probabilities based on the marginal probabilities because there are multiple options.\n\nAre there plausible assumptions/restrictions that would assist or\n  facilitate the calculations for finding the joint probabilities\n\nPossibly. For instance in the 2 x 2 table this can be some measure for the degree of dependence. However in the case of larger tables this becomes more and more complicated. There will be $(n_R-1)(n_C-1)$ parameters that you need to fix in order to be able to derive the joint probabilities. And for every extra dimension there will be more factors.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/229159/402101 \n An oracle knows the truth: it knows the true subset and is willing to act on it. The oracle property is that the asymptotic distribution of the estimator is the same as the asymptotic distribution of the MLE on only the true support. That is, the estimator adapts to knowing the true support \nwithout\n paying a price (in terms of the asymptotic distribution.)\n\nBy the asymptotic optimality properties of the MLE discussed in, for instance, Keener's theoretical statistics in theorem 9.14, we know, under some technical conditions which hold when, for instance, the error is Gaussian, that $$\\sqrt{n} \\left( \\hat\\beta_S - \\beta^*_S \\right) \\to \\mathcal{N} (0, I^{-1}(\\beta^*_S)),$$ where we assume that $\\beta^*_S$ is the true coefficient on the true support $S$. Notice that the variance of the asymptotic distribution is the inverse of the Fisher information, showing that $\\hat\\beta_S$ is asymptotically efficient. Since the MLE knowing the true support achieves this, it is also required as part of the oracle property.\n\nHowever, we do pay a steep nonasymptotic price: see, for instance,\n\nHannes Leeb, Benedikt M. P\u00f6tscher, Sparse estimators and the oracle property, or the return of Hodges\u2019 estimator, Journal of Econometrics, Volume 142, Issue 1, 2008, Pages 201-211,\n\nwhich shows that the risk of any \"oracle estimator\" (in the sense of Fan and Li, 2001) has a supremum which diverges to infinity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1828/402101 \n Consider the following situation:\n\nI want to catch the subway to go to my office. My plan is to take my car, park at the subway and then take the train to go to my office. My goal is to catch the train at 8.15 am every day so that I can reach my office on time. I need to decide the following: (a) the time at which I need to leave from my home and (b) the route I will take to drive to the station.\n\nIn the above example, I have two parameters (i.e., time of departure from home and route to take to the station) and I need to choose these parameters such that I reach the station by 8.15 am.\n\nIn order to solve the above problem I may try out different sets of 'parameters' (i.e., different combination of times of departure and route) on Mondays, Wednesdays, and Fridays, to see which combination is the 'best' one. The idea is that once I have identified the best combination I can use it every day so that I achieve my objective.\n\nProblem of Overfitting\n\nThe problem with the above approach is that I may overfit which essentially means that the best combination I identify may in some sense may be unique to Mon, Wed and Fridays and that combination may not work for Tue and Thu. Overfitting may happen if in my search for the best combination of times and routes I exploit some aspect of the traffic situation on Mon/Wed/Fri which does not occur on Tue and Thu.\n\nOne Solution to Overfitting: Cross-Validation\n\nCross-validation is one solution to overfitting. The idea is that once we have identified our best combination of parameters (in our case time and route) we test the performance of that set of parameters in a different context. Therefore, we may want to test on Tue and Thu as well to ensure that our choices work for those days as well.\n\nExtending the analogy to statistics\n\nIn statistics, we have a similar issue. We often use a limited set of data to estimate the unknown parameters we do not know. If we overfit then our parameter estimates will work very well for the existing data but not as well for when we use them in another context. Thus, cross-validation helps in avoiding the above issue of overfitting by proving us some reassurance that the parameter estimates are not unique to the data we used to estimate them.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1828/402101 \n Extending the analogy to statistics\n\nIn statistics, we have a similar issue. We often use a limited set of data to estimate the unknown parameters we do not know. If we overfit then our parameter estimates will work very well for the existing data but not as well for when we use them in another context. Thus, cross-validation helps in avoiding the above issue of overfitting by proving us some reassurance that the parameter estimates are not unique to the data we used to estimate them.\n\nOf course, cross validation is not perfect. Going back to our example of the subway, it can happen that even after cross-validation, our best choice of parameters may not work one month down the line because of various issues (e.g., construction, traffic volume changes over time etc).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/92066/402101 \n When you fit a regression model such as $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1x_i + \\hat\\beta_2x^2_i$, the model and the OLS estimator doesn't 'know' that $x^2_i$ is simply the square of $x_i$, it just 'thinks' it's another variable.  Of course there is some collinearity, and that gets incorporated into the fit (e.g., the standard errors are larger than they might otherwise be), but lots of pairs of variables can be somewhat collinear without one of them being a function of the other.\n\nWe don't recognize that there are really two separate variables in the model, because \nwe\n know that $x^2_i$ is ultimately the same variable as $x_i$ that we transformed and included in order to capture a curvilinear relationship between $x_i$ and $y_i$.  That knowledge of the true nature of $x^2_i$, coupled with our belief that there is a curvilinear relationship between $x_i$ and $y_i$ is what makes it difficult for us to understand the way that it is still linear from the model's perspective.  In addition, we visualize $x_i$ and $x^2_i$ together by looking at the marginal projection of the 3D function onto the 2D $x, y$ plane.\n\nIf you only have $x_i$ and $x^2_i$, you can try to visualize them in the full 3D space (although it is still rather hard to really see what is going on).  If you did look at the fitted function in the full 3D space, you would see that the fitted function is a 2D plane, and moreover that it is a flat plane.  As I say, it is hard to see well because the $x_i, x^2_i$ data exist only along a curved line going through that 3D space (that fact is the visual manifestation of their collinearity).  We can try to do that here.  Imagine this is the fitted model:\n\nx     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")\n\nx     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/92066/402101 \n x     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")\n\nx     = seq(from=0, to=10, by=.5)\nx2    = x**2\ny     = 3 + x - .05*x2\nd.mat = data.frame(X1=x, X2=x2, Y=y)\n\n# 2D plot\nplot(x, y, pch=1, ylim=c(0,11), col=\"red\", \n     main=\"Marginal projection onto the 2D X,Y plane\")\nlines(x, y, col=\"lightblue\")\n\n\n\n# 3D plot\nlibrary(scatterplot3d)\ns = scatterplot3d(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, color=\"gray\", pch=1, \n              xlab=\"X1\", ylab=\"X2\", zlab=\"Y\", xlim=c(0, 11), ylim=c(0,101), \n              zlim=c(0, 11), type=\"h\", main=\"In pseudo-3D space\")\ns$points(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, col=\"red\", pch=1)\ns$plane3d(Intercept=3, x.coef=1, y.coef=-.05, col=\"lightblue\")\n\n# 3D plot\nlibrary(scatterplot3d)\ns = scatterplot3d(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, color=\"gray\", pch=1, \n              xlab=\"X1\", ylab=\"X2\", zlab=\"Y\", xlim=c(0, 11), ylim=c(0,101), \n              zlim=c(0, 11), type=\"h\", main=\"In pseudo-3D space\")\ns$points(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, col=\"red\", pch=1)\ns$plane3d(Intercept=3, x.coef=1, y.coef=-.05, col=\"lightblue\")\n\n\n\nIt may be easier to see in these images, which are screenshots of a rotated 3D figure made with the same data using the \nrgl\n package.\n\nrgl\n\n\n\nWhen we say that a model that is \"linear in the parameters\" really is linear, this isn't just some mathematical sophistry.  With $p$ variables, you are fitting a $p$-dimensional hyperplane in a $p\\!+\\!1$-dimensional hyperspace (in our example a 2D plane in a 3D space).  That hyperplane really is 'flat' / 'linear'; it isn't just a metaphor.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/168703/402101 \n Considering multicollineariy is important in regression analysis because, \nin extrema\n, it directly bears on whether or not your coefficients are uniquely identified in the data. In less severe cases, it can still mess with your coefficient estimates; small changes in the data used for estimation may cause wild swings in estimated coefficients. These can be problematic from an inferential standpoint: If two variables are highly correlated, increases in one may be offset by decreases in another so the combined effect is to negate each other. With more than two variables, the effect can be even more subtle, but if the \npredictions\n are stable, that is often enough for machine learning applications.\n\nConsider why we regularize in a regression context: We need to constrict the model from being \ntoo\n flexible. Applying the correct amount of regularization will slightly increase the bias for a larger reduction in variance. The classic example of this is adding polynomial terms and interaction effects to a regression: In the degenerate case, the prediction equation will interpolate data points, but probably be terrible when attempting to predict the values of unseen data points. Shrinking those coefficients will likely minimize or entirely eliminate some of those coefficients and improve generalization.\n\nA random forest, however, could be seen to have a regularization parameter through the number of variables sampled at each split: you get better splits the larger the \nmtry\n (more features to choose from; some of them are better than others), but that also makes each tree more highly correlated with each other tree, somewhat mitigating the diversifying effect of estimating multiple trees in the first place. This dilemma compels one to find the right balance, usually achieved using cross-validation. Importantly, and in contrast to a regression analysis, the predictions of the random forest model are not harmed by highly collinear variables: even if two of the variables provide the same child node purity, you can just pick one.\n\nmtry",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/168703/402101 \n mtry\n\nLikewise, for something like an SVM, you can include more predictors than features because the kernel trick lets you  operate solely on the inner product of those feature vectors. Having more features than observations would be a problem in regressions, but the kernel trick means we only estimate a coefficient for each exemplar, while the regularization parameter \n$C$\n reduces the flexibility of the solution -- which is decidedly a good thing, since estimating \n$N$\n parameters for \n$N$\n observations in an unrestricted way will always produce a perfect model on test data -- and we come full circle, back to the ridge/LASSO/elastic net regression scenario where we have the model flexibility constrained as a check against an overly optimistic model. A review of the KKT conditions of the SVM problem reveals that the SVM solution is unique, so we don't have to worry about the identification problems which arose in the regression case.\n\nFinally, consider the actual \nimpact\n of multicollinearity. It doesn't change the predictive power of the model (at least, on the training data) but it does screw with our coefficient estimates. In most ML applications, we don't care about coefficients \nthemselves\n, just the loss of our model predictions, so in that sense, checking VIF doesn't actually answer a consequential question. (But if a slight change in the data causes a huge fluctuation in coefficients [a classic symptom of multicollinearity], it may also change predictions, in which case we do care -- but all of this [we hope!] is characterized when we perform cross-validation, which is a part of the modeling process anyway.) A regression is more easily interpreted, but interpretation might not be the most important goal for some tasks.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/258035/402101 \n I am not aware of situations, in which stepwise regression would be the preferred approach. It may be okay (particularly in its step-down version starting from the full model) with bootstrapping of the whole stepwise process on extremely large datasets with $n>>p$. Here $n$ is the number of observations in an continuous outcome (or number of records with an event in survival analysis) $p$ is the number of candidate predictors including all considered interactions - i.e. when any even small effects become very clear and it does not matter so much how your do your model building (that would mean that $n$ would be much larger than $p$ than by substantially more than the sometimes quoted factor of 20).\n\nOf course the reason most people are tempted to do something like stepwise regression is,\n\nbecause it is not computationally intensive (if you do not do the proper bootstrapping, but then your results are pretty unreliable), \n\n\nbecause it provides clear cut \"is in the model\" versus \"is not in the model\" statements (which are very unreliable in standard stepwise regression; something that proper bootstrapping will usually make clear so that these statements will usually not be so clear) and \n\n\nbecause often $n$ is smaller, close to or just a bit larger than $p$.\n\nI.e. a method like stepwise regression would (if it had good operating characteristics) be especially attractive in those situations, when it does not have good operating characteristics.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/496387/402101 \n Omitted variable bias (OVB) is agnostic to the causal relationship between \n$X$\n and \n$Z$\n. It concerns only the ability to estimate \n$\\tau$\n in the structural model for \n$Y$\n. The joint distribution of \n$Y$\n, \n$X$\n, and \n$Z$\n is compatible both with a data-generating process in which \n$Z$\n is a confounder of the \n$X \\rightarrow Y$\n relationship, so that \n$\\tau$\n represents the total effect of \n$X$\n on \n$Y$\n, and with a data-generating process in which \n$Z$\n is a mediator of the \n$X \\rightarrow Y$\n relationship, so that \n$\\tau$\n represents the direct effect of \n$X$\n on \n$Y$\n.\n\nIn the confounding model, the data-generating process for \n$X$\n and \n$Z$\n is:\n\n$$\nZ := \\epsilon_Z \\\\\nX := \\gamma Z + \\epsilon_X\n$$\n\nIn the mediation model, the data-genertaing process for \n$X$\n and \n$Z$\n is:\n\n$$\nZ := \\alpha X + \\epsilon_Z \\\\\nX := \\epsilon_X\n$$\n\nFor the confounding process, omitting \n$Z$\n from the model for \n$Y$\n yields a biased estimate of \n$\\tau$\n, the total effect of \n$X$\n on \n$Y$\n. Thisis the classic bias due to an omitted confounder.\n\nFor the mediation process, the \n$X \\rightarrow Y$\n relationship is not confounded. The estimated coefficient \n$\\hat \\tau$\n in the model omitting \n$Z$\n is unbiased for the \ntotal\n causal effect of \n$X$\n on \n$Y$\n. However, it is biased for \n$\\tau$\n, the \ndirect\n effect of \n$X$\n on \n$Y$\n.\n\nThis is all to say that it's possible to have OVB without confounding if the coefficient you are trying to estimate is a direct effect, in which case omitting the mediator yields a biased estimate of this quantity. In the absence of confounding, the model omitting the mediator yields the total effect. The formula for the bias is the same regardless of the data-generating process of \n$X$\n and \n$Z$\n, but the interpretation of the biased parameter depends on the causal relationship between \n$X$\n and \n$Z$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/474027/402101 \n I agree that this is can be a little confusing.\n\nWill including hospital as a random intercept adjust the estimated coefficient for treatment even though the expected value of the random effect for hospital is 0?\n\nYes. Just because the random intercepts have a mean of zero does not mean that it doesn't control for confounding. Very often an analyst has difficulty deciding whether to model a factor as fixed or random. Very often there are different arguments in favour of both, but there is no argument as to whether one adjusts for confounding or not.\n\nWhen we have clustered data with correlations within the clusters, we can control for this by eithe using a random intercept for the cluster ID, or fitting a fixed effect for the cluster ID [Generalised estimating equations are another option, but that's not relevant to this answer].\n\nA simple simulation shows this:\n\nset.seed(15)\n\nn <- 50\n\nX <- rbinom(n, 10, 0.5)\n\nE <- (X/5) + rnorm(n)\n\nY <- E + X + rnorm(n)\n\nset.seed(15)\n\nn <- 50\n\nX <- rbinom(n, 10, 0.5)\n\nE <- (X/5) + rnorm(n)\n\nY <- E + X + rnorm(n)\n\nHere we have an exposure \nE\n and an outcome \nY\n, but the association is confounded by \nX\n. The \"true\" value for the estimate for \nE\n is 1:\n\nE\n\nY\n\nX\n\nE\n\nX <- as.factor(X) \nlm1 <- lm(Y ~ E)\nlm2 <- lm(Y ~ E + X)  \nlmm <- lmer(Y ~ E + (1|X))\n\n\n> summary(lm1)\n\nE             1.5232\n\nX <- as.factor(X) \nlm1 <- lm(Y ~ E)\nlm2 <- lm(Y ~ E + X)  \nlmm <- lmer(Y ~ E + (1|X))\n\n\n> summary(lm1)\n\nE             1.5232\n\nwhere I have omitted all but the the essential output. Evidently this is confounded.  But, if we include the confounder \nX\n as a fixed effect we get:\n\nX\n\n> summary(lm2)\n\nE            1.0446\n\n> summary(lm2)\n\nE            1.0446\n\nas expected. And we also we find\n\n> summary(lmm)\n\nE             1.0661\n\n> summary(lmm)\n\nE             1.0661",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/339327/402101 \n Another way to think about this is that the statistical experiment is the protocol we follow to generate data and the statistical model is the protocol we use to analyze these data.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/175290/402101 \n ANOVA and linear regression are equivalent when the two models test against the same hypotheses and use an identical encoding. The models differ in their basic aim: ANOVA is mostly concerned to present differences between categories' means in the data while linear regression is mostly concern to estimate a sample mean response and an associated $\\sigma^2$.\n\nSomewhat aphoristically one can describe ANOVA as a regression with dummy variables. We can easily see that this is the case in the simple regression with categorical variables. A categorical variable will be encoded as a indicator matrix (a matrix of \n0/1\n depending on whether a subject is part of a given group or not) and then used directly for the solution of the linear system described by a linear regression.\nLet's see an example with 5 groups. For the sake of argument I will assume that the mean of \ngroup1\n equals 1,  the mean of \ngroup2\n equals 2, ... and the mean of \ngroup5\n equals 5. (I use MATLAB, but the exact same thing is equivalent in R.)\n\n0/1\n\ngroup1\n\ngroup2\n\ngroup5\n\nrng(123);               % Fix the seed\nX = randi(5,100,1);     % Generate 100 random integer U[1,5]\nY = X + randn(100,1);   % Generate my response sample\nXcat = categorical(X);  % Treat the integers are categories\n\n% One-way ANOVA\n[anovaPval,anovatab,stats] = anova1(Y,Xcat);\n% Linear regression\nfitObj = fitlm(Xcat,Y);\n\n% Get the group means from the ANOVA\nANOVAgroupMeans = stats.means\n% ANOVAgroupMeans =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Get the beta coefficients from the linear regression\nLRbetas = [fitObj.Coefficients.Estimate'] \n% LRbetas =\n% 1.0953    0.7468    1.6398    3.1368    3.9565\n\n% Rescale the betas according the intercept\nscaledLRbetas = [LRbetas(1) LRbetas(1)+LRbetas(2:5)]\n% scaledLRbetas =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Check if the two results are numerically equivalent\nabs(max( scaledLRbetas - ANOVAgroupMeans)) \n% ans =\n% 2.6645e-15\n\nrng(123);               % Fix the seed\nX = randi(5,100,1);     % Generate 100 random integer U[1,5]\nY = X + randn(100,1);   % Generate my response sample\nXcat = categorical(X);  % Treat the integers are categories\n\n% One-way ANOVA\n[anovaPval,anovatab,stats] = anova1(Y,Xcat);\n% Linear regression\nfitObj = fitlm(Xcat,Y);\n\n% Get the group means from the ANOVA\nANOVAgroupMeans = stats.means\n% ANOVAgroupMeans =\n% 1.0953    1.8421    2.7350    4.2321    5.0517",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/175290/402101 \n rng(123);               % Fix the seed\nX = randi(5,100,1);     % Generate 100 random integer U[1,5]\nY = X + randn(100,1);   % Generate my response sample\nXcat = categorical(X);  % Treat the integers are categories\n\n% One-way ANOVA\n[anovaPval,anovatab,stats] = anova1(Y,Xcat);\n% Linear regression\nfitObj = fitlm(Xcat,Y);\n\n% Get the group means from the ANOVA\nANOVAgroupMeans = stats.means\n% ANOVAgroupMeans =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Get the beta coefficients from the linear regression\nLRbetas = [fitObj.Coefficients.Estimate'] \n% LRbetas =\n% 1.0953    0.7468    1.6398    3.1368    3.9565\n\n% Rescale the betas according the intercept\nscaledLRbetas = [LRbetas(1) LRbetas(1)+LRbetas(2:5)]\n% scaledLRbetas =\n% 1.0953    1.8421    2.7350    4.2321    5.0517\n\n% Check if the two results are numerically equivalent\nabs(max( scaledLRbetas - ANOVAgroupMeans)) \n% ans =\n% 2.6645e-15\n\nAs it can be seen in this scenario the results where exactly the same. The minute numerical difference is due to the design not being perfectly balanced as well as the underlaying estimation procedure; the ANOVA accumulates numerical errors a bit more aggressively. To that respect we fit an intercept, \nLRbetas(1)\n; we could fit an intercept-free model but that would not be a \"standard\" linear regression. (The results would be even closer to ANOVA in that case though.)\n\nLRbetas(1)\n\nThe $F$-statistic (a ratio of the means) in the case of the ANOVA and in the case of linear regression will be also be the same for the above example:\n\nabs( fitObj.anova.F(1) - anovatab{2,5} )\n% ans =\n% 2.9132e-13\n\nabs( fitObj.anova.F(1) - anovatab{2,5} )\n% ans =\n% 2.9132e-13\n\nThis is because procedures test the same hypothesis but with different wordings: ANOVA will qualitatively check if \"\nthe ratio is high enough to suggest that no grouping is implausible\n\" while linear regression will qualitatively check if \"\nthe ratio is high enough to suggest an intercept only model is possibly inadequate\n\".\n\n(This is a somewhat free interpretation of the \"\npossibility to see a value equal or greater than the one observed under the null hypothesis\n\" and it is not meant to be a text-book definition.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/175290/402101 \n (This is a somewhat free interpretation of the \"\npossibility to see a value equal or greater than the one observed under the null hypothesis\n\" and it is not meant to be a text-book definition.)\n\nComing back to the final part of your question about \"\nANOVA tell(ing) you nothing about the coefficients of the linear model (assuming the means are not equal\n\") I hope you can now see that the ANOVA, in the case that your design is simple/\nbalanced\n enough, tells you everything that a linear model would. The confidence intervals for group means will be the same you have for your $\\beta$, etc. Clearly when ones starts adding multiple covariate in his regression model, a simple one-way ANOVA does not have a direct equivalence. In that case one augments the information used to calculate the linear regression's mean response with information that are not directly available for a one way ANOVA. I believe that one can re-express things in ANOVA terms once more but it is mostly an academic exercise.\n\nAn interesting paper on the matter is Gelman's 2005 paper titled: \nAnalysis of Variance - Why it is more important than ever\n. Some important points raised; I am not fully supportive of the paper (I think I personally align much more with McCullach's view) but it can be a constructive read.\n\nAs a final note: The plot thickens when you have \nmixed effects models\n. There you have different concepts about what can be considered a nuisance or actual information regarding the grouping of your data. These issues are outside the scope of this question but I think they are worthy of a nod.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/321373/402101 \n I think what you're looking for is a copula. You've got two marginal distributions (specified by either parametric or empirical cdfs) and now you want to specify the dependence between the two. For the bivariate case there are all kinds of choices, but the basic recipe is the same. I'll use a Gaussian copula for ease of interpretation.\n\nTo draw from the Gaussian copula with correlation matrix $C$\n\nDraw $(Z=(Z_1, Z_2)\\sim N(0, C)$\n\n\nSet $U_i = \\Phi(Z_i)$ for $i=1, 2$ (with $\\Phi$ the standard normal cdf). Now $U_1, U_2\\sim U[0,1]$, but they're dependent.\n\n\nSet $Y_i = F_i^{-1}(U_i)$ where $F_i^{-1}$ is the (pseudo) inverse of the marginal cdf for variable $i$. This implies that $Y_i$ follow the desired distribution (this step is just inverse transform sampling).\n\nDraw $(Z=(Z_1, Z_2)\\sim N(0, C)$\n\nSet $U_i = \\Phi(Z_i)$ for $i=1, 2$ (with $\\Phi$ the standard normal cdf). Now $U_1, U_2\\sim U[0,1]$, but they're dependent.\n\nSet $Y_i = F_i^{-1}(U_i)$ where $F_i^{-1}$ is the (pseudo) inverse of the marginal cdf for variable $i$. This implies that $Y_i$ follow the desired distribution (this step is just inverse transform sampling).\n\nVoila! Try it for some simple cases, and look at marginal histograms and scatterpolots, it's fun.\n\nNo guarantee that this is appropriate for your particular application though (in particular, you might need to replace the Gaussian copula with a t copula) but this should get you started. A good reference on copula modeling is Nelsen (1999), \nAn Introduction to Copulas\n, but there are some pretty good introductions online too.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/294/402101 \n You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales.\n\nUsing the correlation matrix is equivalent to \nstandardizing\n each of the variables (to mean 0 and standard deviation 1). In general, PCA with and without standardizing will give different results. Especially when the scales are different.\n\nAs an example, take a look at this R \nheptathlon\n data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (run 800m) are around 120.\n\nheptathlon\n\nlibrary(HSAUR)\nheptathlon[,-8]      # look at heptathlon data (excluding 'score' variable)\n\nlibrary(HSAUR)\nheptathlon[,-8]      # look at heptathlon data (excluding 'score' variable)\n\nThis outputs:\n\nhurdles highjump  shot run200m longjump javelin run800m\nJoyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66  128.51\nJohn (GDR)            12.85     1.80 16.23   23.65     6.71   42.56  126.12\nBehmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54  124.20\nSablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78  132.24\nChoubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46  127.90\n...\n\nhurdles highjump  shot run200m longjump javelin run800m\nJoyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66  128.51\nJohn (GDR)            12.85     1.80 16.23   23.65     6.71   42.56  126.12\nBehmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54  124.20\nSablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78  132.24\nChoubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46  127.90\n...\n\nNow let's do PCA on covariance and on correlation:\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/294/402101 \n Now let's do PCA on covariance and on correlation:\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)\n\n\n\nNotice that PCA on covariance is dominated by \nrun800m\n and \njavelin\n: PC1 is almost equal to \nrun800m\n (and explains \n$82\\%$\n of the variance) and PC2 is almost equal to \njavelin\n (together they explain \n$97\\%$\n). \nPCA on correlation is much more informative\n and reveals some structure in the data and relationships between variables (but note that the explained variances drop to \n$64\\%$\n and \n$71\\%$\n).\n\nrun800m\n\njavelin\n\nrun800m\n\njavelin\n\nNotice also that the outlying individuals (in \nthis\n data set) are outliers regardless of whether the covariance or correlation matrix is used.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/69209/402101 \n It suffices to modify the loss function by adding the penalty. In matrix terms, the initial quadratic loss function becomes\n$$ (Y - X\\beta)^{T}(Y-X\\beta) + \\lambda \\beta^T\\beta.$$\nDeriving with respect to $\\beta$ leads to the normal equation\n$$ X^{T}Y = \\left(X^{T}X + \\lambda I\\right)\\beta $$\nwhich leads to the Ridge estimator.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/268803/402101 \n Answering your question with a question: \nwhat exactly is machine learning?\n Trevor Hastie, Robert Tibshirani and Jerome Friedman in \nThe Elements of\nStatistical Learning\n, Kevin P. Murphy in \nMachine Learning A Probabilistic Perspective\n, Christopher Bishop in \nPattern Recognition and Machine Learning\n,  Ian Goodfellow, Yoshua Bengio and Aaron Courville in \nDeep Learning\n and a number of other machine learning \"bibles\" mention linear regression as one of the machine learning \"algorithms\". Machine learning is partly a buzzword for applied statistics and the distinction between statistics and machine learning is often blurry.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/411707/402101 \n Both the standard Normal and Student t distributions are rather poor approximations to the distribution of\n\n$$Z = \\frac{\\hat p - p}{\\sqrt{\\hat p(1-\\hat p)/n}}$$\n\nfor small \n$n,$\n so poor that the error dwarfs the differences between these two distributions.\n\nHere is a comparison of all three distributions (omitting the cases where \n$\\hat p$\n or \n$1-\\hat p$\n are zero, where the ratio is undefined) for \n$n=10, p=1/2:$\n\n\n\nThe \"empirical\" distribution is that of \n$Z,$\n which must be discrete because the estimates \n$\\hat p$\n are limited to the finite set \n$\\{0, 1/n, 2/n, \\ldots, n/n\\}.$\n\nThe \n$t$\n distribution appears to do a better job of approximation.\n\nFor \n$n=30$\n and \n$p=1/2,$\n you can see the difference between the standard Normal and Student t distributions is completely negligible:\n\n\n\nBecause the Student t distribution is more complicated than the standard Normal\n (it's really an entire family of distributions indexed by the \"degrees of freedom,\" formerly requiring entire chapters of tables rather than a single page), the standard Normal is used for almost all approximations.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n As I see it, there are two related reasons to consider matching instead of regression. The first is assumptions about functional form, and the second is about proving to your audience that functional form assumptions do not affect the resulting effect estimate. The first is a statistical matter and the second is epistemic. Consider the tale below that attempts to illustrate how the choice between matching and regression could play out.\n\nWe'll assume you have measured a sufficient adjustment set to satisfy the backdoor criterion (i.e., all relevant confounders have been measured) with no measurement error or missing data, and that your goal is to estimate the marginal treatment effect of the treatment on an outcome. We'll also assume the standard assumptions of positivity and SUTVA hold. We'll consider a continuous outcome first, but much of the discussion extends to general outcomes.\n\nYou decide to run a regression of the outcome on the treatment and confounders as a way to control for confounding by these variables because that is what linear regression is supposed to do. However, the effect estimate is only unbiased under extremely strict circumstances. First, that the treatment effect is constant across levels of the confounders, and second, that the linear model describes the conditional relationship between the outcome and the confounders. For the first, you might include an interaction between the treatment and each confounder, allowing for heterogeneous treatment effects while estimating the marginal effect. This is equivalent to g-computation (1), which involves using the fitted regression model to generate predicted values under treatment and control for all units and using the difference in the means of these predicted values as the effect estimate.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n That still assumes a linear model for the outcomes under treatment and control. Okay, we'll use a flexible machine-learning method like random forests instead. Well, now we can't claim our estimator is unbiased, only possibly consistent, and it still requires the specific machine learning model to approach the truth at a certain rate. Okay, we'll use Superlearner (2), a stacking method that takes on the rate of convergence of the fastest of its included models. Well, now we don't have a way to conduct inference, and the model might still be wrong. Okay, we'll use a semiparametric efficient doubly-robust estimator like augmented inverse probability weighting (AIPW) (3) or targeted minimum loss-based estimation (TMLE) (4). Well, that's only consistent if the true models fall in the Donsker class of models. Okay, we'll use cross-fitting with AIPW or TMLE to relax that requirement (5).\n\nGreat. You've taken regression to its extreme, relaxing as many assumptions as possible and landing with a multiply-robust estimator (multiply-robust in the sense that if one of many models are correct, the estimator is consistent) with generally good inference properties (but it can be bootstrapped so getting the variance exactly right isn't a big problem). Have we solved causal inference?\n\nYou submit the results of your cross-fit TMLE estimate using Superlearner for the propensity score and potential outcome models with a full library including highly adaptive lasso and many other models, which, under weak assumptions, are all that are required for a truly consistent estimator that converges at a parametric rate.\n\nA reviewer reads the paper and says, \"I don't believe the results of this model.\"\n\n\"Why not?\" you say. \"I used the optimal estimator with the best properties; it is consistent and semiparametric efficient with few, if any, assumptions on the functional forms of the models.\"\n\n\"Your estimator is consistent,\" says the reviewer, \"but not unbiased. That means I can only trust its results in general and as N goes to infinity. How do I know you have successfully eliminated \nbias\n in the effect estimate in \nthis\n dataset?\"\n\n\"...\"",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n \"Why not?\" you say. \"I used the optimal estimator with the best properties; it is consistent and semiparametric efficient with few, if any, assumptions on the functional forms of the models.\"\n\n\"Your estimator is consistent,\" says the reviewer, \"but not unbiased. That means I can only trust its results in general and as N goes to infinity. How do I know you have successfully eliminated \nbias\n in the effect estimate in \nthis\n dataset?\"\n\n\"...\"\n\nYou read about a hot new method called \"propensity score matching\" (6). It was big in 1983, and, even in 2021, you see it in almost every paper published in specialized medical journals. You come across King and Nielsen's influential paper \"Why Propensity Scores Should Not Be Used for Matching\" (7) and Noah's \nanswer\n on CV describing the many drawbacks to using propensity score matching. Okay, you'll use genetic matching instead (8), and minimize the energy distance between the samples (9), including a flexibly estimated propensity score as a covariate to match on. You find that balance can be improved by using substantive knowledge to incorporate exact matching and caliper constraints that prioritize balance on covariates known to be important to the outcome. You decide to use full matching to relax the requirement of 1:1 matching to include more units in the analysis (10).\n\nYou estimate the treatment effect using a simple linear regression of the outcome on the treatment and the covariates, including the matching weights in the regression and using a cluster-robust standard error to account for pair membership (11). You resubmit the result of your full matching analysis using exact matching and calipers for prognostically important variables and a distance matrix estimating using genetic matching on the covariates and a flexibly estimated propensity score.\n\nThe reviewer reads your new manuscript. \"Wow, you've learned a lot. But I still don't believe you've removed bias the in the effect estimate.\"\n\n\"Look at the balance tables,\" you say. \"The covariate distributions are almost identical.\"\n\n\"I see low standardized mean differences,\" says the reviewer, \"but imbalances could remain on other features of the covariate distribution.\"",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n The reviewer reads your new manuscript. \"Wow, you've learned a lot. But I still don't believe you've removed bias the in the effect estimate.\"\n\n\"Look at the balance tables,\" you say. \"The covariate distributions are almost identical.\"\n\n\"I see low standardized mean differences,\" says the reviewer, \"but imbalances could remain on other features of the covariate distribution.\"\n\n\"Look at the balance tables in the appendix which contain balance statistics for pairwise interactions, polynomials up to the 5th power of each covariate, and Kolmogorov-Smirnov statistics to compare the full covariate distributions. There are no meaningful differences between the samples, and no differences at all on the most highly prognostic covariates because of the exact matching constraints and calipers.\"\n\n\"I see...\"\n\n\"Also, I used Branson's randomization test (12) with the energy distance as the balance statistic to show that my sample is better balanced not only than a hypothetical randomized trial using the same data, but also a block randomized trial, and even a covariate balance-constrained randomized trial.\"\n\n\"Wow, I guess I don't have much to say...\"\n\n\"My outcome regression estimator isn't just consistent, it's truly \nunbiased\n in \nthis\n sample. Also, because I incorporated pair membership into the analysis, my standard errors are smaller and more accurate and the resulting estimate is less sensitive to unobserved confounding* (13).\"\n\n\"I get it!\"\n\nFrank Harrell \nbursts\n into the room. \"Wait, by discarding so many units in matching, you have thrown away so much useful data and needlessly decimated your precision.\" Mark van der Laan follows. \"Wait, by using substantive 'expertise' you are not letting the analysis method find the true patterns in the data that might have eluded researchers, and your estimator does not converge at a known rate, let alone a parametric one! And there is no guarantee that your inference is valid!\" I, your humble narrator, too, join in on the dogpile. \"Wait, by using exact matching constraints and calipers, you have shifted your estimand away from the ATE or any \na priori\n describable estimand (14)! Your effect estimate may be unbiased, but unbiased for \nwhat\n?\"\n\nYou stand there, bewildered, defeated, feeling like you have come nowhere since you asked your simple question on CrossValidated what felt like years ago, no closer to understanding whether you should use matching or regression to estimate causal effects.\n\nThe curtains close.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n You stand there, bewildered, defeated, feeling like you have come nowhere since you asked your simple question on CrossValidated what felt like years ago, no closer to understanding whether you should use matching or regression to estimate causal effects.\n\nThe curtains close.\n\nIn the face of uncertainty and scarcity, we are left with tradeoffs. The choice between a regression-based method and matching to estimate a causal effect depends on how you and your audience choose to manage those tradeoffs and prioritize the advantages and drawbacks of each method.\n\nStandard regression requires strong functional form assumptions, but with advanced methods, those can be relaxed, at the cost of giving up on bias and focusing on consistency and asymptotic inference. Many of these advanced methods work best in large samples, and they still require many choices along the way (e.g., which specific estimator to use, which machine learning methods to include in the Superlearner library, how many folds to use for cross-validation and cross-fitting, etc.). Although the multiply-robust methods may guarantee consistency and fast convergence rates in general data, it is not immediately clear how you can assess how well they eliminated bias in your dataset, potentially leaving one skeptical of their actual performance in your one instance.\n\nMatching methods require few functional form assumptions because no models are required (e.g., when using a distance matrix that doesn't depend solely on the propensity score, like that resulting from genetic matching). You can control confounding by adjusting the specification of the match, focusing more effort on hard-to-balance or prognostically important variables. You can come close to guaranteeing unbiasedness by ensuring you have achieved covariate balance, which can and should be measured extremely broadly with a skeptic in mind. You can use tools for analyzing randomized trials and trials with more powerful and robust designs. This comes at the cost of possibly decimating your precision by discarding huge amounts of data, changing your estimand so that your effect estimate doesn't generalize to a meaningful population and isn't replicable, and relying on ad hoc, \"artisanal\" methods with no clear path for valid inference.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n The advantage matching has over regression, and the reason why I think it is so valuable and why I devoted my graduate training to understanding and improving matching and its use by applied researchers as the author of the R package \ncobalt\n, \nWeightIt\n, \nMatchIt\n, and others, is an epistemic advantage. With matching, you can more effectively convince a reader that what you have done is trustworthy and that you have accounted for all possible objections to the observed result, and can at least point to specific assumptions and explain how their violation might affect results. This all centers on covariate balance, the similarity between covariate distributions across the treatment groups. By reporting balance broadly and submitting the resulting matched data to a battery of tests and balance measures, you can convince yourself and your readers that the resulting effect estimate is unbiased and therefore trustworthy (given the assumptions mentioned at the beginning, though these may be tenuous, and neither matching nor regression can solve that problem).\n\ncobalt\n\nWeightIt\n\nMatchIt\n\nHowever, not everyone agrees that this advantage so important, or more important than consistency and valid asymptotic inference. There can never be consensus on this matter, because consensus requires knowing the truth, and science (including statistics research) is about searching for an inherently unknowable truth (i.e., the true parameters that govern or describe our world). That is, if we knew the true causal effect, we could know the best method to estimate it, but we don't, so we can't. We can only do our best using the knowledge we have and try to manage the inherent constraints and tradeoffs as well as we can as we fumble around in the dark using the pinpoint of light the universe has shown us.\n\n*Only when using a special method of inference for matched samples.\n\nSnowden JM, Rose S, Mortimer KM. Implementation of G-Computation on a Simulated Data Set: Demonstration of a Causal Inference Technique. Am J Epidemiol. 2011;173(7):731\u2013738.\n\n\nvan der Laan MJ, Polley EC, Hubbard AE. Super Learner. Statistical Applications in Genetics and Molecular Biology [electronic article]. 2007;6(1). (\nhttps://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1309/sagmb.2007.6.1.1309.xml\n). (Accessed October 8, 2019)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n Snowden JM, Rose S, Mortimer KM. Implementation of G-Computation on a Simulated Data Set: Demonstration of a Causal Inference Technique. Am J Epidemiol. 2011;173(7):731\u2013738.\n\n\nvan der Laan MJ, Polley EC, Hubbard AE. Super Learner. Statistical Applications in Genetics and Molecular Biology [electronic article]. 2007;6(1). (\nhttps://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1309/sagmb.2007.6.1.1309.xml\n). (Accessed October 8, 2019)\n\n\nDaniel RM. Double Robustness. In: Wiley StatsRef: Statistics Reference Online. American Cancer Society; 2018 (Accessed November 9, 2018):1\u201314.(\nhttp://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat08068\n). (Accessed November 9, 2018)\n\n\nGruber S, van der Laan MJ. Targeted Maximum Likelihood Estimation: A Gentle Introduction. 2009;17.\n\n\nZivich PN, Breskin A. Machine Learning for Causal Inference: On the Use of Cross-fit Estimators. Epidemiology. 2021;32(3):393\u2013401.\n\n\nRosenbaum PR, Rubin DB. The central role of the propensity score in observational studies for causal effects. Biometrika. 1983;70(1):41\u201355.\n\n\nKing G, Nielsen R. Why Propensity Scores Should Not Be Used for Matching. Polit. Anal. 2019;1\u201320.\n\n\nDiamond A, Sekhon JS. Genetic matching for estimating causal effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics. 2013;95(3):932\u2013945.\n\n\nHuling JD, Mak S. Energy Balancing of Covariate Distributions. arXiv:2004.13962 [stat] [electronic article]. 2020;(\nhttp://arxiv.org/abs/2004.13962\n). (Accessed December 22, 2020)\n\n\nStuart EA, Green KM. Using full matching to estimate causal effects in nonexperimental studies: Examining the relationship between adolescent marijuana use and adult outcomes. Developmental Psychology. 2008;44(2):395\u2013406.\n\n\nAbadie A, Spiess J. Robust Post-Matching Inference. Journal of the American Statistical Association. 2020;0(ja):1\u201337.\n\n\nBranson Z. Randomization Tests to Assess Covariate Balance When Designing and Analyzing Matched Datasets. Observational Studies. 2021;7:44\u201380.\n\n\nZubizarreta JR, Paredes RD, Rosenbaum PR. Matching for balance, pairing for heterogeneity in an observational study of the effectiveness of for-profit and not-for-profit high schools in Chile. The Annals of Applied Statistics. 2014;8(1):204\u2013231.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/544958/402101 \n Branson Z. Randomization Tests to Assess Covariate Balance When Designing and Analyzing Matched Datasets. Observational Studies. 2021;7:44\u201380.\n\n\nZubizarreta JR, Paredes RD, Rosenbaum PR. Matching for balance, pairing for heterogeneity in an observational study of the effectiveness of for-profit and not-for-profit high schools in Chile. The Annals of Applied Statistics. 2014;8(1):204\u2013231.\n\n\nGreifer N, Stuart EA. Choosing the Estimand When Matching or Weighting in Observational Studies. arXiv:2106.10577 [stat] [electronic article]. 2021;(\nhttp://arxiv.org/abs/2106.10577\n). (Accessed September 17, 2021)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18217/402101 \n Yes, there are some simple relationships between confidence interval comparisons and hypothesis tests in a wide range of practical settings.\n  However, in addition to verifying the CI procedures and t-test are appropriate for our data, we must check that the sample sizes are not too different and that the two sets have similar standard deviations.  We also should not attempt to derive highly precise p-values from comparing two confidence intervals, but should be glad to develop effective approximations.\n\nIn trying to reconcile the two replies already given (by @John and @Brett), it helps to be mathematically explicit.  A formula for a symmetric two-sided confidence interval appropriate for the setting of this question is\n\n$$\\text{CI} = m \\pm \\frac{t_\\alpha(n) s}{\\sqrt{n}}$$\n\nwhere \n$m$\n is the sample mean of \n$n$\n independent observations, \n$s$\n is the sample standard deviation, \n$2\\alpha$\n is the desired test size (maximum false positive rate), and \n$t_\\alpha(n)$\n is the upper \n$1-\\alpha$\n percentile of the Student t distribution with \n$n-1$\n degrees of freedom.  (This slight deviation from conventional notation simplifies the exposition by obviating any need to fuss over the \n$n$\n \nvs\n \n$n-1$\n distinction, which will be inconsequential anyway.)\n\nUsing subscripts \n$1$\n and \n$2$\n to distinguish two independent sets of data for comparison, with \n$1$\n corresponding to the larger of the two means, a \nnon\n-overlap of confidence intervals is expressed by the inequality (lower confidence limit 1) \n$\\gt$\n (upper confidence limit 2); \nviz.\n,\n\n$$m_1 - \\frac{t_\\alpha(n_1) s_1}{\\sqrt{n_1}} \\gt m_2 + \\frac{t_\\alpha(n_2) s_2}{\\sqrt{n_2}}.$$\n\nThis can be made to look like the t-statistic of the corresponding hypothesis test (to compare the two means) with simple algebraic manipulations, yielding\n\n$$\\frac{m_1-m_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}} \\gt \\frac{s_1\\sqrt{n_2}t_\\alpha(n_1) + s_2\\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 s_2^2 + n_2 s_1^2}}.$$\n\nThe left hand side is the statistic used in the hypothesis test; it is usually compared to a percentile of a Student t distribution with \n$n_1+n_2$\n degrees of freedom: that is, to \n$t_\\alpha(n_1+n_2)$\n.  The right hand side is a biased weighted average of the original t distribution percentiles.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18217/402101 \n $$\\frac{m_1-m_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}} \\gt \\frac{s_1\\sqrt{n_2}t_\\alpha(n_1) + s_2\\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 s_2^2 + n_2 s_1^2}}.$$\n\nThe left hand side is the statistic used in the hypothesis test; it is usually compared to a percentile of a Student t distribution with \n$n_1+n_2$\n degrees of freedom: that is, to \n$t_\\alpha(n_1+n_2)$\n.  The right hand side is a biased weighted average of the original t distribution percentiles.\n\nThe analysis so far justifies the reply by @Brett: there appears to be no simple relationship available.\n  However, let's probe further.  I am inspired to do so because, intuitively, a non-overlap of confidence intervals \nought\n to say something!\n\nFirst, notice that this form of the hypothesis test is valid only when we expect \n$s_1$\n and \n$s_2$\n to be at least approximately equal.  (Otherwise we face the notorious \nBehrens-Fisher problem\n and its complexities.)  Upon checking the approximate equality of the \n$s_i$\n, we could then create an approximate simplification in the form\n\n$$\\frac{m_1-m_2}{s\\sqrt{1/n_1 + 1/n_2}} \\gt \\frac{\\sqrt{n_2}t_\\alpha(n_1) + \\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 + n_2}}.$$\n\nHere, \n$s \\approx s_1 \\approx s_2$\n.  Realistically, we should not expect this informal comparison of confidence limits to have the same size as \n$\\alpha$\n.  Our question then is whether there exists an \n$\\alpha'$\n such that the right hand side is (at least approximately) equal to the correct t statistic.  Namely, for what \n$\\alpha'$\n is it the case that\n\n$$t_{\\alpha'}(n_1+n_2) = \\frac{\\sqrt{n_2}t_\\alpha(n_1) + \\sqrt{n_1}t_\\alpha(n_2)}{\\sqrt{n_1 + n_2}}\\text{?}$$\n\nIt turns out that for equal sample sizes, \n$\\alpha$\n and \n$\\alpha'$\n are connected (to pretty high accuracy) by a power law.\n  For instance, here is a log-log plot of the two for the cases \n$n_1=n_2=2$\n (lowest blue line), \n$n_1=n_2=5$\n (middle red line), \n$n_1=n_2=\\infty$\n (highest gold line).  The middle green dashed line is an approximation described below.  The straightness of these curves belies a power law.  It varies with \n$n=n_1=n_2$\n, but not much.\n\n\n\nThe answer does depend on the set \n$\\{n_1, n_2\\}$\n, but it is natural to wonder how much it really varies with changes in the sample sizes.  In particular, we could hope that for moderate to large sample sizes (maybe \n$n_1 \\ge 10, n_2 \\ge 10$\n or thereabouts) the sample size makes little difference.  In this case, we could develop a \nquantitative\n way to relate \n$\\alpha'$\n to \n$\\alpha$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18217/402101 \n The answer does depend on the set \n$\\{n_1, n_2\\}$\n, but it is natural to wonder how much it really varies with changes in the sample sizes.  In particular, we could hope that for moderate to large sample sizes (maybe \n$n_1 \\ge 10, n_2 \\ge 10$\n or thereabouts) the sample size makes little difference.  In this case, we could develop a \nquantitative\n way to relate \n$\\alpha'$\n to \n$\\alpha$\n.\n\nThis approach turns out to work provided the sample sizes are not too different from each other.  In the spirit of simplicity, I will report an omnibus formula for computing the test size \n$\\alpha'$\n corresponding to the confidence interval size \n$\\alpha$\n.  It is\n\n$$\\alpha' \\approx e \\alpha^{1.91};$$\n\nthat is,\n\n$$\\alpha' \\approx \\exp(1 + 1.91\\log(\\alpha)).$$\n\nThis formula works reasonably well in these common situations:\n\nBoth sample sizes are close to each other, \n$n_1 \\approx n_2$\n, and \n$\\alpha$\n is not too extreme (\n$\\alpha \\gt .001$\n or so).\n\n\n\n\nOne sample size is within about three times the other and the smallest isn't too small (roughly, greater than \n$10$\n) and again \n$\\alpha$\n is not too extreme.\n\n\n\n\nOne sample size is within three times the other and \n$\\alpha \\gt .02$\n or so.\n\nBoth sample sizes are close to each other, \n$n_1 \\approx n_2$\n, and \n$\\alpha$\n is not too extreme (\n$\\alpha \\gt .001$\n or so).\n\nOne sample size is within about three times the other and the smallest isn't too small (roughly, greater than \n$10$\n) and again \n$\\alpha$\n is not too extreme.\n\nOne sample size is within three times the other and \n$\\alpha \\gt .02$\n or so.\n\nThe relative error (correct value divided by the approximation) in the first situation is plotted here, with the lower (blue) line showing the case \n$n_1=n_2=2$\n, the middle (red) line the case \n$n_1=n_2=5$\n, and the upper (gold) line the case \n$n_1=n_2=\\infty$\n.  Interpolating between the latter two, we see that the approximation is excellent for a wide range of practical values of \n$\\alpha$\n when sample sizes are moderate (around 5-50) and otherwise is reasonably good.\n\n\n\nThis is more than good enough for eyeballing a bunch of confidence intervals.\n\nTo summarize,\n the failure of two \n$2\\alpha$\n-size confidence intervals of means to overlap is significant evidence of a difference in means at a level equal to \n$2e \\alpha^{1.91}$\n, provided the two samples have approximately equal standard deviations and are approximately the same size.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18217/402101 \n This is more than good enough for eyeballing a bunch of confidence intervals.\n\nTo summarize,\n the failure of two \n$2\\alpha$\n-size confidence intervals of means to overlap is significant evidence of a difference in means at a level equal to \n$2e \\alpha^{1.91}$\n, provided the two samples have approximately equal standard deviations and are approximately the same size.\n\nI'll end with a tabulation of the approximation for common values of \n$2\\alpha$\n.  In the left hand column is the nominal size \n$2\\alpha$\n of the original confidence interval; in the right hand column is the actual size \n$2\\alpha^\\prime$\n of the comparison of two such intervals:\n\n$$\\begin{array}{ll}\n2\\alpha & 2\\alpha^\\prime \\\\   \\hline \n0.1   &0.02\\\\  \n0.05  &0.005\\\\  \n0.01  &0.0002\\\\  \n0.005 &0.00006\\\\  \n\\end{array}$$\n\nFor example, when a pair of two-sided 95% CIs (\n$2\\alpha=.05$\n) for samples of approximately equal sizes do not overlap, we should take the means to be significantly different, \n$p \\lt .005$\n.  The correct p-value (for equal sample sizes \n$n$\n) actually lies between \n$.0037$\n (\n$n=2$\n) and \n$.0056$\n (\n$n=\\infty$\n).\n\nThis result justifies (and I hope improves upon) the reply by @John.  Thus, although the previous replies appear to be in conflict, both are (in their own ways) correct.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n I am going to change the order of questions about.\n\nI've found textbooks and lecture notes frequently disagree, and would like a system to work through the choice that can safely be recommended as best practice, and especially a textbook or paper this can be cited to.\n\nUnfortunately, some discussions of this issue in books and so on rely on received wisdom. Sometimes that received wisdom is reasonable, sometimes it is less so (at the least in the sense that it tends to focus on a smaller issue when a larger problem is ignored); we should examine the justifications offered for the advice (if any justification is offered at all) with care.\n\nMost guides to choosing a t-test or non-parametric test focus on the normality issue.\n\nThat\u2019s true, but it\u2019s somewhat misguided for several reasons that I address in this answer.\n\nIf performing an \"unrelated samples\" or \"unpaired\" t-test, whether to use a Welch correction?\n\nThis (to use it unless you have reason to think variances should be equal) is the advice of numerous references. I point to some in this answer.\n\nSome people use a hypothesis test for equality of variances, but here it would have low power. Generally I just eyeball whether the sample SDs are \"reasonably\" close or not (which is somewhat subjective, so there must be a more principled way of doing it) but again, with low n it may well be that the population SDs are rather further apart than the sample ones.\n\nIs it safer simply to always use the Welch correction for small samples, unless there is some good reason to believe population variances are equal?\nThat\u2019s what the advice is. The properties of the tests are affected by the choice based on the assumption test.\n\nSome references on this can be seen \nhere\n and \nhere\n,  though there are more that say similar things.\n\nThe equal-variances issue has many similar characteristics to the normality issue \u2013 people want to test it, advice suggests conditioning choice of tests on the results of tests can adversely affect the results of both kinds of subsequent test \u2013 it\u2019s better simply not to assume what you can\u2019t adequately justify (by reasoning about the data, using information from other studies relating to the same variables and so on).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n The equal-variances issue has many similar characteristics to the normality issue \u2013 people want to test it, advice suggests conditioning choice of tests on the results of tests can adversely affect the results of both kinds of subsequent test \u2013 it\u2019s better simply not to assume what you can\u2019t adequately justify (by reasoning about the data, using information from other studies relating to the same variables and so on).\n\nHowever,  there are differences. One is that \u2013 at least in terms of the distribution of the test statistic under the null hypothesis (and hence, its level-robustness) - non-normality is less important in large samples (at least in respect of significance level, though power might still be an issue if you need to find small effects), while the effect of unequal variances under the equal variance assumption doesn\u2019t really go away with large sample size.\n\nWhat principled method can be recommended for choosing which is the most appropriate test when the sample size is \"small\"?\n\nWith hypothesis tests, what matters (under some set of conditions) is primarily two things:\n\nWhat is the actual type I error rate?\n\n\n\n\nWhat is the power behaviour like?\n\nWhat is the actual type I error rate?\n\nWhat is the power behaviour like?\n\nWe also need to keep in mind that if we're comparing two procedures, changing the first will change the second (that is, if they\u2019re not conducted at the same actual significance level, you would expect that higher \n$\\alpha$\n is associated with higher power).\n\n(Of course we're usually not so confident we know what distributions we're dealing with, so the sensitivity of those behaviors to changes in circumstances also matter.)\n\nWith these small-sample issues in mind, is there a good - hopefully citable - checklist to work through when deciding between t and non-parametric tests?\n\nI will consider a number of situations in which I\u2019ll make some recommendations, considering both the possibility of non-normality and unequal variances. In every case, take mention of the t-test to imply the Welch-test:\n\nn medium-large\n\nNon-normal (or unknown), likely to have near-equal variance:\n\nIf the distribution is heavy-tailed, you will generally be better with a Mann-Whitney, though if it\u2019s only slightly heavy, the t-test should do okay. With light-tails the t-test may (often) be preferred.  Permutation tests are a good option (you can even do a permutation test using a t-statistic if you're so inclined). Bootstrap tests are also suitable.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n n medium-large\n\nNon-normal (or unknown), likely to have near-equal variance:\n\nIf the distribution is heavy-tailed, you will generally be better with a Mann-Whitney, though if it\u2019s only slightly heavy, the t-test should do okay. With light-tails the t-test may (often) be preferred.  Permutation tests are a good option (you can even do a permutation test using a t-statistic if you're so inclined). Bootstrap tests are also suitable.\n\nNon-normal (or unknown), unequal variance (or variance relationship unknown):\n\nIf the distribution is heavy-tailed, you will generally be better with a Mann-Whitney\n\nif inequality of variance is only related to inequality of mean - i.e. if H0 is true the difference in spread should also be absent. GLMs are often a good option, especially if there\u2019s skewness and spread is related to the mean. A permutation test is another option, with a similar caveat as for the rank-based tests. Bootstrap tests are a good possibility here.\n\nZimmerman and Zumbo (1993)\n$^{[1]}$\n suggest a Welch-t-test on the ranks which they say performs better that the Wilcoxon-Mann-Whitney in cases where the variances are unequal.\n\nn moderately small\n\nrank tests are reasonable defaults here if you expect non-normality (again with the above caveat). If you have external information about shape or variance, you might consider GLMs . If you expect things not to be too far from normal, t-tests may be fine.\n\nn very small\n\nBecause of the problem with getting suitable significance levels, neither permutation tests nor rank tests may be suitable, and at the smallest sizes, a t-test may be the best option (there\u2019s some possibility of slightly robustifying it). However, there\u2019s a good argument for using higher type I error rates with small samples (otherwise you\u2019re letting type II error rates inflate while holding type I error rates constant).\nAlso see de Winter (2013)\n$^{[2]}$\n.\n\nThe advice must be modified somewhat when the distributions are both strongly skewed and very discrete, such as Likert scale items where most of the observations are in one of the end categories. Then the Wilcoxon-Mann-Whitney isn\u2019t necessarily a better choice than the t-test.\n\nSimulation can help guide choices further when you have some information about likely circumstances.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n The advice must be modified somewhat when the distributions are both strongly skewed and very discrete, such as Likert scale items where most of the observations are in one of the end categories. Then the Wilcoxon-Mann-Whitney isn\u2019t necessarily a better choice than the t-test.\n\nSimulation can help guide choices further when you have some information about likely circumstances.\n\nI appreciate this is something of a perennial topic, but most questions concern the questioner's particular data set, sometimes a more general discussion of power, and occasionally what to do if two tests disagree, but I would like a procedure to pick the correct test in the first place!\n\nThe main problem is how hard it is to check the normality assumption in a small data set:\n\nIt \nis\n difficult to check normality in a small data set, and to some extent that's an important issue, but I think there's another issue of importance that we need to consider. A basic problem is that trying to assess normality as the basis of choosing between tests adversely impacts the properties of the tests you're choosing between.\n\nAny formal test for normality would have low power so violations may well not be detected. (Personally I wouldn't test for this purpose, and I'm clearly not alone, but\nI've found this little use when clients demand a normality test be performed because that's what their textbook or old lecture notes or some website they found once declare should be done. This is one point where a weightier looking citation would be welcome.)\n\nHere\u2019s an example of a reference (there are others)  which is unequivocal (Fay and Proschan, 2010\n$^{[3]}$\n):\n\nThe choice between t- and WMW DRs should not be based on a test of normality.\n\nThey are similarly unequivocal about not testing for equality of variance.\n\nTo make matters worse, it is unsafe to use the Central Limit Theorem as a safety net: for small n we can't rely on the convenient asymptotic normality of the test statistic and t distribution.\n\nNor even in large samples -- asymptotic normality of the numerator doesn\u2019t imply that the t-statistic will have a t-distribution. However, that may not matter so much, since  you should still have asymptotic normality (e.g. CLT for the numerator, and Slutsky\u2019s theorem suggest that eventually the t-statistic should begin to look normal, if the conditions for both hold.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n Nor even in large samples -- asymptotic normality of the numerator doesn\u2019t imply that the t-statistic will have a t-distribution. However, that may not matter so much, since  you should still have asymptotic normality (e.g. CLT for the numerator, and Slutsky\u2019s theorem suggest that eventually the t-statistic should begin to look normal, if the conditions for both hold.)\n\nOne principled response to this is \"safety first\": as there's no way to reliably verify the normality assumption on a small sample, run an equivalent non-parametric test instead.\n\nThat\u2019s actually the advice that the references I mention (or link to mentions of) give.\n\nAnother approach I've seen but feel less comfortable with, is to perform a visual check and proceed with a t-test if nothing untowards is observed (\"no reason to reject normality\", ignoring the low power of this check). My personal inclination is to consider whether there are any grounds for assuming normality, theoretical (e.g. variable is sum of several random components and CLT applies) or empirical (e.g. previous studies with larger n suggest variable is normal).\n\nBoth those are good arguments, especially when backed up with the fact that the t-test is reasonably robust against moderate deviations from normality.\n(One should keep in mind, however, that \"moderate deviations\" is a tricky phrase; certain kinds of deviations from normality may impact the power performace of the t-test quite a bit even though those deviations are visually very small - the t-test is less robust to some deviations than others. We should keep this in mind whenever we're discussing small deviations from normality.)\n\nBeware, however, the phrasing \"suggest the variable is normal\". Being reasonably consistent with normality is not the same thing as normality. We can often reject actual normality with no need even to see the data \u2013 for example, if the data cannot be negative, the distribution cannot be normal. Fortunately, what matters is closer to what we might actually have from previous studies or reasoning about how the data are composed, which is that the deviations from normality should be small.\n\nIf so, I would use a t-test if data passed visual inspection, and otherwise stick to non-parametrics. But any theoretical or empirical grounds usually only justify assuming approximate normality, and on low degrees of freedom it's hard to judge how near normal it needs to be to avoid invalidating a t-test.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n If so, I would use a t-test if data passed visual inspection, and otherwise stick to non-parametrics. But any theoretical or empirical grounds usually only justify assuming approximate normality, and on low degrees of freedom it's hard to judge how near normal it needs to be to avoid invalidating a t-test.\n\nWell, that\u2019s something we can assess the impact of fairly readily (such as via simulations, as I mentioned earlier). From what I've seen, skewness seems to matter more than heavy tails (but on the other hand I have seen some claims of the opposite - though I don't know what that's based on).\n\nFor people who see the choice of methods as a trade-off between power and robustness, claims about the asymptotic efficiency of the non-parametric methods are unhelpful. For instance, the rule of thumb that \"Wilcoxon tests have about 95% of the power of a t-test if the data really are normal, and are often far more powerful if the data is not, so just use a Wilcoxon\" is sometimes heard, but if the 95% only applies to large n, this is flawed reasoning for smaller samples.\n\nBut we can check small-sample power quite easily! It\u2019s easy enough to simulate to obtain power curves \nas here\n.\n\n(Again, also see de Winter (2013)\n$^{[2]}$\n).\n\nHaving done such simulations under a variety of circumstances, both for the two-sample and one-sample/paired-difference cases, the small sample efficiency at the normal in both cases seems to be a little lower than the asymptotic efficiency, but the efficiency of the signed rank and Wilcoxon-Mann-Whitney tests is still very high even at very small sample sizes.\n\nAt least that's if the tests are done at the same actual significance level; you can't do a 5% test with very small samples (and least not without randomized tests for example), but if you're prepared to perhaps do (say) a 5.5% or a 3.2% test instead, then the rank tests hold up very well indeed compared with a t-test at that significance level.\n\nSmall samples may make it very difficult, or impossible, to assess whether a transformation is appropriate for the data since it's hard to tell whether the transformed data belong to a (sufficiently) normal distribution. So if a QQ plot reveals very positively skewed data, which look more reasonable after taking logs, is it safe to use a t-test on the logged data? On larger samples this would be very tempting, but with small n I'd probably hold off unless there had been grounds to expect a log-normal distribution in the first place.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n There\u2019s another alternative: make a different parametric assumption. For example, if there\u2019s skewed data, one might, for example, in some situations reasonably consider a gamma distribution, or some other skewed family as a better approximation - in moderately large samples, we might just use a GLM, but in very small samples it may be necessary to look to a small-sample test - in many cases simulation can be useful.\n\nAlternative 2: robustify the t-test (but taking care about the choice of robust procedure so as not to heavily discretize the resulting distribution of the test statistic)  -  this has some advantages over a very-small-sample nonparametric procedure such as the ability to consider tests with low type I error rate.\n\nHere I'm thinking along the lines of using say M-estimators of location (and related estimators of scale) in the t-statistic to smoothly robustify against deviations from normality. Something akin to the Welch, like:\n\n$$\\frac{\\stackrel{\\sim}{x}-\\stackrel{\\sim}{y}}{\\stackrel{\\sim}{S}_p}$$\n\nwhere \n$\\stackrel{\\sim}{S}_p^2=\\frac{\\stackrel{\\sim}{s}_x^2}{n_x}+\\frac{\\stackrel{\\sim}{s}_y^2}{n_y}$\n and \n$\\stackrel{\\sim}{x}$\n, \n$\\stackrel{\\sim}{s}_x$\n etc being robust estimates of location and scale respectively.\n\nI'd aim to reduce any tendency of the statistic to discreteness - so I'd avoid things like trimming and Winsorizing, since if the original data were discrete, trimming etc will exacerbate this; by using M-estimation type approaches with a smooth \n$\\psi$\n-function you achieve similar effects without contributing to the discreteness. Keep in mind we're trying to deal with the situation where \n$n$\n is very small indeed (around 3-5, in each sample, say), so even M-estimation potentially has its issues.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n You could, for example, use simulation at the normal to get p-values (if sample sizes are very small, I'd suggest that over bootstrapping - if sample sizes aren't so small, a carefully-implemented bootstrap may do quite well, but then we might as well go back to Wilcoxon-Mann-Whitney). There's be a scaling factor as well as a d.f. adjustment to get to what I'd imagine would then be a reasonable t-approximation. This means we should get the kind of properties we seek very close to the normal, and should have reasonable robustness in the broad vicinity of the normal. There are a number of issues that come up that would be outside the scope of the present question, but I think in very small samples the benefits should outweigh the costs and the extra effort required.\n\n[I haven't read the literature on this stuff for a very long time, so I don't have suitable references to offer on that score.]\n\nOf course if you didn't expect the distribution to be somewhat normal-like, but rather similar to some other distribution, you could undertake a suitable robustification of a different parametric test.\n\nWhat if you want to check assumptions for the non-parametrics? Some sources recommend verifying a symmetric distribution before applying a Wilcoxon test, which brings up similar problems to checking normality.\n\nIndeed. I assume you mean the signed rank test*. In the case of using it on paired data, if you are prepared to assume that the two distributions are the same shape apart from location shift you are safe, since the differences should then be symmetric. Actually, we don't even need that much; for the test to work you need symmetry under the null; it's not required under the alternative (e.g. consider a paired situation with identically-shaped right skewed continuous distributions on the positive half-line, where the scales differ under the alternative but not under the null; the signed rank test should work essentially as expected in that case). The interpretation of the test is easier if the alternative is a location shift though.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/123389/402101 \n *(Wilcoxon\u2019s name is  associated with both the one and two sample rank tests \u2013 signed rank and rank sum; with their U test, Mann and Whitney generalized the situation studied by Wilcoxon, and introduced important new ideas for evaluating the null distribution, but the priority between the two sets of authors on the Wilcoxon-Mann-Whitney is clearly Wilcoxon\u2019s -- so at least if we only consider Wilcoxon vs Mann&Whitney, Wilcoxon goes first in my book. However, it seems \nStigler's Law\n beats me yet again, and Wilcoxon should perhaps share some of that priority with a number of earlier contributors, and (besides Mann and Whitney) should share credit with several discoverers of an equivalent test.[4][5] )\n\nReferences\n\n[1]: Zimmerman DW and Zumbo BN, (1993),\n\n\nRank transformations and the power of the Student t-test and Welch t\u2032-test for non-normal populations,\n\nCanadian Journal Experimental Psychology, \n47\n: 523\u201339.\n\n[2]: J.C.F. de Winter (2013),\n\n\"Using the Student\u2019s t-test with extremely small sample sizes,\"\n\n\nPractical Assessment, Research and Evaluation\n,  \n18\n:10, August, ISSN 1531-7714\n\n\nhttps://openpublishing.library.umass.edu/pare/article/id/1434/\n\n[3]: Michael P. Fay and Michael A. Proschan (2010),\n\n\"Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules,\"\n\n\nStat Surv\n; \n4\n: 1\u201339.\n\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/\n\n[4]: Berry, K.J., Mielke, P.W. and Johnston, J.E. (2012),\n\n\"The Two-sample Rank-sum Test: Early Development,\"\n\n\nElectronic Journal for History of Probability and Statistics\n, Vol.8, December\n\n\npdf\n\n[5]: Kruskal, W. H. (1957),\n\n\"Historical notes on the Wilcoxon unpaired two-sample test,\"\n\n\nJournal of the American Statistical Association\n, \n52\n, 356\u2013360.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/203310/402101 \n I have argued that variable importance is a \nslippery concept\n, as this question posits. The tautological first type of response that you get to your question and the unrealistic hopes of those who would interpret variable-importance results in terms of causality, as noted by @DexGroves, need little elaboration.\n\nIn fairness to those who would use backward selection, however, even Frank Harrell allows for it as part of a modeling strategy. From page 97 of his \nRegression Modeling Strategies\n, 2nd edition (a similar statement is on page 131 of the associated \ncourse notes\n):\n\nDo limited backwards step-down variable selection if parsimony is more\n  important than accuracy.\n\nThis limited potential use of backward selection, however, is step 13, the last step before the final model (step 14). It comes well after the crucial first steps:\n\nAssemble as much accurate pertinent data as possible, with wide distributions for predictor values...\n\n\nFormulate good hypotheses that lead to specification of relevant candidate predictors and possible interactions...\n\nIn my experience people often want to bypass step 2, and let some automated procedure replace intelligent application of subject-matter knowledge. This may lead to some of the emphasis placed on variable importance.\n\nThe full model of Harrell's step 14 is followed by 5 further steps of validation and adjustment, with a last step:\n\nDevelop simplifications to the full model by approximating it to any desired degrees of accuracy.\n\nAs other answers have noted, there are issues of actionability, cost, and simplicity that enter into the practical application of modeling results. For example, if I develop a new cancer biomarker that improves prognostication but costs $100,000 per test, it might be difficult to convince insurers or the government to pay for the test unless it is spectacularly useful. So it's not unreasonable for someone to want to focus on variables that are \"most important,\" or to simplify an accurate model into one that is somewhat less accurate but is easier or less expensive to implement.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/203310/402101 \n But this variable selection and model simplification should be \nfor a specific purpose\n, and I think that is where the difficulty arises. The issue is similar to assessing classification schemes solely on the basis of percent of cases correctly classified. Just as different classification errors can have different costs, different model simplification schemes can have different costs that balance against their hoped-for benefits.\n\nSo I think that the issue to focus on as the analyst is the ability to estimate and illustrate these costs and benefits reliably with statistical modeling procedures, rather than worrying too much about an abstract concept of statistically validity per se. For example, pages 157-8 of Harrell's class notes linked above has an example of using the bootstrap to show the vagaries of ranking predictors in least squares; similar results can be found for variables sets selected by LASSO.\n\nIf that type of variability in variable selection doesn't get in the way of a particular practical application of the model that's OK. The job is to estimate how much and what type of trouble that simplification will lead to.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230419/402101 \n A probability space $\\mathcal{P}$ is by definition a tripple $(\\Omega, \\mathcal{F}, \\mathbb{P} )$ where $\\Omega$ is a set of outcomes, $\\mathcal{F}$ is a $\\sigma$-algebra on the subsets of $\\Omega$ and $\\mathbb{P}$ is a probability-measure that fulfills the axioms of Kolmogorov, i.e. $\\mathbb{P}$ is a function from $\\mathcal{F}$ to $[0,1]$ such that $\\mathbb{P}(\\Omega)=1$ and for disjoint $E_1, E_2, \\dots$ in $\\mathcal{F}$ it holds that $P \\left( \\cup_{j=1}^\\infty E_j \\right)=\\sum_{j=1}^\\infty \\mathbb{P}(E_j)$.\n\nWithin such a probability space one can, for two events $E_1, E_2$ in $\\mathcal{F}$ define the conditional probability as $\\mathbb{P}(E_1|_{E_2})\\stackrel{def}{=}\\frac{\\mathbb{P}(E_1 \\cap E_2)}{\\mathbb{P}(E_2)}$\n\nNote that:\n\nthis ''conditional probability'' is only defined when $\\mathbb{P}$ is defined on $\\mathcal{F}$, so we need a probability space to be able to define conditional probabilities. \n\n\nA probability space is defined in very general terms (\na\n set $\\Omega$, \na\n $\\sigma$-algebra $\\mathcal{F}$ and \na\n probability measure $\\mathbb{P}$), the only requirement is that certain properties should be fulfilled but apart from that these three elements can be ''anything''.\n\nMore detail can be found in \nthis link\n\nFrom the definition of conditional probability it also holds that $\\mathbb{P}(E_2|_{E_1})=\\frac{\\mathbb{P}(E_2 \\cap E_1)}{\\mathbb{P}(E_1)}$. And from the two latter equations we find Bayes' rule.  So Bayes' rule holds (by definition of conditional probabilty) in any probability space (to show it, derive $\\mathbb{P}(E_1 \\cap E_2)$ and $\\mathbb{P}(E_2 \\cap E_1)$ from each equation and equate them (they are equal because intersection is commutative)).\n\nAs Bayes rule is the basis for Bayesian inference, one can do Bayesian analysis in any valid (i.e. fulfilling all conditions, a.o. Kolmogorov's axioms) probability space.\n\nThe above holds ''in general'', i.e. we have no specific $\\Omega$, $\\mathcal{F}$, $\\mathbb{P}$ in mind as long as $\\mathcal{F}$ is a $\\sigma$-algebra on subsets of $\\Omega$ and $\\mathbb{P}$ fulfills Kolmogorov's axioms.\n\nWe will now show that a ''frequentist'' definition of $\\mathbb{P}$ fulfills Kolomogorov's axioms.  If that is the case then ''frequentist'' probabilities are only a special case of Kolmogorov's general and abstract probability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230419/402101 \n The above holds ''in general'', i.e. we have no specific $\\Omega$, $\\mathcal{F}$, $\\mathbb{P}$ in mind as long as $\\mathcal{F}$ is a $\\sigma$-algebra on subsets of $\\Omega$ and $\\mathbb{P}$ fulfills Kolmogorov's axioms.\n\nWe will now show that a ''frequentist'' definition of $\\mathbb{P}$ fulfills Kolomogorov's axioms.  If that is the case then ''frequentist'' probabilities are only a special case of Kolmogorov's general and abstract probability.\n\nLet's take an example and roll the dice. Then the set of all possible outcomes $\\Omega$ is $\\Omega=\\{1,2,3,4,5,6\\}$. We also need a $\\sigma$-algebra on this set $\\Omega$ and we take $\\mathcal{F}$ the set of all subsets of $\\Omega$, i.e. $\\mathcal{F}=2^\\Omega$.\n\nWe still have to define the probability measure $\\mathbb{P}$ in a frequentist way. Therefore we define $\\mathbb{P}(\\{1\\})$ as $\\mathbb{P}(\\{1\\}) \\stackrel{def}{=} \\lim_{n \\to +\\infty} \\frac{n_1}{n}$ where $n_1$ is the number of $1$'s obtained in $n$ rolls of the dice. Similar for $\\mathbb{P}(\\{2\\})$, ... $\\mathbb{P}(\\{6\\})$.\n\nIn this way $\\mathbb{P}$ is defined for all singletons in $\\mathcal{F}$.  For any other set in $\\mathcal{F}$, e.g. $\\{1,2\\}$ we define $\\mathbb{P}(\\{1,2\\})$ in a frequentist way i.e. \n$\\mathbb{P}(\\{1,2\\}) \\stackrel{def}{=} \\lim_{n \\to +\\infty} \\frac{n_1+n_2}{n}$, but by the linearity of the 'lim', this is equal to $\\mathbb{P}(\\{1\\})+\\mathbb{P}(\\{2\\})$, which implies that Kolmogorov's axioms hold.\n\nSo the frequentist definition of probability is only a special case of Kolomogorov's general and abstract definition of a probability measure.\n\nNote that there are other ways to define a probability measure that fulfills Kolmogorov's axioms, so the frequentist definition is not the only possible one.\n\nThe probability in Kolmogorov's axiomatic system is ''abstract'', it has no real meaning, it only has to fulfill conditions called ''axioms''.  Using only these axioms Kolmogorov was able to derive a very rich set of theorems.\n\nThe frequentist definition of probability fullfills the axioms and therefore replacing the abstract, ''meaningless'' $\\mathbb{P}$ by a probability defined in a frequentist way, all these theorems are valid because the \n''frequentist probability'' is only a special case of Kolmogorov's abstract probability (i.e. it fulfills the axioms).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230419/402101 \n The frequentist definition of probability fullfills the axioms and therefore replacing the abstract, ''meaningless'' $\\mathbb{P}$ by a probability defined in a frequentist way, all these theorems are valid because the \n''frequentist probability'' is only a special case of Kolmogorov's abstract probability (i.e. it fulfills the axioms).\n\nOne of the properties that can be derived in Kolmogorov's general framework is Bayes rule.  As it holds in the general and abstract framework, it will also hold (cfr supra) in the specific case that the probabilities are defined in a frequentist way (because the frequentist definition fulfills the axioms and these axioms were the only thing that is needed to derive all theorems).  \nSo one can do Bayesian analysis with a frequentist definition of probability.\n\nDefining $\\mathbb{P}$ in a frequentist way is not the only possibility, there are other ways to define it such that it fulfills the abstract axioms of Kolmogorov.  Bayes' rule will also hold in these ''specific cases''. \nSo one can also do Bayesian analysis with a \nnon\n-frequentist definition of probability.\n\n@mpiktas reaction to your comment:\n\nAs I said, the sets $\\Omega, \\mathcal{F}$ and the probability measure $\\mathbb{P}$ have no particular meaning in the axiomatic system, they are abstract.\n\nIn order to apply this theory you have to give further \ndefinitions\n (so what you say in your comment \n\"no need to muddle it further with some bizarre definitions''\n is \nwrong, you need additional definitions\n).\n\nLet's apply it to the case of tossing a fair coin.  The set $\\Omega$ in Kolmogorov's theory has no particular meaning, it just has to be ''a set''.  So we must specify what this set is in case of the fair coin, i.e. we must define the set $\\Omega$.  If we represent head as H and tail as T, then the set $\\Omega$ is \nby definition\n $\\Omega\\stackrel{def}{=}\\{H,T\\}$.\n\nWe also have to \ndefine\n the events, i.e. the $\\sigma$-algebra $\\mathcal{F}$. We define is as $\\mathcal{F} \\stackrel{def}{=} \\{\\emptyset, \\{H\\},\\{T\\},\\{H,T\\} \\}$. It is easy to verify that $\\mathcal{F}$ is a $\\sigma$-algebra.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230419/402101 \n We also have to \ndefine\n the events, i.e. the $\\sigma$-algebra $\\mathcal{F}$. We define is as $\\mathcal{F} \\stackrel{def}{=} \\{\\emptyset, \\{H\\},\\{T\\},\\{H,T\\} \\}$. It is easy to verify that $\\mathcal{F}$ is a $\\sigma$-algebra.\n\nNext we must define for every event in $E \\in \\mathcal{F}$ its measure.  So we need to \ndefine\n a map from $\\mathcal{F}$ in $[0,1]$.  I will define it in the frequentist way, for a fair coin, if I toss it a huge number of times, then the fraction of heads will be 0.5, so I define $\\mathbb{P}(\\{H\\})\\stackrel{def}{=}0.5$.  Similarly I define $\\mathbb{P}(\\{T\\})\\stackrel{def}{=}0.5$, $\\mathbb{P}(\\{H,T\\})\\stackrel{def}{=}1$ and $\\mathbb{P}(\\emptyset)\\stackrel{def}{=}0$. Note that $\\mathbb{P}$ is a map from $\\mathcal{F}$ in $[0,1]$ and that it fulfills Kolmogorov's axioms.\n\nFor a reference with the frequentist definition of probability see \nthis link\n (at the end of the section 'definition') and \nthis link\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3909/402101 \n I think it's useful to review what we know about cross-validation.  Statistical results around CV fall into two classes: efficiency and consistency.\n\nEfficiency is what we're usually concerned with when building predictive models.  The idea is that we use CV to determine a model with asymtptotic guarantees concerning the loss function.  The most famous result here is due to \nAn asymptotic equivalence of choice of model by cross\u2010validation and Akaike's criterion\n (Stone 1977) and shows that LOO CV is asymptotically equivalent to AIC.  But, Brett provides a good example where you can find a predictive model which doesn't inform you on the causal mechanism.\n\nConsistency is what we're concerned with if our goal is to find the \"true\" model.  The idea is that we use CV to determine a model with asymptotic guarantees that, given that our model space includes the true model, we'll discover it with a large enough sample.  The most famous result here is due to \nLinear Model Selection by Cross-Validation\n (Shao 1993) concerning linear models, but as he states in his abstract, his \"shocking discovery\" is opposite of the result for LOO.  For linear models, you can achieve consistency using LKO CV as long as \n$k/n \\rightarrow 1$\n as \n$n \\rightarrow \\infty$\n.  Beyond linear mdoels, it's harder to derive statistical results.\n\nBut suppose you can meet the consistency criteria and your CV procedure leads to the true model: \n$Y = \\beta X + e$\n.  What have we learned about the causal mechanism?  We simply know that there's a well defined correlation between \n$Y$\n and \n$X$\n, which doesn't say much about causal claims.  From a traditional perspective, you need to bring in experimental design with the mechanism of control/manipulation to make causal claims.  From the perspective of Judea Pearl's framework, you can bake causal assumptions into a structural model and use the probability based calculus of counterfactuals to derive some claims, but you'll need to satisfy \ncertain properties\n.\n\nPerhaps you could say that CV can help with causal inference by identifying the true model (provided you can satisfy consistency criteria!).  But it only gets you so far; CV by itself isn't doing any of the work in either framework of causal inference.\n\nIf you're interested further in what we can say with cross-validation, I would recommend Shao 1997 over the widely cited 1993 paper:\n\nAn Asymptotic Theory for Linear Model Selection\n (Shao, 1997)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/3909/402101 \n Perhaps you could say that CV can help with causal inference by identifying the true model (provided you can satisfy consistency criteria!).  But it only gets you so far; CV by itself isn't doing any of the work in either framework of causal inference.\n\nIf you're interested further in what we can say with cross-validation, I would recommend Shao 1997 over the widely cited 1993 paper:\n\nAn Asymptotic Theory for Linear Model Selection\n (Shao, 1997)\n\nYou can skim through the major results, but it's interesting to read the discussion that follows.  I thought the comments by Rao & Tibshirani, and by Stone, were particularly insightful.  But note that while they discuss consistency, no claims are ever made regarding causality.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/29618/402101 \n The general rule of thumb (based on stuff in Frank Harrell's book, \nRegression Modeling Strategies\n) is that \nif you expect to be able to detect reasonable-size effects with reasonable power\n, you need 10-20 observations per parameter (covariate) estimated.  Harrell discusses a lot of options for \"dimension reduction\" (getting your number of covariates down to a more reasonable size), such as PCA, but the most important thing is that in order to have any confidence in the results \ndimension reduction must be done without looking at the response variable\n. Doing the regression again with just the significant variables, as you suggest above, is in almost every case a bad idea.\n\nHowever, since you're stuck with a data set and a set of covariates you're interested in, I don't think that running the multiple regression this way is inherently wrong. I think the best thing would be to accept the results as they are, from the full model (don't forget to look at the point estimates and confidence intervals to see whether the significant effects are estimated to be \"large\" in some real-world sense, and whether the non-significant effects are actually estimated to be smaller than the significant effects or not).\n\nAs to whether it makes any sense to do an analysis without the predictor that your field considers important: I don't know. It depends what kind of inferences you want to make based on the model. In the narrow sense, the regression model is still well-defined (\"what are the marginal effects of these predictors on this response?\"), but someone in your field might quite rightly say that the analysis just doesn't make sense.  It would help a little bit if you knew that the predictors you have are uncorrelated from the well-known predictor (whatever it is), or that well-known predictor is constant or nearly constant for your data: then at least you could say that something other than the well-known predictor does have an effect on the response.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/20527/402101 \n They mainly differ in the link function.\n\nIn Logit:\n\n$\\Pr(Y=1 \\mid X) = [1 + e^{-X'\\beta}]^{-1} $\n\nIn Probit:\n\n$\\Pr(Y=1 \\mid X) = \\Phi(X'\\beta)$\n   (Cumulative standard normal pdf)\n\nIn other way, logistic has slightly flatter tails. i.e the probit curve approaches the axes more quickly than the logit curve.\n\nLogit has easier interpretation than probit. Logistic regression can be interpreted as modelling log odds (i.e those who smoke >25 cigarettes a day are 6 times more likely to die before 65 years of age). Usually people start the modelling with logit. You could use the likelihood value of each model to decide for logit vs probit.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/218158/402101 \n We can ignore the matrix formulation, and just consider two vectors $x$ and $y$ (since the matrix formulation is just the vector operation repeated over different pairs of vectors). One intuitive/geometric distinction between covariance/correlation/cosine similarity is their invariance to different transformations of the input. That is, if we transform $x$ and $y$, under what types of transformations will the scores keep the same value?\n\nCovariance subtracts the means before taking the dot product. Therefore, it's invariant to shifts.\n\nPearson correlation subtracts the means and divides by the standard deviations before taking the dot product. Therefore, it's invariant to shifts and scaling.\n\nCosine similarity divides by the norms before taking the dot product. Therefore it's invariant to scaling, but not shifts. Geometrically, it can be thought of as measuring the size of the angle between the two vectors (as its name suggests, it's the cosine of the angle).\n\nAll of these quantities depend on the dot product, so they can only detect linear structure. To address a question from the comments, mutual information is fully general, and can detect structure for any distribution. But, it's harder to estimate from finite data than other quantities, and more care must be taken. Also, it measures dependence, but doesn't indicate the direction of a relationship (e.g. variables that are correlated or anticorrelated can have the same same mutual information). Mutual information is a valid measure of dependence when no 'direction of relationship' even exists (non-monotonic relationships). If the goal is to detect relationships that are nonlinear but monotonic, then Spearman rank correlation and Kendall's tau are good options.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/292413/402101 \n To add to the previous responses. You should definitely check out the recent work by Tibshirani and colleagues. They have developed a rigorous framework for inferring selection-corrected p-values and confidence intervals for lasso-type methods and also provide an R-package.\n\nSee:\n\nLee, Jason D., et al. \"Exact post-selection inference, with application to the lasso.\" The Annals of Statistics 44.3 (2016): 907-927.\n(\nhttps://projecteuclid.org/euclid.aos/1460381681\n)\n\nTaylor, Jonathan, and Robert J. Tibshirani. \"Statistical learning and selective inference.\" Proceedings of the National Academy of Sciences 112.25 (2015): 7629-7634.\n\nR-package:\n\nhttps://cran.r-project.org/web/packages/selectiveInference/index.html",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/355657/402101 \n The other answers already here do a great job of explaining why Gaussian RVs don't converge to anything as the variance increases without bound, but I want to point out a seemingly-uniform property that such a collection of Gaussians \ndoes\n satisfy that I think might be enough for someone to guess that they are becoming uniform, but that turns out to not be strong enough to conclude that.\n$\\newcommand{\\len}{\\text{len}}$\n\nConsider a collection of random variables $\\{X_1,X_2,\\dots\\}$ where $X_n \\sim \\mathcal N(0, n^2)$. Let $A = [a_1,a_2]$ be a fixed interval of finite length, and for some $c \\in \\mathbb R$ define $B = A +c$, i.e. $B$ is $A$ but just shifted over by $c$. For an interval $I = [i_1,i_2]$ define $\\len (I) = i_2-i_1$ to be the length of $I$, and note that $\\len(A) = \\len(B)$.\n\nI'll now prove the following result:\n\nResult\n:  $\\vert P(X_n \\in A) - P(x_n\\in B)\\vert \\to 0$ as $n \\to \\infty$.\n\nI call this uniform-like because it says that the distribution of $X_n$ increasingly has two fixed intervals of equal length having equal probability, no matter how far apart they may be. That's definitely a very uniform feature, but as we'll see this doesn't say anything about the actual distribution of the $X_n$ converging to a uniform one.\n\nPf: note that $X_n = n X_1$ where $X_1 \\sim \\mathcal N(0, 1)$ so\n$$\nP(X_n \\in A) = P(a_1 \\leq n X_1 \\leq a_2) = P\\left(\\frac{a_1}{n} \\leq X_1 \\leq \\frac{a_2}n\\right)\n$$\n$$\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{a_1/n}^{a_2/n} e^{-x^2/2}\\,\\text dx.\n$$\nI can use the (very rough) bound that $e^{-x^2/2} \\leq 1$ to get\n$$\n\\frac{1}{\\sqrt{2\\pi}}\\int_{a_1/n}^{a_2/n} e^{-x^2/2}\\,\\text dx \\leq \\frac{1}{\\sqrt{2\\pi}}\\int_{a_1/n}^{a_2/n} 1\\,\\text dx\n$$\n$$\n= \\frac{\\text{len}(A)}{n\\sqrt{2\\pi}}.\n$$\n\nI can do the same thing for $B$ to get\n$$\nP(X_n \\in B) \\leq \\frac{\\text{len}(B)}{n\\sqrt{2\\pi}}.\n$$\n\nPutting these together I have\n$$\n\\left\\vert P(X_n \\in A) - P(X_n \\in B)\\right\\vert \\leq \\frac{\\sqrt 2 \\text{len}(A) }{n\\sqrt{\\pi}} \\to 0\n$$\nas $n\\to\\infty$ (I'm using the triangle inequality here).\n\n$\\square$\n\nHow is this different from $X_n$ converging on a uniform distribution? I just proved that the probabilities given to any two fixed intervals of the same finite length get closer and closer, and intuitively that makes sense that as the densities are \"flattening out\" from $A$ and $B$'s perspectives.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/355657/402101 \n $\\square$\n\nHow is this different from $X_n$ converging on a uniform distribution? I just proved that the probabilities given to any two fixed intervals of the same finite length get closer and closer, and intuitively that makes sense that as the densities are \"flattening out\" from $A$ and $B$'s perspectives.\n\nBut in order for $X_n$ to converge on a uniform distribution, I'd need $P(X_n \\in I)$ to head towards being proportional to $\\text{len}(I)$ for \nany\n interval $I$, and that is a very different thing because this needs to apply to any $I$, not just one fixed in advance (and as mentioned elsewhere, this is also not even possible for a distribution with unbounded support).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/397295/402101 \n Questions\n\nQ0: The time series looks rather right-skewed and the level shift is accompanied by a scale shift. Hence, I would analyze the time series in logs rather than levels, i.e., with multiplicative rather than additive errors. In logs, it seems that an AR(1) model works quite well in each segment. See e.g. \nacf()\n and \npacf()\n before and after the break.\n\nacf()\n\npacf()\n\npacf(log(window(myts1, end = c(2018, 136))))\npacf(log(window(myts1, start = c(2018, 137))))\n\npacf(log(window(myts1, end = c(2018, 136))))\npacf(log(window(myts1, start = c(2018, 137))))\n\nQ1: For a time series without breaks in the mean, you can simply use the squared (or absolute) residuals and run a test for level shifts again. Alternatively, you can run tests and breakpoint estimation based on a maximum likelihood model where the error variance is another model parameter in addition to the regression coefficients. This is Zeileis \net al.\n (2010, \ndoi:10.1016/j.csda.2009.12.005\n). The corresponding score-based CUSUM tests are available in \nstrucchange\n as well but the breakpoint estimation is in \nfxregime\n. Finally, in the absence of regressors when looking only for changes in mean and variance the \nchangepoint\n R package also provides dedicated functions.\n\nstrucchange\n\nfxregime\n\nchangepoint\n\nHaving said that, it seems that a least-squares approach (treating the variance as a nuisance parameter) is sufficient for the time series you posted. See below.\n\nQ2: Yes. I would simply fit separate models to each segment and analyze these \"as usual\" Bai & Perron (2003, \nJournal of Applied Econometrics\n) also argue that this is justified asymptotically due to the faster convergence of the breakpoint estimates (with rate \n$n$\n rather than \n$\\sqrt{n}$\n).\n\nQ3: I'm not fully sure what you are looking for here. If you want to run the tests sequentially to monitor incoming data, then you should adopt a formal monitoring approach. This is also discussed in Zeileis \net al.\n (2010).\n\nAnalysis code snippets:\n\nCombine log series with its lags for subsequent regression.\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nTesting with supF and score-based CUSUM tests:\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\n\n\nsc <- efp(y ~ y1, data = d, type = \"Score-CUSUM\")\nplot(sc, functional = NULL)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/397295/402101 \n Analysis code snippets:\n\nCombine log series with its lags for subsequent regression.\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nd <- ts.intersect(y = log(myts1), y1 = lag(log(myts1), -1))\n\nTesting with supF and score-based CUSUM tests:\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\nfs <- Fstats(y ~ y1, data = d)\nplot(fs)\nlines(breakpoints(fs))\n\n\n\nsc <- efp(y ~ y1, data = d, type = \"Score-CUSUM\")\nplot(sc, functional = NULL)\n\nsc <- efp(y ~ y1, data = d, type = \"Score-CUSUM\")\nplot(sc, functional = NULL)\n\n\n\nThis highlights that both intercept and autocorrelation coefficient change significantly at the time point visible in the original time series. There is also some fluctuation in the variance but this is not significant at 5% level.\n\nA BIC-based dating also clearly finds this one breakpoint:\n\nbp <- breakpoints(y ~ y1, data = d)\ncoef(bp)\n##                       (Intercept)        y1\n## 2016(123) - 2018(136)    3.926381 0.3858473\n## 2018(137) - 2019(1)      3.778685 0.2845176\n\nbp <- breakpoints(y ~ y1, data = d)\ncoef(bp)\n##                       (Intercept)        y1\n## 2016(123) - 2018(136)    3.926381 0.3858473\n## 2018(137) - 2019(1)      3.778685 0.2845176\n\nClearly, the mean drops but also the autocorrelation slightly. The fitted model in logs is then:\n\nplot(log(myts1), col = \"lightgray\", lwd = 2)\nlines(fitted(bp))\nlines(confint(bp))\n\nplot(log(myts1), col = \"lightgray\", lwd = 2)\nlines(fitted(bp))\nlines(confint(bp))\n\n\n\nRe-fitting the model to each segments can then be done via:\n\nsummary(lm(y ~ y1, data = window(d, end = c(2018, 136))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, end = c(2018, 136)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.73569 -0.18457 -0.04354  0.12042  1.89052 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.92638    0.21656   18.13   <2e-16 ***\n## y1           0.38585    0.03383   11.40   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2999 on 742 degrees of freedom\n## Multiple R-squared:  0.1491, Adjusted R-squared:  0.148 \n## F-statistic: 130.1 on 1 and 742 DF,  p-value: < 2.2e-16",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/397295/402101 \n summary(lm(y ~ y1, data = window(d, end = c(2018, 136))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, end = c(2018, 136)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.73569 -0.18457 -0.04354  0.12042  1.89052 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.92638    0.21656   18.13   <2e-16 ***\n## y1           0.38585    0.03383   11.40   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2999 on 742 degrees of freedom\n## Multiple R-squared:  0.1491, Adjusted R-squared:  0.148 \n## F-statistic: 130.1 on 1 and 742 DF,  p-value: < 2.2e-16\n\n\n\nsummary(lm(y ~ y1, data = window(d, start = c(2018, 137))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, start = c(2018, 137)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.43663 -0.13953 -0.03408  0.09028  0.99777 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.61558    0.33468   10.80  < 2e-16 ***\n## y1           0.31567    0.06327    4.99  1.2e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2195 on 227 degrees of freedom\n## Multiple R-squared:  0.09883,    Adjusted R-squared:  0.09486 \n## F-statistic:  24.9 on 1 and 227 DF,  p-value: 1.204e-06\n\nsummary(lm(y ~ y1, data = window(d, start = c(2018, 137))))\n## Call:\n## lm(formula = y ~ y1, data = window(d, start = c(2018, 137)))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.43663 -0.13953 -0.03408  0.09028  0.99777 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.61558    0.33468   10.80  < 2e-16 ***\n## y1           0.31567    0.06327    4.99  1.2e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2195 on 227 degrees of freedom\n## Multiple R-squared:  0.09883,    Adjusted R-squared:  0.09486 \n## F-statistic:  24.9 on 1 and 227 DF,  p-value: 1.204e-06",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/289193/402101 \n It's really just a convenience for loglikelihood, nothing more.\n\nI mean the convenience of the sums vs. products: $\\ln (\\prod_i x_i) =\\sum_i\\ln x_i$, the sums are easier to deal with in many respects, such as differentialtion or integration. It's not a convenience for only exponential families, I'm trying to say.\n\nWhen you deal with a random sample, the likelihoods are of the form: $\\mathrm{L}=\\prod_ip_i$, so the loglikelihood would break this product into the sum instead, which is easier to manipulate and analyze. It helps that all we care is the point of the maximum, the value at the maximum is not important, se we can apply any monotonous transformation such as the logarithm.\n\nOn the curvature intuition. It's basically the same thing in the end as the second derivative of loglikelihood.\n\nUPDATE:\nThis is what I meant on the curvature. If you have a function $y=f(x)$, then it's curvature would be (\nsee (14)\n on Wolfram):\n$$\\kappa=\\frac{f''(x)}{(1+f'(x)^2)^{3/2}}$$\n\nThe second derivative of the log likelihood:\n$$A=(\\ln f(x))''=\\frac{f''(x)}{f(x)}-\\left(\\frac{f'(x)}{f(x)}\\right)^2$$\n\nAt the point of the maximum, the first derivative is obviously zero, so we get:\n$$\\kappa_{max}=f''(x_{max})=Af(x_{max})$$\nHence, my quip that the curvature of the likelihood and the second derivative of loglikelihood are the same thing, sort of.\n\nOn the other hand, if the first derivative of likelihood is small not only at but around the point of maximum, i.e. the likelihood function is flat then we get:\n$$\\kappa\\approx f''(x)\\approx A f(x)$$\nNow the flat likelihood is not a good thing for us, because it makes finding the maximum more difficult numerically, and the maximum likelihood is not that better than other points around it, i.e. the parameter estimation errors are high.\n\nAnd again, we still have the curvature and second derivative relationship. So why didn't Fisher look at the curvature of the likelihood function? I think it's for the same reason of convenience. It's easier to manipulate the loglikelihood because of sums instead of the product. So, he could study the curvature of the likelihood by analyzing the second derivative of the loglikelihood. Although the equation looks very simple for the curvature $\\kappa_{max}=f''(x_{max})$, in actuality you're taking a second derivative of the product, which is messier than the sum of second derivatives.\n\nUPDATE 2:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/289193/402101 \n UPDATE 2:\n\nHere's a demonstration. I draw a (completely made up) likelihood function, its a) curvature and b) the 2nd derivative of its log. On the left side you see the narrow likelihood and on the right side it's wide. You see how at the point of the max likelihood a) and b) converge, as they should. More importantly though, you can study the width (or flatness) of the likelihood function by examining the 2nd derivative of its log-likelihood. As I wrote earlier the latter is technically simpler than the former to analyze.\n\nNot surprisingly deeper 2nd derivative of loglikelihood signals flatter likelihood function around its max, which is not desired for it causes bigger parameter estimation error.\n\n\n\nMATLAB code in case you want to reproduce the plots:\n\nf=@(x,a)a.^2./(a.^2+x.^2);\nc = @(x,a)(-2*a.^2.*(a.^2-3*x.^2)./(a.^2+x.^2).^3/(4*a.^4.*x.^2/(a.^2+x.^2).^4+1).^(3/2));\nll2d = @(x,a)(2*(x.^2-a.^2)./(a.^2+x.^2).^2);\n\nh = 0.1;\nx=-10:h:10;\n\n% narrow peak\nfigure\nsubplot(1,2,1)\na = 1;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Narrow Likelihood'\nylim([-2 1])\n\n% wide peak\nsubplot(1,2,2)\na=2;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Wide Likelihood'\nlegend('likelihood','curvature','2nd derivative LogL','location','best')\nylim([-2 1])\n\nf=@(x,a)a.^2./(a.^2+x.^2);\nc = @(x,a)(-2*a.^2.*(a.^2-3*x.^2)./(a.^2+x.^2).^3/(4*a.^4.*x.^2/(a.^2+x.^2).^4+1).^(3/2));\nll2d = @(x,a)(2*(x.^2-a.^2)./(a.^2+x.^2).^2);\n\nh = 0.1;\nx=-10:h:10;\n\n% narrow peak\nfigure\nsubplot(1,2,1)\na = 1;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Narrow Likelihood'\nylim([-2 1])\n\n% wide peak\nsubplot(1,2,2)\na=2;\ny = f(x,a);\nplot(x,y,'LineWidth',2)\n%dy = diff(y)/h;\nhold on\n%plot(x(2:end),dy)\nplot(x,c(x,a),'LineWidth',2)\nplot(x,ll2d(x,a),'LineWidth',2)\ntitle 'Wide Likelihood'\nlegend('likelihood','curvature','2nd derivative LogL','location','best')\nylim([-2 1])\n\nUPDATE 3:\n\nIn the code above I plugged some arbitrary bell shaped function into the curvature equation, then calculated the second derivative of its log. I didn't re-scale anything, the values are straight from equations to show the equivalence that I mentioned earlier.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/289193/402101 \n UPDATE 3:\n\nIn the code above I plugged some arbitrary bell shaped function into the curvature equation, then calculated the second derivative of its log. I didn't re-scale anything, the values are straight from equations to show the equivalence that I mentioned earlier.\n\nHere's\n the very first paper on likelihood that Fisher published while still in the university, \"On an Absolute Criterion for Fitting Frequency Curves\", Messenger of Mathmatics, 41: 155-160 (1912)\n\nAs I was insisting all along he doesn't mention any \"deeper\" connections of log probabilities to entropy and other fancy subjects, neither does he offer his information criterion yet. He simply puts the equation $\\log P'=\\sum_1^n\\log p$ on p.54 then proceeds to talk about maximizing the probabilities. In my opinion, this shows that he was using the logarithm just as a convenient method of analyzing the joint probabilities themselves. It is especially useful in the continuous curve fitting, for which he gives an obvious formula on p.55:\n$$\\log P=\\int_{-\\infty}^\\infty\\log fdx$$\nGood luck analyzing this likelihood (or probability $P$ as per Fisher) without the log!\n\nOne thing to note when reading the paper he was only starting with maximum likelihood estimation work, and did more work in subsequent 10 years, so even the term MLE wasn't coined yet, as far as I know.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/241950/402101 \n This question has been partially discussed at this site as below, and opinions seem mixed.\n\nWhat is the difference between fixed effect, random effect and mixed effect models?\n\n\nWhat is the mathematical difference between random- and fixed-effects?\n\n\nConcepts behind fixed/random effects models\n\nAll terms are generally related to longitudinal / panel / clustered / hierarchical data and repeated measures (in the format of advanced regression and ANOVA), but have multiple meanings in different context. I would like to answer the question in formulas based on my knowledge.\n\nIn biostatistics, fixed-effects, denoted as $\\color{red}{\\boldsymbol\\beta}$ in Equation (*) below, usually comes together with random effects. But the fixed-effects model is also defined to assume that the observations are independent, like cross-sectional setting, as in \nLongitudinal Data Analysis\n of Hedeker and Gibbons (2006).\n\n\nIn econometrics, the fixed-effects model can be written as\n$$ y_{ij}=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta+\\color{red}{u_i}+\\epsilon_{ij}$$\nwhere $\\color{red}{u_i}$ is fixed (not random) intercept for each subject ($i$), or we can also have a fixed-effect as $u_j$ for each repeated measurement ($j$); $\\boldsymbol x_{ij}$ denotes covariates.\n\n\nIn meta-analysis, the fixed-effect model assumes underlying effect is the same across all studies (e.g. Mantel and Haenszel, 1959).\n\nIn biostatistics, the random-effects model (Laird and Ware, 1982) can be written as\n$$\\tag{*} y_{ij}=\\boldsymbol x_{ij}^{'}\\color{red}{\\boldsymbol\\beta}+\\boldsymbol z_{ij}^{'}\\color{blue}{\\boldsymbol u_i}+e_{ij}$$\nwhere $\\color{blue}{\\boldsymbol u_i}$ is assumed to follow a distribution. $\\boldsymbol x_{ij}$ denotes covariates for fixed effects, and $\\boldsymbol z_{ij}$ denotes covariates for random effects.\n\n\nIn econometrics, the random-effects model may only refer to \nrandom intercept model\n as in biostatistics, i.e. $\\boldsymbol z_{ij}^{'}=1$ and $\\boldsymbol u_i$ is a scalar.\n\n\nIn meta-analysis, the random-effect model assumes heterogeneous effects across studies (DerSimonian and Laird, 1986).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/241950/402101 \n In econometrics, the random-effects model may only refer to \nrandom intercept model\n as in biostatistics, i.e. $\\boldsymbol z_{ij}^{'}=1$ and $\\boldsymbol u_i$ is a scalar.\n\n\nIn meta-analysis, the random-effect model assumes heterogeneous effects across studies (DerSimonian and Laird, 1986).\n\nMarginal model is generally compared to conditional model (random-effects model), and the former focuses on the population mean (take linear model for an example) $$E(y_{ij})=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta,$$ while the latter deals with the conditional mean $$E(y_{ij}|\\boldsymbol u_i)=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta + \\boldsymbol z_{ij}^{'}\\boldsymbol u_i.$$ The interpretation and scale of the regression coefficients between marginal model and random-effects model would be different for nonlinear models (e.g. logistic regression). Let $h(E(y_{ij}|\\boldsymbol u_i))=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta + \\boldsymbol z_{ij}^{'}\\boldsymbol u_i$, then $$E(y_{ij})=E(E(y_{ij}|\\boldsymbol u_i))=E(h^{-1}(\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta + \\boldsymbol z_{ij}^{'}\\boldsymbol u_i))\\neq h^{-1}(\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta),$$ unless trivially the link function $h$ is the identity link (linear model), or $u_i=0$ (no random-effects). Good examples include generalized estimating equations (GEE; Zeger, Liang and Albert, 1988) and marginalized multilevel models (Heagerty and Zeger, 2000).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473596/402101 \n This is quite a ubiquitous misunderstanding of the central limit theorem, which I have also encountered in my statistical teaching.  Over the years I have encountered this problem so often that I have developed a Socratic method to deal with it.  I identify a student that has accepted this idea and then engage the student to tease out what this would logically imply.  It is fairly simple to get to the \nreductio ad absurdum\n of the false version of the theorem, which is that \nevery sequence of IID random variables has a normal distribution\n.  A typical conversation would go something like this.\n\nTeacher:\n I noticed in this assignment question that you said that because \n$n$\n is large, the data are approximately normally distributed.  Can you take me through your reasoning for that bit?\n\nStudent:\n Is that wrong?\n\nTeacher:\n I don't know.  Let's have a look at it.\n\nStudent:\n Well, I used that theorem you talked about in class; that main one you mentioned a bunch of times.  I forget the name.\n\nTeacher:\n The central limit theorem?\n\nStudent:\n Yeah, the central limit theorem.\n\nTeacher:\n Great, and when does that theorem apply?\n\nStudent:\n I think if the variables are IID.\n\nTeacher:\n And have finite variance.\n\nStudent:\n Yeah, and finite variance.\n\nTeacher:\n Okay, so the random variables have some \nfixed\n distribution with finite variance, is that right?\n\nStudent:\n Yeah.\n\nTeacher:\n And the distribution isn't changing or anything?\n\nStudent:\n No, they're IID with a fixed distribution.\n\nTeacher:\n Okay great, so let me see if I can state the theorem.  The central limit theorem says that if you have an IID sequence of random variables with finite variance, and you take a sample of \n$n$\n of them, then as that sample size \n$n$\n gets large the distribution of the random variables converges to a normal distribution.  Is that right?\n\nStudent:\n Yeah, I think so.\n\nTeacher:\n Okay great, so let's think about what that would mean.  Suppose I have a sequence like that.  If I take say, a thousand sample values, what is the distribution of those random variables?\n\nStudent:\n It's approximately a normal distribution.\n\nTeacher:\n How close?\n\nStudent:\n Pretty close I think.\n\nTeacher:\n Okay, what if I take a billion sample values.  How close now?\n\nStudent:\n Really close I'd say.\n\nTeacher:\n And if we have a sequence of these things, then in theory we can take \n$n$\n as high as we want can't we?  So we can make the distribution as close to a normal distribution as we want.\n\nStudent:\n Yeah.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473596/402101 \n Student:\n It's approximately a normal distribution.\n\nTeacher:\n How close?\n\nStudent:\n Pretty close I think.\n\nTeacher:\n Okay, what if I take a billion sample values.  How close now?\n\nStudent:\n Really close I'd say.\n\nTeacher:\n And if we have a sequence of these things, then in theory we can take \n$n$\n as high as we want can't we?  So we can make the distribution as close to a normal distribution as we want.\n\nStudent:\n Yeah.\n\nTeacher:\n So let's say we take \n$n$\n big enough that we're happy to say that the random variables basically have a normal distribution.  And that's a fixed distribution right?\n\nStudent:\n Yeah.\n\nTeacher:\n And they're IID right?  These random variables are IID?\n\nStudent:\n Yeah, they're IID.\n\nTeacher:\n Okay, so they all have the same distribution.\n\nStudent:\n Yeah.\n\nTeacher:\n Okay, so that means the \nfirst\n value in the sequence, it also has a normal distribution.  Is that right?\n\nStudent:\n Yeah.  I mean, it's an approximation, but yeah, if \n$n$\n is really large then it effectively has a normal distribution.\n\nTeacher:\n Okay great.  And so does the second value in the sequence, and so on, right?\n\nStudent:\n Yeah.\n\nTeacher:\n Okay, so really, as soon as we started sampling, we were already getting values that are essentially normal distributed.  We didn't really need to wait until \n$n$\n gets large before that started happening.\n\nStudent:\n Hmmm.  I'm not sure.  That sounds wrong.  The theorem says you need a large \n$n$\n, so I guess I think you can't apply it if you only sampled a small number of values.\n\nTeacher:\n Okay, so let's say we are sampling a billion values.  Then we have large \n$n$\n.  And we've established that this means that the first few random variables in the sequence are normally distributed, to a very close approximation.  If that's true, can't we just stop sampling early?  Say we were going to sample a billion values, but then we stop sampling after the first value.  Was that random variable still normally distributed?\n\nStudent:\n I think maybe it isn't.\n\nTeacher:\n Okay, so at some point its distribution changes?\n\nStudent:\n I'm not sure.  I'm a bit confused about it now.\n\nTeacher:\n Hmmm, well it seems we have something strange going on here.  Why don't you have another read of the material on the central limit theorem and see if you can figure out how to resolve that contradiction.  Let's talk more about it then.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473596/402101 \n Student:\n I think maybe it isn't.\n\nTeacher:\n Okay, so at some point its distribution changes?\n\nStudent:\n I'm not sure.  I'm a bit confused about it now.\n\nTeacher:\n Hmmm, well it seems we have something strange going on here.  Why don't you have another read of the material on the central limit theorem and see if you can figure out how to resolve that contradiction.  Let's talk more about it then.\n\nThat is one possible approach, which seeks to reduce the false theorem down to the \nreductio\n which says that every IID sequence (with finite variance) must be composed of normal random variables.  (You should be prepared to encounter some variations in the student's views, which might include confusion between the probability distribution of a random variable and the sampling distribution of a sample.  Additional confusions or issues may take the conversation off on a tangent, but the above captures a common flow of argument.)  Assuming that the conversation goes as it should, either the student will get to the false (and absurd) conclusion and realise something is wrong, or they will defend against this conclusion by saying that the distribution changes as \n$n$\n gets large (or they may handwave a bit, and you might have to lawyer them to a conclusion).  Either way, this usually provokes some further thinking that can lead them to re-read the theorem.  Here is another approach:\n\nTeacher:\n Let's look at this another way.  Suppose we have an IID sequence of random variables from some other distribution; one that is \nnot\n a normal distribution.  Is that possible?  For example, could we have a sequence of random variables representing outcome of coin flip, from the Bernoulli distribution?\n\nStudent:\n Yeah, we can have that.\n\nTeacher:\n Okay, great.  And these are all IID values, so again, they all have the same distribution.  So every random variable in that sequence is going to have a distribution that is \nnot\n a normal distribution, right?\n\nStudent:\n Yeah.\n\nTeacher:\n In fact, in this case, every value in the sequence will be the outcome of a coin flip, which we set as zero or one.  Is that right?\n\nStudent:\n Yeah, as long as we label them that way.\n\nTeacher:\n Okay, great.  So if all the values in the sequence are zeroes or ones,\nno matter how many of them we sample, we are always going to get a histogram showing values at zero and one, right?\n\nStudent:\n Yeah.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/473596/402101 \n Student:\n Yeah.\n\nTeacher:\n In fact, in this case, every value in the sequence will be the outcome of a coin flip, which we set as zero or one.  Is that right?\n\nStudent:\n Yeah, as long as we label them that way.\n\nTeacher:\n Okay, great.  So if all the values in the sequence are zeroes or ones,\nno matter how many of them we sample, we are always going to get a histogram showing values at zero and one, right?\n\nStudent:\n Yeah.\n\nTeacher:\n Okay.  And do you think if we sample more and more values, we will get closer and closer to the true distribution?  Like, if it is a fair coin, does the histogram eventually converge to where the relative frequency bars are the same height?\n\nStudent:\n I guess so.  I think it does.\n\nTeacher:\n I think you're right.  In fact, we call that result the \"law of large numbers\".  Anyway, it seems like we have a bit of a problem here doesn't it.  If we sample a large number of the values then the central limit theorem says we converge to a normal distribution, but it sounds like the \"law of large numbers\" says we actually converge to the true distribution, which isn't a normal distribution.  In fact, it's a distribution that is just probabilities on the zero value and the one value, which looks nothing like the normal distribution.  So which is it?\n\nStudent:\n I think when \n$n$\n is large it looks like a normal distribution.\n\nTeacher:\n So describe it to me.  Let's say we have flipped the coin a billion times.  Describe the distribution of the outcomes and explain why that looks like a normal distribution.\n\nStudent:\n I'm not really sure how to do that.\n\nTeacher:\n Okay.  Well, do you agree that if we have a billion coin flips, all those outcomes are zeroes and ones?\n\nStudent:\n Yeah.\n\nTeacher:\n Okay, so describe what its histogram looks like.\n\nStudent:\n It's just two bars on those values.\n\nTeacher:\n Okay, so not \"bell curve\" shaped?\n\nStudent:\n Yeah, I guess not.\n\nTeacher:\n Hmmm, so perhaps the central limit theorem doesn't say what we thought.  Why don't you read the material on the central limit theorem again and see if you can figure out what it says.  Let's talk more about it then.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49446/402101 \n Methods of \ncensored regression\n can handle data like this.\n  They assume the \nresiduals\n behave as in ordinary linear regression but have been modified so that\n\n(Left censoring): all values smaller than a low threshold, which is independent of the data, (but can vary from one case to the other) have not been quantified; and/or\n\n\n(Right censoring): all values larger than than a high threshold, which is independent of the data (but can vary from one case to the other) have not been quantified.\n\n(Left censoring): all values smaller than a low threshold, which is independent of the data, (but can vary from one case to the other) have not been quantified; and/or\n\n(Right censoring): all values larger than than a high threshold, which is independent of the data (but can vary from one case to the other) have not been quantified.\n\n\"Not quantified\" means we know whether or not a value falls below (or above) its threshold, \nbut that's all.\n\nThe fitting methods typically use maximum likelihood.  When the model for the response $Y$ corresponding to a vector $X$ is in the form\n\n$$Y \\sim X \\beta + \\varepsilon$$\n\nwith iid $\\varepsilon$ having a common distribution $F_\\sigma$ with PDF $f_\\sigma$ (where $\\sigma$ are unknown \"nuisance parameters\"), then--in the absence of censoring--the log likelihood of observations $(x_i, y_i)$ is\n\n$$\\Lambda = \\sum_{i=1}^n \\log f_\\sigma(y_i - x_i\\beta).$$\n\nWith censoring present we may divide the cases into three (possibly empty) classes: for indexes $i=1$ to $n_1$, the $y_i$ contain the \nlower threshold\n values and represent \nleft censored\n data; for indexes $i=n_1+1$ to $n_2$, the $y_i$ are quantified; and for the remaining indexes, the $y_i$ contain the \nupper threshold\n values and represent \nright censored\n data.  The log likelihood is obtained in the same way as before: it is the log of the product of the probabilities.\n\n$$\\Lambda = \\sum_{i=1}^{n_1} \\log F_\\sigma(y_i - x_i\\beta) + \\sum_{i=n_1+1}^{n_2} \\log f_\\sigma(y_i - x_i\\beta) + \\sum_{i=n_2+1}^n \\log (1 - F_\\sigma(y_i - x_i\\beta)).$$\n\nThis is maximized numerically as a function of $(\\beta, \\sigma)$.\n\nIn my experience, such methods can work well when less than half the data are censored; otherwise, the results can be unstable.\n\nHere is a simple \nR\n example\n using the \ncensReg\n package\n to illustrate how OLS and censored results can differ (a lot) even with plenty of data.  It qualitatively reproduces the data in the question.\n\nR\n\ncensReg",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49446/402101 \n This is maximized numerically as a function of $(\\beta, \\sigma)$.\n\nIn my experience, such methods can work well when less than half the data are censored; otherwise, the results can be unstable.\n\nHere is a simple \nR\n example\n using the \ncensReg\n package\n to illustrate how OLS and censored results can differ (a lot) even with plenty of data.  It qualitatively reproduces the data in the question.\n\nR\n\ncensReg\n\nlibrary(\"censReg\")\nset.seed(17)\nn.data <- 2960\ncoeff  <- c(-0.001, 0.005)\nsigma  <- 0.005\nx      <- rnorm(n.data, 0.5)\ny      <- as.vector(coeff %*% rbind(rep(1, n.data), x) + rnorm(n.data, 0, sigma))\ny.cen           <- y\ny.cen[y < 0]    <- 0\ny.cen[y > 0.01] <- 0.01\ndata = data.frame(list(x, y.cen))\n\nlibrary(\"censReg\")\nset.seed(17)\nn.data <- 2960\ncoeff  <- c(-0.001, 0.005)\nsigma  <- 0.005\nx      <- rnorm(n.data, 0.5)\ny      <- as.vector(coeff %*% rbind(rep(1, n.data), x) + rnorm(n.data, 0, sigma))\ny.cen           <- y\ny.cen[y < 0]    <- 0\ny.cen[y > 0.01] <- 0.01\ndata = data.frame(list(x, y.cen))\n\nThe key things to notice are the parameters: the \ntrue\n slope is $0.005$, the \ntrue\n intercept is $-0.001$, and the \ntrue\n error SD is $0.005$.\n\nLet's use both \nlm\n and \ncensReg\n to fit a line:\n\nlm\n\ncensReg\n\nfit <- censReg(y.cen ~ x, data=data, left=0.0, right=0.01)\nsummary(fit)\n\nfit <- censReg(y.cen ~ x, data=data, left=0.0, right=0.01)\nsummary(fit)\n\nThe results of this censored regression, given by \nprint(fit)\n, are\n\nprint(fit)\n\n(Intercept)           x       sigma \n  -0.001028    0.004935    0.004856\n\n(Intercept)           x       sigma \n  -0.001028    0.004935    0.004856\n\nThose are remarkably close to the correct values of $-0.001$, $0.005$, and $0.005$, respectively.\n\nfit.OLS <- lm(y.cen ~ x, data=data)\nsummary(fit.OLS)\n\nfit.OLS <- lm(y.cen ~ x, data=data)\nsummary(fit.OLS)\n\nThe OLS fit, given by \nprint(fit.OLS)\n, is\n\nprint(fit.OLS)\n\n(Intercept)            x  \n   0.001996     0.002345\n\n(Intercept)            x  \n   0.001996     0.002345\n\nNot even remotely close!  The estimated standard error reported by \nsummary\n is $0.002864$, less than half the true value.  \nThese kinds of biases are typical of regressions with lots of censored data.\n\nsummary\n\nFor comparison, let's limit the regression to the quantified data:\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)\n\n(Intercept)  x[0 <= y & y <= 0.01]  \n   0.003240               0.001461\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49446/402101 \n summary\n\nFor comparison, let's limit the regression to the quantified data:\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)\n\n(Intercept)  x[0 <= y & y <= 0.01]  \n   0.003240               0.001461\n\nfit.part <- lm(y[0 <= y & y <= 0.01] ~ x[0 <= y & y <= 0.01])\nsummary(fit.part)\n\n(Intercept)  x[0 <= y & y <= 0.01]  \n   0.003240               0.001461\n\nEven worse!\n\nA few pictures summarize the situation.\n\nlineplot <- function() {\n  abline(coef(fit)[1:2], col=\"Red\", lwd=2)\n  abline(coef(fit.OLS), col=\"Blue\", lty=2, lwd=2)\n  abline(coef(fit.part), col=rgb(.2, .6, .2), lty=3, lwd=2)\n}\npar(mfrow=c(1,4))\nplot(x,y, pch=19, cex=0.5, col=\"Gray\", main=\"Hypothetical Data\")\nlineplot()\nplot(x,y.cen, pch=19, cex=0.5, col=\"Gray\", main=\"Censored Data\")\nlineplot()\nhist(y.cen, breaks=50, main=\"Censored Data\")\nhist(y[0 <= y & y <= 0.01], breaks=50, main=\"Quantified Data\")\n\nlineplot <- function() {\n  abline(coef(fit)[1:2], col=\"Red\", lwd=2)\n  abline(coef(fit.OLS), col=\"Blue\", lty=2, lwd=2)\n  abline(coef(fit.part), col=rgb(.2, .6, .2), lty=3, lwd=2)\n}\npar(mfrow=c(1,4))\nplot(x,y, pch=19, cex=0.5, col=\"Gray\", main=\"Hypothetical Data\")\nlineplot()\nplot(x,y.cen, pch=19, cex=0.5, col=\"Gray\", main=\"Censored Data\")\nlineplot()\nhist(y.cen, breaks=50, main=\"Censored Data\")\nhist(y[0 <= y & y <= 0.01], breaks=50, main=\"Quantified Data\")\n\n\n\nThe difference between the \"hypothetical data\" and \"censored data\" plots is that all y-values below $0$ or above $0.01$ in the former have been moved to their respective thresholds to produce the latter plot.  As a result, you can see the censored data all lined up along the bottom and top.\n\nSolid red lines are the censored fits, dashed blue lines the OLS fits, \nboth of them based on the censored data only\n. The dashed green lines are the fits to the quantified data only. It is clear which is best: the blue and green lines are noticeably poor and only the red (for the censored regression fit) looks about right.  The histograms at the right confirm that the $Y$ values of this synthetic dataset are indeed qualitatively like those of the question (mean = $0.0032$, SD = $0.0037$).  The rightmost histogram shows the center (quantified) part of the histogram in detail.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1869/402101 \n You really need to figure out what is the question that you are trying to answer- or what question is management most interested in. Then you can select the survey questions that are most relevant to your problem.\n\nWithout knowing anything about your problem or dataset, here are some generic solutions:\n\nVisually represent the answers as clusters. My favorite is by either using dendrograms or just plotting on an xy axis (Google \"cluster analysis r\" and go to the first result by statmethods.net)\n\n\nRank the questions from greatest to least \"daily or more frequently\" responses. This is an example that may not exactly work for you but perhaps it will inspire you \nhttp://www.programmingr.com/content/building-scoring-and-ranking-systems-r\n\n\nCrosstabs: if for example, you have a question \"How often do you come in late for work?\" and \"How often do you use Facebook?,\" by crosstabbing the two questions you can find out the percentage of people who rarely do both, or who do both everyday.(Google \"r frequency crosstabs\" or go to the aforementioned statmethods.net)\n\n\nCorrelograms. I don't have any experience with these but I saw it also on the statmethods.net website. Basically you find which questions have the highest correlation and then create a table. You may find this useful although it looks kind of \"busy.\"",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/151305/402101 \n Since you ask for \ninsights\n, I'm going to take a fairly intuitive approach rather than a more mathematical tack:\n\nFollowing the concepts in my answer \nhere\n, we can formulate a ridge regression as a regression with dummy data by adding \n$p$\n (in your formulation) observations, where \n$y_{n+j}=0$\n, \n$x_{j,n+j}=\\sqrt{\\lambda}$\n and \n$x_{i,n+j}=0$\n for \n$i\\neq j$\n. If you write out the new RSS for this expanded data set, you'll see the additional observations each add a term of the form \n$(0-\\sqrt{\\lambda}\\beta_j)^2=\\lambda\\beta_j^2$\n, so the new RSS is the original \n$\\text{RSS} + \\lambda \\sum_{j=1}^p\\beta_j^2$\n -- and minimizing the RSS on this new, expanded data set is the same as minimizing the ridge regression criterion.\n\n\nSo what can we see here? As \n$\\lambda$\n increases, the additional \n$x$\n-rows  each have one component that increases, and so the influence of these points also increases. They pull the fitted hyperplane toward themselves. Then as \n$\\lambda$\n and the corresponding components of the \n$x$\n's go off to infinity, all the involved coefficients \"flatten out\" to \n$0$\n.\n\n\nThat is, as \n$\\lambda\\to\\infty$\n, the penalty will dominate the minimization, so the \n$\\beta$\ns will go to zero. If the intercept is not penalized (the usual case) then the model shrinks more and more toward the mean of the response.\n\n\nI'll give an intuitive sense of why we're talking about ridges first (which also suggests why it's needed), then tackle a little history. The first is adapted from my answer \nhere\n:\n\n\nIf there's multicollinearity, you get a \"ridge\" in the likelihood function (likelihood is a function of the \n$\\beta$\n's). This in turn yields a long \"valley\" in the RSS (since RSS=\n$-2\\log\\mathcal{L}$\n). \n\n\nRidge\n regression \"fixes\" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:\n\n\n\n[\nClearer image\n]",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/151305/402101 \n If there's multicollinearity, you get a \"ridge\" in the likelihood function (likelihood is a function of the \n$\\beta$\n's). This in turn yields a long \"valley\" in the RSS (since RSS=\n$-2\\log\\mathcal{L}$\n). \n\n\nRidge\n regression \"fixes\" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:\n\n\n\n[\nClearer image\n]\n\n\nThe actual story behind the name is a little more complicated. In 1959 A.E. Hoerl [1] introduced \nridge analysis\n for response surface methodology, and it very soon [2] became adapted to dealing with multicollinearity in regression ('ridge regression'). See for example, the discussion by R.W. Hoerl in [3], where it  describes Hoerl's (A.E. not R.W.) use of contour plots of the response surface* in the identification of where to head to find local optima (where one 'heads up the ridge'). In ill-conditioned problems, the issue of a very long ridge arises, and insights and methodology from ridge analysis are adapted to the related issue with the likelihood/RSS in regression, producing ridge regression.\n\nFollowing the concepts in my answer \nhere\n, we can formulate a ridge regression as a regression with dummy data by adding \n$p$\n (in your formulation) observations, where \n$y_{n+j}=0$\n, \n$x_{j,n+j}=\\sqrt{\\lambda}$\n and \n$x_{i,n+j}=0$\n for \n$i\\neq j$\n. If you write out the new RSS for this expanded data set, you'll see the additional observations each add a term of the form \n$(0-\\sqrt{\\lambda}\\beta_j)^2=\\lambda\\beta_j^2$\n, so the new RSS is the original \n$\\text{RSS} + \\lambda \\sum_{j=1}^p\\beta_j^2$\n -- and minimizing the RSS on this new, expanded data set is the same as minimizing the ridge regression criterion.\n\nSo what can we see here? As \n$\\lambda$\n increases, the additional \n$x$\n-rows  each have one component that increases, and so the influence of these points also increases. They pull the fitted hyperplane toward themselves. Then as \n$\\lambda$\n and the corresponding components of the \n$x$\n's go off to infinity, all the involved coefficients \"flatten out\" to \n$0$\n.\n\nThat is, as \n$\\lambda\\to\\infty$\n, the penalty will dominate the minimization, so the \n$\\beta$\ns will go to zero. If the intercept is not penalized (the usual case) then the model shrinks more and more toward the mean of the response.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/151305/402101 \n That is, as \n$\\lambda\\to\\infty$\n, the penalty will dominate the minimization, so the \n$\\beta$\ns will go to zero. If the intercept is not penalized (the usual case) then the model shrinks more and more toward the mean of the response.\n\nI'll give an intuitive sense of why we're talking about ridges first (which also suggests why it's needed), then tackle a little history. The first is adapted from my answer \nhere\n:\n\nIf there's multicollinearity, you get a \"ridge\" in the likelihood function (likelihood is a function of the \n$\\beta$\n's). This in turn yields a long \"valley\" in the RSS (since RSS=\n$-2\\log\\mathcal{L}$\n).\n\nRidge\n regression \"fixes\" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:\n\n[\nClearer image\n]\n\nThe actual story behind the name is a little more complicated. In 1959 A.E. Hoerl [1] introduced \nridge analysis\n for response surface methodology, and it very soon [2] became adapted to dealing with multicollinearity in regression ('ridge regression'). See for example, the discussion by R.W. Hoerl in [3], where it  describes Hoerl's (A.E. not R.W.) use of contour plots of the response surface* in the identification of where to head to find local optima (where one 'heads up the ridge'). In ill-conditioned problems, the issue of a very long ridge arises, and insights and methodology from ridge analysis are adapted to the related issue with the likelihood/RSS in regression, producing ridge regression.\n\n* examples of response surface contour plots (in the case of quadratic response) can be seen \nhere\n (Fig 3.9-3.12).\n\nThat is, \"ridge\" actually refers to the characteristics of the function we were attempting to optimize, rather than to adding a \"ridge\" (+ve diagonal) to the \n$X^TX$\n matrix (so while ridge regression does add to the diagonal, that's not why we call it 'ridge' regression).\n\nFor some additional information on the need for ridge regression, see the first link under list item 2. above.\n\nReferences:\n\n[1]: Hoerl, A.E. (1959). Optimum solution of many variables equations. \nChemical Engineering Progress\n, \n\n55\n (11) 69-78.\n\n[2]: Hoerl, A.E. (1962). Applications of ridge analysis to regression problems. \nChemical Engineering Progress\n, \n\n58\n (3) 54-59.\n\n[3] Hoerl, R.W. (1985). Ridge Analysis 25 Years Later.\n\nAmerican Statistician\n, \n39\n (3), 186-192",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230943/402101 \n A probability space $\\mathcal{P}$ is by definition a tripple $(\\Omega, \\mathcal{F}, \\mathbb{P} )$ where $\\Omega$ is a set of outcomes, $\\mathcal{F}$ is a $\\sigma$-algebra on the subsets of $\\Omega$ and $\\mathbb{P}$ is a probability-measure that fulfills the axioms of Kolmogorov, i.e. $\\mathbb{P}$ is a function from $\\mathcal{F}$ to $[0,1]$ such that $\\mathbb{P}(\\Omega)=1$ and for disjoint $E_1, E_2, \\dots$ in $\\mathcal{F}$ it holds that $P \\left( \\cup_{j=1}^\\infty E_j \\right)=\\sum_{j=1}^\\infty \\mathbb{P}(E_j)$.\n\nWithin such a probability space one can, for two events $E_1, E_2$ in $\\mathcal{F}$ define the conditional probability as $\\mathbb{P}(E_1|_{E_2})\\stackrel{def}{=}\\frac{\\mathbb{P}(E_1 \\cap E_2)}{\\mathbb{P}(E_2)}$\n\nNote that:\n\nthis ''conditional probability'' is only defined when $\\mathbb{P}$ is defined on $\\mathcal{F}$, so we need a probability space to be able to define conditional probabilities. \n\n\nA probability space is defined in very general terms (\na\n set $\\Omega$, \na\n $\\sigma$-algebra $\\mathcal{F}$ and \na\n probability measure $\\mathbb{P}$), the only requirement is that certain properties should be fulfilled but apart from that these three elements can be ''anything''.\n\nMore detail can be found in \nthis link\n\nFrom the definition of conditional probability it also holds that $\\mathbb{P}(E_2|_{E_1})=\\frac{\\mathbb{P}(E_2 \\cap E_1)}{\\mathbb{P}(E_1)}$. And from the two latter equations we find Bayes' rule.  So Bayes' rule holds (by definition of conditional probabilty) in any probability space (to show it, derive $\\mathbb{P}(E_1 \\cap E_2)$ and $\\mathbb{P}(E_2 \\cap E_1)$ from each equation and equate them (they are equal because intersection is commutative)).\n\nAs Bayes rule is the basis for Bayesian inference, one can do Bayesian analysis in any valid (i.e. fulfilling all conditions, a.o. Kolmogorov's axioms) probability space.\n\nThe above holds ''in general'', i.e. we have no specific $\\Omega$, $\\mathcal{F}$, $\\mathbb{P}$ in mind as long as $\\mathcal{F}$ is a $\\sigma$-algebra on subsets of $\\Omega$ and $\\mathbb{P}$ fulfills Kolmogorov's axioms.\n\nWe will now show that a ''frequentist'' definition of $\\mathbb{P}$ fulfills Kolomogorov's axioms.  If that is the case then ''frequentist'' probabilities are only a special case of Kolmogorov's general and abstract probability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230943/402101 \n The above holds ''in general'', i.e. we have no specific $\\Omega$, $\\mathcal{F}$, $\\mathbb{P}$ in mind as long as $\\mathcal{F}$ is a $\\sigma$-algebra on subsets of $\\Omega$ and $\\mathbb{P}$ fulfills Kolmogorov's axioms.\n\nWe will now show that a ''frequentist'' definition of $\\mathbb{P}$ fulfills Kolomogorov's axioms.  If that is the case then ''frequentist'' probabilities are only a special case of Kolmogorov's general and abstract probability.\n\nLet's take an example and roll the dice. Then the set of all possible outcomes $\\Omega$ is $\\Omega=\\{1,2,3,4,5,6\\}$. We also need a $\\sigma$-algebra on this set $\\Omega$ and we take $\\mathcal{F}$ the set of all subsets of $\\Omega$, i.e. $\\mathcal{F}=2^\\Omega$.\n\nWe still have to define the probability measure $\\mathbb{P}$ in a frequentist way. Therefore we define $\\mathbb{P}(\\{1\\})$ as $\\mathbb{P}(\\{1\\}) \\stackrel{def}{=} \\lim_{n \\to +\\infty} \\frac{n_1}{n}$ where $n_1$ is the number of $1$'s obtained in $n$ rolls of the dice. Similar for $\\mathbb{P}(\\{2\\})$, ... $\\mathbb{P}(\\{6\\})$.\n\nIn this way $\\mathbb{P}$ is defined for all singletons in $\\mathcal{F}$.  For any other set in $\\mathcal{F}$, e.g. $\\{1,2\\}$ we define $\\mathbb{P}(\\{1,2\\})$ in a frequentist way i.e. \n$\\mathbb{P}(\\{1,2\\}) \\stackrel{def}{=} \\lim_{n \\to +\\infty} \\frac{n_1+n_2}{n}$, but by the linearity of the 'lim', this is equal to $\\mathbb{P}(\\{1\\})+\\mathbb{P}(\\{2\\})$, which implies that Kolmogorov's axioms hold.\n\nSo the frequentist definition of probability is only a special case of Kolomogorov's general and abstract definition of a probability measure.\n\nNote that there are other ways to define a probability measure that fulfills Kolmogorov's axioms, so the frequentist definition is not the only possible one.\n\nThe probability in Kolmogorov's axiomatic system is ''abstract'', it has no real meaning, it only has to fulfill conditions called ''axioms''.  Using only these axioms Kolmogorov was able to derive a very rich set of theorems.\n\nThe frequentist definition of probability fullfills the axioms and therefore replacing the abstract, ''meaningless'' $\\mathbb{P}$ by a probability defined in a frequentist way, all these theorems are valid because the \n''frequentist probability'' is only a special case of Kolmogorov's abstract probability (i.e. it fulfills the axioms).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230943/402101 \n The frequentist definition of probability fullfills the axioms and therefore replacing the abstract, ''meaningless'' $\\mathbb{P}$ by a probability defined in a frequentist way, all these theorems are valid because the \n''frequentist probability'' is only a special case of Kolmogorov's abstract probability (i.e. it fulfills the axioms).\n\nOne of the properties that can be derived in Kolmogorov's general framework is Bayes rule.  As it holds in the general and abstract framework, it will also hold (cfr supra) in the specific case that the probabilities are defined in a frequentist way (because the frequentist definition fulfills the axioms and these axioms were the only thing that is needed to derive all theorems).  \nSo one can do Bayesian analysis with a frequentist definition of probability.\n\nDefining $\\mathbb{P}$ in a frequentist way is not the only possibility, there are other ways to define it such that it fulfills the abstract axioms of Kolmogorov.  Bayes' rule will also hold in these ''specific cases''. \nSo one can also do Bayesian analysis with a \nnon\n-frequentist definition of probability.\n\n@mpiktas reaction to your comment:\n\nAs I said, the sets $\\Omega, \\mathcal{F}$ and the probability measure $\\mathbb{P}$ have no particular meaning in the axiomatic system, they are abstract.\n\nIn order to apply this theory you have to give further \ndefinitions\n (so what you say in your comment \n\"no need to muddle it further with some bizarre definitions''\n is \nwrong, you need additional definitions\n).\n\nLet's apply it to the case of tossing a fair coin.  The set $\\Omega$ in Kolmogorov's theory has no particular meaning, it just has to be ''a set''.  So we must specify what this set is in case of the fair coin, i.e. we must define the set $\\Omega$.  If we represent head as H and tail as T, then the set $\\Omega$ is \nby definition\n $\\Omega\\stackrel{def}{=}\\{H,T\\}$.\n\nWe also have to \ndefine\n the events, i.e. the $\\sigma$-algebra $\\mathcal{F}$. We define is as $\\mathcal{F} \\stackrel{def}{=} \\{\\emptyset, \\{H\\},\\{T\\},\\{H,T\\} \\}$. It is easy to verify that $\\mathcal{F}$ is a $\\sigma$-algebra.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/230943/402101 \n We also have to \ndefine\n the events, i.e. the $\\sigma$-algebra $\\mathcal{F}$. We define is as $\\mathcal{F} \\stackrel{def}{=} \\{\\emptyset, \\{H\\},\\{T\\},\\{H,T\\} \\}$. It is easy to verify that $\\mathcal{F}$ is a $\\sigma$-algebra.\n\nNext we must define for every event in $E \\in \\mathcal{F}$ its measure.  So we need to \ndefine\n a map from $\\mathcal{F}$ in $[0,1]$.  I will define it in the frequentist way, for a fair coin, if I toss it a huge number of times, then the fraction of heads will be 0.5, so I define $\\mathbb{P}(\\{H\\})\\stackrel{def}{=}0.5$.  Similarly I define $\\mathbb{P}(\\{T\\})\\stackrel{def}{=}0.5$, $\\mathbb{P}(\\{H,T\\})\\stackrel{def}{=}1$ and $\\mathbb{P}(\\emptyset)\\stackrel{def}{=}0$. Note that $\\mathbb{P}$ is a map from $\\mathcal{F}$ in $[0,1]$ and that it fulfills Kolmogorov's axioms.\n\nFor a reference with the frequentist definition of probability see \nthis link\n (at the end of the section 'definition') and \nthis link\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/107821/402101 \n I often hear the claim that Bayesian statistics can be highly subjective.\n\nSo do I.  But notice that there's a major ambiguity in calling something subjective.\n\nSubjectivity (both senses)\n\nSubjective can mean (at least) one of\n\ndepends on the idiosyncracies of the researcher\n\n\nexplicitly concerned with the state of knowledge of an individual\n\nBayesianism is subjective in the second sense because it is always offering a way to update beliefs represented by probability distributions by conditioning on information.  (Note that whether those beliefs are beliefs that some subject actually has or just beliefs that a subject \ncould\n have is irrelevant to deciding whether it is 'subjective'.)\n\nThe main argument being that inference depends on the choice of a prior\n\nActually, if a prior represents your personal belief about something then you almost certainly didn't \nchoose\n it at any more than you chose most of your beliefs.  And if it represents somebody's beliefs then it can be a more or less accurate representation of those beliefs, so ironically there will be a rather 'objective' fact about how well it represents them.\n\n(even though one could use the principle of indifference o maximum entropy to choose a prior).\n\nOne could, though this doesn't tend to generalize very smoothly to continuous domains.  Also, arguably it's impossible to be flat or 'indifferent' in all parameterisations at once (though I've never been quite sure why you'd want to be).\n\nIn comparison, the claim goes, frequentist statistics is in general more objective. How much truth is there in this statement?\n\nSo how might we evaluate this claim?\n\nI suggest that in the second second sense of subjective: it's mostly correct.  And in the first sense of subjective: it's probably false.\n\nFrequentism as subjective (second sense)\n\nSome historical detail is helpful to map the issues\n\nFor Neyman and Pearson there is only inductive \nbehaviour\n not inductive \ninference\n and all statistical evaluation works with long run sampling properties of estimators.  (Hence alpha and power analysis, but not p values).  That's pretty unsubjective in both senses.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/107821/402101 \n Frequentism as subjective (second sense)\n\nSome historical detail is helpful to map the issues\n\nFor Neyman and Pearson there is only inductive \nbehaviour\n not inductive \ninference\n and all statistical evaluation works with long run sampling properties of estimators.  (Hence alpha and power analysis, but not p values).  That's pretty unsubjective in both senses.\n\nIndeed it's possible, and I think quite reasonable, to argue along these lines that Frequentism is actually not an inference framework at all but rather a collection of \nevaluation criteria\n for all  possible inference procedures that emphasises their behaviour in repeated application.  Simple examples would be consistency, unbiasedness, etc.  This makes it obviously unsubjective in sense 2.  However, it also risks being subjective in sense 1 when we have to decide what to do when those crteria do not apply (e.g. when there isn't an unbiased estimator to be had) or when they apply but contradict.\n\nFisher offered a less unsubjective Frequentism that is interesting.  For Fisher, there is such a thing as inductive inference, in the sense that a subject, the scientist, makes inferences on the basis of a data analysis, done by the statistician. (Hence p-values but not alpha and power analysis).  However, the decisions about how to behave, whether to carry on with research etc. are made by the scientist on the basis of her understanding of domain theory, not by the statistician applying the inference paradigm.  Because of this Fisherian division of labour, both the subjectiveness (sense 2) and the individual subject (sense 1) sit on the science side, not the statistical side.\n\nLegalistically speaking, the Fisher's Frequentism \nis\n subjective. It's just that the subject who is subjective is not the statistician.\n\nThere are various syntheses of these available, both the barely coherent mix of these two you find in applied statistics textbooks and more nuanced versions, e.g. the 'Error Statistics' pushed by Deborah Mayo.  This latter is pretty unsubjective in sense 2, but highly subjective in sense 1, because the researcher has to use scientific judgement - Fisher style - to figure out what error probabilities matter and shoudl be tested.\n\nFrequentism as subjective (first sense)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/107821/402101 \n There are various syntheses of these available, both the barely coherent mix of these two you find in applied statistics textbooks and more nuanced versions, e.g. the 'Error Statistics' pushed by Deborah Mayo.  This latter is pretty unsubjective in sense 2, but highly subjective in sense 1, because the researcher has to use scientific judgement - Fisher style - to figure out what error probabilities matter and shoudl be tested.\n\nFrequentism as subjective (first sense)\n\nSo is Frequentism less subjective in the first sense?  It depends.  Any inference procedure can be riddled with idiosyncracies as actually applied.  So perhaps it's more useful to ask whether Frequentism \nencourages\n a less subjective (first sense) approach?  I doubt it - I think the self conscious application of subjective (second sense) methods leads to less subjective (first sense) outcomes, but it can be argued either way.\n\nAssume for a moment that subjectiveness (first sense) sneaks into an analysis via 'choices'.  Bayesianism does seem to involve more 'choices'.  In the simplest case the choices tally up as: one set of potentially idiosyncratic assumptions for the Frequentist (the Likelihood function or equivalent) and two sets for the Bayesian (the Likelihood and a prior over the unknowns).\n\nHowever, Bayesians \nknow\n they're being subjective (in the second sense) about all these choices so they are liable to be more self conscious about the implications which should lead to less subjectiveness (in the first sense).\n\nIn contrast, if one looks up a test in a big book of tests, then one could get the feeling that the result is less subjective (first sense), but arguably that's a result of substituting some other subject's understanding of the problem for one's own.  It's not clear that one has gotten less subjective this way, but it might feel that way.  I think most would agree that that's unhelpful.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4274/402101 \n Short answer: Whenever you are facing one of these situations:\n\nlarge number of variables or low ratio of no. observations to no. variables (including the $n\\ll p$ case), \n\n\nhigh collinearity,\n\n\nseeking for a sparse solution (i.e., embed feature selection when estimating model parameters), or \n\n\naccounting for variables grouping in high-dimensional data set.\n\nRidge regression generally yields better predictions than OLS solution, through a better compromise between bias and variance. Its main drawback is that all predictors are kept in the model, so it is not very interesting if you seek a parsimonious model or want to apply some kind of feature selection.\n\nTo achieve sparsity, the lasso is more appropriate but it will not necessarily yield good results in presence of high collinearity (it has been observed that if predictors are highly correlated, the prediction performance of the lasso is dominated by ridge regression). The second problem with L1 penalty is that the lasso solution is not uniquely determined when the number of variables is greater than the number of subjects (this is not the case of ridge regression). The last drawback of lasso is that it tends to select only one variable among a group of predictors with high pairwise correlations. In this case, there are alternative solutions like the \ngroup\n (i.e., achieve shrinkage on block of covariates, that is some blocks of regression coefficients are exactly zero) or \nfused\n lasso. The \nGraphical Lasso\n also offers promising features for GGMs (see the R \nglasso\n package).\n\nBut, definitely, the \nelasticnet\n criteria, which is a combination of L1 and L2 penalties achieve both shrinkage and automatic variable selection, and it allows to keep $m>p$ variables in the case where $n\\ll p$. Following Zou and Hastie (2005), it is defined as the argument that minimizes (over $\\beta$)\n\n$$\nL(\\lambda_1,\\lambda_2,\\mathbf{\\beta}) = \\|Y-X\\beta\\|^2 + \\lambda_2\\|\\beta\\|^2 + \\lambda_1\\|\\beta\\|_1\n$$\n\nwhere $\\|\\beta\\|^2=\\sum_{j=1}^p\\beta_j^2$ and $\\|\\beta\\|^1=\\sum_{j=1}^p|\\beta_j |$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4274/402101 \n But, definitely, the \nelasticnet\n criteria, which is a combination of L1 and L2 penalties achieve both shrinkage and automatic variable selection, and it allows to keep $m>p$ variables in the case where $n\\ll p$. Following Zou and Hastie (2005), it is defined as the argument that minimizes (over $\\beta$)\n\n$$\nL(\\lambda_1,\\lambda_2,\\mathbf{\\beta}) = \\|Y-X\\beta\\|^2 + \\lambda_2\\|\\beta\\|^2 + \\lambda_1\\|\\beta\\|_1\n$$\n\nwhere $\\|\\beta\\|^2=\\sum_{j=1}^p\\beta_j^2$ and $\\|\\beta\\|^1=\\sum_{j=1}^p|\\beta_j |$.\n\nThe lasso can be computed with an algorithm based on coordinate descent as described in the recent paper by Friedman and coll., \nRegularization Paths for Generalized Linear Models via Coordinate Descent\n (JSS, 2010) or the LARS algorithm. In R, the \npenalized\n, \nlars\n or \nbiglars\n, and \nglmnet\n packages are useful packages; in Python, there's the \nscikit.learn\n toolkit, with extensive \ndocumentation\n on the algorithms used to apply all three kind of regularization schemes.\n\nAs for general references, the \nLasso page\n contains most of what is needed to get started with lasso regression and technical details about L1-penalty, and this related question features essential references, \nWhen should I use lasso vs ridge?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/133171/402101 \n Pearson correlation \nis\n used to look at correlation between series ... but being time series the correlation is looked at across different lags -- the \ncross-correlation function\n.\n\nThe cross-correlation is impacted by dependence within-series, so in many cases\n$^{\\dagger}$\n the within-series dependence should be removed first. So to use this correlation, rather than \nsmoothing\n the series, it's actually more common (because it's meaningful) to look at dependence between residuals - the rough part that's left over after a suitable model is found for the variables.\n\nYou probably want to begin with some basic resources on time series models before delving into trying to figure out whether a Pearson correlation across (presumably) non-stationary, smoothed series is interpretable.\n\nIn particular, you'll probably want to look into the phenomenon \nhere\n.\n\n[Edit -- the Wikipedia landscape keeps changing; the above para. should probably be revised to reflect what's there now.]\n\ne.g. see some discussions\n\n[Why Do We Sometimes Get Nonsense-Correlations between Time Series? A Study in Sampling and the Nature of Time Series][1]  (the opening quote of Yule, in a paper presented in 1925 but published the following year, summarizes the problem quite well)\n\n\n\n\nChristos Agiakloglou and Apostolos Tsimpanos, \nSpurious Correlations for Stationary AR(1) Processes\n \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.611.5055&rep=rep1&type=pdf\n  (this shows that you can even get the problem between stationary series; hence the tendency to pre-whiten)\n\n\n\n\nThe classic reference of Yule, (1926) [1] mentioned above.\n\n[Why Do We Sometimes Get Nonsense-Correlations between Time Series? A Study in Sampling and the Nature of Time Series][1]  (the opening quote of Yule, in a paper presented in 1925 but published the following year, summarizes the problem quite well)\n\nChristos Agiakloglou and Apostolos Tsimpanos, \nSpurious Correlations for Stationary AR(1) Processes\n \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.611.5055&rep=rep1&type=pdf\n  (this shows that you can even get the problem between stationary series; hence the tendency to pre-whiten)\n\nThe classic reference of Yule, (1926) [1] mentioned above.\n\nYou may also find the discussion \nhere\n useful, as well as the discussion \nhere\n\n--\n\nUsing Pearson correlation in a meaningful way between time series is difficult and sometimes surprisingly subtle.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/133171/402101 \n The classic reference of Yule, (1926) [1] mentioned above.\n\nYou may also find the discussion \nhere\n useful, as well as the discussion \nhere\n\n--\n\nUsing Pearson correlation in a meaningful way between time series is difficult and sometimes surprisingly subtle.\n\nI looked up spurious correlation, but I don't care if my A series is the cause of my B series or vice versa. I only want to know if you can learn something about series A by looking at what series B is doing (or vice versa). In other words - do they have an correlation.\n\nTake note of my previous comment about the narrow use of the term spurious correlation in the Wikipedia article.\n\nThe point about spurious correlation is that series can \nappear\n correlated, but the correlation itself is not meaningful. Consider two people tossing two distinct coins counting number of heads so far minus number of tails so far as the value of their series.\n\n(So if person 1 tosses \n$\\text{HTHH...}$\n they have 3-1 = 2 for the value at the 4th time step, and their series goes \n$1, 0, 1, 2,...$\n.)\n\nObviously there's no connection between the two series. \nClearly\n neither can tell you the first thing about the other!\n\nBut look at the sort of correlations you get between pairs of coins:\n\n\n\nIf I didn't tell you what those were, and you took any pair of those series by themselves, those would be impressive correlations would they not?\n\nBut they're all \nmeaningless\n. Utterly spurious. None of the three pairs are really any more positively or negatively related to each other than any of the others -- its just \ncumulated noise\n (and for people thinking an edit would improve this, yes, I really do mean to write, \ncumulated\n, as in the inverse of differencing, not \naccumulated\n, which would be the total, please desist from 'fixing' this). The \nspuriousness\n isn't just about prediction, the whole \nnotion\n of considering association between series without taking account of the within-series dependence is misplaced.\n\nAll\n you have here is \nwithin-series\n dependence. There's no actual cross-series relation at all.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/133171/402101 \n All\n you have here is \nwithin-series\n dependence. There's no actual cross-series relation at all.\n\nOnce you deal properly with the issue that makes these series auto-dependent - they're all integrated (\nBernoulli random walks\n), so you need to difference (yes, it's definitely \ndifference\n NOT \ndifferentiate\n, I do not understand why people keep replacing a perfectly correct word with a very-much-incorrect word in my various time series posts, and other people blithely approve it) them - the \"apparent\" association disappears (the largest absolute cross-series correlation of the three is 0.048).\n\nWhat that tells you is the truth -- the apparent association is a mere illusion caused by the dependence within-series.\n\nYour question asked \"how to use Pearson correlation correctly with time series\" -- so please understand: if there's within-series dependence and you \ndon't\n deal with it first, you won't be using it correctly.\n\nFurther, \nsmoothing\n won't reduce the problem of serial dependence; quite the opposite -- it makes it even worse! Here are the correlations after smoothing (default loess smooth - of series vs index - performed in R):\n\ncoin1      coin2     \ncoin2   0.9696378 \ncoin3  -0.8829326 -0.7733559\n\ncoin1      coin2     \ncoin2   0.9696378 \ncoin3  -0.8829326 -0.7733559\n\nThey all got further from 0. They're \nall still nothing but meaningless noise\n, though now it's smoothed, accumulated noise. (By smoothing, we reduce the variability in the series we put into the correlation calculation, so that may be why the correlation goes up.)\n\n${\\dagger}$\n \nCointegrated series\n are an obvious exception.\n\n[1]: Yule, G.U. (1926) \"Why do we Sometimes get Nonsense-Correlations between Time-Series?\" \nJ.Roy.Stat.Soc.\n, \n89\n, \n1\n, pp. 1-63 (\nhttps://www.math.mcgill.ca/~dstephens/OldCourses/204-2007/Handouts/Yule1926.pdf\n)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/30205/402101 \n The bivariate normal distribution is the \nexception\n, not the rule!\n\nIt is important to recognize that \"almost all\" joint distributions with normal marginals are \nnot\n the bivariate normal distribution. That is, the common viewpoint that joint distributions with normal marginals that are not the bivariate normal are somehow \"pathological\", is a bit misguided.\n\nCertainly, the multivariate normal is \nextremely\n important due to its stability under linear transformations, and so receives the bulk of attention in applications.\n\nExamples\n\nIt is useful to start with some examples. The figure below contains heatmaps of six bivariate distributions, \nall\n of which have standard normal marginals. The left and middle ones in the top row are bivariate normals, the remaining ones are not (as should be apparent). They're described further below.\n\n\n\nThe bare bones of copulas\n\nProperties of dependence are often efficiently analyzed using \ncopulas\n. A \nbivariate copula\n is just a fancy name for a probability distribution on the unit square \n$[0,1]^2$\n with \nuniform\n marginals.\n\nSuppose \n$C(u,v)$\n is a bivariate copula. Then, immediately from the above, we know that \n$C(u,v) \\geq 0$\n, \n$C(u,1) = u$\n and \n$C(1,v) = v$\n, for example.\n\nWe can construct bivariate random variables on the Euclidean plane with \nprespecified\n marginals by a simple transformation of a bivariate copula. Let \n$F_1$\n and \n$F_2$\n be prescribed marginal distributions for a pair of random variables \n$(X,Y)$\n. Then, if \n$C(u,v)$\n is a bivariate copula,\n\n$$\nF(x,y) = C(F_1(x), F_2(y))\n$$\n\nis a bivariate distribution function with marginals \n$F_1$\n and \n$F_2$\n. To see this last fact, just note that\n\n$$\n\\renewcommand{\\Pr}{\\mathbb P}\n\\Pr(X \\leq x) = \\Pr(X \\leq x, Y < \\infty) = C(F_1(x), F_2(\\infty)) = C(F_1(x),1) = F_1(x) \\>.\n$$\n\nThe same argument works for \n$F_2$\n.\n\nFor continuous \n$F_1$\n and \n$F_2$\n, \nSklar's theorem\n asserts a converse implying uniqueness. That is, given a bivariate distribution \n$F(x,y)$\n with continuous marginals \n$F_1$\n, \n$F_2$\n, the corresponding copula is unique (on the appropriate range space).\n\nThe bivariate normal is \nexceptional\n\nSklar's theorem tells us (essentially) that there is only one copula that produces the bivariate normal distribution. This is, aptly named, the \nGaussian copula\n which has density on \n$[0,1]^2$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/30205/402101 \n For continuous \n$F_1$\n and \n$F_2$\n, \nSklar's theorem\n asserts a converse implying uniqueness. That is, given a bivariate distribution \n$F(x,y)$\n with continuous marginals \n$F_1$\n, \n$F_2$\n, the corresponding copula is unique (on the appropriate range space).\n\nThe bivariate normal is \nexceptional\n\nSklar's theorem tells us (essentially) that there is only one copula that produces the bivariate normal distribution. This is, aptly named, the \nGaussian copula\n which has density on \n$[0,1]^2$\n\n\n$$\nc_\\rho(u,v) := \\frac{\\partial^2}{\\partial u \\, \\partial v} C_\\rho(u,v) = \\frac{\\varphi_{2,\\rho}(\\Phi^{-1}(u),\\Phi^{-1}(v))}{\\varphi(\\Phi^{-1}(u)) \\varphi(\\Phi^{-1}(v))} \\>,\n$$\n\nwhere the numerator is the bivariate normal distribution with correlation \n$\\rho$\n evaluated at \n$\\Phi^{-1}(u)$\n and \n$\\Phi^{-1}(v)$\n.\n\nBut, there are \nlots\n of other copulas and \nall\n of them will give a bivariate distribution with normal marginals which is \nnot\n the bivariate normal by using the transformation described in the previous section.\n\nSome details on the examples\n\nNote that if \n$C(u,v)$\n is an \narbitrary\n copula with density \n$c(u,v)$\n, the corresponding bivariate density with standard normal marginals under the transformation \n$F(x,y) = C(\\Phi(x),\\Phi(y))$\n is\n\n$$\nf(x,y) = \\varphi(x) \\varphi(y) c(\\Phi(x), \\Phi(y)) \\> .\n$$\n\nNote that by applying the Gaussian copula in the above equation, we recover the bivariate normal density. But, for any other choice of \n$c(u,v)$\n, we will not.\n\nThe examples in the figure were constructed as follows (going across each row, one column at a time):\n\nBivariate normal with independent components.\n\n\nBivariate normal with \n$\\rho = -0.4$\n.\n\n\nThe \nexample given in this answer\n of \nDilip Sarwate\n. It can easily be seen to be induced by the copula \n$C(u,v)$\n with density \n$c(u,v) = 2 (\\mathbf 1_{(0 \\leq u \\leq 1/2, 0 \\leq v \\leq 1/2)} + \\mathbf 1_{(1/2 < u \\leq 1, 1/2 < v \\leq 1)})$\n.\n\n\nGenerated from the \nFrank copula\n with parameter \n$\\theta = 2$\n.\n\n\nGenerated from the \nClayton copula\n with parameter \n$\\theta = 1$\n.\n\n\nGenerated from an asymmetric modification of the Clayton copula with parameter \n$\\theta = 3$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/122191/402101 \n The dichotomy between the cases $d < 3$ and $d \\geq 3$ for the admissibility of the MLE of the mean of a $d$-dimensional multivariate normal random variable is certainly shocking.\n\nThere is another very famous example in probability and statistics in which there is a dichotomy between the $d < 3$ and $d \\geq 3$ cases. This is the recurrence of a simple random walk on the lattice $\\mathbb{Z}^d$. That is, the $d$-dimensional simple random walk is recurrent in 1 or 2 dimensions, but is transient in $d \\geq 3$ dimensions. The continuous-time analogue (in the form of Brownian motion) also holds.\n\nIt turns out that the two are closely related.\n\nLarry Brown\n proved that the two questions are essentially equivalent. That is, the best invariant estimator $\\hat{\\mu} \\equiv \\hat{\\mu}(X) = X$ of a $d$-dimensional multivariate normal mean vector is admissible if and only if the $d$-dimensional Brownian motion is recurrent.\n\nIn fact, his results go \nmuch\n further. For \nany\n sensible (i.e., generalized Bayes) estimator $\\tilde{\\mu} \\equiv \\tilde{\\mu}(X)$ with bounded (generalized) $L_2$ risk, there is an explicit(!) corresponding $d$-dimensional diffusion such that the estimator $\\tilde{\\mu}$ is admissible if and only if its corresponding diffusion is recurrent.\n\nThe local mean of this diffusion is essentially the discrepancy between the two estimators, i.e., $\\tilde{\\mu} - \\hat{\\mu}$ and the covariance of the diffusion is $2 I$. From this, it is easy to see that for the case of the MLE $\\tilde{\\mu} = \\hat{\\mu} = X$, we recover (rescaled) Brownian motion.\n\nSo, in some sense, we can view the question of admissibility through the lens of stochastic processes and use well-studied properties of diffusions to arrive at the desired conclusions.\n\nReferences\n\nL. Brown (1971). \nAdmissible estimators, recurrent diffusions, and insoluble boundary value problems\n. \nAnn. Math. Stat.\n, vol. 42, no. 3, pp. 855\u2013903. \n\n\nR. N. Bhattacharya (1978). \nCriteria for recurrence and existence of invariant measures for multidimensional diffusions\n. \nAnn. Prob.\n, vol. 6, no. 4, 541\u2013553.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/184022/402101 \n How to decide which regularization (L1 or L2) to use?\n\nWhat is your goal? Both can improve model generalization by penalizing coefficients, since features with opposite relationship to the outcome can \"offset\" each other (a large positive value is counterbalanced by a large negative value). This can arise when there are collinear features. Small changes in the data can result in dramatically different parameter estimates (high variance estimates). Penalization can restrain both coefficients to be smaller. (Hastie et al, \nElements of Statistical Learning\n, 2nd edition, p. 63)\n\nWhat are the pros & cons of each of L1 / L2 regularization?\n\nL1 regularization can address the multicollinearity problem by constraining the coefficient norm and pinning some coefficient values to 0. Computationally, Lasso regression (regression with an L1 penalty) is a quadratic program which requires some special tools to solve. \nWhen you have more features than observations \n$N$\n, lasso will keep at most \n$N$\n non-zero coefficients\n. Depending on context, that might not be what you want.\n\nL1 regularization is sometimes used as a feature selection method. Suppose you have some kind of hard cap on the number of features you can use (because data collection for \nall\n features is expensive, or you have tight engineering constraints on how many values you can store, etc.). You can try to tune the L1 penalty to hit your desired number of non-zero features.\n\nL2 regularization can address the multicollinearity problem by constraining the coefficient norm and keeping all the variables. It's unlikely to estimate a coefficient to be exactly 0. This isn't necessarily a drawback, unless a sparse coefficient vector is important for some reason.\n\nIn the regression setting, it's the \"classic\" solution to the problem of estimating a regression with more features than observations. L2 regularization can estimate a coefficient for each feature even if there are more features than observations (indeed, this was the original motivation for \"ridge regression\").\n\nAs an alternative, \nelastic net allows L1 and L2 regularization as special cases.\n A typical use-case in for a data scientist in industry is that you just want to pick the best model, but don't necessarily care if it's penalized using L1, L2 or both. Elastic net is nice in situations like these.\n\nIs it recommended to 1st do feature selection using L1 & then apply L2 on these selected variables?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/184022/402101 \n As an alternative, \nelastic net allows L1 and L2 regularization as special cases.\n A typical use-case in for a data scientist in industry is that you just want to pick the best model, but don't necessarily care if it's penalized using L1, L2 or both. Elastic net is nice in situations like these.\n\nIs it recommended to 1st do feature selection using L1 & then apply L2 on these selected variables?\n\nI'm not familiar with a publication proposing an L1-then-L2 pipeline, but this is probably just ignorance on my part. There doesn't seem to be anything wrong with it. I'd conduct a literature review.\n\nA few examples of similar \"phased\" pipelines exist. One is the \"relaxed lasso\", which applies lasso regression \ntwice\n, once to down-select from a large group to a small group of features, and second to estimate coefficients for use in a model. This uses cross-validation at each step to choose the magnitude of the penalty. The reasoning is that in the first step, you cross-validate and will likely choose a large penalty to screen out irrelevant predictors; in the second step, you cross-validate and will likely pick a smaller penalty (and hence larger coefficients). This is mentioned briefly in \nElements of Statistical Learning\n with a citation to Nicolai Meinshausen (\"Relaxed Lasso.\" \nComputational Statistics & Data Analysis\n Volume 52, Issue 1, 15 September 2007, pp 374-393).\n\nUser @amoeba also suggests an L1-then-OLS pipeline; this might be nice because it only has 1 hyperparameter for the magnitude of the L1 penalty, so less fiddling would be required.\n\nOne problem that can arise with any \"phased\" analysis pipeline (that is, a pipeline which does some steps, and then some other steps separately) is that there's no \"visibility\" between those different phases (algorithms applied at each step). This means that one process inherits any data snooping that happened at the previous steps. This effect is not negligible; poorly-conceived modeling can result in garbage models.\n\nOne way to hedge against data-snooping side-effects is to cross-validate all of your choices. However, the increased computational costs can be prohibitive, depending on the scale of the data and the complexity of each step.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/84107/402101 \n To add to @Peter Flom's answer, it is worth defining the other terms that were used:\n\nDeductive reasoning:\n  Derive conclusions or predictions about specific cases from fundamental rules or theories.\n\nInductive reasoning:\n  Derive universal rules or theories from observation of many cases.\n\nInferential statistics use both inductive and deductive reasoning.  You are trying to establish rules about the behaviour of a system based on evidence, but you are testing models against probability theories derived deductively (i.e., probability distributions in parametric models or the combinatorics that are the basis of non-parametric models).\n\nDescriptive statistics don't really qualify as \"reasoning\" in my book.  Saying the average of something is \nx\n and the standard deviation is \ns\n isn't any more of an argument than saying the colour of something is blue.  You're describing what you have in front of you, not drawing any conclusions beyond it.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18435/402101 \n It is possible that this question is homework but I felt this classical elementary probability question was still lacking a complete answer after several months, so I'll give one here.\n\nFrom the problem statement, we want the distribution of\n\n$$Y = \\max \\{ X_1, ..., X_n \\}$$\n\nwhere \n$X_1, ..., X_n$\n are iid \n${\\rm Uniform}(a,b)$\n. We know that \n$Y < x$\n if and only if every element of the sample is less than \n$x$\n. Then this, as indicated in @varty's hint, combined with the fact that the \n$X_i$\n's are independent, allows us to deduce\n\n$$ P(Y \\leq x) = P(X_1 \\leq x, ..., X_n \\leq x) = \\prod_{i=1}^{n} P(X_i \\leq x) = F_{X}(x)^n$$\n\nwhere \n$F_{X}(x)$\n is the \nCDF of the uniform distribution\n that is \n$\\frac{y-a}{b-a}$\n. Therefore the CDF of \n$Y$\n is\n\n$$F_{Y}(y) = P(Y \\leq y) = \\begin{cases} \n0 & y \\leq a \\\\ \n\\phantom{} \\left( \\frac{y-a}{b-a} \\right)^n & y\\in(a,b) \\\\\n1 & y \\geq b \\\\ \n\\end{cases}$$\n\nSince \n$Y$\n has an absolutely continuous distribution \nwe can derive its density by differentiating the CDF\n. Therefore the density of \n$Y$\n is\n\n$$ p_{Y}(y) = \\frac{n(y-a)^{n-1}}{(b-a)^{n}}$$\n\nIn the special case where \n$a=0,b=1$\n, we have that \n$p_{Y}(y)=ny^{n-1}$\n, which is the density of a \nBeta distribution\n with \n$\\alpha=n$\n and \n$\\beta=1$\n, since \n${\\rm Beta}(n,1) = \\frac{\\Gamma(n+1)}{\\Gamma(n)\\Gamma(1)}=\\frac{n!}{(n-1)!} = n$\n.\n\nAs a note, the sequence you get if you were to sort your sample in increasing order - \n$X_{(1)}, ..., X_{(n)}$\n - are called the \norder statistics\n. A generalization of this answer is that \nall order statistics of a \n${\\rm Uniform}(0,1)$\n distributed sample have a Beta distribution\n, as noted in @bnaul's answer.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/876/402101 \n Keep in mind that ridge regression can't zero out coefficients; thus, you either end up including all the coefficients in the model, or none of them. In contrast, the LASSO does both parameter shrinkage and variable selection automatically. If some of your covariates are highly correlated, you may want to look at the Elastic Net [3] instead of the LASSO.\n\nI'd personally recommend using the Non-Negative Garotte (NNG) [1] as it's consistent in terms of estimation and variable selection [2]. Unlike LASSO and ridge regression, NNG requires an initial estimate that is then shrunk towards the origin. In the original paper, Breiman recommends the least-squares solution for the initial estimate (you may however want to start the search from a ridge regression solution and use something like GCV to select the penalty parameter).\n\nIn terms of available software, I've implemented the original NNG in MATLAB (based on Breiman's original FORTRAN code). You can download it from:\n\nhttp://www.emakalic.org/blog/wp-content/uploads/2010/04/nngarotte.zip\n\nBTW, if you prefer a Bayesian solution, check out [4,5].\n\nReferences:\n\n[1] Breiman, L. Better Subset Regression Using the Nonnegative Garrote Technometrics, 1995, 37, 373-384\n\n[2] Yuan, M. & Lin, Y. On the non-negative garrotte estimator Journal of the Royal Statistical Society (Series B), 2007, 69, 143-161\n\n[3] Zou, H. & Hastie, T. Regularization and variable selection via the elastic net Journal of the Royal Statistical Society (Series B), 2005, 67, 301-320\n\n[4] Park, T. & Casella, G. The Bayesian Lasso Journal of the American Statistical Association, 2008, 103, 681-686\n\n[5] Kyung, M.; Gill, J.; Ghosh, M. & Casella, G. Penalized Regression, Standard Errors, and Bayesian Lassos Bayesian Analysis, 2010, 5, 369-412",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/36037/402101 \n You can mechanically check that the expected value does not exist, but this should be physically intuitive, at least if you accept \nHuygens' principle\n and the \nLaw of Large Numbers\n. The conclusion of the Law of Large Numbers fails for a Cauchy distribution, so it can't have a mean. If you average $n$ independent Cauchy random variables, the result does not converge to $0$ as $n\\to \\infty$ with probability $1$. It stays a Cauchy distribution of the same size. This is important in optics.\n\nThe Cauchy distribution is the normalized intensity of light on a line from a point source. Huygens' principle says that you can determine the intensity by assuming that the light is re-emitted from any line between the source and the target. So, the intensity of light on a line $2$ meters away can be determined by assuming that the light first hits a line $1$ meter away, and is re-emitted at any forward angle. The intensity of light on a line $n$ meters away can be expressed as the $n$-fold convolution of the distribution of light on a line $1$ meter away. That is, the sum of $n$ independent Cauchy distributions is a Cauchy distribution scaled by a factor of $n$.\n\nIf the Cauchy distribution had a mean, then the $25$th percentile of the $n$-fold convolution divided by $n$ would have to converge to $0$ by the Law of Large Numbers. Instead it stays constant. If you mark the $25$th percentile on a (transparent) line $1$ meter away, $2$ meters away, etc. then these points form a straight line, at $45$ degrees. They don't bend toward $0$.\n\nThis tells you about the Cauchy distribution in particular, but you should know the integral test because there are other distributions with no mean which don't have a clear physical interpretation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/102800/402101 \n The reviewer should have told you why the Spearman $\\rho$ is not appropriate.  Here is one version of that:  Let the data be $(Z_i, I_i)$ where $Z$ is the measured variable and $I$ is the gender indicator, say it is 0 (man), 1 (woman). Then Spearman's $\\rho$ is calculated based on the ranks of $Z, I$ respectively. Since there are only two possible values for the indicator $I$, there will be a lot of ties, so this formula is not appropriate. If you replace rank with mean rank,  then you will get only two different values, one for men, another for women.  Then $\\rho$ will become basically some rescaled version of the mean ranks between the two groups. It would be simpler (more interpretable) to simply compare the means!   Another approach is the following.\n\nLet $X_1, \\dots, X_n$ be the observations of the continuous variable among men, $Y_1, \\dots, Y_m$ same among women. Now, if the distribution of $X$ and of $Y$ are the same, then $P(X>Y)$ will be 0.5 (let's assume the distribution is purely absolutely continuous, so there are no ties). In the general case, define\n$$\n   \\theta = P(X>Y)\n$$\nwhere $X$ is a random draw among men, $Y$ among women. Can we estimate $\\theta$ from our sample? Form all pairs $(X_i, Y_j)$ (assume no ties) and count for how many we have  \"man is larger\" ($X_i > Y_j$)($M$) and for how many \"woman is larger\"  ($ X_i < Y_j$) ($W$).  Then one sample estimate of $\\theta$ is\n$$\n  \\frac{M}{M+W}\n$$\nThat is one reasonable measure of correlation!  (If there are only a few ties, just ignore them).  But I am not sure what that is called,  if it has a name. \nThis one may be close:       \nhttps://en.wikipedia.org/wiki/Goodman_and_Kruskal%27s_gamma",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/52841/402101 \n As mentioned by @Nick this is a consequence of \nWilks' theorem\n. But note that the test statistic is \nasymptotically\n $\\chi^2$-distributed, not $\\chi^2$-distributed.\n\nI am very impressed by this theorem because it holds in a very wide context. Consider a statistical model with likelihood $l(\\theta \\mid y)$ where $y$ is the vector observations of  $n$ independent replicated observations from a distribution with parameter $\\theta$ belonging to a submanifold $B_1$ of $\\mathbb{R}^d$ with dimension $\\dim(B_1)=s$. Let $B_0 \\subset B_1$ be a submanifold with dimension $\\dim(B_0)=m$. Imagine you are interested in testing $H_0\\colon\\{\\theta \\in B_0\\}$.\n\nThe \nlikelihood ratio\n is \n$$lr(y) = \\frac{\\sup_{\\theta \\in B_1}l(\\theta \\mid y)}{\\sup_{\\theta \\in B_0}l(\\theta \\mid y)}. $$\nDefine the \ndeviance\n $d(y)=2 \\log \\big(lr(y)\\big)$. Then \nWilks' theorem\n says that, under usual regularity assumptions, $d(y)$ is asymptotically $\\chi^2$-distributed with $s-m$ degrees of freedom when $H_0$ holds true.\n\nIt is proven in \nWilk's original paper\n mentioned by @Nick. I think this paper is not easy to read. Wilks published a book later, perhaps with an easiest presentation of his theorem. A short heuristic proof is given in \nWilliams' excellent book\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/258117/402101 \n I am not aware of situations, in which stepwise regression would be the preferred approach. It may be okay (particularly in its step-down version starting from the full model) with bootstrapping of the whole stepwise process on extremely large datasets with $n>>p$. Here $n$ is the number of observations in an continuous outcome (or number of records with an event in survival analysis) $p$ is the number of candidate predictors including all considered interactions - i.e. when any even small effects become very clear and it does not matter so much how your do your model building (that would mean that $n$ would be much larger than $p$ than by substantially more than the sometimes quoted factor of 20).\n\nOf course the reason most people are tempted to do something like stepwise regression is,\n\nbecause it is not computationally intensive (if you do not do the proper bootstrapping, but then your results are pretty unreliable), \n\n\nbecause it provides clear cut \"is in the model\" versus \"is not in the model\" statements (which are very unreliable in standard stepwise regression; something that proper bootstrapping will usually make clear so that these statements will usually not be so clear) and \n\n\nbecause often $n$ is smaller, close to or just a bit larger than $p$.\n\nI.e. a method like stepwise regression would (if it had good operating characteristics) be especially attractive in those situations, when it does not have good operating characteristics.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/161649/402101 \n Assume \n$F_X$\n is continuous and increasing. Define \n$Z = F_X(X)$\n and note that \n$Z$\n takes values in \n$[0, 1]$\n. Then\n\n$$F_Z(x) = P(F_X(X) \\leq x) = P(X \\leq F_X^{-1}(x)) = F_X(F_X^{-1}(x)) = x.$$\n\nThe derivative of \n$\\frac{d}{dz}F_Z(x) = f_Z(x) = 1$\n so \n$Z$\n is uniformly distributed \n$[0, 1]$\n.\n\nAnother way to see this: A uniform random variable \n$U$\n taking values in \n$[0, 1]$\n has \n$F_U(x) =  \\int_R f_U(u)\\,du =\\int_0^x \\,du =x$\n. So \n$F_Z(x) = F_U(x)$\n for every \n$x\\in[0, 1]$\n. Since \n$Z$\n and \n$U$\n have the same distribution function \n$Z$\n must also be uniform on \n$[0, 1]$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/342809/402101 \n 'Y must be normally distributed'\n\nmust?\n\nIn the cases that you mention it is sloppy language (abbreviating \n'the error in Y must be normally distributed'\n), but they don't really (strongly) say that \nthe response must\n be normally distributed, or at least it does not seem to me that their words were intended like that.\n\nspeaks about \n\"a continuous variable \n$Y$\n\"\n, but also about \"\n$Y_i$\n\" as in \n$$E(Y_i) = \\beta_0 + \\beta_1 x_i$$\n where we could regard \n$Y_i$\n, which is as amoeba called in the comments 'conditional', normally distributed,\n\n$$Y_i \\sim N(\\beta_0 + \\beta_1x_i,\\sigma^2)$$\n\nThe article uses \n$Y$\n and \n$Y_i$\n interchangeably. Throughout the entire article one speaks about the 'distribution of Y', for instance:\n\nwhen explaining some variant of GLM (binary logistic regression), \n\n\n\n\nRandom component\n: The distribution of \n$Y$\n is assumed to be \n$Binomial(n,\\pi)$\n,...\n\n\n\n\nin some definition\n\n\n\n\nRandom Component\n \u2013 refers to the probability distribution of the response variable (\n$Y$\n); e.g. normal distribution for \n$Y$\n in the linear regression, or binomial distribution for \n$Y$\n in the binary logistic regression.\n\nwhen explaining some variant of GLM (binary logistic regression),\n\nRandom component\n: The distribution of \n$Y$\n is assumed to be \n$Binomial(n,\\pi)$\n,...\n\nin some definition\n\nRandom Component\n \u2013 refers to the probability distribution of the response variable (\n$Y$\n); e.g. normal distribution for \n$Y$\n in the linear regression, or binomial distribution for \n$Y$\n in the binary logistic regression.\n\nhowever at some other point they also refer to \n$Y_i$\n instead of \n$Y$\n:\n\nThe dependent variable \n$Y_i$\n does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...)\n\nThe dependent variable \n$Y_i$\n does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...)\n\nis an extremely brief, simplified, stylized description. I am not sure you should take this serious. For instance, it speaks about\n\n..requires \nall\n variables to be multivariate normal...\n\nso that is not just the response variable,\n\nand also the the 'multivariate' descriptor is vague. I am not sure \nhow to get that interpreted.\n\nhas an additional context explained in brackets:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/342809/402101 \n is an extremely brief, simplified, stylized description. I am not sure you should take this serious. For instance, it speaks about\n\n..requires \nall\n variables to be multivariate normal...\n\nso that is not just the response variable,\n\nand also the the 'multivariate' descriptor is vague. I am not sure \nhow to get that interpreted.\n\nhas an additional context explained in brackets:\n\nOrdinary linear regression predicts the expected value of a given\n  unknown quantity (the response variable, a random variable) as \na\n  linear combination of a set of observed values (predictors)\n. This\n  implies that a constant change in a predictor leads to a constant\n  change in the response variable (i.e. a linear-response model).\n  This is appropriate when the response variable has a normal\n  distribution \n(intuitively, when a response variable can vary\n  essentially indefinitely in either direction with no fixed \"zero\n  value\", or more generally for any quantity that only varies by a\n  relatively small amount, e.g. human heights).\n\nThis 'no fixed zero value' seems to point to the case that a \nlinear combination\n \n$y+\\epsilon$\n when \n$\\epsilon \\sim N(0,\\sigma)$\n has an infinite domain (from minus infinity to plus infinity) whereas often many variables have some finite cut-off value (such as counts not allowing negative values).\n\nThe particular line has been added on \nMarch 8 2012\n, but note that the first line of the Wikipedia article still reads \n\"a flexible generalization of ordinary linear regression that allows for response variables that have \nerror distribution models\n other than a normal distribution\"\n and is not so much (not everywhere) wrong.\n\nSo, based on these three examples (which indeed could \ngenerate\n misconceptions, or at least could be misunderstood) I would not say that \n\"this misconception has spread\"\n. Or at least it does not seem to me that the intention of those three examples is to argue that Y must be normally distributed (although I do remember this issue has arised before here on stackexchange, the swap between normally distributed errors and normally distributed response variable is easy to make).\n\nSo, the assumption that 'Y must be normally distributed' seems to me not like a \nwidespread\n believe/misconception (as in something that spreads like a red herring), but more like a common error (which is not \nspread\n but made independently each time).\n\nAn example of the mistake on this website is in the following question",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/342809/402101 \n So, the assumption that 'Y must be normally distributed' seems to me not like a \nwidespread\n believe/misconception (as in something that spreads like a red herring), but more like a common error (which is not \nspread\n but made independently each time).\n\nAn example of the mistake on this website is in the following question\n\nWhat if residuals are normally distributed, but y is not?\n\nI would consider this as a beginners question. It is not present in the materials like the Penn State course material, the Wikipedia website, and recently noted in the comments the book  'Extending the Linear Regression with R'.\n\nThe writers of those works do correctly understand the material. Indeed, they use phrases such as 'Y must be normally distributed', but based on the context and the used formulas you can see that they all mean 'Y, conditional on X, must be normally distributed' and not 'the marginal Y must be normally distributed'. They are not misconceiving the idea themselves, and at least the idea is not widespread among statisticians and people that write books and other course materials. But misreading their ambiguous words may indeed cause the misconception.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/182754/402101 \n You are conflating the features of a process with its representation. Consider the (return) process $(Y_t)_{t=0}^\\infty$.\n\nAn ARMA(p,q) model specifies the \nconditional mean\n of the process as\n\n$$\n\\begin{align}\n\\mathbb{E}(Y_t \\mid \\mathcal{I}_t) &= \\alpha_0 + \\sum_{j=1}^p \\alpha_j Y_{t-j}+ \\sum_{k=1}^q \\beta_k\\epsilon_{t-k}\\\\\n\\end{align}\n$$\nHere, $\\mathcal{I}_t$ is the information set at time $t$, which is the $\\sigma$-algebra generated by the lagged values of the outcome process $(Y_t)$.\n\nThe GARCH(r,s) model specifies the \nconditional variance\n of the process\n$$\n\\begin{alignat}{2}\n& \\mathbb{V}(Y_t \\mid \\mathcal{I}_t) &{}={}& \\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t) \\\\\n\\equiv \\,& \\sigma^2_t&{}={}& \\delta_0 + \\sum_{l=1}^r \\delta_j \\sigma^2_{t-l} + \\sum_{m=1}^s \\gamma_k \\epsilon^2_{t-m}\n\\end{alignat}\n$$\n\nNote in particular the first equivalence $ \\mathbb{V}(Y_t \\mid \\mathcal{I}_t)= \\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t)$.\n\nAside\n: Based on this representation, you can write\n$$\n\\epsilon_t \\equiv \\sigma_t Z_t\n$$\nwhere $Z_t$ is a strong white noise process, but this follows from the way the process is defined.\n\nThe two models (for the conditional mean and the variance) are perfectly compatible with each other, in that the mean of the process can be modeled as ARMA, and the variances as GARCH. This leads to the complete specification of an ARMA(p,q)-GARCH(r,s) model for the  process as in the following representation\n$$\n\\begin{align}\nY_t &= \\alpha_0 + \\sum_{j=1}^p \\alpha_j Y_{t-j} + \\sum_{k=1}^q \\beta_k\\epsilon_{t-k} +\\epsilon_t\\\\\n \\mathbb{E}(\\epsilon_t\\mid \\mathcal{I}_t) &=0,\\, \\forall t \\\\\n\\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t) &= \\delta_0 + \\sum_{l=1}^r \\delta_l \\sigma^2_{t-l} + \\sum_{m=1}^s \\gamma_m \\epsilon^2_{t-m}\\, \\forall t\n\\end{align}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/47776/402101 \n The short version is that the Beta distribution can be understood as representing a distribution \nof probabilities\n, that is, it represents all the possible values of a probability when we don't know what that probability is. Here is my favorite intuitive explanation of this:\n\nAnyone who follows baseball is familiar with \nbatting averages\n\u2014simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it's just a percentage between \n0\n and \n1\n). \n.266\n is in general considered an average batting average, while \n.300\n is considered an excellent one.\n\n0\n\n1\n\n.266\n\n.300\n\nImagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly \n1.000\n, while if he strikes out, his batting average is \n0.000\n. It doesn't get much better if you go up to bat five or six times- you could get a lucky streak and get an average of \n1.000\n, or an unlucky streak and get an average of \n0\n, neither of which are a remotely good predictor of how you will bat that season.\n\n1.000\n\n0.000\n\n1.000\n\n0\n\nWhy is your batting average in the first few hits not a good predictor of your eventual batting average? When a player's first at-bat is a strikeout, why does no one predict that he'll never get a hit all season? Because we're going in with \nprior expectations.\n We know that in history, most batting averages over a season have hovered between something like \n.215\n and \n.360\n, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he'll end up a bit worse than average, but we know he probably won't deviate from that range.\n\n.215\n\n.360\n\nGiven our batting average problem, which can be represented with a \nbinomial distribution\n (a series of successes and failures), the best way to represent these prior expectations (what we in statistics just call a \nprior\n) is with the Beta distribution- it's saying, before we've seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is \n(0, 1)\n, just like a probability, so we already know we're on the right track, but the appropriateness of the Beta for this task goes far beyond that.\n\n(0, 1)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/47776/402101 \n (0, 1)\n\nWe expect that the player's season-long batting average will be most likely around \n.27\n, but that it could reasonably range from \n.21\n to \n.35\n. This can be represented with a Beta distribution with parameters \n$\\alpha=81$\n and \n$\\beta=219$\n:\n\n.27\n\n.21\n\n.35\n\ncurve(dbeta(x, 81, 219))\n\ncurve(dbeta(x, 81, 219))\n\n\n\nI came up with these parameters for two reasons:\n\nThe mean is \n$\\frac{\\alpha}{\\alpha+\\beta}=\\frac{81}{81+219}=.270$\n\n\nAs you can see in the plot, this distribution lies almost entirely within \n(.2, .35)\n- the reasonable range for a batting average.\n\n(.2, .35)\n\nYou asked what the x axis represents in a beta distribution density plot\u2014here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution \nof probabilities\n.\n\nBut here's why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now \n1 hit; 1 at bat\n. We have to then \nupdate\n our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (\nit's shown here\n), the result is \nvery simple\n. The new Beta distribution will be:\n\n1 hit; 1 at bat\n\n$\\mbox{Beta}(\\alpha_0+\\mbox{hits}, \\beta_0+\\mbox{misses})$\n\nWhere \n$\\alpha_0$\n and \n$\\beta_0$\n are the parameters we started with- that is, 81 and 219. Thus, in this case, \n$\\alpha$\n  has increased by 1 (his one hit), while \n$\\beta$\n has not increased at all (no misses yet). That means our new distribution is \n$\\mbox{Beta}(81+1, 219)$\n, or:\n\ncurve(dbeta(x, 82, 219))\n\ncurve(dbeta(x, 82, 219))\n\n\n\nNotice that it has barely changed at all- the change is indeed invisible to the naked eye! (That's because one hit doesn't really mean anything).\n\nHowever, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be \n$\\mbox{Beta}(81+100, 219+200)$\n, or:\n\ncurve(dbeta(x, 81+100, 219+200))\n\ncurve(dbeta(x, 81+100, 219+200))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/47776/402101 \n However, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be \n$\\mbox{Beta}(81+100, 219+200)$\n, or:\n\ncurve(dbeta(x, 81+100, 219+200))\n\ncurve(dbeta(x, 81+100, 219+200))\n\n\n\nNotice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player's batting average is.\n\nOne of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is \n$\\frac{\\alpha}{\\alpha+\\beta}$\n. Thus, after 100 hits of 300 \nreal\n at-bats, the expected value of the new Beta distribution is \n$\\frac{81+100}{81+100+219+200}=.303$\n- notice that it is lower than the naive estimate of \n$\\frac{100}{100+200}=.333$\n, but higher than the estimate you started the season with (\n$\\frac{81}{81+219}=.270$\n). You might notice that this formula is equivalent to adding a \"head start\" to the number of hits and non-hits of a player- you're saying \"start him off in the season with 81 hits and 219 non hits on his record\").\n\nThus, the Beta distribution is best for representing a probabilistic distribution \nof probabilities\n: the case where we don't know what a probability is in advance, but we have some reasonable guesses.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/211595/402101 \n Two things to consider\n\nThe Gini is scale independent whereas the SD is in the original units\n\nSuppose we have a measure bounded above and below. SD takes on its maximum value if half measurements are at each bound whereas Gini takes on the maximum is one is at one bound and all the rest at the other.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32518/402101 \n The \nshortest\n answer: \nnever\n, unless you are \nsure\n that your linear approximation of the data generating process (linear regression model) either by some theoretical or any other reasons \nis forced to go through the origin\n. If not the other regression parameters will be biased even if intercept is statistically insignificant (strange but it is so, consult \nBrooks\n \nIntroductory Econometrics\n for instance). Finally, as I do often explain to my students, by leaving the intercept term you insure that the residual term is zero-mean.\n\nFor your two models case we need more context. It may happen that linear model is not suitable here. For example, you need to log transform first if the model is multiplicative. Having exponentially growing processes it may occasionally happen that $R^2$ for the model without the intercept is \"much\" higher.\n\nScreen the data, test the model with RESET test or any other linear specification test, this may help to see if my guess is true. And, building the models highest $R^2$ is one of the last statistical properties I do really concern about, but it is nice to present to the people who are not so well familiar with econometrics (there are many dirty tricks to make determination close to 1 :)).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/127044/402101 \n Logistic regression is emphatically \nnot\n a classification algorithm on its own. It is only a classification algorithm \nin combination with\n a decision rule that makes dichotomous the predicted probabilities of the outcome. Logistic regression \nis\n a regression model because it estimates the probability of class membership as a (transformation of a) multilinear function of the features.\n\nFrank Harrell\n has posted a number of answers on this website enumerating the pitfalls of regarding logistic regression as a classification algorithm. Among them:\n\nClassification is a decision\n. To make an optimal decision, you need to asses a utility function, which implies that you need to account for the uncertainty in the outcome, i.e. a probability.\n\n\nThe costs of misclassification are not uniform across all units.\n\n\nDon't use cutoffs.\n\n\nUse proper scoring rules.\n\n\nThe problem is actually risk estimation, not classification.\n\nIf I recall correctly, he once pointed me to his book on regression strategies for more elaboration on these (and more!) points, but I can't seem to find that particular post.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/58243/402101 \n A footnote in \nPankratz (1983)\n, on page 48, says:\n\nThe label \"moving average\" is technically incorrect since the MA\n  coefficients may be negative and may not sum to unity. This label is\n  used by convention.\n\nBox and Jenkins (1976)\n also says something similar. On page 10:\n\nThe name \"moving average\" is somewhat misleading because the weights\n  $1, -\\theta_{1}, -\\theta_{2}, \\ldots, -\\theta_{q}$, which multiply the\n  $a$'s, need not total unity nor need that be positive. However, this\n  nomenclature is in common use, and therefore we employ it.\n\nI hope this helps.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74569/402101 \n Let's consider a very simple model: \n$y = \\beta x + e$\n, with an L1 penalty on \n$\\hat{\\beta}$\n and a least-squares loss function on \n$\\hat{e}$\n.  We can expand the expression to be minimized as:\n\n$\\min y^Ty -2 y^Tx\\hat{\\beta} + \\hat{\\beta} x^Tx\\hat{\\beta} + 2\\lambda|\\hat{\\beta}|$\n\nKeep in mind this is a univariate example, with \n$\\beta$\n and \n$x$\n being scalars, to show how LASSO can send a coefficient to zero. This can be generalized to the multivariate case.\n\nLet us assume the least-squares solution is some \n$\\hat{\\beta} > 0$\n, which is equivalent to assuming that \n$y^Tx > 0$\n, and see what happens when we add the L1 penalty.  With \n$\\hat{\\beta}>0$\n, \n$|\\hat{\\beta}| = \\hat{\\beta}$\n, so the penalty term is equal to \n$2\\lambda\\beta$\n.  The derivative of the objective function w.r.t. \n$\\hat{\\beta}$\n is:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} + 2\\lambda$\n\nwhich evidently has solution \n$\\hat{\\beta} = (y^Tx - \\lambda)/(x^Tx)$\n.\n\nObviously by increasing \n$\\lambda$\n we can drive \n$\\hat{\\beta}$\n to zero (at \n$\\lambda = y^Tx$\n).   However, once \n$\\hat{\\beta} = 0$\n, increasing \n$\\lambda$\n won't drive it negative, because, writing loosely, the instant \n$\\hat{\\beta}$\n becomes negative, the derivative of the objective function changes to:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} - 2\\lambda$\n\nwhere the flip in the sign of \n$\\lambda$\n is due to the absolute value nature of the penalty term; when \n$\\beta$\n becomes negative, the penalty term becomes equal to \n$-2\\lambda\\beta$\n, and taking the derivative w.r.t. \n$\\beta$\n results in \n$-2\\lambda$\n.  This leads to the solution \n$\\hat{\\beta} = (y^Tx + \\lambda)/(x^Tx)$\n, which is obviously inconsistent with \n$\\hat{\\beta} < 0$\n (given that the least squares solution \n$> 0$\n, which implies \n$y^Tx > 0$\n, and \n$\\lambda > 0$\n).  There is an increase in the L1 penalty AND an increase in the squared error term (as we are moving farther from the least squares solution) when moving \n$\\hat{\\beta}$\n from \n$0$\n to \n$ < 0$\n, so we don't, we just stick at \n$\\hat{\\beta}=0$\n.\n\nIt should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with \n$\\hat{\\beta} < 0$\n.\n\nWith the least squares penalty \n$\\lambda\\hat{\\beta}^2$\n, however, the derivative becomes:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} + 2\\lambda\\hat{\\beta}$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/74569/402101 \n It should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with \n$\\hat{\\beta} < 0$\n.\n\nWith the least squares penalty \n$\\lambda\\hat{\\beta}^2$\n, however, the derivative becomes:\n\n$-2y^Tx +2x^Tx\\hat{\\beta} + 2\\lambda\\hat{\\beta}$\n\nwhich evidently has solution \n$\\hat{\\beta} = y^Tx/(x^Tx + \\lambda)$\n.  Obviously no increase in \n$\\lambda$\n will drive this all the way to zero.  So the L2 penalty can't act as a variable selection tool without some mild ad-hockery such as \"set the parameter estimate equal to zero if it is less than \n$\\epsilon$\n\".\n\nObviously things can change when you move to multivariate models, for example, moving one parameter estimate around might force another one to change sign, but the general principle is the same: the L2 penalty function can't get you all the way to zero, because, writing very heuristically, it in effect adds to the \"denominator\" of the expression for \n$\\hat{\\beta}$\n, but the L1 penalty function can, because it in effect adds to the \"numerator\".",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/49127/402101 \n This figure shows the distributions of the means of $n=1$ (blue), $10$ (red), and $100$ (gold) independent and identically distributed (\niid\n) normal distributions (of unit variance and mean $\\mu$):\n\n\n\nAs $n$ increases, the distribution of the mean becomes more \"focused\" on $\\mu$.  (The sense of \"focusing\" is easily quantified: given any fixed open interval $(a,b)$ surrounding $\\mu$, the amount of the distribution within $[a,b]$ increases with $n$ and has a limiting value of $1$.)\n\nHowever, \nwhen we standardize\n these distributions, we rescale each of them to have a mean of $0$ and a unit variance: they are all the same then.  This is how we see that although the PDFs of the means themselves are spiking upwards and focusing around $\\mu$, nevertheless every one of these distributions is still has a Normal \nshape,\n even though they differ individually.\n\nThe Central Limit Theorem says that when you start with \nany\n distribution--not just a normal distribution--that has a finite variance, and play the same game with means of $n$ iid values as $n$ increases, you see the same thing: the mean distributions focus around the original mean (the Weak Law of Large Numbers), but the \nstandardized\n mean distributions converge to a standard Normal distribution (the Central Limit Theorem).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/496319/402101 \n This is all philosophical (as it must be):\n\nWhen a probability is a \nfrequency\n you will have difficulty applying it to something that can't happen more than once, like Thursday's match between Liverpool and Arsenal. The problem is that a frequency is of the form \n$k$\n out of every \n$n$\n. You could conceive of a class of Thursdays sufficiently similar to this one (weather, &c.) where Liverpool plays Arsenal in a sufficiently similar way (team compositions, &c.) that you would consider them (almost) repetitions of the same event and so have an idea of how this would go \non average\n, though finding the data might be difficult. But never for a given match, which can only happen this once. Similarly, you can't assign a probability to \nthe next\n coin toss, because there is only one, but you can say something about the class of sufficiently similar coin tosses.\n\nIf a probability is a \ndegree of belief\n, you can assign it to anything that you can have an opinion on if you follow the proper rules of such assignments. It is my belief that the next coin toss is about as probable to come up heads as it is to come up tails.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214315/402101 \n The i.i.d. assumption about the pairs \n$(\\mathbf{X}_i, y_i)$\n, \n$i = 1, \\ldots, N$\n, is often made in statistics and in machine learning. Sometimes for a good reason, sometimes out of convenience and sometimes just because we usually make this assumption. To satisfactorily answer if the assumption is really necessary, and what the consequences are of not making this assumption, I would easily end up writing a book (if you ever easily end up doing something like that). Here I will try to give a brief overview of what I find to be the most important aspects.\n\nLet's assume that we want to learn a probability model of \n$y$\n given \n$\\mathbf{X}$\n, which we call \n$p(y \\mid \\mathbf{X})$\n. We do not make any assumptions about this model a priori, but we will make the minimal assumption that such a model exists such that\n\nthe conditional distribution of \n$y_i$\n given \n$\\mathbf{X}_i$\n is \n$p(y_i \\mid \\mathbf{X}_i)$\n.\n\nWhat is worth noting about this assumption is that the conditional distribution of \n$y_i$\n depends on \n$i$\n only through \n$\\mathbf{X}_i$\n. This is what makes the model useful, e.g. for prediction. The assumption holds as a consequence of the \nidentically distributed\n part under the i.i.d. assumption, but it is weaker because we don't make any assumptions about the \n$\\mathbf{X}_i$\n's.\n\nIn the following the focus will mostly be on the role of independence.\n\nThere are two major approaches to learning a model of \n$y$\n given \n$\\mathbf{X}$\n. One approach is known as \ndiscriminative\n modelling and the other as \ngenerative\n modelling.\n\nDiscriminative modelling\n: We model \n$p(y \\mid \\mathbf{X})$\n directly, e.g. a logistic regression model, a neural network, a tree or a random forest. The \nworking modelling assumption\n will typically be that the \n$y_i$\n's are conditionally independent given the \n$\\mathbf{X}_i$\n's, though estimation techniques relying on subsampling or bootstrapping make most sense under the i.i.d. or the weaker exchangeability assumption (see below). But generally, for discriminative modelling we don't need to make distributional assumptions about the \n$\\mathbf{X}_i$\n's.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214315/402101 \n Generative modelling\n: We model the joint distribution, \n$p(\\mathbf{X}, y)$\n, of \n$(\\mathbf{X}, y)$\n typically by modelling the conditional distribution \n$p(\\mathbf{X} \\mid y)$\n and the marginal distribution \n$p(y)$\n. Then we use Bayes's formula for computing \n$p(y \\mid \\mathbf{X})$\n. Linear discriminant analysis and naive Bayes methods are examples. The \nworking modelling assumption\n will typically be the i.i.d. assumption.\n\nFor both modelling approaches the working modelling assumption is used to derive or propose learning methods (or estimators). That could be by maximising the (penalised) log-likelihood, minimising the empirical risk or by using Bayesian methods. Even if the working modelling assumption is wrong, the resulting method can still provide a sensible fit of \n$p(y \\mid \\mathbf{X})$\n.\n\nSome techniques used together with discriminative modelling, such as bagging (bootstrap aggregation), work by fitting many models to data sampled randomly from the dataset. Without the i.i.d. assumption (or exchangeability) the resampled datasets will not have a joint distribution similar to that of the original dataset. Any dependence structure has become \"messed up\" by the resampling. I have not thought deeply about this, but I don't see why that should necessarily break the method as a method for learning \n$p(y \\mid \\mathbf{X})$\n. At least not for methods based on the working independence assumptions. I am happy to be proved wrong here.\n\nA central question for all learning methods is whether they result in models close to \n$p(y \\mid \\mathbf{X})$\n. There is a vast theoretical literature in statistics and machine learning dealing with consistency and error bounds. A main goal of this literature is to prove that the learned model is close to \n$p(y \\mid \\mathbf{X})$\n when \n$N$\n is large. Consistency is a qualitative assurance, while error bounds provide (semi-) explicit quantitative control of the closeness and give rates of convergence.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214315/402101 \n The theoretical results all rely on assumptions about the joint distribution of the observations in the dataset. Often the working modelling assumptions mentioned above are made (that is, conditional independence for discriminative modelling and i.i.d. for generative modelling). For discriminative modelling, consistency and error bounds will require that the \n$\\mathbf{X}_i$\n's fulfil certain conditions. In classical regression one such condition is that \n$\\frac{1}{N} \\mathbb{X}^T \\mathbb{X} \\to \\Sigma$\n for \n$N \\to \\infty$\n, where \n$\\mathbb{X}$\n denotes the design matrix with rows \n$\\mathbf{X}_i^T$\n. Weaker conditions may be enough for consistency. In sparse learning another such condition is the restricted eigenvalue condition, see e.g. \nOn the conditions used to prove oracle results for the Lasso\n. The i.i.d. assumption together with some technical distributional assumptions imply that some such sufficient conditions are fulfilled with large probability, and thus the i.i.d. assumption may prove to be a sufficient but not a necessary assumption to get consistency and error bounds for discriminative modelling.\n\nThe working modelling assumption of independence may be wrong for either of the modelling approaches. As a rough rule-of-thumb one can still expect consistency if the data comes from an \nergodic process\n, and one can still expect some error bounds if the process is \nsufficiently fast mixing\n. A precise mathematical definition of these concepts would take us too far away from the main question. It is enough to note that there exist dependence structures besides the i.i.d. assumption for which the learning methods can be proved to work as \n$N$\n tends to infinity.\n\nIf we have more detailed knowledge about the dependence structure, we may choose to replace the working independence assumption used for modelling with a model that captures the dependence structure as well. This is often done for time series. A better working model may result in a more efficient method.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/214315/402101 \n If we have more detailed knowledge about the dependence structure, we may choose to replace the working independence assumption used for modelling with a model that captures the dependence structure as well. This is often done for time series. A better working model may result in a more efficient method.\n\nRather than proving that the learning method gives a model close to \n$p(y \\mid \\mathbf{X})$\n it is of great practical value to obtain a (relative) assessment of \"how good a learned model is\". Such assessment scores are comparable for two or more learned models, but they will not provide an absolute assessment of how close a learned model is to \n$p(y \\mid \\mathbf{X})$\n. Estimates of assessment scores are typically computed empirically based on splitting the dataset into a training and a test dataset or by using cross-validation.\n\nAs with bagging, a random splitting of the dataset will \"mess up\" any dependence structure. However, for methods based on the working independence assumptions, ergodicity assumptions weaker than i.i.d. should be sufficient for the assessment estimates to be reasonable, though standard errors on these estimates will be very difficult to come up with.\n\n[\nEdit:\n Dependence among the variables will result in a distribution of the learned model that differs from the distribution under the i.i.d. assumption. The estimate produced by cross-validation is not obviously related to the generalization error. If the dependence is strong, it will most likely be a poor estimate.]\n\nAll the above is under the assumption that there is a fixed conditional probability model, \n$p(y \\mid \\mathbf{X})$\n. Thus there cannot be trends or sudden changes in the conditional distribution not captured by \n$\\mathbf{X}$\n.\n\nWhen learning a model of \n$y$\n given \n$\\mathbf{X}$\n, independence plays a role as\n\na useful working modelling assumption that allows us to derive learning methods\n\n\na sufficient but not necessary assumption for proving consistency and providing error bounds\n\n\na sufficient but not necessary assumption for using random data splitting techniques such as bagging for learning and cross-validation for assessment.\n\nTo understand precisely what alternatives to i.i.d. that are also sufficient is non-trivial and to some extent a research subject.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/35895/402101 \n ...the relationship is nonlinear yet there is a clear relation between x and y, how can I test the association and label its nature?\n\nOne way of doing this would be to fit $y$ as a semi-parametrically estimated function of $x$ using, for example, a \ngeneralized additive model\n and testing whether or not that functional estimate is constant, which would indicate no relationship between $y$ and $x$. This approach frees you from having to do polynomial regression and making sometimes arbitrary decisions about the order of the polynomial, etc.\n\nSpecifically, if you have observations, $(Y_i, X_i)$, you could fit the model:\n\n$$ E(Y_i | X_i) = \\alpha + f(X_i) + \\varepsilon_i $$\n\nand test the hypothesis $H_{0} : f(x) = 0, \\ \\forall x$. In \nR\n, you can do this using the \ngam()\n function. If \ny\n is your outcome and \nx\n is your predictor, you could type:\n\nR\n\ngam()\n\ny\n\nx\n\nlibrary(mgcv) \ng <- gam(y ~ s(x))\n\nlibrary(mgcv) \ng <- gam(y ~ s(x))\n\nTyping \nsummary(g)\n will give you the result of the hypothesis test above. As far as characterizing the nature of the relationship, this would be best done with a plot. One way to do this in \nR\n (assuming the code above has already been entered)\n\nsummary(g)\n\nR\n\nplot(g,scheme=2)\n\nplot(g,scheme=2)\n\nIf your response variable is discrete (e.g. binary), you can accommodate that within this framework by fitting a logistic GAM (in \nR\n, you'd add \nfamily=binomial\n to your call to \ngam\n). Also, if you have multiple predictors, you can include multiple additive terms (or ordinary linear terms), or fit multivariable functions, e.g. $f(x,z)$ if you had predictors \nx, z\n. The complexity of the relationship is automatically selected by cross validation if you use the default methods, although there is a lot of flexibility here - see the \ngam\n help file\n if interested.\n\nR\n\nfamily=binomial\n\ngam\n\nx, z\n\ngam",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/196580/402101 \n Trying to complement the other answers... What kind of information is Fisher information?  Start with the loglikelihood function\n\n$$\n   \\ell (\\theta) = \\log f(x;\\theta)\n$$\n\nas a function of \n$\\theta$\n for \n$\\theta \\in \\Theta$\n, the parameter space.\nAssuming some regularity conditions we do not discuss here, we have\n\n$\\DeclareMathOperator{\\E}{\\mathbb{E}}  \\E \\frac{\\partial}{\\partial \\theta} \\ell (\\theta) = \\E_\\theta \\dot{\\ell}(\\theta) = 0$\n (we will write derivatives with respect to the parameter as dots as here).  The variance is the Fisher information\n\n$$\n    I(\\theta) = \\E_\\theta ( \\dot{\\ell}(\\theta) )^2= -\\E_\\theta \\ddot{\\ell}(\\theta)\n$$\n\nthe last formula showing that it is the (negative) curvature of the loglikelihood function.  One often finds the maximum likelihood estimator (mle) of \n$\\theta$\n by solving the likelihood equation \n$\\dot{\\ell}(\\theta)=0$\n when the Fisher information as the variance of the score \n$\\dot{\\ell}(\\theta)$\n is large, then the solution to that equation will be very sensitive to the data, giving a hope for high precision of the mle.  That is confirmed at least asymptotically, the asymptotic variance of the mle being the inverse of Fisher information.\n\nHow can we interpret this?   \n$\\ell(\\theta)$\n is the likelihood information about the parameter \n$\\theta$\n from the sample. This can really only be interpreted in a relative sense, like when we use it to compare the plausibilities of two distinct possible parameter values via the likelihood ratio test \n$\\ell(\\theta_0) - \\ell(\\theta_1)$\n.  The rate of change of the loglikelihood is the score function \n$\\dot{\\ell}(\\theta)$\n tells us how fast the likelihood changes, and its variance \n$I(\\theta)$\n how much this varies from sample to sample, at a given parameter value, say \n$\\theta_0$\n.  The equation (which is really surprising!)\n\n$$\n    I(\\theta) = - \\E_\\theta \\ddot{\\ell}(\\theta)\n$$\n\ntells us there is a relationship (equality) between the variability in the information (likelihood) for a given parameter value, \n$\\theta_0$\n, and the curvature of the likelihood function for that parameter value.  This is a surprising relationship between the variability (variance) of ths statistic \n$\\dot{\\ell}(\\theta) \\mid_{\\theta=\\theta_0}$\n and the expected change in likelihood when we vary the parameter \n$\\theta$\n in some interval around \n$\\theta_0$\n (for the same data).   This is really both strange, surprising and powerful!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/196580/402101 \n So what is the likelihood function?  We usually think of the statistical model \n$\\{ f(x;\\theta), \\theta \\in \\Theta \\} $\n as a family of probability distributions for data \n$x$\n, indexed by the parameter \n$\\theta$\n some element in the parameter space \n$\\Theta$\n. We think of this model as being true if there exists some value \n$\\theta_0 \\in \\Theta$\n such that the data \n$x$\n actually have the probability distribution \n$f(x;\\theta_0)$\n. So we get a statistical model by imbedding the true data-generating probability distribution \n$f(x;\\theta_0)$\n in a family of probability distributions. But, it is clear that such an imbedding can be done in many different ways, and each such imbedding will be a \"true\" model, and they will give different likelihood functions. And, without such an imbedding, there is no likelihood function. It seems that we really do need some help, some principles for how to choose an imbedding wisely!\n\nSo, what does this mean? It means that the choice of likelihood function tells us how we would expect the data to change, if the truth changed a little bit. But, this cannot really be verified by the data, as the data only gives information about the true model function \n$f(x;\\theta_0)$\n which actually generated the data, and not nothing about all the other elements in the choosen model. This way we see that choice of the likelihood function is similar to choice of a prior in Bayesian analysis, it injects non-data information into the analysis. Let us look at this in a simple (somewhat artificial) example, and look at the effect of imbedding \n$f(x;\\theta_0)$\n in a model in different ways.\n\nLet us assume that \n$X_1, \\dotsc, X_n$\n are iid as \n$N(\\mu=10, \\sigma^2=1)$\n. So, that is the true, data-generating distribution. Now, let us embed this in a model in two different ways, model A and model B.\n\n$$\nA \\colon X_1, \\dotsc, X_n ~\\text{iid}~N(\\mu, \\sigma^2=1),\\mu \\in \\mathbb{R} \\\\\nB \\colon X_1, \\dotsc, X_n ~\\text{iid}~N(\\mu, \\mu/10), \\mu>0\n$$\n\nyou can check that this coincides for \n$\\mu=10$\n.\n\nThe loglikelihood functions become\n\n$$\n\\ell_A(\\mu) = -\\frac{n}{2} \\log (2\\pi) -\\frac12\\sum_i (x_i-\\mu)^2 \\\\\n\\ell_B(\\mu) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2}\\log(\\mu/10) - \\frac{10}{2}\\sum_i \\frac{(x_i-\\mu)^2}{\\mu}\n$$\n\nThe score functions: (loglikelihood derivatives):\n\n$$\n\\dot{\\ell}_A(\\mu) = n (\\bar{x}-\\mu)       \\\\\n\\dot{\\ell}_B(\\mu) = -\\frac{n}{2\\mu}- \\frac{10}{2}\\sum_i \\left(\\frac{x_i}{\\mu}\\right)^2 - \n15 n\n$$\n\nand the curvatures",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/196580/402101 \n you can check that this coincides for \n$\\mu=10$\n.\n\nThe loglikelihood functions become\n\n$$\n\\ell_A(\\mu) = -\\frac{n}{2} \\log (2\\pi) -\\frac12\\sum_i (x_i-\\mu)^2 \\\\\n\\ell_B(\\mu) = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2}\\log(\\mu/10) - \\frac{10}{2}\\sum_i \\frac{(x_i-\\mu)^2}{\\mu}\n$$\n\nThe score functions: (loglikelihood derivatives):\n\n$$\n\\dot{\\ell}_A(\\mu) = n (\\bar{x}-\\mu)       \\\\\n\\dot{\\ell}_B(\\mu) = -\\frac{n}{2\\mu}- \\frac{10}{2}\\sum_i \\left(\\frac{x_i}{\\mu}\\right)^2 - \n15 n\n$$\n\nand the curvatures\n\n$$\n   \\ddot{\\ell}_A(\\mu) = -n   \\\\\n   \\ddot{\\ell}_B(\\mu) =  \\frac{n}{2\\mu^2} + \\frac{10}{2}\\sum_i \\frac{2 x_i^2}{\\mu^3}\n$$\n\nso, the Fisher information do really depend on the imbedding. Now, we calculate the Fisher information at the true value \n$\\mu=10$\n,\n\n$$\n  I_A(\\mu=10) = n, \\\\\n  I_B(\\mu=10) = n \\cdot \\left(\\frac1{200}+\\frac{2020}{2000}\\right) > n\n$$\n\nso the Fisher information about the parameter is somewhat larger in model B.\n\nThis illustrates that, in some sense, the Fisher information tells us how fast the information from the data about the parameter \nwould have changed\n if the governing parameter changed \nin the way postulated by the imbedding in a model family\n.  The explanation of higher information in model B is that our model family B postulates \nthat if the expectation would have increased, then the variance too would have increased\n.  So that, under model B, the sample variance will also carry information about \n$\\mu$\n, which it will not do under model A.\n\nAlso, this example illustrates that we really do need some theory for helping us in how to construct model families.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/222128/402101 \n Here is an example with 3 variables, \n$y$\n, \n$x_1$\n and \n$x_2$\n, related by the equation\n\n$$ y = x_1 + x_2 + \\varepsilon $$\n\nwhere \n$\\varepsilon \\sim N(0,1)$\n\nThe particular data are\n\ny x1 x2\n1 4.520866  1  2\n2 6.849811  2  4\n3 6.539804  3  6\n\ny x1 x2\n1 4.520866  1  2\n2 6.849811  2  4\n3 6.539804  3  6\n\nSo it is evident that \n$x_2$\n is a multiple of \n$x_1$\n hence we have perfect collinearity.\n\nWe can write the model as\n\n$$ Y = X \\beta + \\varepsilon$$\n\nwhere:\n\n$$ Y = \\begin{bmatrix}4.52 \\\\6.85 \\\\6.54\\end{bmatrix}$$\n\n$$ X = \\begin{bmatrix}1 & 1 & 2\\\\1 & 2 & 4 \\\\1 & 3 & 6\\end{bmatrix}$$\n\nSo we have\n\n$$ XX' = \\begin{bmatrix}1 & 1 & 2\\\\1 & 2 & 4 \\\\1 & 3 & 6\\end{bmatrix}\n\\begin{bmatrix}1 & 1 & 1\\\\1 & 2 & 3 \\\\2 & 4 & 6\\end{bmatrix}\n= \\begin{bmatrix}6 & 11 & 16\\\\11 & 21 & 31 \\\\16 & 31 & 46\\end{bmatrix}\n$$\n\nNow we calculate the determinant of \n$XX'$\n :\n\n$$ \\det XX' = 6\\begin{vmatrix}21 & 31 \\\\31 & 46\\end{vmatrix} - 11 \\begin{vmatrix}11 & 31 \\\\16 & 46\\end{vmatrix} + 16\\begin{vmatrix}11 & 21 \\\\16 & 31\\end{vmatrix}= 0$$\n\nIn R we can show this as follows:\n\n> x1 <- c(1,2,3)\n\n> x1 <- c(1,2,3)\n\ncreate \nx2\n, a multiple of \nx1\n\nx2\n\nx1\n\n> x2 <- x1*2\n\n> x2 <- x1*2\n\ncreate y, a linear combination of \nx1\n, \nx2\n and some randomness\n\nx1\n\nx2\n\n> y <- x1 + x2 + rnorm(3,0,1)\n\n> y <- x1 + x2 + rnorm(3,0,1)\n\nobserve that\n\n> summary(m0 <- lm(y~x1+x2))\n\n> summary(m0 <- lm(y~x1+x2))\n\nfails to estimate a value for the \nx2\n coefficient:\n\nx2\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   3.9512     1.6457   2.401    0.251\nx1            1.0095     0.7618   1.325    0.412\nx2                NA         NA      NA       NA\n\nResidual standard error: 0.02583 on 1 degrees of freedom\nMultiple R-squared:      1,     Adjusted R-squared:  0.9999 \nF-statistic: 2.981e+04 on 1 and 1 DF,  p-value: 0.003687\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   3.9512     1.6457   2.401    0.251\nx1            1.0095     0.7618   1.325    0.412\nx2                NA         NA      NA       NA\n\nResidual standard error: 0.02583 on 1 degrees of freedom\nMultiple R-squared:      1,     Adjusted R-squared:  0.9999 \nF-statistic: 2.981e+04 on 1 and 1 DF,  p-value: 0.003687\n\nThe model matrix \n$X$\n is:\n\n> (X <- model.matrix(m0))\n\n(Intercept) x1 x2\n1           1  1  2\n2           1  2  4\n3           1  3  6\n\n> (X <- model.matrix(m0))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/222128/402101 \n Residual standard error: 0.02583 on 1 degrees of freedom\nMultiple R-squared:      1,     Adjusted R-squared:  0.9999 \nF-statistic: 2.981e+04 on 1 and 1 DF,  p-value: 0.003687\n\nThe model matrix \n$X$\n is:\n\n> (X <- model.matrix(m0))\n\n(Intercept) x1 x2\n1           1  1  2\n2           1  2  4\n3           1  3  6\n\n> (X <- model.matrix(m0))\n\n(Intercept) x1 x2\n1           1  1  2\n2           1  2  4\n3           1  3  6\n\nSo \n$XX'$\n is\n\n> (XXdash <- X %*% t(X))\n   1  2  3\n1  6 11 16\n2 11 21 31\n3 16 31 46\n\n> (XXdash <- X %*% t(X))\n   1  2  3\n1  6 11 16\n2 11 21 31\n3 16 31 46\n\nwhich is not invertible, as shown by\n\n> solve(XXdash)\nError in solve.default(XXdash) : \n  Lapack routine dgesv: system is exactly singular: U[3,3] = 0\n\n> solve(XXdash)\nError in solve.default(XXdash) : \n  Lapack routine dgesv: system is exactly singular: U[3,3] = 0\n\nOr:\n\n> det(XXdash)\n[1] 0\n\n> det(XXdash)\n[1] 0",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27310/402101 \n The basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (\nloadings\n). You may recall that PCA seeks to replace $p$ (more or less correlated) variables by $k<p$ uncorrelated linear combinations (projections) of the original variables. Let us ignore how to choose an optimal $k$ for the problem at hand. Those $k$ \nprincipal components\n are ranked by importance through their explained variance, and each variable contributes with varying degree to each component. Using the largest variance criteria would be akin to \nfeature extraction\n, where principal component are used as new features, instead of the original variables. However, we can decide to keep only the first component and select the $j<p$ variables that have the highest absolute coefficient; the number $j$ might be based on the proportion of the number of variables (e.g., keep only the top 10% of the $p$ variables), or a fixed cutoff (e.g., considering a threshold on the normalized coefficients). This approach bears some resemblance with the \nLasso\n operator in penalized regression (or \nPLS\n regression). Neither the value of $j$, nor the number of components to retain are obvious choices, though.\n\nThe problem with using PCA is that (1) measurements from all of the original variables are used in the projection to the lower dimensional space, (2) only linear relationships are considered, and (3) PCA or SVD-based methods, as well as univariate screening methods (t-test, correlation, etc.), do not take into account the potential multivariate nature of the data structure (e.g., higher order interaction between variables).\n\nAbout point 1, some more elaborate screening methods have been proposed, for example \nprincipal feature analysis\n or stepwise method, like the one used for '\ngene shaving\n' in gene expression studies. Also, \nsparse PCA\n might be used to perform dimension reduction and variable selection based on the resulting variable loadings. About point 2, it is possible to use kernel PCA (using the \nkernel trick\n) if one needs to embed nonlinear relationships into a lower dimensional space. \nDecision trees\n, or better the \nrandom forest\n algorithm, are probably better able to solve Point 3. The latter allows to derive Gini- or permutation-based measures of \nvariable importance\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/27310/402101 \n A last point: If you intend to perform feature selection before applying a classification or regression model, be sure to cross-validate the whole process (see \u00a77.10.2 of the \nElements of Statistical Learning\n, or \nAmbroise and McLachlan, 2002\n).\n\nAs you seem to be interested in R solution, I would recommend taking a look at the \ncaret\n package which includes a lot of handy functions for data preprocessing and variable selection in a classification or regression context.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18523/402101 \n You're going to have a little bit of trouble modeling a series with 2 levels of seasonality using an ARIMA model.  Getting this right is going highly dependent on setting things up correctly.  Have you considered a simple linear model yet?  They're a lot faster and easier to fit than ARIMA models, and if you use dummy variables for your different seasonality levels they are often quite accurate.\n\nI'm assuming you have hourly data, so make sure your TS object is setup with a frequency of 24.\n\n\nYou can model other levels of seasonality using dummy variables.  For example, you might want a set of 0/1 dummies representing the month of the year.\n\n\nInclude the dummy variables in the \nxreg\n argument, along with any covariates (like temperature).\n\n\nFit the model with the arima function in base R.  This function can handle ARMAX models through the use of the \nxreg\n argument.\n\n\nTry the \nArima\n and \nauto.arima\n functions in the forecast package.  auto.arima is nice because it will automatically find good parameters for your arima model. However, it will take FOREVER to fit on your dataset.\n\n\nTry the tslm function in the arima package, using dummy variables for each level of seasonality.  This will fit a lot faster than the Arima model, and may even work better in your situation.\n\n\nIf 4/5/6 don't work, THEN start worrying about transfer functions.  You have to crawl before you can walk.\n\n\nIf you are planning to forecast into the future, you will first need to forecast your xreg variables.  This is easy for seasonal dummies, but you'll have to think about how to make a good weather forecasts.  Maybe use the median of historical data?\n\nxreg\n\nxreg\n\nHere is an example of how I would approach this:\n\n#Setup a fake time series\nset.seed(1)\nlibrary(lubridate)\nindex <- ISOdatetime(2010,1,1,0,0,0)+1:8759*60*60\nmonth <- month(index)\nhour <- hour(index)\nusage <- 1000+10*rnorm(length(index))-25*(month-6)^2-(hour-12)^2\nusage <- ts(usage,frequency=24)\n\n#Create monthly dummies.  Add other xvars to this matrix\nxreg <- model.matrix(~as.factor(month))[,2:12]\ncolnames(xreg) <- c('Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')\n\n#Fit a model\nlibrary(forecast)\nmodel <- Arima(usage, order=c(0,0,0), seasonal=list(order=c(1,0,0), period=24), xreg=xreg)\nplot(usage)\nlines(fitted(model),col=2)\n\n#Benchmark against other models\nmodel2 <- tslm(usage~as.factor(month)+as.factor(hour))\nmodel3 <- tslm(usage~as.factor(month))\nmodel4 <- rep(mean(usage),length(usage))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18523/402101 \n #Fit a model\nlibrary(forecast)\nmodel <- Arima(usage, order=c(0,0,0), seasonal=list(order=c(1,0,0), period=24), xreg=xreg)\nplot(usage)\nlines(fitted(model),col=2)\n\n#Benchmark against other models\nmodel2 <- tslm(usage~as.factor(month)+as.factor(hour))\nmodel3 <- tslm(usage~as.factor(month))\nmodel4 <- rep(mean(usage),length(usage))\n\n#Compare the 4 models\nlibrary(plyr) #for rbind.fill\nACC <- rbind.fill(  data.frame(t(accuracy(model))),\n                    data.frame(t(accuracy(model2))),\n                    data.frame(t(accuracy(model3))),\n                    data.frame(t(accuracy(model4,usage)))\n                )\nACC <- round(ACC,2)\nACC <- cbind(Type=c('Arima','LM1','Monthly Mean','Mean'),ACC)\nACC[order(ACC$MAE),]\n\n#Setup a fake time series\nset.seed(1)\nlibrary(lubridate)\nindex <- ISOdatetime(2010,1,1,0,0,0)+1:8759*60*60\nmonth <- month(index)\nhour <- hour(index)\nusage <- 1000+10*rnorm(length(index))-25*(month-6)^2-(hour-12)^2\nusage <- ts(usage,frequency=24)\n\n#Create monthly dummies.  Add other xvars to this matrix\nxreg <- model.matrix(~as.factor(month))[,2:12]\ncolnames(xreg) <- c('Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')\n\n#Fit a model\nlibrary(forecast)\nmodel <- Arima(usage, order=c(0,0,0), seasonal=list(order=c(1,0,0), period=24), xreg=xreg)\nplot(usage)\nlines(fitted(model),col=2)\n\n#Benchmark against other models\nmodel2 <- tslm(usage~as.factor(month)+as.factor(hour))\nmodel3 <- tslm(usage~as.factor(month))\nmodel4 <- rep(mean(usage),length(usage))\n\n#Compare the 4 models\nlibrary(plyr) #for rbind.fill\nACC <- rbind.fill(  data.frame(t(accuracy(model))),\n                    data.frame(t(accuracy(model2))),\n                    data.frame(t(accuracy(model3))),\n                    data.frame(t(accuracy(model4,usage)))\n                )\nACC <- round(ACC,2)\nACC <- cbind(Type=c('Arima','LM1','Monthly Mean','Mean'),ACC)\nACC[order(ACC$MAE),]",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/63867/402101 \n A concise introduction is \nT. Schmidt 2008 - Copulas and dependent measurement\n.\nAlso noteworthy is \nEmbrechts 2009 - Copulas - A personal view\n.\n\nFor Schmidt I could not provide a better summary than the section titles. It provides basic definitions, intuition and examples. Discussion of sampling is bare-bone, and a brief literature review covers the must-have. As for Embrechts apart from the obligatory definitions, properties and examples the discussion is interesting since it touches drawbacks and some critical remarks made to copula modeling over the years. The bibliography is here more extensive and covers most works that one shall read",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/54894/402101 \n A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as \"state\", and then the random variable is a function of the state.\n\nExample:\n\nSuppose we have three dice rolls ($D_{1}$,$D_{2}$,$D_{3}$).  Then the state $S=(D_{1},D_{2},D_{3})$.\n\nOne random variable $X$ is the number of 5s. This is:\n\n$$ X=(D_{1}=5?)+(D_{2}=5?)+(D_{3}=5?)$$\n\nAnother random variable $Y$ is the sum of the dice rolls. This is:\n\n$$ Y=D_{1}+D_{2}+D_{3}  $$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2536/402101 \n Correlation is not sufficient for causation. One can get around the Wikipedia example by imagining that those twins always cheated in their tests by having a device that gives them the answers. The twin that goes to the amusement park loses the device, hence the low grade.\n\nA good way to get this stuff straight is to think of the structure of Bayesian network that may be generating the measured quantities, as done by Pearl in his book \nCausality\n. His basic point is to look for hidden variables. If there is a hidden variable that happens not to vary in the measured sample, then the correlation would not imply causation. Expose all hidden variables and you have causation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/202827/402101 \n I have argued that variable importance is a \nslippery concept\n, as this question posits. The tautological first type of response that you get to your question and the unrealistic hopes of those who would interpret variable-importance results in terms of causality, as noted by @DexGroves, need little elaboration.\n\nIn fairness to those who would use backward selection, however, even Frank Harrell allows for it as part of a modeling strategy. From page 97 of his \nRegression Modeling Strategies\n, 2nd edition (a similar statement is on page 131 of the associated \ncourse notes\n):\n\nDo limited backwards step-down variable selection if parsimony is more\n  important than accuracy.\n\nThis limited potential use of backward selection, however, is step 13, the last step before the final model (step 14). It comes well after the crucial first steps:\n\nAssemble as much accurate pertinent data as possible, with wide distributions for predictor values...\n\n\nFormulate good hypotheses that lead to specification of relevant candidate predictors and possible interactions...\n\nIn my experience people often want to bypass step 2, and let some automated procedure replace intelligent application of subject-matter knowledge. This may lead to some of the emphasis placed on variable importance.\n\nThe full model of Harrell's step 14 is followed by 5 further steps of validation and adjustment, with a last step:\n\nDevelop simplifications to the full model by approximating it to any desired degrees of accuracy.\n\nAs other answers have noted, there are issues of actionability, cost, and simplicity that enter into the practical application of modeling results. For example, if I develop a new cancer biomarker that improves prognostication but costs $100,000 per test, it might be difficult to convince insurers or the government to pay for the test unless it is spectacularly useful. So it's not unreasonable for someone to want to focus on variables that are \"most important,\" or to simplify an accurate model into one that is somewhat less accurate but is easier or less expensive to implement.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/202827/402101 \n But this variable selection and model simplification should be \nfor a specific purpose\n, and I think that is where the difficulty arises. The issue is similar to assessing classification schemes solely on the basis of percent of cases correctly classified. Just as different classification errors can have different costs, different model simplification schemes can have different costs that balance against their hoped-for benefits.\n\nSo I think that the issue to focus on as the analyst is the ability to estimate and illustrate these costs and benefits reliably with statistical modeling procedures, rather than worrying too much about an abstract concept of statistically validity per se. For example, pages 157-8 of Harrell's class notes linked above has an example of using the bootstrap to show the vagaries of ranking predictors in least squares; similar results can be found for variables sets selected by LASSO.\n\nIf that type of variability in variable selection doesn't get in the way of a particular practical application of the model that's OK. The job is to estimate how much and what type of trouble that simplification will lead to.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/101290/402101 \n If the values lie along a line the distribution has the same shape (up to location and scale) as the theoretical distribution we have supposed.\n\nLocal behaviour\n: When  looking at sorted sample values on the y-axis and (approximate) expected quantiles on the x-axis, we can identify from how the values in some section of the plot differ  locally from an overall linear trend by seeing whether the values are more or less concentrated than the theoretical distribution would suppose in that section of a plot:\n\n\n\nAs we see, less concentrated points increase more and more concentrated points increase less rapidly than an overall linear relation would suggest, and in the extreme cases correspond to a gap in the density of the sample (shows as a near-vertical jump) or a spike of constant values (values aligned horizontally). This allows us to spot a heavy tail or a light tail and hence, skewness greater or smaller than the theoretical distribution, and so on.\n\nOverall apppearance:\n\nHere's what QQ-plots look like (for particular choices of distribution) \non average\n:\n\n\n\nBut randomness tends to obscure things, especially with small samples:\n\n\n\nNote that at \n$n=21$\n the results may be much more variable than shown there - I generated several such sets of six plots and chose a 'nice' set where you could kind of see the shape in all six plots at the same time. Sometimes straight relationships look curved, curved relationships look straight, heavy-tails just look skew, and so on - with such small samples, often the situation may be much less clear:\n\n\n\nIt's possible to discern more features than those (such as discreteness, for one example), but with \n$n=21$\n, even such basic features may be hard to spot; we shouldn't try to 'over-interpret' every little wiggle. As sample sizes become larger, generally speaking the plots 'stabilize' and the features become more clearly interpretable rather than representing noise. [With some very heavy-tailed distributions, the rare large outlier might prevent the picture stabilizing nicely even at quite large sample sizes.]\n\nYou may also find the suggestion \nhere\n useful when trying to decide how much you should worry about a particular amount of curvature or wiggliness.\n\nA more suitable guide for interpretation in general would also include displays at smaller and larger sample sizes.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45644/402101 \n Consider the vector \n$\\vec{x}=(1,\\varepsilon)\\in\\mathbb{R}^2$\n where \n$\\varepsilon>0$\n is small. The \n$l_1$\n and \n$l_2$\n norms of \n$\\vec{x}$\n, respectively, are given by\n\n$$\\Vert \\vec{x}\\Vert_1 = 1+\\varepsilon,\\ \\ \\Vert\\vec{x}\\Vert_2^2 = 1+\\varepsilon^2$$\n\nNow say that, as part of some regularization procedure, we are going to reduce the magnitude of one of the elements of \n$\\vec{x}$\n by \n$\\delta\\leq\\varepsilon$\n. If we change \n$x_1$\n to \n$1-\\delta$\n, the resulting norms are\n\n$$\\Vert\\vec{x}-(\\delta,0)\\Vert_1 = 1-\\delta+\\varepsilon,\\ \\ \\Vert\\vec{x}-(\\delta,0)\\Vert_2^2 = 1-2\\delta+\\delta^2+\\varepsilon^2$$\n\nOn the other hand, reducing \n$x_2$\n by \n$\\delta$\n gives norms\n\n$$\\Vert\\vec{x}-(0,\\delta)\\Vert_1 = 1-\\delta+\\varepsilon,\\ \\ \\Vert\\vec{x}-(0,\\delta)\\Vert_2^2 = 1-2\\varepsilon\\delta+\\delta^2+\\varepsilon^2$$\n\nThe thing to notice here is that, for an \n$l_2$\n penalty, regularizing the larger term \n$x_1$\n results in a much greater reduction in norm than doing so to the smaller term \n$x_2\\approx 0$\n. For the \n$l_1$\n penalty, however, the reduction is the same. Thus, when penalizing a model using the \n$l_2$\n norm, it is highly unlikely that anything will ever be set to zero, since the reduction in \n$l_2$\n norm going from \n$\\varepsilon$\n to \n$0$\n is almost nonexistent when \n$\\varepsilon$\n is small. On the other hand, the reduction in \n$l_1$\n norm is always equal to \n$\\delta$\n, regardless of the quantity being penalized.\n\nAnother way to think of it: it's not so much that \n$l_1$\n penalties encourage sparsity, but that \n$l_2$\n penalties in some sense \ndiscourage\n sparsity by yielding diminishing returns as elements are moved closer to zero.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/501358/402101 \n Yes, because omitted variable bias depends on the underlying causal question you want to ask.\n\nSuppose you are interested in explaining the causal effect of schooling on earnings (just to give an example I have already discussed elsewhere and need not repeat here, see e.g. \nIs the equation \"$Y=\\mathbb{E}[Y|X] + error$\" an identity?\n, \nIs it true that an estimator will always asymptotically be consistent if it is biased in finite samples?\n \nOmitted variable bias: which predictors do I need to include, and why?\n) in \\$, (\n$Y$\n).\n\nIf you now regress earnings in \\$ on earnings in cents (\n$X$\n), the variables will be perfectly dependent, yet you would surely not argue that someone earns, say, 3000$ \nbecause\n he earns 300,000 cents. The regression still suffers from omitted variable bias when your goal is to estimate the causal effect of schooling on earnings.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/29347/402101 \n Linear regression uses the general linear equation $Y=b_0+\u2211(b_i X_i)+\\epsilon$ where $Y$ is a continuous dependent variable and independent variables $X_i$ are \nusually\n continuous (but can also be binary, e.g. when the linear model is used in a t-test) or other discrete domains. $\\epsilon$ is a term for the variance that is not explained by the model and is usually just called \"error\". Individual dependent values denoted by $Y_j$ can be solved by modifying the equation a little: $Y_j=b_0 + \\sum{(b_i X_{ij})+\\epsilon_j}$\n\nLogistic regression is another generalized linear model (GLM) procedure using the same basic formula, but instead of the continuous $Y$, it is regressing for the probability of a categorical outcome. In simplest form, this means that we're considering just one outcome variable and two states of that variable- either 0 or 1.\n\nThe equation for the probability of $Y=1$ looks like this:\n$$\nP(Y=1) = {1 \\over 1+e^{-(b_0+\\sum{(b_iX_i)})}}\n$$\n\nYour independent variables $X_i$ can be continuous or binary. The regression coefficients $b_i$ can be exponentiated to give you the change in odds of $Y$ per change in $X_i$, i.e., $Odds={P(Y=1) \\over P(Y=0)}={P(Y=1) \\over 1-P(Y=1)}$ and ${\\Delta Odds}=  e^{b_i}$.  $\\Delta Odds$ is called the odds ratio, $Odds(X_i+1)\\over Odds(X_i)$. In English, you can say that the odds of $Y=1$ increase by a factor of $e^{b_i}$ per unit change in $X_i$.\n\nExample: If you wanted to see how body mass index predicts blood cholesterol (a continuous measure), you'd use linear regression as described at the top of my answer. If you wanted to see how BMI predicts the odds of being a diabetic (a binary diagnosis), you'd use logistic regression.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/221930/402101 \n Here is an example with 3 variables, \n$y$\n, \n$x_1$\n and \n$x_2$\n, related by the equation\n\n$$ y = x_1 + x_2 + \\varepsilon $$\n\nwhere \n$\\varepsilon \\sim N(0,1)$\n\nThe particular data are\n\ny x1 x2\n1 4.520866  1  2\n2 6.849811  2  4\n3 6.539804  3  6\n\ny x1 x2\n1 4.520866  1  2\n2 6.849811  2  4\n3 6.539804  3  6\n\nSo it is evident that \n$x_2$\n is a multiple of \n$x_1$\n hence we have perfect collinearity.\n\nWe can write the model as\n\n$$ Y = X \\beta + \\varepsilon$$\n\nwhere:\n\n$$ Y = \\begin{bmatrix}4.52 \\\\6.85 \\\\6.54\\end{bmatrix}$$\n\n$$ X = \\begin{bmatrix}1 & 1 & 2\\\\1 & 2 & 4 \\\\1 & 3 & 6\\end{bmatrix}$$\n\nSo we have\n\n$$ XX' = \\begin{bmatrix}1 & 1 & 2\\\\1 & 2 & 4 \\\\1 & 3 & 6\\end{bmatrix}\n\\begin{bmatrix}1 & 1 & 1\\\\1 & 2 & 3 \\\\2 & 4 & 6\\end{bmatrix}\n= \\begin{bmatrix}6 & 11 & 16\\\\11 & 21 & 31 \\\\16 & 31 & 46\\end{bmatrix}\n$$\n\nNow we calculate the determinant of \n$XX'$\n :\n\n$$ \\det XX' = 6\\begin{vmatrix}21 & 31 \\\\31 & 46\\end{vmatrix} - 11 \\begin{vmatrix}11 & 31 \\\\16 & 46\\end{vmatrix} + 16\\begin{vmatrix}11 & 21 \\\\16 & 31\\end{vmatrix}= 0$$\n\nIn R we can show this as follows:\n\n> x1 <- c(1,2,3)\n\n> x1 <- c(1,2,3)\n\ncreate \nx2\n, a multiple of \nx1\n\nx2\n\nx1\n\n> x2 <- x1*2\n\n> x2 <- x1*2\n\ncreate y, a linear combination of \nx1\n, \nx2\n and some randomness\n\nx1\n\nx2\n\n> y <- x1 + x2 + rnorm(3,0,1)\n\n> y <- x1 + x2 + rnorm(3,0,1)\n\nobserve that\n\n> summary(m0 <- lm(y~x1+x2))\n\n> summary(m0 <- lm(y~x1+x2))\n\nfails to estimate a value for the \nx2\n coefficient:\n\nx2\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   3.9512     1.6457   2.401    0.251\nx1            1.0095     0.7618   1.325    0.412\nx2                NA         NA      NA       NA\n\nResidual standard error: 0.02583 on 1 degrees of freedom\nMultiple R-squared:      1,     Adjusted R-squared:  0.9999 \nF-statistic: 2.981e+04 on 1 and 1 DF,  p-value: 0.003687\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   3.9512     1.6457   2.401    0.251\nx1            1.0095     0.7618   1.325    0.412\nx2                NA         NA      NA       NA\n\nResidual standard error: 0.02583 on 1 degrees of freedom\nMultiple R-squared:      1,     Adjusted R-squared:  0.9999 \nF-statistic: 2.981e+04 on 1 and 1 DF,  p-value: 0.003687\n\nThe model matrix \n$X$\n is:\n\n> (X <- model.matrix(m0))\n\n(Intercept) x1 x2\n1           1  1  2\n2           1  2  4\n3           1  3  6\n\n> (X <- model.matrix(m0))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/221930/402101 \n Residual standard error: 0.02583 on 1 degrees of freedom\nMultiple R-squared:      1,     Adjusted R-squared:  0.9999 \nF-statistic: 2.981e+04 on 1 and 1 DF,  p-value: 0.003687\n\nThe model matrix \n$X$\n is:\n\n> (X <- model.matrix(m0))\n\n(Intercept) x1 x2\n1           1  1  2\n2           1  2  4\n3           1  3  6\n\n> (X <- model.matrix(m0))\n\n(Intercept) x1 x2\n1           1  1  2\n2           1  2  4\n3           1  3  6\n\nSo \n$XX'$\n is\n\n> (XXdash <- X %*% t(X))\n   1  2  3\n1  6 11 16\n2 11 21 31\n3 16 31 46\n\n> (XXdash <- X %*% t(X))\n   1  2  3\n1  6 11 16\n2 11 21 31\n3 16 31 46\n\nwhich is not invertible, as shown by\n\n> solve(XXdash)\nError in solve.default(XXdash) : \n  Lapack routine dgesv: system is exactly singular: U[3,3] = 0\n\n> solve(XXdash)\nError in solve.default(XXdash) : \n  Lapack routine dgesv: system is exactly singular: U[3,3] = 0\n\nOr:\n\n> det(XXdash)\n[1] 0\n\n> det(XXdash)\n[1] 0",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7950/402101 \n The \nshortest\n answer: \nnever\n, unless you are \nsure\n that your linear approximation of the data generating process (linear regression model) either by some theoretical or any other reasons \nis forced to go through the origin\n. If not the other regression parameters will be biased even if intercept is statistically insignificant (strange but it is so, consult \nBrooks\n \nIntroductory Econometrics\n for instance). Finally, as I do often explain to my students, by leaving the intercept term you insure that the residual term is zero-mean.\n\nFor your two models case we need more context. It may happen that linear model is not suitable here. For example, you need to log transform first if the model is multiplicative. Having exponentially growing processes it may occasionally happen that $R^2$ for the model without the intercept is \"much\" higher.\n\nScreen the data, test the model with RESET test or any other linear specification test, this may help to see if my guess is true. And, building the models highest $R^2$ is one of the last statistical properties I do really concern about, but it is nice to present to the people who are not so well familiar with econometrics (there are many dirty tricks to make determination close to 1 :)).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/172258/402101 \n I'd like to provide a straightforward answer.\n\nWhat is the main difference between maximum likelihood estimation\n(MLE) vs. least squares estimation (LSE) ?\n\nAs @TrynnaDoStat commented, minimizing squared error is equivalent to maximizing the likelihood in this case. As said in \nWikipedia\n,\n\nIn a linear model, if the errors belong to a normal distribution the\nleast squares estimators are also the maximum likelihood estimators.\n\nthey can be viewed as almost the same in your case since the conditions of the least square methods are \nthese four\n: 1) linearity; 2) linear normal residuals; 3) constant variability/homoscedasticity; 4) independence.\n\nLet me detail it a bit. Since we know that the response variable \n$y$\n.\n\n$$y=w^T X +\\epsilon \\quad\\text{ where }\\epsilon\\thicksim N(0,\\sigma^2)$$\n\nfollows a normal distribution (normal residuals),\n\n\n$$P(y|w, X)=\\mathcal{N}(y|w^TX, \\sigma^2I)$$\n\nthen the likelihood function (independence) is,\n\n\\begin{align}\nL(y^{(1)},\\dots,y^{(N)};w, X^{(1)},\\dots,X^{(N)}) &= \\prod_{i=1}^N \\mathcal{N}(y^{(i)}|w^TX^{(i)}, \\sigma^2I) \\\\ &= \n\\frac{1}{(2\\pi)^{\\frac{N}{2}}\\sigma^N}\\exp\\left(\\frac{-1}{2\\sigma^2}\\left(\\sum_{i=1}^N(y^{(i)}-w^TX^{(i)})^2\\right)\\right).\n\\end{align}\n\nMaximizing L is equivalent to minimizing (since other stuff are all constants, homoscedasticity)\n\n$$\\sum_{i=1}^n(y^{(i)}-w^TX^{(i)})^2.$$\n\nThat's the least-squares method, the difference between the expected \n$\\hat{Y_i}$\n and the actual \n$Y_i$\n.\n\nWhy can't we use MLE for predicting \n$y$\n values in linear regression\nand vice versa?\n\nAs explained above we're actually (more precisely equivalently) using the MLE for predicting \n$y$\n values. And if the response variable has arbitrary distributions rather than the normal distribution, like\nBernoulli distribution or anyone from the \nexponential family\n we map the linear predictor to the response variable distribution using a \nlink function\n (according to the response distribution), then the likelihood function becomes the product of all the outcomes (probabilities between 0 and 1) after the transformation. We can treat the link function in the linear regression as the identity function (since the response is already a probability).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4223/402101 \n That Wiki page is abusing language by referring to this number as a probability.  You are correct that it is not.  It is actually a \nprobability per foot\n.  Specifically, the value of 1.5789 (for a height of 6 feet) implies that the probability of a height between, say, 5.99 and 6.01 feet is close to the following unitless value:\n\n$$1.5789\\, [1/\\text{foot}] \\times (6.01 - 5.99)\\, [\\text{feet}] = 0.0316$$\n\nThis\n value \nmust\n not exceed 1, as you know.  (The small range of heights (0.02 in this example) is a crucial part of the probability apparatus.  It is the \"differential\" of height, which I will abbreviate $d(\\text{height})$.)  Probabilities per unit of something are called \ndensities\n by analogy to other densities, like mass per unit volume.\n\nBona fide\n probability \ndensities\n can have arbitrarily large values, even infinite ones.\n\n\n\nThis example shows the probability density function for a Gamma distribution (with shape parameter of $3/2$ and scale of $1/5$). Because most of the density is less than $1$, the curve has to rise higher than $1$ in order to have a total area of $1$ as required for all probability distributions.\n\n\n\nThis density (for a beta distribution with parameters $1/2, 1/10$) becomes infinite at $0$ and at $1$.  The total area still is finite (and equals $1$)!\n\nThe value of 1.5789 /foot is obtained in that example by estimating that the heights of males have a normal distribution with mean 5.855 feet and variance 3.50e-2 square feet.  (This can be found in a previous table.) The square root of that variance is the standard deviation, 0.18717 feet.  We re-express 6 feet as the number of SDs from the mean:\n\n$$z = (6 - 5.855) / 0.18717 = 0.7747$$\n\nThe division by the standard deviation produces a relation\n\n$$dz = d(\\text{height})/0.18717$$\n\nThe Normal probability density, by definition, equals\n\n$$\\frac{1}{\\sqrt{2 \\pi}}\\exp(-z^2/2)dz = 0.29544\\ d(\\text{height}) / 0.18717 = 1.5789\\  d(\\text{height}).$$\n\n(Actually, I cheated: I simply asked Excel to compute NORMDIST(6, 5.855, 0.18717, FALSE).  But then I really did check it against the formula, just to be sure.)  When we strip the \nessential\n differential $d(\\text{height})$ from the formula only the number $1.5789$ remains, like the Cheshire Cat's smile.  We, the readers, need to understand that the number has to be multiplied by a small difference in heights in order to produce a probability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163039/402101 \n Logistic regression can be described as a linear combination\n\n$$ \\eta = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k $$\n\nthat is passed through the link function $g$:\n\n$$ g(E(Y)) = \\eta $$\n\nwhere the link function is a \nlogit\n function\n\n$$ E(Y|X,\\beta) = p = \\text{logit}^{-1}( \\eta ) $$\n\nwhere $Y$ take only values in $\\{0,1\\}$ and inverse logit functions transforms linear combination $\\eta$ to this range. This is where classical logistic regression ends.\n\nHowever if you recall that $E(Y) = P(Y = 1)$ for variables that take only values in $\\{0,1\\}$, than $E(Y | X,\\beta)$ can be considered as $P(Y = 1 | X,\\beta)$. In this case, the logit function output could be thought as conditional probability of \"success\", i.e. $P(Y=1|X,\\beta)$. \nBernoulli distribution\n is a distribution that describes probability of observing binary outcome, with some $p$ parameter, so we can describe $Y$ as\n\n$$ y_i \\sim \\text{Bernoulli}(p) $$\n\nSo with logistic regression we look for some parameters $\\beta$ that togeder with independent variables $X$ form a linear combination $\\eta$. In classical regression $E(Y|X,\\beta) = \\eta$ (we assume link function to be identity function), however to model $Y$ that takes values in $\\{0,1\\}$ we need to transform $\\eta$ so to fit in $[0,1]$ range.\n\nNow, to estimate logistic regression in Bayesian way you pick up some priors for $\\beta_i$ parameters as with linear regression (see \nKruschke et al, 2012\n), then use logit function to transform the linear combination $\\eta$, so to use its output as a $p$ parameter of Bernoulli distribution that describes your $Y$ variable. So, yes, you actually use the equation and the logit link function the same way as in frequentionist case, and the rest works (e.g. choosing priors) like with estimating linear regression the Bayesian way.\n\nThe simple approach for choosing priors is to choose Normal distributions (but you can also use other distributions, e.g. $t$- or Laplace distribution for more robust model) for $\\beta_i$'s with parameters $\\mu_i$ and $\\sigma_i^2$ that are preset or taken from \nhierarchical priors\n. Now, having the model definition you can use software such as \nJAGS\n to perform \nMarkov Chain Monte Carlo\n simulation for you to estimate the model. Below I post JAGS code for simple logistic model (check \nhere\n for more examples).\n\nmodel {\n   # setting up priors\n   a ~ dnorm(0, .0001)\n   b ~ dnorm(0, .0001)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163039/402101 \n model {\n   # setting up priors\n   a ~ dnorm(0, .0001)\n   b ~ dnorm(0, .0001)\n\n   for (i in 1:N) {\n      # passing the linear combination through logit function\n      logit(p[i]) <- a + b * x[i]\n\n      # likelihood function\n      y[i] ~ dbern(p[i])\n   }\n}\n\nmodel {\n   # setting up priors\n   a ~ dnorm(0, .0001)\n   b ~ dnorm(0, .0001)\n\n   for (i in 1:N) {\n      # passing the linear combination through logit function\n      logit(p[i]) <- a + b * x[i]\n\n      # likelihood function\n      y[i] ~ dbern(p[i])\n   }\n}\n\nAs you can see, the code directly translates to model definition. What the software does is it draws some values from Normal priors for \na\n and \nb\n, then it uses those values to estimate \np\n and finally, uses likelihood function to assess how likely is your data given those parameters (this is when you use Bayes theorem, see \nhere\n for more detailed description).\n\na\n\nb\n\np\n\nThe basic logistic regression model can be extended to model the dependency between the predictors using a hierarchical model (including \nhyperpriors\n). In this case you can draw $\\beta_i$'s from \nMultivariate Normal distribution\n that enables us to include information about covariance $\\boldsymbol{\\Sigma}$ between independent variables\n\n$$ \\begin{pmatrix} \\beta_0  \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k  \\end{pmatrix} \\sim \\mathrm{MVN} \\left(\n\\begin{bmatrix} \\mu_0 \\\\ \\mu_1 \\\\ \\vdots \\\\ \\mu_k \\end{bmatrix},\n\\begin{bmatrix} \\sigma^2_0 & \\sigma_{0,1} & \\ldots & \\sigma_{0,k} \\\\\n               \\sigma_{1,0} & \\sigma^2_1 & \\ldots &\\sigma_{1,k} \\\\\n               \\vdots & \\vdots & \\ddots & \\vdots \\\\\n               \\sigma_{k,0} & \\sigma_{k,1} & \\ldots & \\sigma^2_k\n\\end{bmatrix}\n\\right)$$\n\n...but this is going into details, so let's stop right here.\n\nThe \"Bayesian\" part in here is choosing priors, using Bayes theorem and defining model in probabilistic terms. See here for \ndefinition of \"Bayesian model\"\n and here for some \ngeneral intuition on Bayesian approach\n. What you can also notice is that defining models is pretty straightforward and flexible with this approach.\n\nKruschke, J. K., Aguinis, H., & Joo, H. (2012). \nThe time has come: Bayesian methods for data analysis in the organizational sciences.\n \nOrganizational Research Methods, 15\n(4), 722-752.\n\nGelman, A., Jakulin, A., Pittau, G.M., and Su, Y.-S. (2008). \nA weakly informative default prior distribution for logistic and other regression models.\n \nThe Annals of Applied Statistics, 2\n(4), 1360\u20131383.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/136199/402101 \n There are two different typical situations for these kind of problems:\n\ni) you want to generate a sample from a given distribution whose population characteristics match the ones specified (but due to sampling variation, you don't have the sample characteristics exactly matching).\n\nii) you want to generate a sample whose sample characteristics match the ones specified (but, due to the constraints of exactly matching sample quantities to a prespecified set of values, don't really come from the distribution you want).\n\nYou want the second case -- but you get it by following the same approach as the first case, with an extra standardization step.\n\nSo for multivariate normals, either can be done in a fairly straightforward manner:\n\nWith first case you could use random normals without the population structure (such as iid standard normal which have expectation 0 and identity covariance matrix) and then impose it - transform to get the covariance matrix and mean you want. If $\\mu$ and $\\Sigma$ are the population mean and covariance you need and $z$ are iid standard normal, you calculate $y=Lz+\\mu$, for some $L$ where $LL'=\\Sigma$ (e.g. a suitable $L$ could be obtained via Cholesky decomposition). Then $y$ has the desired population characteristics.\n\nWith the second, you have to first transform your random normals to remove even the random variation away from the zero mean and identity covariance (making the sample mean zero and sample covariance $I_n$), then proceed as before. But that initial step of removing the sample deviation from exact mean $0$, variance $I$ interferes with the distribution. (In small samples it can be quite severe.)\n\nThis can be done by subtracting the sample mean of $z$ ($z^*=z-\\bar z$) and calculating the Cholesky decomposition of $z^*$. If $L^*$ is the left Cholesky factor, then $z^{(0)}=(L^*)^{-1}z^*$ should have sample mean 0 and identity sample covariance. You can then calculate $y=Lz^{(0)}+\\mu$ and have a sample with the desired sample moments. (Depending on how your sample quantities are defined, there may be an extra small fiddle involved with multiplying/dividing by factors like $\\sqrt{\\frac{n-1}{n}}$, but it's easy enough to identify that need.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/22720/402101 \n The best way to think about this is to imagine a scatterplot of points with $y$ on the vertical axis and $x$ represented by the horizontal axis.  Given this framework, you see a cloud of points, which may be vaguely circular, or may be elongated into an ellipse.  What you are trying to do in regression is find what might be called the 'line of best fit'.  However, while this seems straightforward, we need to figure out what we mean by 'best', and that means we must define what it would be for a line to be good, or for one line to be better than another, etc.  Specifically, we must stipulate a \nloss function\n.  A loss function gives us a way to say how 'bad' something is, and thus, when we minimize that, we make our line as 'good' as possible, or find the 'best' line.\n\nTraditionally, when we conduct a regression analysis, we find estimates of the slope and intercept so as to minimize the \nsum of squared errors\n.  These are defined as follows:\n\n$$\nSSE=\\sum_{i=1}^N(y_i-(\\hat\\beta_0+\\hat\\beta_1x_i))^2\n$$\n\nIn terms of our scatterplot, this means we are minimizing the (sum of the squared) \nvertical distances\n between the observed data points and the line.\n\n\n\nOn the other hand, it is perfectly reasonable to regress $x$ onto $y$, but in that case, we would put $x$ on the vertical axis, and so on.  If we kept our plot as is (with $x$ on the horizontal axis), regressing $x$ onto $y$ (again, using a slightly adapted version of the above equation with $x$ and $y$ switched) means that we would be minimizing the sum of the \nhorizontal distances\n between the observed data points and the line.  This sounds very similar, but is not quite the same thing.  (The way to recognize this is to do it both ways, and then algebraically convert one set of parameter estimates into the terms of the other.  Comparing the first model with the rearranged version of the second model, it becomes easy to see that they are not the same.)\n\n\n\nNote that neither way would produce the same line we would intuitively draw if someone handed us a piece of graph paper with points plotted on it.  In that case, we would draw a line straight through the center, but minimizing the vertical distance yields a line that is slightly \nflatter\n (i.e., with a shallower slope), whereas minimizing the horizontal distance yields a line that is slightly \nsteeper\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/22720/402101 \n Note that neither way would produce the same line we would intuitively draw if someone handed us a piece of graph paper with points plotted on it.  In that case, we would draw a line straight through the center, but minimizing the vertical distance yields a line that is slightly \nflatter\n (i.e., with a shallower slope), whereas minimizing the horizontal distance yields a line that is slightly \nsteeper\n.\n\nA correlation is symmetrical; $x$ is as correlated with $y$ as $y$ is with $x$.  The Pearson product-moment correlation can be understood within a regression context, however.  The correlation coefficient, $r$, is the slope of the regression line when both variables have been \nstandardized\n first.  That is, you first subtracted off the mean from each observation, and then divided the differences by the standard deviation.  The cloud of data points will now be centered on the origin, and the slope would be the same whether you regressed $y$ onto $x$, or $x$ onto $y$ (but note the comment by @DilipSarwate below).\n\n\n\nNow, why does this matter?  Using our traditional loss function, we are saying that all of the error is in only \none\n of the variables (viz., $y$).  That is, we are saying that $x$ is measured without error and constitutes the set of values we care about, but that $y$ has \nsampling error\n.  This is very different from saying the converse.  This was important in an interesting historical episode:  In the late 70's and early 80's in the US, the case was made that there was discrimination against women in the workplace, and this was backed up with regression analyses showing that women with equal backgrounds (e.g., qualifications, experience, etc.) were paid, on average, less than men.  Critics (or just people who were extra thorough) reasoned that if this was true, women who were paid equally with men would have to be more highly qualified, but when this was checked, it was found that although the results were 'significant' when assessed the one way, they were not 'significant' when checked the other way, which threw everyone involved into a tizzy.  See \nhere\n for a famous paper that tried to clear the issue up.\n\n(Updated much later)\n  Here's another way to think about this that approaches the topic through the formulas instead of visually:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/22720/402101 \n (Updated much later)\n  Here's another way to think about this that approaches the topic through the formulas instead of visually:\n\nThe formula for the slope of a simple regression line is a consequence of the loss function that has been adopted.  If you are using the standard \nOrdinary Least Squares\n loss function (noted above), you can derive the formula for the slope that you see in every intro textbook.  This formula can be presented in various forms; one of which I call the 'intuitive' formula for the slope.  Consider this form for both the situation where you are regressing $y$ on $x$, and where you are regressing $x$ on $y$:\n$$\n\\overbrace{\\hat\\beta_1=\\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}}^{y\\text{ on } x}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\overbrace{\\hat\\beta_1=\\frac{\\text{Cov}(y,x)}{\\text{Var}(y)}}^{x\\text{ on }y}\n$$\nNow, I hope it's obvious that these would not be the same unless $\\text{Var}(x)$ equals $\\text{Var}(y)$.  If the variances \nare\n equal (e.g., because you standardized the variables first), then so are the standard deviations, and thus the variances would both also equal $\\text{SD}(x)\\text{SD}(y)$.  In this case, $\\hat\\beta_1$ would equal Pearson's $r$, which is the same either way by virtue of \nthe principle of commutativity\n: \n$$\n\\overbrace{r=\\frac{\\text{Cov}(x,y)}{\\text{SD}(x)\\text{SD}(y)}}^{\\text{correlating }x\\text{ with }y}~~~~~~~~~~~~~~~~~~~~~~~~~~~\\overbrace{r=\\frac{\\text{Cov}(y,x)}{\\text{SD}(y)\\text{SD}(x)}}^{\\text{correlating }y\\text{ with }x}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73499/402101 \n It is a measure of precision just as $\\Sigma$ is a measure of dispersion.\n\nMore elaborately, $\\Sigma$ is a measure of how the variables are dispersed around the mean (the diagonal elements) and how they co-vary with other variables (the off-diagonal) elements. The more the dispersion the farther apart they are from the mean and the more they co-vary (in absolute value) with the other variables the stronger is the tendency for them to 'move together' (in the same or opposite direction depending on the sign of the covariance).\n\nSimilarly,  $\\Sigma^{-1}$ is a measure of how tightly clustered the variables are around the mean (the diagonal elements) and the extent to which they do not co-vary with the other variables (the off-diagonal elements). Thus, the higher the diagonal element, the tighter the variable is clustered around the mean. The interpretation of the off-diagonal elements is more subtle and I refer you to the other answers for that interpretation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/385766/402101 \n Xi'an's answer proved (or at least hinted a proof) that there are different distributions with the same mean, variance, skewness and kurtosis. I just want to show an example of three visually distinct discrete distributions with the same moments (mean=skewness=0, variance=1 and kurtosis=2):\n\n\n\nThe code to generate them is:\n\nlibrary(moments)\n\nn <- 1e6\n\nx <- c(-sqrt(2), 0, +sqrt(2))\np <- c(1,2,1)\nmostra1 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.4629338416371, -0.350630832572269, 0.350630832573386, 1.46293384163564)\np <- c(1, 1.3, 1.3, 1)\nmostra2 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.5049621442915, -0.457635862316285, 0.457635862316022, 1.50496214429192)\np <- c(1, 1.6, 1.6, 1)\nmostra3 <- sample(x, size=n, prob=p, replace=TRUE)\n\nmostra <- rbind(data.frame(x=mostra1, grup=\"a\"),\n                data.frame(x=mostra2, grup=\"b\"),\n                data.frame(x=mostra3, grup=\"c\"))\naggregate(x~grup, data=mostra, mean)\naggregate(x~grup, data=mostra, var)\naggregate(x~grup, data=mostra, skewness)\naggregate(x~grup, data=mostra, kurtosis)\n\nlibrary(ggplot2)\nggplot(mostra)+\n  geom_histogram(aes(x, fill=grup), bins=100)\n\nlibrary(moments)\n\nn <- 1e6\n\nx <- c(-sqrt(2), 0, +sqrt(2))\np <- c(1,2,1)\nmostra1 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.4629338416371, -0.350630832572269, 0.350630832573386, 1.46293384163564)\np <- c(1, 1.3, 1.3, 1)\nmostra2 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.5049621442915, -0.457635862316285, 0.457635862316022, 1.50496214429192)\np <- c(1, 1.6, 1.6, 1)\nmostra3 <- sample(x, size=n, prob=p, replace=TRUE)\n\nmostra <- rbind(data.frame(x=mostra1, grup=\"a\"),\n                data.frame(x=mostra2, grup=\"b\"),\n                data.frame(x=mostra3, grup=\"c\"))\naggregate(x~grup, data=mostra, mean)\naggregate(x~grup, data=mostra, var)\naggregate(x~grup, data=mostra, skewness)\naggregate(x~grup, data=mostra, kurtosis)\n\nlibrary(ggplot2)\nggplot(mostra)+\n  geom_histogram(aes(x, fill=grup), bins=100)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/176759/402101 \n A fitted regression model uses the parameters to generate point estimate predictions which are the means of observed responses if you were to replicate the study with the same $X$ values an infinite number of times (and when the linear model is true). The difference between these predicted values and the ones used to fit the model are called \"residuals\" which, when replicating the data collection process, have properties of random variables with 0 means.\n\nThe observed residuals are then used to subsequently estimate the variability in these values and to estimate the sampling distribution of the parameters. When the residual standard error is exactly 0 then the model fits the data perfectly (likely due to overfitting). If the residual standard error can not be shown to be significantly different from the variability in the unconditional response, then there is little evidence to suggest the linear model has any predictive ability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45158/402101 \n Viewed through the lens of probability inequalities and connections to the multiple-observation case, this result might not seem so impossible, or, at least, it might seem more plausible.\n\nLet $\\renewcommand{\\Pr}{\\mathbb P}\\newcommand{\\Ind}[1]{\\mathbf 1_{(#1)}}X \\sim \\mathcal N(\\mu,\\sigma^2)$ with $\\mu$ and $\\sigma^2$ unknown. We can write $X = \\sigma  Z + \\mu$ for $Z \\sim \\mathcal N(0,1)$.\n\nMain Claim\n: $[0,X^2/q_\\alpha)$ is a $(1-\\alpha)$ confidence interval for $\\sigma^2$ where $q_\\alpha$ is the $\\alpha$-level\n  quantile of a chi-squared distribution with one degree of freedom. Furthermore, since this interval has \nexactly\n $(1-\\alpha)$ coverage when $\\mu = 0$, it is the narrowest possible interval of the form $[0,b X^2)$ for some $b \\in \\mathbb R$.\n\nA reason for optimism\n\nRecall that in the $n \\geq 2$ case, with $T = \\sum_{i=1}^n (X_i - \\bar X)^2$, the \ntypical\n $(1-\\alpha)$ confidence interval for $\\sigma^2$ is\n$$\n\\Big(\\frac{T}{q_{n-1,(1-\\alpha)/2}}, \\frac{T}{q_{n-1,\\alpha/2}} \\Big) \\>,\n$$\nwhere $q_{k,a}$ is the $a$-level quantile of a chi-squared with $k$ degrees of freedom. This, of course, holds for any $\\mu$. While this is the \nmost popular\n interval (called the \nequal-tailed interval\n for obvious reasons), it is neither the only one nor even the one of smallest width! As should be apparent, another valid selection is\n$$\n\\Big(0,\\frac{T}{q_{n-1,\\alpha}}\\Big) \\>.\n$$\n\nSince, $T \\leq \\sum_{i=1}^n X_i^2$, then\n$$\n\\Big(0,\\frac{\\sum_{i=1}^n X_i^2}{q_{n-1,\\alpha}}\\Big) \\>,\n$$\nalso has coverage of at least $(1-\\alpha)$.\n\nViewed in this light, we might then be optimistic that the interval in the main claim is true for $n = 1$. The main difference is that there is no zero-degree-of-freedom chi-squared distribution for the case of a single observation, so we must hope that using a one-degree-of-freedom quantile will work.\n\nA half step toward our destination\n (\nExploiting the right tail\n)\n\nBefore diving into a proof of the main claim, let's first look at a preliminary claim that is not nearly as strong or satisfying statistically, but perhaps gives some additional insight into what is going on. You can skip down to the proof of the main claim below, without much (if any) loss. In this section and the next, the proofs\u2014while slightly subtle\u2014are based on only elementary facts: monotonicity of probabilities, and symmetry and unimodality of the normal distribution.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45158/402101 \n Before diving into a proof of the main claim, let's first look at a preliminary claim that is not nearly as strong or satisfying statistically, but perhaps gives some additional insight into what is going on. You can skip down to the proof of the main claim below, without much (if any) loss. In this section and the next, the proofs\u2014while slightly subtle\u2014are based on only elementary facts: monotonicity of probabilities, and symmetry and unimodality of the normal distribution.\n\nAuxiliary claim\n: $[0,X^2/z^2_\\alpha)$ is a $(1-\\alpha)$ confidence interval for $\\sigma^2$ as long as $\\alpha > 1/2$. Here $z_\\alpha$ is the $\\alpha$-level quantile of a standard normal.\n\nProof\n. $|X| = |-X|$ and $|\\sigma Z + \\mu| \\stackrel{d}{=} |-\\sigma Z+\\mu|$ by symmetry, so in what follows we can take $\\mu \\geq 0$ without loss of generality. Now, for $\\theta \\geq 0$ and $\\mu \\geq 0$,\n$$\n\\Pr(|X| > \\theta) \\geq \\Pr( X > \\theta) = \\Pr( \\sigma Z + \\mu > \\theta) \\geq \\Pr( Z > \\theta/\\sigma) \\>,\n$$\nand so with $\\theta = z_{\\alpha} \\sigma$, we see that \n$$\n\\Pr(0 \\leq \\sigma^2 < X^2 / z^2_\\alpha) \\geq 1 - \\alpha \\>.\n$$\nThis works only for $\\alpha > 1/2$, since that is what is needed for $z_\\alpha > 0$.\n\nThis proves the auxiliary claim. While illustrative, it is unsatifying from a statistical perspective since it requires an absurdly large $\\alpha$ to work.\n\nProving the main claim\n\nA refinement of the above argument leads to a result that will work for an arbitrary confidence level. First, note that\n$$\n\\Pr(|X| > \\theta) = \\Pr(|Z + \\mu/\\sigma| > \\theta / \\sigma ) \\>.\n$$\nSet $a = \\mu/\\sigma \\geq 0$ and $b = \\theta / \\sigma \\geq 0$. Then,\n$$\n\\Pr(|Z + a| > b) = \\Phi(a-b) + \\Phi(-a-b) \\>.\n$$\nIf we can show that the right-hand side increases in $a$ for every fixed $b$, then we can employ a similar argument as in the previous argument. This is at least plausible, since we'd like to believe that if the mean increases, then it becomes more probable that we see a value with a modulus that exceeds $b$. (However, we have to watch out for how quickly the mass is decreasing in the left tail!)\n\nSet $f_b(a) = \\Phi(a-b) + \\Phi(-a-b)$. Then\n$$\nf'_b(a) = \\varphi(a-b) - \\varphi(-a-b) = \\varphi(a-b) - \\varphi(a+b) \\>.\n$$\nNote that $f'_b(0) = 0$ and for positive $u$, $\\varphi(u)$ is decreasing in $u$. Now, for $a \\in (0,2b)$, it is easy to see that $\\varphi(a-b) \\geq \\varphi(-b) = \\varphi(b)$. These facts taken together easily imply that\n$$\nf'_b(a) \\geq 0\n$$\nfor all $a \\geq 0$ and any fixed $b \\geq 0$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45158/402101 \n Set $f_b(a) = \\Phi(a-b) + \\Phi(-a-b)$. Then\n$$\nf'_b(a) = \\varphi(a-b) - \\varphi(-a-b) = \\varphi(a-b) - \\varphi(a+b) \\>.\n$$\nNote that $f'_b(0) = 0$ and for positive $u$, $\\varphi(u)$ is decreasing in $u$. Now, for $a \\in (0,2b)$, it is easy to see that $\\varphi(a-b) \\geq \\varphi(-b) = \\varphi(b)$. These facts taken together easily imply that\n$$\nf'_b(a) \\geq 0\n$$\nfor all $a \\geq 0$ and any fixed $b \\geq 0$.\n\nHence, we have shown that for $a \\geq 0$ and $b \\geq 0$,\n$$\n\\Pr(|Z + a| > b) \\geq \\Pr(|Z| > b) = 2\\Phi(-b) \\>.\n$$\n\nUnraveling all of this, if we take $\\theta = \\sqrt{q_\\alpha} \\sigma$, we get\n$$\n\\Pr(X^2 > q_\\alpha \\sigma^2) \\geq \\Pr(Z^2  > q_\\alpha) = 1 - \\alpha \\>,\n$$\nwhich establishes the main claim.\n\nClosing remark\n: A careful reading of the above argument shows that it uses only the symmetric and unimodal properties of the normal distribution. Hence, the approach works analogously for obtaining confidence intervals from a single observation from any symmetric unimodal location-scale family, e.g., Cauchy or Laplace distributions.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2629/402101 \n There may be varying opinions on this, but I would treat the population data as a sample and assume a hypothetical population, then make inferences in the usual way.  One way to think about this is that there is an underlying data generating process responsible for the collected data, the \"population\" distribution.\n\nIn your particular case, this might make even more sense since you will have cohorts in the future.  Then your population is really cohorts who take the test even in the future.  In this way, you could account for time based variations if you have data for more than a year, or try to account for latent factors through your error model.  In short, you can develop richer models with greater explanatory power.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n There exist a number of frequenly mentioned regressional effects which conceptually are different but share much in common when seen purely statistically (see e.g. \nthis paper\n \"Equivalence of the Mediation, Confounding and Suppression\nEffect\" by David MacKinnon et al., or Wikipedia articles):\n\nMediator: IV which conveys effect (totally of partly) of another IV\nto the DV.\n\n\nConfounder: IV which constitutes or precludes, totally or\npartly, effect of another IV to the DV.\n\n\nModerator: IV which, varying,\nmanages the strength of the effect of another IV on the DV.\nStatistically, it is known as interaction between the two IVs.\n\n\nSuppressor: IV (a mediator or a moderator conceptually) which inclusion\nstrengthens the effect of another IV on the DV.\n\nI'm not going to discuss to what extent some or all of them are technically similar (for that, read the paper linked above). My aim is to try to show graphically what \nsuppressor\n is. The above definition that \"suppressor is a variable which inclusion strengthens the effect of another IV on the DV\" seems to me \npotentially\n broad because it does not tell anything about mechanisms of such enhancement. Below I'm discussing one mechanism - the only one I consider to be suppression. If there are \nother\n mechanisms as well (as for right now, I haven't tried to meditate of any such other) then either the above \"broad\" definition should be considered imprecise or my definition of suppression should be considered too narrow.\n\nSuppressor is the independent variable which, when added to the model, raises observed R-square \nmostly due to its accounting for the residuals\n left by the model without it, and not due to its own association with the DV (which is comparatively weak). We know that the increase in R-square in response to adding a IV is the squared part correlation of that IV in that new model. This way, if the part correlation of the IV with the DV \nis greater\n (by absolute value) than the zero-order \n$r$\n between them, that IV is a suppressor.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n So, a suppressor mostly \"suppresses\" the error of the reduced model, being weak as a predictor itself. The error term is the complement to the prediction. The prediction is \"projected on\" or \"shared between\" the IVs (regression coefficients), and so is the error term (\"complements\" to the coefficients). The suppressor suppresses such error components unevenly: greater for some IVs, lesser for other IVs. For those IVs \"whose\" such components it suppresses greatly it lends considerable facilitating aid by actually \nraising their regression coefficients\n.\n\nNot strong suppressing effects occurs often and wildly (an \nexample\n on this site). Strong suppression is typically introduced consciously. A researcher seeks for a characteristic which must correlate with the DV as weak as possible and at the same time would correlate with something in the IV of interest which is considered irrelevant, prediction-void, in respect to the DV. He enters it to the model and gets considerable increase in that IV's predictive power. The suppressor's coefficient is typically not interpreted.\n\nI could summarize my \ndefinition\n as follows [up on @Jake's answer and @gung's comments]:\n\nFormal (statistical) definition: suppressor is IV with part\ncorrelation larger than zero-order correlation (with the dependent).\n\n\nConceptual (practical) definition: the above formal definition + the zero-order\ncorrelation is small, so that the suppressor is not a sound predictor\nitself.\n\n\"Suppessor\" is a role of a IV in a specific \nmodel\n only, not the characteristic of the separate variable. When other IVs are added or removed, the suppressor can suddenly stop suppressing or resume suppressing or change the focus of its suppressing activity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n Conceptual (practical) definition: the above formal definition + the zero-order\ncorrelation is small, so that the suppressor is not a sound predictor\nitself.\n\n\"Suppessor\" is a role of a IV in a specific \nmodel\n only, not the characteristic of the separate variable. When other IVs are added or removed, the suppressor can suddenly stop suppressing or resume suppressing or change the focus of its suppressing activity.\n\nThe first picture below shows a typical regression with two predictors (we'll speak of linear regression). The picture is copied from \nhere\n where it is explained in more details. In short, moderately correlated (= having acute angle between them) predictors \n$X_1$\n and \n$X_2$\n span 2-dimesional space \"plane X\". The dependent variable \n$Y$\n is projected onto it orthogonally, leaving the predicted variable \n$Y'$\n and the residuals with st. deviation equal to the length of \n$e$\n. R-square of the regression is the angle between \n$Y$\n and \n$Y'$\n, and the two regression coefficients are directly related to the skew coordinates \n$b_1$\n and \n$b_2$\n, respectively. This situation I've called normal or typical because both \n$X_1$\n and \n$X_2$\n correlate with \n$Y$\n (oblique angle exists between each of the independents and the dependent) and the predictors compete for the prediction because they are correlated.\n\n\n\nIt is shown on the next picture. This one is like the previous; however \n$Y$\n vector now directs somewhat away from the viewer and \n$X_2$\n changed its direction considerably. \n$X_2$\n acts as a suppressor. Note first of all that it hardly correlates with \n$Y$\n. Hence it cannot be a valuable \npredictor\n itself. Second. Imagine \n$X_2$\n is absent and you predict only by \n$X_1$\n; the prediction of this one-variable regression is depicted as \n$Y^*$\n red vector, the error as \n$e^*$\n vector, and the coefficient is given by \n$b^*$\n coordinate (which is the endpoint of \n$Y^*$\n).\n\n\n\nNow bring yourself back to the full model and notice that \n$X_2$\n is fairly correlated with \n$e^*$\n. Thus, \n$X_2$\n when introduced in the model, can explain a considerable portion of that error of the reduced model, cutting down \n$e^*$\n to \n$e$\n. This constellation: (1) \n$X_2$\n is not a rival to \n$X_1$\n as a \npredictor\n; and (2) \n$X_2$\n is a dustman to pick up \nunpredictedness\n left by \n$X_1$\n, - makes \n$X_2$\n a \nsuppressor\n. As a result of its effect, predictive strength of \n$X_1$\n has grown to some extent: \n$b_1$\n is larger than \n$b^*$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n Well, why is \n$X_2$\n called a suppressor to \n$X_1$\n and how can it reinforce it when \"suppressing\" it? Look at the next picture.\n\n\n\nIt is exactly the same as the previous. Think again of the model with the single predictor \n$X_1$\n. This predictor could of course be decomposed in two parts or components (shown in grey): the part which is \"responsible\" for prediction of \n$Y$\n (and thus coinciding with that vector) and the part which is \"responsible\" for the unpredictedness (and thus parallel to \n$e^*$\n). It is \nthis\n second part of \n$X_1$\n - the part irrelevant to \n$Y$\n - is suppressed by \n$X_2$\n when that suppressor is added to the model. The irrelevant part is suppressed and thus, given that the suppressor doesn't itself predict \n$Y$\n any much, the relevant part looks stronger. A suppressor is not a predictor but rather a facilitator for another/other predictor/s. Because it competes with what impedes them to predict.\n\nIt is the sign of the correlation between the suppressor and the error variable \n$e^*$\n left by the reduced (without-the-suppressor) model. In the depiction above, it is positive. In other settings (for example, revert the direction of \n$X_2$\n) it could be negative.\n\nExample data:\n\ny         x1         x2\n\n1.64454000  .35118800 1.06384500\n1.78520400  .20000000 -1.2031500\n-1.3635700 -.96106900 -.46651400\n .31454900  .80000000 1.17505400\n .31795500  .85859700 -.10061200\n .97009700 1.00000000 1.43890400\n .66438800  .29267000 1.20404800\n-.87025200 -1.8901800 -.99385700\n1.96219200 -.27535200 -.58754000\n1.03638100 -.24644800 -.11083400\n .00741500 1.44742200 -.06923400\n1.63435300  .46709500  .96537000\n .21981300  .34809500  .55326800\n-.28577400  .16670800  .35862100\n1.49875800 -1.1375700 -2.8797100\n1.67153800  .39603400 -.81070800\n1.46203600 1.40152200 -.05767700\n-.56326600 -.74452200  .90471600\n .29787400 -.92970900  .56189800\n-1.5489800 -.83829500 -1.2610800\n\ny         x1         x2",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n y         x1         x2\n\n1.64454000  .35118800 1.06384500\n1.78520400  .20000000 -1.2031500\n-1.3635700 -.96106900 -.46651400\n .31454900  .80000000 1.17505400\n .31795500  .85859700 -.10061200\n .97009700 1.00000000 1.43890400\n .66438800  .29267000 1.20404800\n-.87025200 -1.8901800 -.99385700\n1.96219200 -.27535200 -.58754000\n1.03638100 -.24644800 -.11083400\n .00741500 1.44742200 -.06923400\n1.63435300  .46709500  .96537000\n .21981300  .34809500  .55326800\n-.28577400  .16670800  .35862100\n1.49875800 -1.1375700 -2.8797100\n1.67153800  .39603400 -.81070800\n1.46203600 1.40152200 -.05767700\n-.56326600 -.74452200  .90471600\n .29787400 -.92970900  .56189800\n-1.5489800 -.83829500 -1.2610800\n\nLinear regression results:\n\n\n\nObserve that \n$X_2$\n served as suppressor. Its zero-order correlation with \n$Y$\n is practically zero but its part correlation is much larger by magnitude, \n$-.224$\n. It strengthened to some extent the predictive force of \n$X_1$\n (from r \n$.419$\n, a would-be beta in simple regression with it, to beta \n$.538$\n in the multiple regression).\n\nAccording to the \nformal\n definition, \n$X_1$\n appeared a suppressor too, because its part correlation is greater than its zero-order correlation. But that is because we have only two IV in the simple example. Conceptually, \n$X_1$\n isn't a suppressor because its \n$r$\n with \n$Y$\n is not about \n$0$\n.\n\nBy way, sum of squared part correlations exceeded R-square: \n.4750^2+(-.2241)^2 = .2758 > .2256\n, which would not occur in normal regressional situation (see the \nVenn diagram\n below).\n\n.4750^2+(-.2241)^2 = .2758 > .2256\n\nAdding a variable that will serve a supressor may as well as may not change the sign of some other variables' coefficients. \"Suppression\" and \"change sign\" effects are not the same thing. Moreover, I believe that a suppressor can never change sign of \nthose\n predictors whom they serve suppressor. (It would be a shocking discovery to add the suppressor on purpose to facilitate a variable and then to find it having become indeed stronger but in the opposite direction! I'd be thankful if somebody could show me it is possible.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n To cite an earlier passage: \"For those IVs \"whose\" such components [error components] it suppresses greatly the suppressor lends considerable facilitating aid by actually \nraising their regression coefficients\n\". Indeed, in our Example above, \n$X_2$\n, the suppressor, raised the coefficient for \n$X_1$\n. Such enhancement of the unique predictive power of another regressor is often the \naim\n of a suppressor to a model but it is not the \ndefinition\n of suppressor or of suppression effect. For, the aforementioned enhancement of another predictor's capacity via adding more regressors can easily occure in a normal regressional situation without those regressors being suppressors. Here is an example.\n\ny       x1       x2       x3\n\n   1        1        1        1\n   3        2        2        6\n   2        3        3        5\n   3        2        4        2\n   4        3        5        9\n   3        4        4        2\n   2        5        3        3\n   3        6        4        4\n   4        7        5        5\n   5        6        6        6\n   4        5        7        5\n   3        4        5        5\n   4        5        3        5\n   5        6        4        6\n   6        7        5        4\n   5        8        6        6\n   4        2        7        7\n   5        3        8        8\n   6        4        9        4\n   5        5        3        3\n   4        6        4        2\n   3        2        1        1\n   4        3        5        4\n   5        4        6        5\n   6        9        5        4\n   5        8        3        3\n   3        5        5        2\n   2        6        6        1\n   3        7        7        5\n   5        8        8        8\n\ny       x1       x2       x3",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n y       x1       x2       x3\n\n   1        1        1        1\n   3        2        2        6\n   2        3        3        5\n   3        2        4        2\n   4        3        5        9\n   3        4        4        2\n   2        5        3        3\n   3        6        4        4\n   4        7        5        5\n   5        6        6        6\n   4        5        7        5\n   3        4        5        5\n   4        5        3        5\n   5        6        4        6\n   6        7        5        4\n   5        8        6        6\n   4        2        7        7\n   5        3        8        8\n   6        4        9        4\n   5        5        3        3\n   4        6        4        2\n   3        2        1        1\n   4        3        5        4\n   5        4        6        5\n   6        9        5        4\n   5        8        3        3\n   3        5        5        2\n   2        6        6        1\n   3        7        7        5\n   5        8        8        8\n\nRegressions results without and with \n$X_3$\n:\n\n\n\nInclusion of \n$X_3$\n in the model raised the beta of \n$X_1$\n from \n$.381$\n to \n$.399$\n (and its corresponding partial correlation with \n$Y$\n from \n$.420$\n to \n$.451$\n). Still, we find no suppressor in the model. \n$X_3$\n's part correlation (\n$.229$\n) is not greater than its zero-order correlation (\n$.427$\n). Same is for the other regressors. \"Facilitation\" effect was there, but not due to \"suppression\" effect. Definition of a suppessor is different from just strenghtening/facilitation; and it is about picking up mostly errors, due to which the part correlation exceeds the zero-order one.\n\nNormal regressional situation is often explained with the help of Venn diagram.\n\n\n\nA+B+C+D\n = 1, all \n$Y$\n variability. \nB+C+D\n area is the variability accounted by the two IV (\n$X_1$\n and \n$X_2$\n), the R-square; the remaining area \nA\n is the error variability. \nB+C\n = \n$r_{YX_1}^2$\n; \nD+C\n = \n$r_{YX_2}^2$\n, Pearson zero-order correlations. \nB\n and \nD\n are the squared part (semipartial) correlations: \nB\n = \n$r_{Y(X_1.X_2)}^2$\n; \nD\n = \n$r_{Y(X_2.X_1)}^2$\n. \nB/(A+B)\n = \n$r_{YX_1.X_2}^2$\n and \nD/(A+D)\n = \n$r_{YX_2.X_1}^2$\n are the squared partial correlations which have the \nsame basic meaning\n as the standardized regression coefficients betas.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73876/402101 \n According to the above definition (which I stick to) that a suppressor is the IV with part correlation greater than zero-order correlation, \n$X_2$\n is the suppressor if \nD\n area > \nD+C\n area. That \ncannot\n be displayed on Venn diagram. (It would imply that \nC\n from the view of \n$X_2$\n is not \"here\" and is not the same entity than \nC\n from the view of \n$X_1$\n. One must invent perhaps something like multilayered Venn diagram to wriggle oneself to show it.)\n\nP.S.\n Upon finishing my answer I found \nthis\n answer (by @gung) with a nice simple (schematic) diagram, which seems to be in agreement with what I showed above by vectors.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7645/402101 \n In the \"marginalisation\" integral, the lower limit for $x_1$ is not $0$ but $x_2$ (because of the $0<x_2<x_1$ condition).\n\nSo the integral should be:\n\n$$p(x_2)=\\int p(x_1,x_2) dx_1=\\int \\frac{I(0\\leq x_2\\leq x_1\\leq 1)}{x_1} dx_1=\\int_{x_2}^{1} \\frac{dx_1}{x_1}=log\\big(\\frac{1}{x_2}\\big)$$\n\nYou have stumbled across, what I think is one of the hardest parts of statistical integrals - determining the limits of integration.\n\nNOTE: This is consistent with Henry's answer, mine is the PDF, and his is the CDF.  Differentiating his answer gives you mine, which shows we are both right.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/151351/402101 \n Since you ask for \ninsights\n, I'm going to take a fairly intuitive approach rather than a more mathematical tack:\n\nFollowing the concepts in my answer \nhere\n, we can formulate a ridge regression as a regression with dummy data by adding \n$p$\n (in your formulation) observations, where \n$y_{n+j}=0$\n, \n$x_{j,n+j}=\\sqrt{\\lambda}$\n and \n$x_{i,n+j}=0$\n for \n$i\\neq j$\n. If you write out the new RSS for this expanded data set, you'll see the additional observations each add a term of the form \n$(0-\\sqrt{\\lambda}\\beta_j)^2=\\lambda\\beta_j^2$\n, so the new RSS is the original \n$\\text{RSS} + \\lambda \\sum_{j=1}^p\\beta_j^2$\n -- and minimizing the RSS on this new, expanded data set is the same as minimizing the ridge regression criterion.\n\n\nSo what can we see here? As \n$\\lambda$\n increases, the additional \n$x$\n-rows  each have one component that increases, and so the influence of these points also increases. They pull the fitted hyperplane toward themselves. Then as \n$\\lambda$\n and the corresponding components of the \n$x$\n's go off to infinity, all the involved coefficients \"flatten out\" to \n$0$\n.\n\n\nThat is, as \n$\\lambda\\to\\infty$\n, the penalty will dominate the minimization, so the \n$\\beta$\ns will go to zero. If the intercept is not penalized (the usual case) then the model shrinks more and more toward the mean of the response.\n\n\nI'll give an intuitive sense of why we're talking about ridges first (which also suggests why it's needed), then tackle a little history. The first is adapted from my answer \nhere\n:\n\n\nIf there's multicollinearity, you get a \"ridge\" in the likelihood function (likelihood is a function of the \n$\\beta$\n's). This in turn yields a long \"valley\" in the RSS (since RSS=\n$-2\\log\\mathcal{L}$\n). \n\n\nRidge\n regression \"fixes\" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:\n\n\n\n[\nClearer image\n]",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/151351/402101 \n If there's multicollinearity, you get a \"ridge\" in the likelihood function (likelihood is a function of the \n$\\beta$\n's). This in turn yields a long \"valley\" in the RSS (since RSS=\n$-2\\log\\mathcal{L}$\n). \n\n\nRidge\n regression \"fixes\" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:\n\n\n\n[\nClearer image\n]\n\n\nThe actual story behind the name is a little more complicated. In 1959 A.E. Hoerl [1] introduced \nridge analysis\n for response surface methodology, and it very soon [2] became adapted to dealing with multicollinearity in regression ('ridge regression'). See for example, the discussion by R.W. Hoerl in [3], where it  describes Hoerl's (A.E. not R.W.) use of contour plots of the response surface* in the identification of where to head to find local optima (where one 'heads up the ridge'). In ill-conditioned problems, the issue of a very long ridge arises, and insights and methodology from ridge analysis are adapted to the related issue with the likelihood/RSS in regression, producing ridge regression.\n\nFollowing the concepts in my answer \nhere\n, we can formulate a ridge regression as a regression with dummy data by adding \n$p$\n (in your formulation) observations, where \n$y_{n+j}=0$\n, \n$x_{j,n+j}=\\sqrt{\\lambda}$\n and \n$x_{i,n+j}=0$\n for \n$i\\neq j$\n. If you write out the new RSS for this expanded data set, you'll see the additional observations each add a term of the form \n$(0-\\sqrt{\\lambda}\\beta_j)^2=\\lambda\\beta_j^2$\n, so the new RSS is the original \n$\\text{RSS} + \\lambda \\sum_{j=1}^p\\beta_j^2$\n -- and minimizing the RSS on this new, expanded data set is the same as minimizing the ridge regression criterion.\n\nSo what can we see here? As \n$\\lambda$\n increases, the additional \n$x$\n-rows  each have one component that increases, and so the influence of these points also increases. They pull the fitted hyperplane toward themselves. Then as \n$\\lambda$\n and the corresponding components of the \n$x$\n's go off to infinity, all the involved coefficients \"flatten out\" to \n$0$\n.\n\nThat is, as \n$\\lambda\\to\\infty$\n, the penalty will dominate the minimization, so the \n$\\beta$\ns will go to zero. If the intercept is not penalized (the usual case) then the model shrinks more and more toward the mean of the response.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/151351/402101 \n That is, as \n$\\lambda\\to\\infty$\n, the penalty will dominate the minimization, so the \n$\\beta$\ns will go to zero. If the intercept is not penalized (the usual case) then the model shrinks more and more toward the mean of the response.\n\nI'll give an intuitive sense of why we're talking about ridges first (which also suggests why it's needed), then tackle a little history. The first is adapted from my answer \nhere\n:\n\nIf there's multicollinearity, you get a \"ridge\" in the likelihood function (likelihood is a function of the \n$\\beta$\n's). This in turn yields a long \"valley\" in the RSS (since RSS=\n$-2\\log\\mathcal{L}$\n).\n\nRidge\n regression \"fixes\" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:\n\n[\nClearer image\n]\n\nThe actual story behind the name is a little more complicated. In 1959 A.E. Hoerl [1] introduced \nridge analysis\n for response surface methodology, and it very soon [2] became adapted to dealing with multicollinearity in regression ('ridge regression'). See for example, the discussion by R.W. Hoerl in [3], where it  describes Hoerl's (A.E. not R.W.) use of contour plots of the response surface* in the identification of where to head to find local optima (where one 'heads up the ridge'). In ill-conditioned problems, the issue of a very long ridge arises, and insights and methodology from ridge analysis are adapted to the related issue with the likelihood/RSS in regression, producing ridge regression.\n\n* examples of response surface contour plots (in the case of quadratic response) can be seen \nhere\n (Fig 3.9-3.12).\n\nThat is, \"ridge\" actually refers to the characteristics of the function we were attempting to optimize, rather than to adding a \"ridge\" (+ve diagonal) to the \n$X^TX$\n matrix (so while ridge regression does add to the diagonal, that's not why we call it 'ridge' regression).\n\nFor some additional information on the need for ridge regression, see the first link under list item 2. above.\n\nReferences:\n\n[1]: Hoerl, A.E. (1959). Optimum solution of many variables equations. \nChemical Engineering Progress\n, \n\n55\n (11) 69-78.\n\n[2]: Hoerl, A.E. (1962). Applications of ridge analysis to regression problems. \nChemical Engineering Progress\n, \n\n58\n (3) 54-59.\n\n[3] Hoerl, R.W. (1985). Ridge Analysis 25 Years Later.\n\nAmerican Statistician\n, \n39\n (3), 186-192",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/68753/402101 \n This question has been partially discussed at this site as below, and opinions seem mixed.\n\nWhat is the difference between fixed effect, random effect and mixed effect models?\n\n\nWhat is the mathematical difference between random- and fixed-effects?\n\n\nConcepts behind fixed/random effects models\n\nAll terms are generally related to longitudinal / panel / clustered / hierarchical data and repeated measures (in the format of advanced regression and ANOVA), but have multiple meanings in different context. I would like to answer the question in formulas based on my knowledge.\n\nIn biostatistics, fixed-effects, denoted as $\\color{red}{\\boldsymbol\\beta}$ in Equation (*) below, usually comes together with random effects. But the fixed-effects model is also defined to assume that the observations are independent, like cross-sectional setting, as in \nLongitudinal Data Analysis\n of Hedeker and Gibbons (2006).\n\n\nIn econometrics, the fixed-effects model can be written as\n$$ y_{ij}=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta+\\color{red}{u_i}+\\epsilon_{ij}$$\nwhere $\\color{red}{u_i}$ is fixed (not random) intercept for each subject ($i$), or we can also have a fixed-effect as $u_j$ for each repeated measurement ($j$); $\\boldsymbol x_{ij}$ denotes covariates.\n\n\nIn meta-analysis, the fixed-effect model assumes underlying effect is the same across all studies (e.g. Mantel and Haenszel, 1959).\n\nIn biostatistics, the random-effects model (Laird and Ware, 1982) can be written as\n$$\\tag{*} y_{ij}=\\boldsymbol x_{ij}^{'}\\color{red}{\\boldsymbol\\beta}+\\boldsymbol z_{ij}^{'}\\color{blue}{\\boldsymbol u_i}+e_{ij}$$\nwhere $\\color{blue}{\\boldsymbol u_i}$ is assumed to follow a distribution. $\\boldsymbol x_{ij}$ denotes covariates for fixed effects, and $\\boldsymbol z_{ij}$ denotes covariates for random effects.\n\n\nIn econometrics, the random-effects model may only refer to \nrandom intercept model\n as in biostatistics, i.e. $\\boldsymbol z_{ij}^{'}=1$ and $\\boldsymbol u_i$ is a scalar.\n\n\nIn meta-analysis, the random-effect model assumes heterogeneous effects across studies (DerSimonian and Laird, 1986).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/68753/402101 \n In econometrics, the random-effects model may only refer to \nrandom intercept model\n as in biostatistics, i.e. $\\boldsymbol z_{ij}^{'}=1$ and $\\boldsymbol u_i$ is a scalar.\n\n\nIn meta-analysis, the random-effect model assumes heterogeneous effects across studies (DerSimonian and Laird, 1986).\n\nMarginal model is generally compared to conditional model (random-effects model), and the former focuses on the population mean (take linear model for an example) $$E(y_{ij})=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta,$$ while the latter deals with the conditional mean $$E(y_{ij}|\\boldsymbol u_i)=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta + \\boldsymbol z_{ij}^{'}\\boldsymbol u_i.$$ The interpretation and scale of the regression coefficients between marginal model and random-effects model would be different for nonlinear models (e.g. logistic regression). Let $h(E(y_{ij}|\\boldsymbol u_i))=\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta + \\boldsymbol z_{ij}^{'}\\boldsymbol u_i$, then $$E(y_{ij})=E(E(y_{ij}|\\boldsymbol u_i))=E(h^{-1}(\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta + \\boldsymbol z_{ij}^{'}\\boldsymbol u_i))\\neq h^{-1}(\\boldsymbol x_{ij}^{'}\\boldsymbol\\beta),$$ unless trivially the link function $h$ is the identity link (linear model), or $u_i=0$ (no random-effects). Good examples include generalized estimating equations (GEE; Zeger, Liang and Albert, 1988) and marginalized multilevel models (Heagerty and Zeger, 2000).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4536/402101 \n Yes, it is possible.  What you're interested is is called \"Multivariate Multiple Regression\" or just \"Multivariate Regression\".  I don't know what software you are using, but you can do this in R.\n\nHere's a link that provides examples\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/188904/402101 \n A (metric) distance $D$ must be symmetric, i.e. $D(P,Q) = D(Q,P)$.\nBut, from definition, $KL$ is not.\n\nExample: $\\Omega = \\{A,B\\}$, $P(A) = 0.2, P(B) = 0.8$, $Q(A) = Q(B) = 0.5$.\n\nWe have:\n\n$$KL(P,Q) = P(A)\\log \\frac{P(A)}{Q(A)} + P(B) \\log \\frac{P(B)}{Q(B)} \\approx 0.19$$\n\nand\n\n$$KL(Q,P) = Q(A)\\log \\frac{Q(A)}{P(A)} + Q(B) \\log \\frac{Q(B)}{P(B)} \\approx 0.22$$\n\nthus $KL(P,Q) \\neq KL(Q,P)$ and therefore $KL$ is not a (metric) distance.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/320/402101 \n A saturated model is one in which there are as many estimated parameters as data points. By definition, this will lead to a perfect fit, but will be of little use statistically, as you have no data left to estimate variance.\n\nFor example, if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7005/402101 \n For i.i.d. random variables $X_1, \\dotsc, X_n$, the unbiased estimator for the variance $s^2$ (the one with denominator $n-1$) has variance:\n\n$$\\mathrm{Var}(s^2) = \\sigma^4 \\left(\\frac{2}{n-1} + \\frac{\\kappa}{n}\\right)$$\n\nwhere $\\kappa$ is the excess kurtosis of the distribution (reference: \nWikipedia\n). So now you need to estimate the kurtosis of your distribution as well. You can use a quantity sometimes described as $\\gamma_2$ (also from \nWikipedia\n):\n\n$$\\gamma_2 = \\frac{\\mu_4}{\\sigma_4} - 3$$\n\nI would assume that if you use $s$ as an estimate for $\\sigma$ and $\\gamma_2$ as an estimate for $\\kappa$, that you get a reasonable estimate for $\\mathrm{Var}(s^2)$, although I don't see a guarantee that it is unbiased. See if it matches with the variance among the subsets of your 500 data points reasonably, and if it does don't worry about it anymore :)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/18439/402101 \n Let \n$F$\n be the CDF of the random variable \n$X$\n, so the inverse CDF can be written \n$F^{-1}$\n.  In your integral make the substitution \n$p = F(x)$\n, \n$dp = F'(x)dx = f(x)dx$\n to obtain\n\n$$\\int_0^1F^{-1}(p)dp = \\int_{-\\infty}^{\\infty}x f(x) dx = \\mathbb{E}_F[X].$$\n\nThis is valid for continuous distributions.  Care must be taken for other distributions because an inverse CDF hasn't a unique definition.\n\nWhen the variable is not continuous, it does not have a distribution that is absolutely continuous with respect to Lebesgue measure, requiring care in the definition of the inverse CDF and care in computing integrals.  Consider, for instance, the case of a discrete distribution.  By definition, this is one whose CDF \n$F$\n is a step function with steps of size \n$\\Pr_F(x)$\n at each possible value \n$x$\n.\n\n\n\nThis figure shows the CDF of a Bernoulli\n$(2/3)$\n distribution scaled by \n$2$\n.  That is, the random variable has a probability \n$1/3$\n of equalling \n$0$\n and a probability of \n$2/3$\n of equalling \n$2$\n.  The heights of the jumps at \n$0$\n and \n$2$\n give their probabilities.  The expectation of this variable evidently equals \n$0\\times(1/3)+2\\times(2/3)=4/3$\n.\n\nWe could define an \"inverse CDF\" \n$F^{-1}$\n by requiring\n\n$$F^{-1}(p) = x \\text{ if } F(x) \\ge p \\text{ and } F(x^{-}) \\lt p.$$\n\nThis means that \n$F^{-1}$\n is also a step function.  For any possible value \n$x$\n of the random variable, \n$F^{-1}$\n will attain the value \n$x$\n over an interval of length \n$\\Pr_F(x)$\n.  Therefore its integral is obtained by summing the values \n$x\\Pr_F(x)$\n, which is just the expectation.\n\n\n\nThis is the graph of the inverse CDF of the preceding example.  The jumps of \n$1/3$\n and \n$2/3$\n in the CDF become horizontal lines of these lengths at heights equal to \n$0$\n and \n$2$\n, the values to whose probabilities they correspond.  (The Inverse CDF is not defined beyond the interval \n$[0,1]$\n.)  Its integral is the sum of two rectangles, one of height \n$0$\n and base \n$1/3$\n, the other of height \n$2$\n and base \n$2/3$\n, totaling \n$4/3$\n, as before.\n\nIn general, for a mixture of a continuous and a discrete distribution, we need to define the inverse CDF to parallel this construction: at each discrete jump of height \n$p$\n we must form a horizontal line of length \n$p$\n as given by the preceding formula.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/161651/402101 \n Assume \n$F_X$\n is continuous and increasing. Define \n$Z = F_X(X)$\n and note that \n$Z$\n takes values in \n$[0, 1]$\n. Then\n\n$$F_Z(x) = P(F_X(X) \\leq x) = P(X \\leq F_X^{-1}(x)) = F_X(F_X^{-1}(x)) = x.$$\n\nThe derivative of \n$\\frac{d}{dz}F_Z(x) = f_Z(x) = 1$\n so \n$Z$\n is uniformly distributed \n$[0, 1]$\n.\n\nAnother way to see this: A uniform random variable \n$U$\n taking values in \n$[0, 1]$\n has \n$F_U(x) =  \\int_R f_U(u)\\,du =\\int_0^x \\,du =x$\n. So \n$F_Z(x) = F_U(x)$\n for every \n$x\\in[0, 1]$\n. Since \n$Z$\n and \n$U$\n have the same distribution function \n$Z$\n must also be uniform on \n$[0, 1]$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/38722/402101 \n Is there any way of doing this without expanding out $E[(X-E[X])(aA+......)]$?\n\nYes. There is a property of covariance called \nbilinearity\n which is that the covariance of a linear combination\n\n$$ {\\rm cov}(aX + bY, cW + dZ) $$\n\n(where $a,b,c,d$ are constants and $X,Y,W,Z$ are random variables) can be decomposed as\n\n$$ \nac\\cdot {\\rm cov}(X,W) + \nad\\cdot {\\rm cov}(X,Z) + \nbc\\cdot {\\rm cov}(Y,W) + \nbd\\cdot {\\rm cov}(Y,Z) $$\n\nIn the example you've given, you can use this property to write $\\textrm{cov}(X,aA + bB + cC + dD)$ as\n\n$$ a\\ {\\rm cov}(X, A) +b\\ {\\rm cov}(X, B) +c\\ {\\rm cov}(X, C) +d\\ {\\rm cov}(X, D) $$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/411706/402101 \n Both the standard Normal and Student t distributions are rather poor approximations to the distribution of\n\n$$Z = \\frac{\\hat p - p}{\\sqrt{\\hat p(1-\\hat p)/n}}$$\n\nfor small \n$n,$\n so poor that the error dwarfs the differences between these two distributions.\n\nHere is a comparison of all three distributions (omitting the cases where \n$\\hat p$\n or \n$1-\\hat p$\n are zero, where the ratio is undefined) for \n$n=10, p=1/2:$\n\n\n\nThe \"empirical\" distribution is that of \n$Z,$\n which must be discrete because the estimates \n$\\hat p$\n are limited to the finite set \n$\\{0, 1/n, 2/n, \\ldots, n/n\\}.$\n\nThe \n$t$\n distribution appears to do a better job of approximation.\n\nFor \n$n=30$\n and \n$p=1/2,$\n you can see the difference between the standard Normal and Student t distributions is completely negligible:\n\n\n\nBecause the Student t distribution is more complicated than the standard Normal\n (it's really an entire family of distributions indexed by the \"degrees of freedom,\" formerly requiring entire chapters of tables rather than a single page), the standard Normal is used for almost all approximations.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/14528/402101 \n As Rob mentions, this occurs when you have highly correlated variables. The standard example I use is predicting weight from shoe size. You can predict weight equally well with the right or left shoe size. But together it doesn't work out.\n\nBrief simulation example\n\nRSS = 3:10 #Right shoe size\nLSS = rnorm(RSS, RSS, 0.1) #Left shoe size - similar to RSS\ncor(LSS, RSS) #correlation ~ 0.99\n\nweights = 120 + rnorm(RSS, 10*RSS, 10)\n\n##Fit a joint model\nm = lm(weights ~ LSS + RSS)\n\n##F-value is very small, but neither LSS or RSS are significant\nsummary(m)\n\n##Fitting RSS or LSS separately gives a significant result. \nsummary(lm(weights ~ LSS))\n\nRSS = 3:10 #Right shoe size\nLSS = rnorm(RSS, RSS, 0.1) #Left shoe size - similar to RSS\ncor(LSS, RSS) #correlation ~ 0.99\n\nweights = 120 + rnorm(RSS, 10*RSS, 10)\n\n##Fit a joint model\nm = lm(weights ~ LSS + RSS)\n\n##F-value is very small, but neither LSS or RSS are significant\nsummary(m)\n\n##Fitting RSS or LSS separately gives a significant result. \nsummary(lm(weights ~ LSS))",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/501327/402101 \n Yes, because omitted variable bias depends on the underlying causal question you want to ask.\n\nSuppose you are interested in explaining the causal effect of schooling on earnings (just to give an example I have already discussed elsewhere and need not repeat here, see e.g. \nIs the equation \"$Y=\\mathbb{E}[Y|X] + error$\" an identity?\n, \nIs it true that an estimator will always asymptotically be consistent if it is biased in finite samples?\n \nOmitted variable bias: which predictors do I need to include, and why?\n) in \\$, (\n$Y$\n).\n\nIf you now regress earnings in \\$ on earnings in cents (\n$X$\n), the variables will be perfectly dependent, yet you would surely not argue that someone earns, say, 3000$ \nbecause\n he earns 300,000 cents. The regression still suffers from omitted variable bias when your goal is to estimate the causal effect of schooling on earnings.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7206/402101 \n It depends on exactly what you are looking for\n. Below are some brief details and references.\n\nMuch of the literature for approximations centers around the function\n\n$$\nQ(x) = \\int_x^\\infty \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} \\, \\mathrm{d}u\n$$\n\nfor \n$x > 0$\n. This is because the function you provided can be decomposed as a simple difference of the function above (possibly adjusted by a constant). This function is referred to by many names, including \"upper-tail of the normal distribution\", \"right normal integral\", and \"Gaussian \n$Q$\n-function\", to name a few. You'll also see approximations to \nMills' ratio\n, which is\n\n$$\nR(x) = \\frac{Q(x)}{\\varphi(x)}\n$$\n\nwhere \n$\\varphi(x) = (2\\pi)^{-1/2} e^{-x^2 / 2}$\n is the Gaussian pdf.\n\nHere I list some references for various purposes that you might be interested in.\n\nComputational\n\nThe de-facto standard for computing the \n$Q$\n-function or the related complementary error function is\n\nW. J. Cody, \nRational Chebyshev Approximations for the Error Function\n, \nMath. Comp.\n, 1969, pp. 631--637.\n\nEvery\n (self-respecting) implementation uses this paper. (MATLAB, R, etc.)\n\n\"Simple\" Approximations\n\nAbramowitz and Stegun\n have one based on a polynomial expansion of a transformation of the input. Some people use it as a \"high-precision\" approximation. I don't like it for that purpose since it behaves badly around zero. For example, their approximation does \nnot\n yield \n$\\hat{Q}(0) = 1/2$\n, which I think is a big no-no. Sometimes \nbad things\n happen because of this.\n\nBorjesson and Sundberg give a simple approximation which works pretty well for most applications where one only requires a few digits of precision. The \nabsolute relative error\n is never worse than 1%, which is quite good considering its simplicity. The basic approximation is\n\n$$\n\\hat{Q}(x) = \\frac{1}{(1-a) x + a \\sqrt{x^2 + b}} \\varphi(x)\n$$\n\nand their preferred choices of the constants are \n$a = 0.339$\n and \n$b = 5.51$\n. That reference is\n\nP. O. Borjesson and C. E. Sundberg. \nSimple approximations of the error function Q(x) for communications\napplications\n. \nIEEE Trans. Commun.\n, COM-27(3):639\u2013643, March 1979.\n\nHere is a plot of its absolute relative error.\n\n\n\nThe electrical-engineering literature is awash with various such approximations and seem to take an overly intense interest in them. Many of them are poor though or expand to very strange and convoluted expressions.\n\nYou might also look at",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7206/402101 \n P. O. Borjesson and C. E. Sundberg. \nSimple approximations of the error function Q(x) for communications\napplications\n. \nIEEE Trans. Commun.\n, COM-27(3):639\u2013643, March 1979.\n\nHere is a plot of its absolute relative error.\n\n\n\nThe electrical-engineering literature is awash with various such approximations and seem to take an overly intense interest in them. Many of them are poor though or expand to very strange and convoluted expressions.\n\nYou might also look at\n\nW. Bryc. \nA uniform approximation to the right normal integral\n. \nApplied Mathematics and Computation\n,\n127(2-3):365\u2013374, April 2002.\n\nLaplace's continued fraction\n\nLaplace has a beautiful continued fraction which yields successive upper and lower bounds for every value of \n$x > 0$\n. It is, in terms of Mills' ratio,\n\n$$\nR(x) = \\frac{1}{x+}\\frac{1}{x+}\\frac{2}{x+}\\frac{3}{x+}\\cdots ,\n$$\n\nwhere the notation I've used is fairly standard for a \ncontinued fraction\n, i.e., \n$1/(x+1/(x+2/(x+3/(x+\\cdots))))$\n. This expression doesn't converge very fast for small \n$x$\n, though, and it diverges at \n$x = 0$\n.\n\nThis continued fraction actually yields many of the \"simple\" bounds on \n$Q(x)$\n that were \"rediscovered\" in the mid-to-late 1900s. It's easy to see that for a continued fraction in \"standard\" form (i.e., composed of positive integer coefficients), truncating the fraction at odd (even) terms gives an upper (lower) bound.\n\nHence, Laplace tells us immediately that\n\n$$\n\\frac{x}{x^2 + 1} < R(x) < \\frac{1}{x} \\>,\n$$\n\nboth of which are bounds that were \"rediscovered\" in the mid-1900's. In terms of the \n$Q$\n-function, this is equivalent to\n\n$$\n\\frac{x}{x^2 + 1} \\varphi(x) < Q(x) < \\frac{1}{x} \\varphi(x) .\n$$\n\nAn alternative proof of this using simple integration by parts can be found in S. Resnick, \nAdventures in Stochastic Processes\n, Birkhauser, 1992, in Chapter 6 (Brownian motion). The absolute relative error of these bounds is no worse than \n$x^{-2}$\n, as shown in \nthis related answer\n.\n\nNotice, in particular, that the inequalities above immediately imply that \n$Q(x) \\sim \\varphi(x)/x$\n. This fact can be established using L'Hopital's rule as well. This also helps explain the choice of the functional form of the Borjesson-Sundberg approximation. Any choice of \n$a \\in [0,1]$\n maintains the asymptotic equivalence as \n$x \\to \\infty$\n. The parameter \n$b$\n serves as a \"continuity correction\" near zero.\n\nHere is a plot of the \n$Q$\n-function and the two Laplace bounds.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/7206/402101 \n Notice, in particular, that the inequalities above immediately imply that \n$Q(x) \\sim \\varphi(x)/x$\n. This fact can be established using L'Hopital's rule as well. This also helps explain the choice of the functional form of the Borjesson-Sundberg approximation. Any choice of \n$a \\in [0,1]$\n maintains the asymptotic equivalence as \n$x \\to \\infty$\n. The parameter \n$b$\n serves as a \"continuity correction\" near zero.\n\nHere is a plot of the \n$Q$\n-function and the two Laplace bounds.\n\n\n\nC-I. C. Lee has a paper from the early 1990's that does a \"correction\" for small values of \n$x$\n. See\n\nC-I. C. Lee. \nOn Laplace continued fraction for the normal integral\n. \nAnn. Inst. Statist. Math.\n, 44(1):107\u2013120,\nMarch 1992.\n\nDurrett's \nProbability: Theory and Examples\n provides the classical upper and lower bounds on \n$Q(x)$\n on pages 6\u20137 of the 3rd edition. They're meant for larger values of \n$x$\n (say, \n$x > 3$\n) and are asymptotically tight.\n\nHopefully this will get you started. If you have a more specific interest, I might be able to point you somewhere.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/56/402101 \n Here is how I would explain the basic difference to my grandma:\n\nI have misplaced my phone somewhere in the home. I can use the phone locator on the base of the instrument to locate the phone and when I press the phone locator the phone starts beeping.\n\nProblem: Which area of my home should I search?\n\nI can hear the phone beeping. I also have a mental model which helps me identify the area from which the sound is coming. Therefore, upon hearing the beep, I infer the area of my home I must search to locate the phone.\n\nI can hear the phone beeping. Now, apart from a mental model which helps me identify the area from which the sound is coming from, I also know the locations where I have misplaced the phone in the past. So, I combine my inferences using the beeps and my prior information about the locations I have misplaced the phone in the past to identify an area I must search to locate the phone.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/316088/402101 \n One possible choice is the \nbeta distribution\n, but \nre-parametrized\n in terms of mean $\\mu$ and precision $\\phi$, that is, \"for fixed $\\mu$, the larger the value of $\\phi$, the smaller the variance of $y$\" (see Ferrari, and Cribari-Neto, 2004). The probability density function is constructed by replacing the standard parameters of beta distribution with $\\alpha = \\phi\\mu$ and $\\beta = \\phi(1-\\mu)$\n\n$$\nf(y) = \\frac{1}{\\mathrm{B}(\\phi\\mu,\\; \\phi(1-\\mu))}\\; y^{\\phi\\mu-1} (1-y)^{\\phi(1-\\mu)-1}\n$$\n\nwhere $E(Y) = \\mu$ and $\\mathrm{Var}(Y) = \\frac{\\mu(1-\\mu)}{1+\\phi}$.\n\nAlternatively, \nyou can calculate\n appropriate $\\alpha$ and $\\beta$ parameters that would lead to beta distribution with pre-defined mean and variance. However, notice that there are restrictions on possible values of variance that are valid for beta distribution. For me personally, the parametrization using precision is more intuitive (think of $x\\,/\\,\\phi$ proportions in \nbinomially distributed\n $X$, with sample size $\\phi$ and the probability of success $\\mu$).\n\nKumaraswamy distribution\n is another bounded continuous distribution, but it would be harder to re-parametrize like above.\n\nAs others have noticed, it is \nnot\n normal since normal distribution has the $(-\\infty, \\infty)$ support, so at best you could use the \ntruncated normal\n as an approximation.\n\nFerrari, S., & Cribari-Neto, F. (2004). \nBeta regression for modelling rates and proportions.\n Journal of Applied Statistics, 31(7), 799-815.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/240427/402101 \n As essentially discussed in the comments, unbiasedness is a finite sample property, and if it held it would be expressed as\n\n$$E (\\hat \\beta ) = \\beta$$\n\n(where the expected value is the first moment of the finite-sample distribution)\n\nwhile consistency is an asymptotic property expressed as\n\n$$\\text{plim} \\hat \\beta = \\beta$$\n\nThe OP shows that even though OLS in this context is biased, it is still consistent.\n\n$$E (\\hat \\beta ) \\neq \\beta\\;\\;\\; \\text{but}\\;\\;\\; \\text{plim} \\hat \\beta = \\beta$$\n\nNo contradiction here.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/268879/402101 \n Let $x$ by any number.  Consider the event $\\min(X,Y)\\le x$.  It can be expressed as the union of two events\n\n$$\\min(X,Y)\\le x = (X\\le x) \\cup (Y \\le x),$$\n\nshown by the overlapping yellow and green regions in this figure, respectively:\n\n\n\nThe intersection of these events (shown in the bottom left corner where they overlap) obviously is $\\{X\\le x,\\,Y\\le x\\}=\\max(X,Y)\\le x$.  Therefore (by the \nPIE\n),\n\n$$\\Pr\\left(\\min(X,Y)\\le x\\right) = \\Pr(X\\le x) + \\Pr (Y\\le x) - \\Pr\\left(\\max(X,Y)\\le x\\right).$$\n\nAll three probabilities are given directly by $F$\n (answering the main question):\n\n$$\\eqalign{\\Pr\\left(\\min(X,Y)\\le x\\right) &= F_{X,Y}(x,\\infty) + F_{X,Y}(\\infty, x) - F_{X,Y}(x,x)\\\\&= F_X(x) + F_Y(x) - F_{X,Y}(x,x).\\tag{1}}$$\n\nThe use of \"$\\infty$\" as an argument refers to the limit; thus, \ne.g.\n, $F_X(x)=F_{X,Y}(x,\\infty)=\\lim_{y\\to\\infty} F_{X,Y}(x,y).$\n\nThe result can be expressed in terms of the marginal distributions (only) when $X$ and $Y$ are independent, for then $(1)$ becomes\n\n$$\\eqalign{\\Pr\\left(\\min(X,Y)\\le x\\right) &= F_X(x) + F_Y(x) - F_X(x)F_Y(x) \\\\&= 1 - (1-F_X(x))(1-F_Y(x)).\\tag{2}}$$\n\nThe latter expression is recognizable as computing the chance that independent variables $X$ and $Y$ are both \nnot\n less than or equal to $x$, given by $(1-F_X(x))(1-F_Y(x))$: the subtraction from $1$ then gives the complementary chance that at least one of those variables is less than or equal to $x$, which is precisely what $\\min(X,Y)\\le x$ means.  Thus $(1)$ is the natural generalization of $(2)$ to \nall\n bivariate distributions.\n\nAs a final comment, please note that care is needed in the use of \"$\\le$\" and \"$\\lt$\".  They can be interchanged in all the preceding calculations when $F$ is continuous, but otherwise they make a difference.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/33895/402101 \n Causal theory offers another explanation for how two variables could be unconditionally independent yet conditionally dependent. I am not an expert on causal theory and am grateful for any criticism that will correct any misguidance below.\n\nTo illustrate, I will use \ndirected acyclic graphs\n (DAG). In these graphs, edges (\n$-$\n) between variables represent direct causal relationships. Arrowheads (\n$\\leftarrow$\n or \n$\\rightarrow$\n) indicate the direction of causal relationships. Thus \n$A \\rightarrow B$\n infers that \n$A$\n directly causes \n$B$\n, and \n$A \\leftarrow B$\n infers that \n$A$\n is directly caused by \n$B$\n. \n$A \\rightarrow B \\rightarrow C$\n is a causal path that infers that \n$A$\n indirectly causes \n$C$\n through \n$B$\n. For simplicity, assume all causal relationships are linear.\n\nFirst, consider a simple example of \nconfounder bias\n:\n\n\n\nHere, a simple bivariable regression will suggest a dependence between \n$X$\n and \n$Y$\n. However, there is no direct causal relationship between \n$X$\n and \n$Y$\n. Instead, both are directly caused by \n$Z$\n, and in the simple bivariable regression, observing \n$Z$\n induces a dependency between \n$X$\n and \n$Y$\n, resulting in bias by confounding. However, a multivariable regression conditioning on \n$Z$\n will remove the bias and suggest no dependence between \n$X$\n and \n$Y$\n.\n\nSecond, consider an example of \ncollider bias\n (also known as Berkson's bias or berksonian bias, of which selection bias is a special type):\n\n\n\nHere, a simple bivariable regression will suggest no dependence between \n$X$\n and \n$Y$\n. This agrees with the DAG, which infers no direct causal relationship between \n$X$\n and \n$Y$\n. However, a multivariable regression conditioning on \n$Z$\n will induce a dependence between \n$X$\n and \n$Y$\n, suggesting that a direct causal relationship between the two variables may exist when in fact, none exist. The inclusion of \n$Z$\n in the multivariable regression results in collider bias.\n\nThird, consider an example of incidental cancellation:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/33895/402101 \n Third, consider an example of incidental cancellation:\n\n\n\nLet us assume that \n$\\alpha$\n, \n$\\beta$\n, and \n$\\gamma$\n are path coefficients and that \n$\\beta = -\\alpha\\gamma$\n. A simple bivariable regression will suggest no dependence between \n$X$\n and \n$Y$\n. Although \n$X$\n is, in fact, a direct cause of \n$Y$\n, the confounding effect of \n$Z$\n on \n$X$\n and \n$Y$\n incidentally cancels out the effect of \n$X$\n on \n$Y$\n. A multivariable regression conditioning on \n$Z$\n will remove the confounding effect of \n$Z$\n on \n$X$\n and \n$Y$\n, allowing for the estimation of the direct effect of \n$X$\n on \n$Y$\n, assuming the DAG of the causal model is correct.\n\nTo summarize:\n\nConfounder example:\n \n$X$\n and \n$Y$\n are dependent in bivariable regression and independent in multivariable regression conditioning on confounder \n$Z$\n.\n\nCollider example:\n \n$X$\n and \n$Y$\n are independent in bivariable regression and dependent in multivariable regression conditioning on collider \n$Z$\n.\n\nIncidental cancellation example:\n \n$X$\n and \n$Y$\n are independent in bivariable regression and dependent in multivariable regression conditioning on confounder \n$Z$\n.\n\nDiscussion:\n\nThe results of your analysis are not compatible with the confounder example but are compatible with both the collider example and the incidental cancellation example. Thus, a potential explanation is that you have incorrectly conditioned on a collider variable in your multivariable regression and have induced an association between \n$X$\n and \n$Y$\n even though \n$X$\n is not a cause of \n$Y$\n and \n$Y$\n is not a cause of \n$X$\n. Alternatively, you might have correctly conditioned on a confounder in your multivariable regression that was incidentally cancelling out the true effect of \n$X$\n on \n$Y$\n in your bivariable regression.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/33895/402101 \n I find using background knowledge to construct causal models helpful when considering which variables to include in statistical models. For example, if previous high-quality randomized studies concluded that \n$X$\n causes \n$Z$\n and \n$Y$\n causes \n$Z$\n, I could make a strong assumption that \n$Z$\n is a collider of \n$X$\n and \n$Y$\n and not condition upon it in a statistical model. However, if I merely had an intuition that \n$X$\n causes \n$Z$\n, and \n$Y$\n causes \n$Z$\n, but no strong scientific evidence to support my intuition, I could only make a weak assumption that \n$Z$\n is a collider of \n$X$\n and \n$Y$\n, as human intuition has a history of being misguided. Subsequently, I would be skeptical of inferring causal relationships between \n$X$\n and \n$Y$\n without further investigating their causal relationships with \n$Z$\n. In lieu of or in addition to background knowledge, there are also algorithms designed to infer causal models from the data using a series of tests of association (e.g. PC algorithm and FCI algorithm, see \nTETRAD\n for Java implementation, \nPCalg\n for R implementation). These algorithms are very interesting, but I would not recommend relying on them without a strong understanding of the power and limitations of causal calculus and causal models in causal theory.\n\nConclusion:\n\nContemplation of causal models does not excuse the investigator from addressing the statistical considerations discussed in other answers here. However, I feel that causal models can nevertheless provide a helpful framework when thinking of potential explanations for observed statistical dependence and independence in statistical models, especially when visualizing potential confounders and colliders.\n\nFurther reading:\n\nGelman, Andrew. 2011. \"\nCausality and Statistical Learning\n.\" Am. J. Sociology 117 (3) (November): 955\u2013966.\n\nGreenland, S, J Pearl, and J M Robins. 1999. \u201c\nCausal Diagrams for Epidemiologic Research\n.\u201d Epidemiology (Cambridge, Mass.) 10 (1) (January): 37\u201348.\n\nGreenland, Sander. 2003. \u201c\nQuantifying Biases in Causal Models: Classical Confounding Vs Collider-Stratification Bias\n.\u201d Epidemiology 14 (3) (May 1): 300\u2013306.\n\nPearl, Judea. 1998. \nWhy There Is No Statistical Test For Confounding, Why Many Think There Is, And Why They Are Almost Right\n.\n\nPearl, Judea. 2009. \nCausality: Models, Reasoning and Inference\n. 2nd ed. Cambridge University Press.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/33895/402101 \n Greenland, Sander. 2003. \u201c\nQuantifying Biases in Causal Models: Classical Confounding Vs Collider-Stratification Bias\n.\u201d Epidemiology 14 (3) (May 1): 300\u2013306.\n\nPearl, Judea. 1998. \nWhy There Is No Statistical Test For Confounding, Why Many Think There Is, And Why They Are Almost Right\n.\n\nPearl, Judea. 2009. \nCausality: Models, Reasoning and Inference\n. 2nd ed. Cambridge University Press.\n\nSpirtes, Peter, Clark Glymour, and Richard Scheines. 2001. \nCausation, Prediction, and Search\n, Second Edition. A Bradford Book.\n\nUpdate:\n Judea Pearl discusses the theory of causal inference and the need to incorporate causal inference into introductory statistics courses in the \nNovember 2012 edition of Amstat News\n. His \nTuring Award Lecture\n, entitled \"The mechanization of causal inference: A 'mini' Turing Test and beyond\" is also of interest.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/179537/402101 \n Correlation measures linear association between two given variables and it has no obligation to detect any other form of association else.\n\nSo those two variables might be associated in several other non-linear ways and correlation could not distinguish from independent case.\n\nAs a very didactic, artificial and non realistic example, one can consider $X$  such that $P(X=x)=1/3$ for $x=-1, 0, 1$ and $Y=X^2$. Notice that they are not only associated, but one is a function of the other. Nonetheless, their correlation is 0, for their association is orthogonal to the association that correlation can detect.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/45240/402101 \n When I see panel data, I think longitudinal data, so observations collected on the same individuals at multiple times, on the same topics. Repeated cross sections should be the same topics, but you get different samples of individuals at each observation.  I'd welcome other descriptions.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/21901/402101 \n I think A and E aren't a good combination, because A says you should pick Mercy and E says you should pick Hope.\n\nA and D have the virtue of advocating the same choice.  But, lets examine the line of reasoning in D in further detail, since that seems to be the confusion.  The probability of success for the surgeries follows the same ordering at both hospitals, with the A type being most likely to be successful and the E type being the least likely.  If we collapse over (i.e., ignore) the hospitals, we can see that the marginal probability of success for the surgeries is:\n\nType     A     B     C     D     E     All  \nProb   .81   .78   .56   .21   .08     .52\n\nType     A     B     C     D     E     All  \nProb   .81   .78   .56   .21   .08     .52\n\nBecause E is \nmuch\n less likely to be successful, it is reasonable to imagine that it is more difficult (although in the real world, other possibilities exist as well).  We can extend that line of thinking to the other four types also.  Now lets look at what proportion of each hospital's total surgeries are of each type:\n\nType     A     B     C     D     E  \nMercy  .08   .39   .06   .44   .03  \nHope   .09   .54   .23   .09   .05\n\nType     A     B     C     D     E  \nMercy  .08   .39   .06   .44   .03  \nHope   .09   .54   .23   .09   .05\n\nWhat we notice here is that Hope tends to do more of the easier surgeries A-C (and especially B & C), and fewer of the harder surgeries like D.  E is pretty uncommon in both hospitals, but, for what it's worth, Hope actually does a higher percentage.  Nonetheless, the Simpson's Paradox effect is going to mostly be driven by B-D here (not actually column E as answer choice D suggested).\n\nSimpson's Paradox occurs because the surgeries vary in difficulty (in general) \nand also\n because the N's differ.  It is the differing base rates of the different types of surgeries that makes this counter-intuitive.  What is happening would be easy to see if both hospitals did exactly the same number of each type of surgery.  We can do that by simply calculating the success probabilities and multiplying by 100; this adjusts for the different frequencies:\n\nType     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225\n\nType     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/21901/402101 \n Type     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225\n\nType     A     B     C     D     E     All  \nMercy   81    79    60    21    09     250  \nHope    80    76    51    14    04     225\n\nNow, because both hospitals did 100 of each surgery (500 total), the answer is obvious: Mercy is the better hospital.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/125016/402101 \n I'm a practitioner, both producer and user of forecasting and NOT a trained statistician. Below I share some of my thoughts on why your mean forecast turned out better than ARIMA by referring to research article that rely on empirical evidence.  One book that time and time again I go back to refer is the \nPrinciples of Forecasting\n book by Armstrong and its \nwebsite\n which I would recommend as an excellent read for any forecaster, provides great insight on usage and guiding principles of extrapolation methods.\n\nTo answer you first question\n  - What I want to know is if this is unusual?\n\nThere is a chapter called Extrapolation for Time-Series and Cross-Sectional Data which also available free in the same \nwebsite\n. The following is the quote from the chapter\n\n\"For example, in the real-time M2-competition, which examined 29\n  monthly series, Box-Jenkins proved to be one of the least-accurate\n  methods and its overall median error was 17% greater than that for a\n  naive forecast\"\n\nThere lies an empirical evidence on why your mean forecasts was better than ARIMA models.\n\nThere is also been study after study in empirical competitions and the third \nM3 competition\n that show Box - Jenkins ARIMA approach fails to produce accurate forecast and lacks evidence that it performs better for univariate trend extrapolation.\n\nThere is also another paper and an ongoing study by Greene and Armstrong entitled \"\nSimple Forecasting: Avoid Tears Before Bedtime\n\" in the same website. The authors of the paper summarize as follows:\n\nIn total we identified 29 papers incorporating 94 formal comparisons\n  of the  accuracy of forecasts from complex methods with those from\n  simple\u2014but not in all cases  sophisticatedly simple\u2014methods.\n  Eighty-three percent of the comparisons found that forecasts from\n  simple methods were more accurate than, or similarly accurate to,\n  those  from complex methods. On average, the errors of forecasts from\n  complex methods were about 32 percent greater than the errors of\n  forecasts from simple methods in the 21 studies that provide\n  comparisons of errors\n\nTo answer your third question\n: does this indicate that I've set something up wrong?\nNo, I would aconsider ARIMA as complex method and Mean forecast as simple methods. There is ample evidence that simple methods like Mean forecast outperform complex methods like ARIMA.\n\nTo answer your second question\n: Does this mean the times series I'm using are strange?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/125016/402101 \n To answer your third question\n: does this indicate that I've set something up wrong?\nNo, I would aconsider ARIMA as complex method and Mean forecast as simple methods. There is ample evidence that simple methods like Mean forecast outperform complex methods like ARIMA.\n\nTo answer your second question\n: Does this mean the times series I'm using are strange?\n\nBelow are what I considered to be experts in real world forecasting:\n\nMakridakis (Pioneered Empirical competition on Forecasting called M, M2 and M3, and paved way for evidence based methods in forecasting)\n\n\nArmstrong (Provides valuable insights in the form of books/articles on Forecasting Practice)\n\n\nGardner (Invented Damped Trend exponential smoothing another simple method which works surprisingly well vs. ARIMA)\n\nAll of the above researchers advocate, simplicity (methods like your mean forecast)   vs. Complex methods like ARIMA. So you should feel comfortable that your forecasts are good and always favor simplicity over complexity based on empirical evidence. These researchers have  all contributed immensely to the field of applied forecasting.\n\nIn addition to Stephan's good list of simple forecasting method. there is also another method called \nTheta forecasting method\n which is a very simple method (basically Simple Exponential smoothing with a drift that equal 1/2 the slope of linear regression)  I would add this to your toolbox. \nForecast package in R\n implements this method.\n\nForecast package in R",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/156352/402101 \n Pearson correlation \nis\n used to look at correlation between series ... but being time series the correlation is looked at across different lags -- the \ncross-correlation function\n.\n\nThe cross-correlation is impacted by dependence within-series, so in many cases\n$^{\\dagger}$\n the within-series dependence should be removed first. So to use this correlation, rather than \nsmoothing\n the series, it's actually more common (because it's meaningful) to look at dependence between residuals - the rough part that's left over after a suitable model is found for the variables.\n\nYou probably want to begin with some basic resources on time series models before delving into trying to figure out whether a Pearson correlation across (presumably) non-stationary, smoothed series is interpretable.\n\nIn particular, you'll probably want to look into the phenomenon \nhere\n.\n\n[Edit -- the Wikipedia landscape keeps changing; the above para. should probably be revised to reflect what's there now.]\n\ne.g. see some discussions\n\n[Why Do We Sometimes Get Nonsense-Correlations between Time Series? A Study in Sampling and the Nature of Time Series][1]  (the opening quote of Yule, in a paper presented in 1925 but published the following year, summarizes the problem quite well)\n\n\n\n\nChristos Agiakloglou and Apostolos Tsimpanos, \nSpurious Correlations for Stationary AR(1) Processes\n \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.611.5055&rep=rep1&type=pdf\n  (this shows that you can even get the problem between stationary series; hence the tendency to pre-whiten)\n\n\n\n\nThe classic reference of Yule, (1926) [1] mentioned above.\n\n[Why Do We Sometimes Get Nonsense-Correlations between Time Series? A Study in Sampling and the Nature of Time Series][1]  (the opening quote of Yule, in a paper presented in 1925 but published the following year, summarizes the problem quite well)\n\nChristos Agiakloglou and Apostolos Tsimpanos, \nSpurious Correlations for Stationary AR(1) Processes\n \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.611.5055&rep=rep1&type=pdf\n  (this shows that you can even get the problem between stationary series; hence the tendency to pre-whiten)\n\nThe classic reference of Yule, (1926) [1] mentioned above.\n\nYou may also find the discussion \nhere\n useful, as well as the discussion \nhere\n\n--\n\nUsing Pearson correlation in a meaningful way between time series is difficult and sometimes surprisingly subtle.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/156352/402101 \n The classic reference of Yule, (1926) [1] mentioned above.\n\nYou may also find the discussion \nhere\n useful, as well as the discussion \nhere\n\n--\n\nUsing Pearson correlation in a meaningful way between time series is difficult and sometimes surprisingly subtle.\n\nI looked up spurious correlation, but I don't care if my A series is the cause of my B series or vice versa. I only want to know if you can learn something about series A by looking at what series B is doing (or vice versa). In other words - do they have an correlation.\n\nTake note of my previous comment about the narrow use of the term spurious correlation in the Wikipedia article.\n\nThe point about spurious correlation is that series can \nappear\n correlated, but the correlation itself is not meaningful. Consider two people tossing two distinct coins counting number of heads so far minus number of tails so far as the value of their series.\n\n(So if person 1 tosses \n$\\text{HTHH...}$\n they have 3-1 = 2 for the value at the 4th time step, and their series goes \n$1, 0, 1, 2,...$\n.)\n\nObviously there's no connection between the two series. \nClearly\n neither can tell you the first thing about the other!\n\nBut look at the sort of correlations you get between pairs of coins:\n\n\n\nIf I didn't tell you what those were, and you took any pair of those series by themselves, those would be impressive correlations would they not?\n\nBut they're all \nmeaningless\n. Utterly spurious. None of the three pairs are really any more positively or negatively related to each other than any of the others -- its just \ncumulated noise\n (and for people thinking an edit would improve this, yes, I really do mean to write, \ncumulated\n, as in the inverse of differencing, not \naccumulated\n, which would be the total, please desist from 'fixing' this). The \nspuriousness\n isn't just about prediction, the whole \nnotion\n of considering association between series without taking account of the within-series dependence is misplaced.\n\nAll\n you have here is \nwithin-series\n dependence. There's no actual cross-series relation at all.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/156352/402101 \n All\n you have here is \nwithin-series\n dependence. There's no actual cross-series relation at all.\n\nOnce you deal properly with the issue that makes these series auto-dependent - they're all integrated (\nBernoulli random walks\n), so you need to difference (yes, it's definitely \ndifference\n NOT \ndifferentiate\n, I do not understand why people keep replacing a perfectly correct word with a very-much-incorrect word in my various time series posts, and other people blithely approve it) them - the \"apparent\" association disappears (the largest absolute cross-series correlation of the three is 0.048).\n\nWhat that tells you is the truth -- the apparent association is a mere illusion caused by the dependence within-series.\n\nYour question asked \"how to use Pearson correlation correctly with time series\" -- so please understand: if there's within-series dependence and you \ndon't\n deal with it first, you won't be using it correctly.\n\nFurther, \nsmoothing\n won't reduce the problem of serial dependence; quite the opposite -- it makes it even worse! Here are the correlations after smoothing (default loess smooth - of series vs index - performed in R):\n\ncoin1      coin2     \ncoin2   0.9696378 \ncoin3  -0.8829326 -0.7733559\n\ncoin1      coin2     \ncoin2   0.9696378 \ncoin3  -0.8829326 -0.7733559\n\nThey all got further from 0. They're \nall still nothing but meaningless noise\n, though now it's smoothed, accumulated noise. (By smoothing, we reduce the variability in the series we put into the correlation calculation, so that may be why the correlation goes up.)\n\n${\\dagger}$\n \nCointegrated series\n are an obvious exception.\n\n[1]: Yule, G.U. (1926) \"Why do we Sometimes get Nonsense-Correlations between Time-Series?\" \nJ.Roy.Stat.Soc.\n, \n89\n, \n1\n, pp. 1-63 (\nhttps://www.math.mcgill.ca/~dstephens/OldCourses/204-2007/Handouts/Yule1926.pdf\n)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/81783/402101 \n $\\hat\\beta_n=(XX^T+\u03bbI)^{\u22121} \\sum\\limits_{i=0}^{n-1} x_iy_i$\n\nLet \n$M_n^{-1} = (XX^T+\u03bbI)^{\u22121}$\n, then\n\n$\\hat\\beta_{n+1}=M_{n+1}^{\u22121} (\\sum\\limits_{i=0}^{n-1} x_iy_i + x_ny_n)$\n , and\n\n$M_{n+1} - M_n = x_nx_n^T$\n,  we can get\n\n$\\hat\\beta_{n+1}=\\hat\\beta_{n}+M_{n+1}^{\u22121} x_n(y_n - x_n^T\\hat\\beta_{n})$\n\nAccording to \nWoodbury formula\n, we have\n\n$M_{n+1}^{-1} = M_{n}^{-1} - \\frac{M_{n}^{-1}x_nx_n^TM_{n}^{-1}}{(1+x_n^TM_n^{-1}x_n)}$\n\nAs a result,\n\n$\\hat\\beta_{n+1}=\\hat\\beta_{n}+\\frac{M_{n}^{\u22121}}{1 + x_n^TM_n^{-1}x_n} x_n(y_n - x_n^T\\hat\\beta_{n})$\n\nPolyak averaging\n indicates you can use \n$\\eta_n = n^{-\\alpha}$\n\nto approximate \n$\\frac{M_{n}^{\u22121}}{1 + x_n^TM_n^{-1}x_n}$\n with \n$\\alpha$\n ranges from \n$0.5$\n to \n$1$\n. You may try in your case to select the best \n$\\alpha$\n for your recursion.\n\nI think it also works if you apply a batch gradient algorithm:\n\n$\\hat\\beta_{n+1}=\\hat\\beta_{n}+\\frac{\\eta_n}{n} \\sum\\limits_{i=0}^{n-1}x_i(y_i - x_i^T\\hat\\beta_{n})$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/268847/402101 \n Time-series (or other intrinsically ordered data) can be problematic for cross-validation.  If some pattern emerges in year 3 and stays for years 4-6, then your model can pick up on it, even though it wasn't part of years 1 & 2.\n\nAn approach that's sometimes more principled for time series is forward chaining, where your procedure would be something like this:\n\nfold 1 : training [1], test [2]\n\n\nfold 2 : training [1 2], test [3]\n\n\nfold 3 : training [1 2 3], test [4]\n\n\nfold 4 : training [1 2 3 4], test [5]\n\n\nfold 5 : training [1 2 3 4 5], test [6]\n\nThat more accurately models the situation you'll see at prediction time, where you'll model on past data and predict on forward-looking data.  It also will give you a sense of the dependence of your modeling on data size.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/389445/402101 \n There are (at least) three senses in which a regression can be considered \"linear.\"\n  To distinguish them, let's start with an extremely general regression model\n\n$$Y = f(X,\\theta,\\varepsilon).$$\n\nTo keep the discussion simple, take the independent variables \n$X$\n to be fixed and accurately measured (rather than random variables).  They model \n$n$\n observations of \n$p$\n attributes each, giving rise to the \n$n$\n-vector of responses \n$Y$\n.  Conventionally, \n$X$\n is represented as an \n$n\\times p$\n matrix and \n$Y$\n as a column \n$n$\n-vector.  The (finite \n$q$\n-vector) \n$\\theta$\n comprises the \nparameters\n.  \n$\\varepsilon$\n is a vector-valued random variable.  It usually has \n$n$\n components, but sometimes has fewer.  The function \n$f$\n is vector-valued (with \n$n$\n components to match \n$Y$\n) and is usually assumed continuous in its last two arguments (\n$\\theta$\n and \n$\\varepsilon$\n).\n\nThe archetypal example\n, of fitting a line to \n$(x,y)$\n data, is the case where \n$X$\n is a vector of numbers \n$(x_i,\\,i=1,2,\\ldots,n)$\n--the x-values; \n$Y$\n is a parallel vector of \n$n$\n numbers \n$(y_i)$\n; \n$\\theta = (\\alpha,\\beta)$\n gives the intercept \n$\\alpha$\n and slope \n$\\beta$\n; and \n$\\varepsilon = (\\varepsilon_1,\\varepsilon_2,\\ldots,\\varepsilon_n)$\n is a vector of \"random errors\" whose components are independent (and usually assumed to have identical but unknown distributions of mean zero).  In the preceding notation,\n\n$$y_i = \\alpha + \\beta x_i +\\varepsilon_i = f(X,\\theta,\\varepsilon)_i$$\n\nwith \n$\\theta = (\\alpha,\\beta)$\n.\n\nThe regression function may be linear in any (or all) of its three arguments:\n\n\"Linear regression, or a \"linear model,\" ordinarily means that \n$f$\n is linear as a function of the \nparameters\n \n$\\theta$\n.  The \nSAS meaning of \"nonlinear regression\"\n is in this sense, with the added assumption that \n$f$\n is differentiable in its second argument (the parameters). This assumption makes it easier to find solutions.\n\n\n\n\nA \"linear relationship between \n$X$\n and \n$Y$\n\" means \n$f$\n is linear as a\nfunction of \n$X$\n.\n\n\n\n\nA model has \nadditive errors\n when \n$f$\n is linear in \n$\\varepsilon$\n.\nIn such cases it is \nalways\n assumed that \n$\\mathbb{E}(\\varepsilon) =\n   0$\n.  (Otherwise, it wouldn't be right to think of \n$\\varepsilon$\n as\n\"errors\" or \"deviations\" from \"correct\" values.)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/389445/402101 \n A \"linear relationship between \n$X$\n and \n$Y$\n\" means \n$f$\n is linear as a\nfunction of \n$X$\n.\n\n\n\n\nA model has \nadditive errors\n when \n$f$\n is linear in \n$\\varepsilon$\n.\nIn such cases it is \nalways\n assumed that \n$\\mathbb{E}(\\varepsilon) =\n   0$\n.  (Otherwise, it wouldn't be right to think of \n$\\varepsilon$\n as\n\"errors\" or \"deviations\" from \"correct\" values.)\n\n\"Linear regression, or a \"linear model,\" ordinarily means that \n$f$\n is linear as a function of the \nparameters\n \n$\\theta$\n.  The \nSAS meaning of \"nonlinear regression\"\n is in this sense, with the added assumption that \n$f$\n is differentiable in its second argument (the parameters). This assumption makes it easier to find solutions.\n\nA \"linear relationship between \n$X$\n and \n$Y$\n\" means \n$f$\n is linear as a\nfunction of \n$X$\n.\n\nA model has \nadditive errors\n when \n$f$\n is linear in \n$\\varepsilon$\n.\nIn such cases it is \nalways\n assumed that \n$\\mathbb{E}(\\varepsilon) =\n   0$\n.  (Otherwise, it wouldn't be right to think of \n$\\varepsilon$\n as\n\"errors\" or \"deviations\" from \"correct\" values.)\n\nEvery possible combination of these characteristics can happen and is useful.\n  Let's survey the possibilities.\n\nA linear model of a linear relationship with additive errors.\n  This is ordinary (multiple) regression, already exhibited above and more generally written as\n\n\n$$Y = X\\theta + \\varepsilon.$$\n\n\n$X$\n has been augmented, if necessary, by adjoining a column of constants, and \n$\\theta$\n is a \n$p$\n-vector.\n\n\n\n\nA linear model of a nonlinear relationship with additive errors.\n  This can be couched as a multiple regression by augmenting the columns of \n$X$\n with nonlinear functions of \n$X$\n itself.  For instance,\n\n\n$$y_i = \\alpha + \\beta x_i^2 + \\varepsilon$$\n\n\nis of this form.  It is linear in \n$\\theta=(\\alpha,\\beta)$\n; it has additive errors; and it is linear in the values \n$(1,x_i^2)$\n even though \n$x_i^2$\n is a nonlinear function of \n$x_i$\n.\n\n\n\n\nA linear model of a linear relationship with nonadditive errors.\n  An example is multiplicative error,\n\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/389445/402101 \n $$y_i = \\alpha + \\beta x_i^2 + \\varepsilon$$\n\n\nis of this form.  It is linear in \n$\\theta=(\\alpha,\\beta)$\n; it has additive errors; and it is linear in the values \n$(1,x_i^2)$\n even though \n$x_i^2$\n is a nonlinear function of \n$x_i$\n.\n\n\n\n\nA linear model of a linear relationship with nonadditive errors.\n  An example is multiplicative error,\n\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i.$$\n\n\n(In such cases the \n$\\varepsilon_i$\n can be interpreted as \"multiplicative errors\" when the location of \n$\\varepsilon_i$\n is \n$1$\n.  However, the proper sense of location is not necessarily the expectation \n$\\mathbb{E}(\\varepsilon_i)$\n anymore: it might be the median or the geometric mean, for instance.  A similar comment about location assumptions applies, \nmutatis mutandis\n, in all other non-additive-error contexts too.)\n\n\n\n\nA linear model of a nonlinear relationship with nonadditive errors.\n \nE.g.\n,\n\n\n$$y_i = (\\alpha + \\beta x_i^2)\\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a linear relationship with additive errors.\n  A nonlinear model involves combinations of its parameters that not only are nonlinear, \nthey cannot even be linearized by re-expressing the parameters.\n\n\n\n\nAs a \nnon-example,\n consider\n\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\n\n\n\nAs an \nexample,\n consider\n\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\n\n\n\n\n\nIt is impossible to find a new parameter \n$\\alpha^\\prime$\n, depending on \n$\\alpha$\n, that will linearize this as a function of \n$\\alpha^\\prime$\n (while keeping it linear in \n$x_i$\n as well).\n\n\n\n\nA nonlinear model of a nonlinear relationship with additive errors.\n\n\n$$y_i = \\alpha + \\alpha^2 x_i^2 + \\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a linear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i)\\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a nonlinear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i^2)\\varepsilon_i.$$\n\nA linear model of a linear relationship with additive errors.\n  This is ordinary (multiple) regression, already exhibited above and more generally written as\n\n$$Y = X\\theta + \\varepsilon.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/389445/402101 \n $$y_i = \\alpha + \\alpha^2 x_i^2 + \\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a linear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i)\\varepsilon_i.$$\n\n\n\n\nA nonlinear model of a nonlinear relationship with nonadditive errors.\n\n\n$$y_i = (\\alpha + \\alpha^2 x_i^2)\\varepsilon_i.$$\n\nA linear model of a linear relationship with additive errors.\n  This is ordinary (multiple) regression, already exhibited above and more generally written as\n\n$$Y = X\\theta + \\varepsilon.$$\n\n$X$\n has been augmented, if necessary, by adjoining a column of constants, and \n$\\theta$\n is a \n$p$\n-vector.\n\nA linear model of a nonlinear relationship with additive errors.\n  This can be couched as a multiple regression by augmenting the columns of \n$X$\n with nonlinear functions of \n$X$\n itself.  For instance,\n\n$$y_i = \\alpha + \\beta x_i^2 + \\varepsilon$$\n\nis of this form.  It is linear in \n$\\theta=(\\alpha,\\beta)$\n; it has additive errors; and it is linear in the values \n$(1,x_i^2)$\n even though \n$x_i^2$\n is a nonlinear function of \n$x_i$\n.\n\nA linear model of a linear relationship with nonadditive errors.\n  An example is multiplicative error,\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i.$$\n\n(In such cases the \n$\\varepsilon_i$\n can be interpreted as \"multiplicative errors\" when the location of \n$\\varepsilon_i$\n is \n$1$\n.  However, the proper sense of location is not necessarily the expectation \n$\\mathbb{E}(\\varepsilon_i)$\n anymore: it might be the median or the geometric mean, for instance.  A similar comment about location assumptions applies, \nmutatis mutandis\n, in all other non-additive-error contexts too.)\n\nA linear model of a nonlinear relationship with nonadditive errors.\n \nE.g.\n,\n\n$$y_i = (\\alpha + \\beta x_i^2)\\varepsilon_i.$$\n\nA nonlinear model of a linear relationship with additive errors.\n  A nonlinear model involves combinations of its parameters that not only are nonlinear, \nthey cannot even be linearized by re-expressing the parameters.\n\nAs a \nnon-example,\n consider\n\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\n\n\n\nAs an \nexample,\n consider\n\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\nAs a \nnon-example,\n consider",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/389445/402101 \n As a \nnon-example,\n consider\n\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\n\n\n\nAs an \nexample,\n consider\n\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\nAs a \nnon-example,\n consider\n\n$$y_i = \\alpha\\beta + \\beta^2 x_i + \\varepsilon_i.$$\n\nBy defining \n$\\alpha^\\prime = \\alpha\\beta$\n and \n$\\beta^\\prime=\\beta^2$\n, and restricting \n$\\beta^\\prime \\ge 0$\n, this model can be rewritten\n\n$$y_i = \\alpha^\\prime + \\beta^\\prime x_i + \\varepsilon_i,$$\n\nexhibiting it as a linear model (of a linear relationship with additive errors).\n\nAs an \nexample,\n consider\n\n$$y_i = \\alpha + \\alpha^2 x_i + \\varepsilon_i.$$\n\nIt is impossible to find a new parameter \n$\\alpha^\\prime$\n, depending on \n$\\alpha$\n, that will linearize this as a function of \n$\\alpha^\\prime$\n (while keeping it linear in \n$x_i$\n as well).\n\nA nonlinear model of a nonlinear relationship with additive errors.\n\n$$y_i = \\alpha + \\alpha^2 x_i^2 + \\varepsilon_i.$$\n\nA nonlinear model of a linear relationship with nonadditive errors.\n\n$$y_i = (\\alpha + \\alpha^2 x_i)\\varepsilon_i.$$\n\nA nonlinear model of a nonlinear relationship with nonadditive errors.\n\n$$y_i = (\\alpha + \\alpha^2 x_i^2)\\varepsilon_i.$$\n\nAlthough these exhibit eight distinct \nforms\n of regression, they do not constitute a \nclassification system\n because some forms can be converted into others.  A standard example is the conversion of a linear model with nonadditive errors (assumed to have positive support)\n\n$$y_i = (\\alpha + \\beta x_i)\\varepsilon_i$$\n\ninto a linear model of a nonlinear relationship with additive errors via the logarithm,\n\n$$\\log(y_i) = \\mu_i + \\log(\\alpha + \\beta x_i) + (\\log(\\varepsilon_i) - \\mu_i)$$\n\nHere, the log geometric mean \n$\\mu_i = \\mathbb{E}\\left(\\log(\\varepsilon_i)\\right)$\n has been removed from the error terms (to ensure they have zero means, as required) and incorporated into the other terms (where its value will need to be estimated). Indeed, one major reason to re-express the dependent variable \n$Y$\n is to create a model with additive errors.  Re-expression can also linearize \n$Y$\n as a function of either (or both) of the parameters and explanatory variables.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/389445/402101 \n Here, the log geometric mean \n$\\mu_i = \\mathbb{E}\\left(\\log(\\varepsilon_i)\\right)$\n has been removed from the error terms (to ensure they have zero means, as required) and incorporated into the other terms (where its value will need to be estimated). Indeed, one major reason to re-express the dependent variable \n$Y$\n is to create a model with additive errors.  Re-expression can also linearize \n$Y$\n as a function of either (or both) of the parameters and explanatory variables.\n\nCollinearity\n (of the column vectors in \n$X$\n) can be an issue in \nany\n form of regression.  The key to understanding this is to recognize that collinearity leads to difficulties in estimating the parameters.  Abstractly and quite generally, compare two models \n$Y = f(X,\\theta,\\varepsilon)$\n and \n$Y=f(X^\\prime,\\theta,\\varepsilon^\\prime)$\n where \n$X^\\prime$\n is \n$X$\n with one column slightly changed.  If this induces enormous changes in the \nestimates\n \n$\\hat\\theta$\n and \n$\\hat\\theta^\\prime$\n, then obviously we have a problem.  One way in which this problem can arise is in a linear model, linear in \n$X$\n (that is, types (1) or (5) above), where the components of \n$\\theta$\n are in one-to-one correspondence with the columns of \n$X$\n.  When one column is a non-trivial linear combination of the others, the estimate of its corresponding parameter can be any real number at all. That is an extreme example of such sensitivity.\n\nFrom this point of view it should be clear that \ncollinearity is a potential problem for linear models of nonlinear relationships\n (regardless of the additivity of the errors) and that this generalized concept of collinearity is potentially a problem in any regression model.  When you have redundant variables, you will have problems identifying some parameters.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/2519/402101 \n It is not true.  If the null hypothesis is true then it will not be rejected more frequently at large sample sizes than small. There is an erroneous rejection rate that's usually set to 0.05 (alpha) but it is independent of sample size. Therefore, taken literally the statement is false. Nevertheless, it's possible that in some situations (even whole fields) all nulls are false and therefore all will be rejected if N is high enough. But is this a bad thing?\n\nWhat is true is that trivially small effects can be found to be \"significant\" with very large sample sizes.  That does not suggest that you shouldn't have such large samples sizes.  What it means is that the way you interpret your finding is dependent upon the effect size and sensitivity of the test.  If you have a very small effect size and highly sensitive test you have to recognize that the statistically significant finding may not be meaningful or useful.\n\nGiven some people don't believe that a test of the null hypothesis, when the null is \ntrue\n, always has an error rate equal to the cutoff point selected for any sample size, here's a simple simulation in \nR\n proving the point. Make N as large as you like and the rate of Type I errors will remain constant.\n\nR\n\n# number of subjects in each condition\nn <- 100\n# number of replications of the study in order to check the Type I error rate\nnsamp <- 10000\n\nps <- replicate(nsamp, {\n    # population mean = 0, sd = 1 for both samples, therefore, no real effect\n    y1 <- rnorm(n, 0, 1) \n    y2 <- rnorm(n, 0, 1)\n    tt <- t.test(y1, y2, var.equal = TRUE)\n    tt$p.value\n})\nsum(ps < .05) / nsamp\n\n# ~ .05 no matter how big n is. Note particularly that it is \n# not an increasing value always finding effects when n is very large.\n\n# number of subjects in each condition\nn <- 100\n# number of replications of the study in order to check the Type I error rate\nnsamp <- 10000\n\nps <- replicate(nsamp, {\n    # population mean = 0, sd = 1 for both samples, therefore, no real effect\n    y1 <- rnorm(n, 0, 1) \n    y2 <- rnorm(n, 0, 1)\n    tt <- t.test(y1, y2, var.equal = TRUE)\n    tt$p.value\n})\nsum(ps < .05) / nsamp\n\n# ~ .05 no matter how big n is. Note particularly that it is \n# not an increasing value always finding effects when n is very large.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/208861/402101 \n Here is a guide to solving this problem (and others like it).\n  I use simulated values to illustrate, so let's begin by simulating a large number of independent realizations from the distribution with density $f$.  (All the code in this answer is written in \nR\n.)\n\nR\n\nn <- 4e4 # Number of trials in the simulation\nx <- matrix(pmax(runif(n*3), runif(n*3)), nrow=3)\n\n# Plot the data\npar(mfrow=c(1,3))\nfor (i in 1:3) {\n  hist(x[i, ], freq=FALSE, main=paste(\"i =\", i))\n  curve(f(x), add=TRUE, col=\"Red\", lwd=2)\n}\n\nn <- 4e4 # Number of trials in the simulation\nx <- matrix(pmax(runif(n*3), runif(n*3)), nrow=3)\n\n# Plot the data\npar(mfrow=c(1,3))\nfor (i in 1:3) {\n  hist(x[i, ], freq=FALSE, main=paste(\"i =\", i))\n  curve(f(x), add=TRUE, col=\"Red\", lwd=2)\n}\n\n\n\nThe histograms show $40,000$ independent realizations of the first, second, and third elements of the datasets.  The red curves graph $f$.  That they coincide with the histograms confirms the simulation is working as intended.\n\nYou need to work out the joint density of $(Y_1, Y_2, Y_3)$.\n Since you're studying order statistics, this should be routine--but the code gives some clues, because it plots their distributions for reference.\n\ny <- apply(x, 2, sort)\n\n# Plot the order statistics.\nf <- function(x) 2*x\nff <- function(x) x^2\nfor (i in 1:3) {\n  hist(y[i, ], freq=FALSE, main=paste(\"i =\", i))\n  k <- factorial(3) / (factorial(3-i)*factorial(1)*factorial(i-1))\n  curve(k * (1-ff(x))^(3-i) * f(x) * ff(x)^(i-1), add=TRUE, col=\"Red\", lwd=2)\n}\n\ny <- apply(x, 2, sort)\n\n# Plot the order statistics.\nf <- function(x) 2*x\nff <- function(x) x^2\nfor (i in 1:3) {\n  hist(y[i, ], freq=FALSE, main=paste(\"i =\", i))\n  k <- factorial(3) / (factorial(3-i)*factorial(1)*factorial(i-1))\n  curve(k * (1-ff(x))^(3-i) * f(x) * ff(x)^(i-1), add=TRUE, col=\"Red\", lwd=2)\n}\n\n\n\nThe same data have been reordered within each of the $40,000$ datasets.  On the left is the histogram of their minima $Y_1$, on the right their maxima $Y_3$, and in the middle their medians $Y_2$.\n\nNext, compute the joint distribution of $(U_1, U_2)$ directly.\n  By definition this is\n\n$$F(u_1, u_2) = \\Pr(U_1 \\le u_1, U_2 \\le u_2) = \\Pr(Y_1 \\le u_1 Y_2, Y_2 \\le u_2 Y_3).$$\n\nSince you have computed the joint density of $(Y_1, Y_2, Y_3)$, this is a routine matter of doing the (triple) integral expressed by the right-hand probability.  The region of integration must be $$0 \\le Y_1 \\le u_1 Y_2,\\ 0 \\le Y_2 \\le u_2 Y_3,\\ 0 \\le Y_3 \\le 1.$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/208861/402101 \n Next, compute the joint distribution of $(U_1, U_2)$ directly.\n  By definition this is\n\n$$F(u_1, u_2) = \\Pr(U_1 \\le u_1, U_2 \\le u_2) = \\Pr(Y_1 \\le u_1 Y_2, Y_2 \\le u_2 Y_3).$$\n\nSince you have computed the joint density of $(Y_1, Y_2, Y_3)$, this is a routine matter of doing the (triple) integral expressed by the right-hand probability.  The region of integration must be $$0 \\le Y_1 \\le u_1 Y_2,\\ 0 \\le Y_2 \\le u_2 Y_3,\\ 0 \\le Y_3 \\le 1.$$\n\nThe simulation can give us an inkling of how $(U_1, U_2)$ are distributed: here is a scatterplot of the realized values of $(U_1, U_2)$.  Your theoretical answer should describe this density.\n\npar(mfrow=c(1,1))\nu <- cbind(y[1, ]/y[2, ], y[2, ]/y[3, ])\nplot(u, pch=16, cex=1/2, col=\"#00000008\", asp=1)\n\npar(mfrow=c(1,1))\nu <- cbind(y[1, ]/y[2, ], y[2, ]/y[3, ])\nplot(u, pch=16, cex=1/2, col=\"#00000008\", asp=1)\n\n\n\nAs a check, we may look at the marginal distributions and compare them to the theoretical solutions.  The marginal densities, shown as red curves, are obtained as $\\partial F(u_1, 1)/\\partial u_1$ and  $\\partial F(1, u_2)/\\partial u_2$.\n\npar(mfrow=c(1,2))\nhist(u[, 1], freq=FALSE); curve(2*x, add=TRUE, col=\"Red\", lwd=2)\nhist(u[, 2], freq=FALSE); curve(4*x^3, add=TRUE, col=\"Red\", lwd=2)\npar(mfrow=c(1,1))\n\npar(mfrow=c(1,2))\nhist(u[, 1], freq=FALSE); curve(2*x, add=TRUE, col=\"Red\", lwd=2)\nhist(u[, 2], freq=FALSE); curve(4*x^3, add=TRUE, col=\"Red\", lwd=2)\npar(mfrow=c(1,1))\n\n\n\nIt is curious that $U_1$ has the same distribution as the original $X_i$.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/31248/402101 \n We'll start with two definitions:\n\nA \nprobability density function (pdf)\n is a non-negative function that integrates to $1$. \n\n\nThe likelihood is defined as the joint density of the observed data as a function of the parameter. But, as pointed out by the reference to Lehmann made by @whuber in a comment below, \nthe likelihood function is a function of the parameter only, with the data held as a fixed constant.\n So the fact that it is a density as a function of the data is irrelevant.\n\nA \nprobability density function (pdf)\n is a non-negative function that integrates to $1$.\n\nThe likelihood is defined as the joint density of the observed data as a function of the parameter. But, as pointed out by the reference to Lehmann made by @whuber in a comment below, \nthe likelihood function is a function of the parameter only, with the data held as a fixed constant.\n So the fact that it is a density as a function of the data is irrelevant.\n\nTherefore, the likelihood function is not a pdf because \nits integral with respect to the parameter does not necessarily equal 1\n (and may not be integrable at all, actually, as pointed out by another comment from @whuber).\n\nTo see this, we'll use a simple example. Suppose you have a single observation, $x$, from a ${\\rm Bernoulli}(\\theta)$ distribution. Then the likelihood function is\n\n$$ L(\\theta) = \\theta^{x} (1 - \\theta)^{1-x} $$\n\nIt is a fact that $\\int_{0}^{1} L(\\theta) d \\theta = 1/2$. Specifically, if $x = 1$, then $L(\\theta) = \\theta$, so $$\\int_{0}^{1} L(\\theta) d \\theta = \\int_{0}^{1} \\theta \\  d \\theta = 1/2$$\n\nand a similar calculation applies when $x = 0$. Therefore, $L(\\theta)$ cannot be a density function.\n\nPerhaps even more important than this technical example showing why the likelihood isn't a probability density is to point out that \nthe likelihood is \nnot\n the probability of the parameter value being correct\n or anything like that - \nit is the probability (density) \nof the data given the parameter value\n, which is a completely different thing. Therefore one should not expect the likelihood function to behave like a probability density.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/163413/402101 \n Let us imagine that you want to infer some parameter \n$\\beta$\n from some observed input-output pairs \n$(x_1,y_1)\\dots,(x_N,y_N)$\n. Let us assume that the outputs are linearly related to the inputs via \n$\\beta$\n and that the data are corrupted by some noise \n$\\epsilon$\n:\n\n$$y_n = \\beta x_n + \\epsilon,$$\n\nwhere \n$\\epsilon$\n is Gaussian noise with mean \n$0$\n and variance \n$\\sigma^2$\n.\nThis gives rise to a Gaussian likelihood:\n\n$$\\prod_{n=1}^N \\mathcal{N}(y_n|\\beta x_n,\\sigma^2).$$\n\nLet us regularise parameter \n$\\beta$\n by imposing the Gaussian prior \n$\\mathcal{N}(\\beta|0,\\lambda^{-1}),$\n where \n$\\lambda$\n is a strictly positive scalar (\n$\\lambda$\n quantifies of by how much we believe that \n$\\beta$\n should be close to zero, i.e. it controls the strength of the regularisation).\nHence, combining the likelihood and the prior we simply have:\n\n$$\\prod_{n=1}^N \\mathcal{N}(y_n|\\beta x_n,\\sigma^2) \\mathcal{N}(\\beta|0,\\lambda^{-1}).$$\n\nLet us take the logarithm of the above expression. Dropping some constants we get:\n\n$$\\sum_{n=1}^N -\\frac{1}{\\sigma^2}(y_n-\\beta x_n)^2 - \\lambda \\beta^2 + \\mbox{const}.$$\n\nIf we maximise the above expression with respect to \n$\\beta$\n, we get the so called maximum a-posteriori estimate for \n$\\beta$\n, or MAP estimate for short. In this expression it becomes apparent why the Gaussian prior can be interpreted as a L2 regularisation term.\n\nThe relationship between the L1 norm and the Laplace prior can be understood in the same fashion. Instead of a Gaussian prior, multiply your likelihood with a Laplace prior and then take the logarithm.\n\nA good reference (perhaps slightly advanced) detailing both issues is the paper \"Adaptive Sparseness for Supervised Learning\", which currently does not seem easy to find online. Alternatively look at \n\"Adaptive Sparseness using Jeffreys Prior\"\n. Another good reference is \n\"On Bayesian classification with Laplace priors\"\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/89532/402101 \n You should think of the algorithm as producing draws from a random variable, to show that the algorithm works, it suffices to show that the algorithm draws from the random variable you want it to.\n\nLet $X$ and $Y$ be scalar random variables with pdfs $f_X$ and $f_Y$ respectively, where $Y$ is something we already know how to sample from. We can also know that we can bound $f_X$ by $Mf_Y$ where $M\\ge1$.\n\nWe now form a new random variable $A$ where $A | y \\sim \\text{Bernoulli } \\left (\\frac{f_X(y)}{Mf_Y(y)}\\right )$, this takes the value $1$ with probability $\\frac{f_X(y)}{Mf_Y(y)} $ and $0$ otherwise. This represents the algorithm 'accepting' a draw from $Y$.\n\nNow we run the algorithm and collect all the draws from $Y$ that are accepted, lets call this random variable $Z = Y|A=1$.\n\nTo show that $Z \\equiv X$, for any event $E$, we must show that $P(Z \\in E) =P(X \\in E)$.\n\nSo let's try that, first use Bayes' rule:\n\n$P(Z \\in E) = P(Y \\in E | A =1) = \\frac{P(Y \\in E \\& A=1)}{P(A=1)}$,\n\nand the top part we write as\n\n\\begin{align*}P(Y \\in E \\& A=1) &= \\int_E f_{Y, A}(y,1) \\, dy \\\\ &= \\int_E f_{A|Y}(1,y)f_Y(y) \\, dy =\\int_E f_Y(y) \\frac{f_X(y)}{Mf_Y(y)} \\, dy  =\\frac{P(X \\in E)}{M}.\\end{align*}\n\nAnd then the bottom part is simply\n\n$P(A=1) = \\int_{-\\infty}^{\\infty}f_{Y,A}(y,1) \\, dy = \\frac{1}{M}$,\n\nby the same reasoning as above, setting $E=(-\\infty, +\\infty)$.\n\nAnd these combine to give $P(X \\in E)$, which is what we wanted, $Z \\equiv X$.\n\nThat is how the algorithm works, but at the end of your question you seem to be concerned about a more general idea, that is when does an empirical distribution converge to the distribution sampled from? This is a general phenomenon concerning any sampling whatsoever if I understand you correctly.\n\nIn this case, let $X_1, \\dots, X_n$ be iid random variables all with distribution $\\equiv X$. Then for any event $E$, $\\frac{\\sum_{i=1}^n1_{X_i \\in E}}{n}$ has expectation $P(X \\in E)$ by the linearity of expectation.\n\nFurthermore, given suitable assumptions you could use the \nstrong law of large numbers\n to show that the empirical probability converges almost surely to the true probability.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86965/402101 \n The topic you are asking about is \nmulticollinearity\n.  You might want to read some of the threads on CV categorized under the \nmulticollinearity\n tag.  @whuber's \nanswer linked above\n in particular is also worth your time.\n\nThe assertion that \"if two predictors are correlated and both are included in a model, one will be insignificant\", is not correct.  If there is a real effect of a variable, the probability that variable will be significant is a function of several things, such as the magnitude of the effect, the magnitude of the error variance, the variance of the variable itself, the amount of data you have, and the number of other variables in the model.  Whether the variables are correlated is also relevant, but it doesn't override these facts.  Consider the following simple demonstration in \nR\n:\n\nR\n\nlibrary(MASS)    # allows you to generate correlated data\nset.seed(4314)   # makes this example exactly replicable\n\n# generate sets of 2 correlated variables w/ means=0 & SDs=1\nX0 = mvrnorm(n=20,   mu=c(0,0), Sigma=rbind(c(1.00, 0.70),    # r=.70\n                                            c(0.70, 1.00)) )\nX1 = mvrnorm(n=100,  mu=c(0,0), Sigma=rbind(c(1.00, 0.87),    # r=.87\n                                            c(0.87, 1.00)) )\nX2 = mvrnorm(n=1000, mu=c(0,0), Sigma=rbind(c(1.00, 0.95),    # r=.95\n                                            c(0.95, 1.00)) )\ny0 = 5 + 0.6*X0[,1] + 0.4*X0[,2] + rnorm(20)    # y is a function of both\ny1 = 5 + 0.6*X1[,1] + 0.4*X1[,2] + rnorm(100)   #  but is more strongly\ny2 = 5 + 0.6*X2[,1] + 0.4*X2[,2] + rnorm(1000)  #  related to the 1st\n\n# results of fitted models (skipping a lot of output, including the intercepts)\nsummary(lm(y0~X0[,1]+X0[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X0[, 1]       0.6614     0.3612   1.831   0.0847 .     # neither variable\n# X0[, 2]       0.4215     0.3217   1.310   0.2075       #  is significant\nsummary(lm(y1~X1[,1]+X1[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X1[, 1]      0.57987    0.21074   2.752  0.00708 **    # only 1 variable\n# X1[, 2]      0.25081    0.19806   1.266  0.20841       #  is significant\nsummary(lm(y2~X2[,1]+X2[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X2[, 1]      0.60783    0.09841   6.177 9.52e-10 ***   # both variables\n# X2[, 2]      0.39632    0.09781   4.052 5.47e-05 ***   #  are significant",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86965/402101 \n library(MASS)    # allows you to generate correlated data\nset.seed(4314)   # makes this example exactly replicable\n\n# generate sets of 2 correlated variables w/ means=0 & SDs=1\nX0 = mvrnorm(n=20,   mu=c(0,0), Sigma=rbind(c(1.00, 0.70),    # r=.70\n                                            c(0.70, 1.00)) )\nX1 = mvrnorm(n=100,  mu=c(0,0), Sigma=rbind(c(1.00, 0.87),    # r=.87\n                                            c(0.87, 1.00)) )\nX2 = mvrnorm(n=1000, mu=c(0,0), Sigma=rbind(c(1.00, 0.95),    # r=.95\n                                            c(0.95, 1.00)) )\ny0 = 5 + 0.6*X0[,1] + 0.4*X0[,2] + rnorm(20)    # y is a function of both\ny1 = 5 + 0.6*X1[,1] + 0.4*X1[,2] + rnorm(100)   #  but is more strongly\ny2 = 5 + 0.6*X2[,1] + 0.4*X2[,2] + rnorm(1000)  #  related to the 1st\n\n# results of fitted models (skipping a lot of output, including the intercepts)\nsummary(lm(y0~X0[,1]+X0[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X0[, 1]       0.6614     0.3612   1.831   0.0847 .     # neither variable\n# X0[, 2]       0.4215     0.3217   1.310   0.2075       #  is significant\nsummary(lm(y1~X1[,1]+X1[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X1[, 1]      0.57987    0.21074   2.752  0.00708 **    # only 1 variable\n# X1[, 2]      0.25081    0.19806   1.266  0.20841       #  is significant\nsummary(lm(y2~X2[,1]+X2[,2]))\n#             Estimate Std. Error t value Pr(>|t|)    \n# X2[, 1]      0.60783    0.09841   6.177 9.52e-10 ***   # both variables\n# X2[, 2]      0.39632    0.09781   4.052 5.47e-05 ***   #  are significant\n\nThe correlation between the two variables is lowest in the first example and highest in the third, yet neither variable is significant in the first example and both are in the last example.  The magnitude of the effects is identical in all three cases, and the variances of the variables and the errors should be similar (they are stochastic, but drawn from populations with the same variance).  The pattern we see here is due primarily to my manipulating the $N$s for each case.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86965/402101 \n The correlation between the two variables is lowest in the first example and highest in the third, yet neither variable is significant in the first example and both are in the last example.  The magnitude of the effects is identical in all three cases, and the variances of the variables and the errors should be similar (they are stochastic, but drawn from populations with the same variance).  The pattern we see here is due primarily to my manipulating the $N$s for each case.\n\nThe key concept to understand to resolve your questions is the \nvariance inflation factor\n (VIF).  The VIF is how much the variance of your regression coefficient is larger than it would otherwise have been if the variable had been completely uncorrelated with all the other variables in the model.  Note that the VIF is a multiplicative factor, if the variable in question is uncorrelated the VIF=1.  A simple understanding of the VIF is as follows: you could fit a model predicting a variable (say, $X_1$) from all other variables in your model (say, $X_2$), and get a multiple $R^2$.  The VIF for $X_1$ would be $1/(1-R^2)$.  Let's say the VIF for $X_1$ were $10$ (often considered a threshold for excessive multicollinearity), then the variance of the sampling distribution of the regression coefficient for $X_1$ would be $10\\times$ larger than it would have been if $X_1$ had been completely uncorrelated with all the other variables in the model.\n\nThinking about what would happen if you included both correlated variables vs. only one is similar, but slightly more complicated than the approach discussed above.  This is because not including a variable means the model uses less degrees of freedom, which changes the residual variance and everything computed from that (including the variance of the regression coefficients).  In addition, if the non-included variable really is associated with the response, the variance in the response due to that variable will be included into the residual variance, making it larger than it otherwise would be.  Thus, several things change simultaneously (the variable is correlated or not with another variable, and the residual variance), and the precise effect of dropping / including the other variable will depend on how those trade off.  The best way to think through this issue is based on the counterfactual of how the model would differ if the variables were uncorrelated instead of correlated, rather than including or excluding one of the variables.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/86965/402101 \n Armed with an understanding of the VIF, here are the answers to your questions:\n\nBecause the variance of the sampling distribution of the regression coefficient would be larger (by a factor of the VIF) if it were correlated with other variables in the model, the p-values would be higher (i.e., less significant) than they otherwise would.  \n\n\nThe variances of the regression coefficients would be larger, as already discussed.  \n\n\nIn general, this is hard to know without solving for the model.  Typically, if only one of two is significant, it will be the one that had the stronger bivariate correlation with $Y$.  \n\n\nHow the predicted values and their variance would change is quite complicated.  It depends on how strongly correlated the variables are and the manner in which they appear to be associated with your response variable in your data.  Regarding this issue, it may help you to read my answer here: \nIs there a difference between 'controlling for' and 'ignoring' other variables in multiple regression?",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/13647/402101 \n The dichotomy between the cases $d < 3$ and $d \\geq 3$ for the admissibility of the MLE of the mean of a $d$-dimensional multivariate normal random variable is certainly shocking.\n\nThere is another very famous example in probability and statistics in which there is a dichotomy between the $d < 3$ and $d \\geq 3$ cases. This is the recurrence of a simple random walk on the lattice $\\mathbb{Z}^d$. That is, the $d$-dimensional simple random walk is recurrent in 1 or 2 dimensions, but is transient in $d \\geq 3$ dimensions. The continuous-time analogue (in the form of Brownian motion) also holds.\n\nIt turns out that the two are closely related.\n\nLarry Brown\n proved that the two questions are essentially equivalent. That is, the best invariant estimator $\\hat{\\mu} \\equiv \\hat{\\mu}(X) = X$ of a $d$-dimensional multivariate normal mean vector is admissible if and only if the $d$-dimensional Brownian motion is recurrent.\n\nIn fact, his results go \nmuch\n further. For \nany\n sensible (i.e., generalized Bayes) estimator $\\tilde{\\mu} \\equiv \\tilde{\\mu}(X)$ with bounded (generalized) $L_2$ risk, there is an explicit(!) corresponding $d$-dimensional diffusion such that the estimator $\\tilde{\\mu}$ is admissible if and only if its corresponding diffusion is recurrent.\n\nThe local mean of this diffusion is essentially the discrepancy between the two estimators, i.e., $\\tilde{\\mu} - \\hat{\\mu}$ and the covariance of the diffusion is $2 I$. From this, it is easy to see that for the case of the MLE $\\tilde{\\mu} = \\hat{\\mu} = X$, we recover (rescaled) Brownian motion.\n\nSo, in some sense, we can view the question of admissibility through the lens of stochastic processes and use well-studied properties of diffusions to arrive at the desired conclusions.\n\nReferences\n\nL. Brown (1971). \nAdmissible estimators, recurrent diffusions, and insoluble boundary value problems\n. \nAnn. Math. Stat.\n, vol. 42, no. 3, pp. 855\u2013903. \n\n\nR. N. Bhattacharya (1978). \nCriteria for recurrence and existence of invariant measures for multidimensional diffusions\n. \nAnn. Prob.\n, vol. 6, no. 4, 541\u2013553.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/493551/402101 \n No, those equations come directly from the mean and variance formulae in terms of expected value, considering the collected data as a population.\n\n$$\\mu = \\mathbb{E}\\big[X\\big]$$\n\n$$\\sigma^2 = \\mathbb{E}\\big[\\big(X-\\mu\\big)^2\\big]$$\n\nSince you have a finite number of observations, the distribution is discrete,\n$^{\\dagger}$\n and the expected value is a sum.\n\n$$\\mu = \\mathbb{E}\\big[X\\big] = \\sum_{i=1}^N p(x_i)x_i = \\sum_{i=1}^N \\dfrac{1}{N}x_i = \\dfrac{1}{N}\\sum_{i=1}^Nx_i$$\n\n$$\\sigma^2 = \\mathbb{E}\\big[\\big(X-\\mu\\big)^2\\big] = \\sum_{i=1}^N p(x_i)(x_i - \\mu)^2 = \\sum_{i=1}^N \\dfrac{1}{N}(x_i - \\mu)^2 = \\dfrac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2$$\n\n(To get from \n$p(x_i)$\n to \n$\\dfrac{1}{N}$\n, note that each individual \n$x_i$\n has probability \n$1/N$\n.)\n\nThis is why the \n$\\dfrac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2$\n gets called the \"population\" variance. It literally is the population variance if you consider the observed data to be the population.\n\n$^{\\dagger}$\n This is a sufficient, but not necessary, condition for a discrete distribution. A Poisson distribution is an example of a discrete distribution with infinitely many values.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/71963/402101 \n Coming from a behavioural sciences background, I associate this terminology particularly with introductory statistics textbooks. In this context the distinction is that :\n\nDescriptive statistics\n  are functions of  the sample data that are intrinsically interesting in describing some feature of the data. Classic descriptive statistics include mean, min, max, standard deviation, median, skew, kurtosis.\n\n\nInferential statistics\n are a function of the sample data that assists you to draw an inference regarding an hypothesis about a population parameter. Classic inferential statistics include z, t, $\\chi^2$, F-ratio, etc.\n\nThe important point is that any statistic, inferential or descriptive, is a function of the sample data. A parameter is a function of the population, where the term population is the same as saying the underlying data generating process.\n\nFrom this perspective the status of a given function of the data as a descriptive or inferential statistic depends on the purpose for which you are using it.\n\nThat said, some statistics are clearly more useful in describing  relevant features of the data, and some are well suited to aiding inference.\n\nInferential statistics:\n  Standard test statistics like t and z, for a given data generating process, where the null hypothesis is false, the expected value is strongly influenced by sample size. Most researchers would not see such statistics as estimating a population parameter of intrinsic interest.\n\n\nDescriptive statistics\n: In contrast descriptive statistics do estimate population parameters that are typically of intrinsic interest. For example the sample mean and standard deviation provide estimates of the equivalent population parameters. Even descriptive statistics like the minimum and maximum provide information about equivalent or similar population parameters, although of course in this case, much more care is required. Furthermore, many descriptive statistics might be biased or otherwise less than ideal estimators. However, they still have some utility in estimating a population parameter of interest.\n\nSo from this perspective, the important things to understand are:\n\nstatistic\n: function of the sample data\n\n\nparameter\n: function of the population (data generating process)\n\n\nestimator\n: function of the sample data used to provide an estimate of a parameter\n\n\ninference\n: process of reaching a conclusion about a parameter",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/71963/402101 \n So from this perspective, the important things to understand are:\n\nstatistic\n: function of the sample data\n\n\nparameter\n: function of the population (data generating process)\n\n\nestimator\n: function of the sample data used to provide an estimate of a parameter\n\n\ninference\n: process of reaching a conclusion about a parameter\n\nThus, you could either define the distinction between descriptive and inferential based on the intention of the researcher using the statistic, or you could define a statistic based on how it is typically used.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/73472/402101 \n It is a measure of precision just as $\\Sigma$ is a measure of dispersion.\n\nMore elaborately, $\\Sigma$ is a measure of how the variables are dispersed around the mean (the diagonal elements) and how they co-vary with other variables (the off-diagonal) elements. The more the dispersion the farther apart they are from the mean and the more they co-vary (in absolute value) with the other variables the stronger is the tendency for them to 'move together' (in the same or opposite direction depending on the sign of the covariance).\n\nSimilarly,  $\\Sigma^{-1}$ is a measure of how tightly clustered the variables are around the mean (the diagonal elements) and the extent to which they do not co-vary with the other variables (the off-diagonal elements). Thus, the higher the diagonal element, the tighter the variable is clustered around the mean. The interpretation of the off-diagonal elements is more subtle and I refer you to the other answers for that interpretation.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4287/402101 \n Imagine some 2D data--let's say height versus weight for students at a high school--plotted on a pair of axes.\n\nNow suppose you fit a straight line through it. This line, which of course represents a set of predicted values, has zero statistical variance. But the bias is (probably) high--i.e., it doesn't fit the data very well.\n\nNext, suppose you model the data with a high-degree polynomial spline. You're not satisfied with the fit, so you increase the polynomial degree until the fit improves (and it will, to arbitrary precision, in fact). Now you have a situation with bias that tends to zero, but the variance is very high.\n\nNote that the bias-variance trade-off doesn't describe a proportional relationship--i.e., if you plot bias versus variance you won't necessarily see a straight line through the origin with slope -1. In the polynomial spline example above, reducing the degree almost certainly increases the variance much less than it decreases the bias.\n\nThe bias-variance tradeoff is also embedded in the sum-of-squares error function. Below, I have rewritten (but not altered) the usual form of this equation to emphasize this:\n\n$$\nE\\left(\\left(y - \\dot{f}(x)\\right)^2\\right) = \\sigma^2 + \\left[f(x) - \\frac{1}{\\kappa}\\sum_{i=0}^nf(x_n)\\right]^2+\\frac{\\sigma^2}{\\kappa}\n$$\n\nOn the right-hand side, there are three terms: the first of these is just the irreducible error (the variance in the data itself); this is beyond our control so ignore it. The \nsecond\n term is the \nsquare of the bias\n; and the \nthird\n is the \nvariance\n. It's easy to see that as one goes up the other goes down--they can't both vary together in the same direction. Put another way, you can think of least-squares regression as (implicitly) finding the optimal combination of bias and variance from among candidate models.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/57766/402101 \n If we consider \"approximation\" in a fairly general sense we can get somewhere.\n\nWe have to assume not that we have an actual normal distribution but something that's approximately normal except the density cannot be nonzero in a neighborhood of 0.\n\nSo let's say that $a$ is \"approximately normal\" (and concentrated near the mean*) in a sense that we can handwave away the concerns about $a$ coming near 0 (and its subsequent impact on the moments of $\\log(a)$, because $a$ doesn't 'get down near 0'), but with the same low order moments as the specified normal distribution, then we could use \nTaylor series to approximate the moments of the transformed random variable\n.\n\nFor some transformation $g(X)$, this involves expanding $g(\\mu_X + X-\\mu_X)$ as a Taylor series (think $g(x+h)$ where $\\mu_X$ is taking the role of '$x$' and $X-\\mu_X$ takes the role of '$h$') and then taking expectations and then either computing the variance or the expectation of the square of the expansion (from which can be obtained the variance).\n\nThe resulting approximate expectation and variance are:\n\n$\\text{E}\\left[g(X)\\right]\\approx g(\\mu_X) +\\frac{g''(\\mu_X)}{2}\\sigma_X^2$ and\n\n$\\text{Var}\\left[g(X)\\right]\\approx \\left(g'(\\mu_X)\\right)^2\\sigma^2_X$\n\nand so (if I didn't make any errors), when $g() = \\log()$:\n\n$$\\text{E}\\left[\\log(a)\\right]\\approx log(\\mu_a) -\\frac{\\sigma_a^2}{2\\mu_a^2}$$\n\n$$\\text{Var}\\left[\\log(a)\\right]\\approx \\sigma^2_a/\\mu_a^2$$\n\n* For this to be a good approximation you generally want the standard deviation of $a$ to be quite small compared to the mean (low coefficient of variation).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/157705/402101 \n Here's an algorithm to sample from an arbitrary mixture $f(x) = \\frac1N \\sum_{i=1}^N f_i(x)$:\n\nPick a mixture component $i$ uniformly at random.\n\n\nSample from $f_i$.\n\nIt should be clear that this produces an exact sample.\n\nA Gaussian kernel density estimate is a mixture $\\frac1N \\sum_{i=1}^N \\mathcal{N}(x; x_i, h^2)$. So you can take a sample of size $N$ by picking a bunch of $x_i$s and adding normal noise with zero mean and variance $h^2$ to it.\n\nYour code snippet is selecting a bunch of $x_i$s, but then it's doing something slightly different:\n\nchanging $x_i$ to\n$\n\\hat\\mu + \\frac{x_i - \\hat\\mu}{\\sqrt{1 + h^2 / \\hat\\sigma^2}}\n$\n\n\nadding zero-mean normal noise with variance $\\frac{h^2}{1 + h^2/\\hat\\sigma^2} = \\frac{1}{\\frac{1}{h^2} + \\frac{1}{\\hat\\sigma^2}}$, the harmonic mean of $h^2$ and $\\sigma^2$.\n\nWe can see that the expected value of a sample according to this procedure is\n$$\n\\frac1N \\sum_{i=1}^N \\frac{x_i}{\\sqrt{1 + h^2/\\hat\\sigma^2}}\n+ \\hat\\mu\n- \\frac{1}{\\sqrt{1 + h^2 /\\hat\\sigma^2}} \\hat\\mu\n= \\hat\\mu\n$$\nsince $\\hat\\mu = \\frac1N \\sum_{i=1}^N x_i$.\n\nI don't think the sampling disribution is the same, though.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/192380/402101 \n The saddlepoint approximation to a probability density function (it works likewise for mass functions, but I will only talk here in terms of densities)\nis a surprisingly well working approximation, that can be seen as a refinement on the central limit theorem. So, it will only work in settings where there is a central limit theorem, but it needs stronger assumptions.\n\nWe start with the assumption that the moment generating function exists and is twice differentiable. This implies in particular that all moments exist. Let \n$X$\n be a random variable with moment generating function (mgf)\n\n$$ \\DeclareMathOperator{\\E}{\\mathbb{E}}\n   M(t) = \\E  e^{t X}\n$$\n\nand cgf (cumulant generating function) \n$K(t)=\\log M(t)$\n (where \n$\\log $\n denotes the natural logarithm). In the development I will follow closely Ronald W Butler: \"Saddlepoint Approximations with Applications\" (CUP). We will develop the saddlepoint approximation using the Laplace approximation to a certain integral.  Write\n\n$$\ne^{K(t)} = \\int_{-\\infty}^\\infty e^{t x} f(x) \\; dx =\\int_{-\\infty}^\\infty \\exp(tx+\\log f(x) ) \\; dx \\\\\n    = \\int_{-\\infty}^\\infty \\exp(-h(t,x)) \\; dx\n$$\n\nwhere\n\n$h(t,x) = -tx - \\log f(x) $\n. Now we will Taylor expand \n$h(t,x)$\n in \n$x$\n considering \n$t$\n as a constant. This gives\n\n$$\n  h(t,x)=h(t,x_0) + h'(t,x_0)(x-x_0) +\\frac12 h''(t,x_0) (x-x_0)^2 +\\dotsm \n$$\n\nwhere \n$'$\n denotes differentiation with respect to \n$x$\n. Note that\n\n$$\nh'(t,x)=-t-\\frac{\\partial}{\\partial x}\\log f(x) \\\\\nh''(t,x)= -\\frac{\\partial^2}{\\partial x^2} \\log f(x) > 0\n$$\n\n(the last inequality by assumption as it is needed for the approximation to work). Let \n$x_t$\n be the solution to \n$h'(t,x_t)=0$\n. We will assume that this gives a minimum for  \n$h(t,x)$\n as a function of \n$x$\n.  Using this expansion in the integral and forgetting about the \n$\\dotsm$\n part, gives\n\n$$\ne^{K(t)} \\approx \\int_{-\\infty}^\\infty \\exp(-h(t,x_t)-\\frac12 h''(t,x_t) (x-x_t)^2 ) \\; dx \\\\\n= e^{-h(t,x_t)} \\int_{-\\infty}^\\infty e^{-\\frac12 h''(t,x_t) (x-x_t)^2} \\; dx\n$$\n\nwhich is a Gaussian integral, giving\n\n$$\ne^{K(t)} \\approx e^{-h(t,x_t)} \\sqrt{\\frac{2\\pi}{h''(t,x_t)}}. \n$$\n\nThis gives (a first version) of the saddlepoint approximation as\n\n$$ \nf(x_t) \\approx \\sqrt{\\frac{h''(t,x_t)}{2\\pi}} \\exp(K(t) -t x_t) \\\\\n     \\tag{*} \\label{*}\n$$\n\nNote that the approximation has the form of an exponential family.\n\nNow we need to do some work to get this in a more useful form.\n\nFrom \n$h'(t,x_t)=0$\n we get",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/192380/402101 \n which is a Gaussian integral, giving\n\n$$\ne^{K(t)} \\approx e^{-h(t,x_t)} \\sqrt{\\frac{2\\pi}{h''(t,x_t)}}. \n$$\n\nThis gives (a first version) of the saddlepoint approximation as\n\n$$ \nf(x_t) \\approx \\sqrt{\\frac{h''(t,x_t)}{2\\pi}} \\exp(K(t) -t x_t) \\\\\n     \\tag{*} \\label{*}\n$$\n\nNote that the approximation has the form of an exponential family.\n\nNow we need to do some work to get this in a more useful form.\n\nFrom \n$h'(t,x_t)=0$\n we get\n\n$$\n    t = -\\frac{\\partial}{\\partial x_t} \\log f(x_t).\n$$\n\nDifferentiating this with respect to \n$x_t$\n gives\n\n$$\n \\frac{\\partial t}{\\partial x_t} = -\\frac{\\partial^2}{\\partial x_t^2} \\log f(x_t) > 0$$\n\n(by our assumptions), so the relationship between \n$t$\n and \n$x_t$\n is monotone, so \n$x_t$\n is well defined. We need an approximation to \n$\\frac{\\partial}{\\partial x_t} \\log f(x_t)$\n. To that end, we get by solving from \\eqref{*}\n\n$$\n\\log f(x_t) = K(t) -t x_t -\\frac12 \\log \\frac{2\\pi}{-\\frac{\\partial^2}{\\partial x_t^2} \\log f(x_t)}.   \\tag{**}  \\label{**}\n$$\n\nAssuming the last term above only depends weakly on \n$x_t$\n, so its derivative with respect to \n$x_t$\n is approximately zero (we will come back to comment on this), we get\n\n$$\n\\frac{\\partial \\log f(x_t)}{\\partial x_t} \\approx \n  (K'(t)-x_t) \\frac{\\partial t}{\\partial x_t} - t\n$$\n\nUp to this approximation we then have that\n\n$$\n0 = t + \\frac{\\partial \\log f(x_t)}{\\partial x_t} \\approx (K'(t)-x_t) \\frac{\\partial t}{\\partial x_t}\n$$\n\nso that \n$t$\n and \n$x_t$\n must be related through the equation\n\n$$\nK'(t) - x_t=0, \\\\\n     \\tag{\u00a7} \\label{\u00a7}\n$$\n\nwhich is called the saddlepoint equation.\n\nWhat we miss now in determining \\eqref{*} is\n\n$$\n  h''(t,x_t) = -\\frac{\\partial^2 \\log f(x_t)}{\\partial x_t^2} \\\\\n = -\\frac{\\partial}{\\partial x_t} \\left(\\frac{\\partial \\log f(x_t)}{\\partial x_t}    \\right)\n  \\\\\n= -\\frac{\\partial}{\\partial x_t}(-t)= \\left(\\frac{\\partial x_t}{\\partial t}\\right)^{-1} \n$$\n\nand that we can find by implicit differentiation of the saddlepoint equation \n$K'(t)=x_t$\n:\n\n$$\n\\frac{\\partial x_t}{\\partial t} = K''(t).\n$$\n\nThe result is that (up to our approximation)\n\n$$\nh''(t,x_t) = \\frac1{K''(t)}\n$$\n\nPutting everything together, we have the final saddlepoint approximation of the density \n$f(x)$\n as\n\n$$\n   f(x_t) \\approx e^{K(t)- t x_t} \\sqrt{\\frac1{2\\pi K''(t)}}. \n$$\n\nNow, to use this practically, to approximate the density at a specific point \n$x_t$\n, we solve the saddlepoint equation for that \n$x_t$\n to find \n$t$\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/192380/402101 \n $$\n\\frac{\\partial x_t}{\\partial t} = K''(t).\n$$\n\nThe result is that (up to our approximation)\n\n$$\nh''(t,x_t) = \\frac1{K''(t)}\n$$\n\nPutting everything together, we have the final saddlepoint approximation of the density \n$f(x)$\n as\n\n$$\n   f(x_t) \\approx e^{K(t)- t x_t} \\sqrt{\\frac1{2\\pi K''(t)}}. \n$$\n\nNow, to use this practically, to approximate the density at a specific point \n$x_t$\n, we solve the saddlepoint equation for that \n$x_t$\n to find \n$t$\n.\n\nThe saddlepoint approximation is often stated as an approximation to the density of the mean based on \n$n$\n iid observations \n$X_1, X_2, \\dotsc, X_n$\n.\nThe cumulant generating function of the mean is simply \n$n K(t)$\n, so the saddlepoint approximation for the mean becomes\n\n$$\nf(\\bar{x}_t) = e^{nK(t) - n t \\bar{x}_t} \\sqrt{\\frac{n}{2\\pi K''(t)}}\n$$\n\nLet us look at a first example. What does we get if we try to approximate the standard normal density\n\n$$\nf(x)=\\frac1{\\sqrt{2\\pi}} e^{-\\frac12 x^2}\n$$\n\nThe mgf is \n$M(t)=\\exp(\\frac12 t^2)$\n so\n\n$$\n   K(t)=\\frac12 t^2 \\\\\n   K'(t)=t  \\\\\n   K''(t)=1\n$$\n\nso the saddlepoint equation is \n$t=x_t$\n and the saddlepoint approximation gives\n\n$$\n  f(x_t) \\approx e^{\\frac12 t^2 -t x_t} \\sqrt{\\frac1{2\\pi \\cdot 1}}\n    = \\frac1{\\sqrt{2\\pi}} e^{-\\frac12 x_t^2} \n$$\n\nso in this case the approximation is exact.\n\nLet us look at a very different application: Bootstrap in the transform domain, we can do bootstrapping analytically using the saddlepoint approximation to the bootstrap distribution of the mean!\n\nAssume we have \n$X_1, X_2, \\dotsc, X_n$\n iid distributed from some density \n$f$\n (in the simulated example we will use a unit exponential distribution). From the sample we calculate the empirical moment generating function\n\n$$\n  \\hat{M}(t)= \\frac1{n} \\sum_{i=1}^n e^{t x_i}\n$$\n\nand then the empirical cgf \n$\\hat{K}(t) = \\log \\hat{M}(t)$\n. We need the empirical mgf for the mean which is \n$\\log ( \\hat{M}(t/n)^n )$\n and the empirical cgf for the mean\n\n$$\n  \\hat{K}_{\\bar{X}}(t) = n \\log \\hat{M}(t/n) \n$$\n\nwhich we use to construct a saddlepoint approximation. In the following some R code (R version 3.2.3):\n\nset.seed(1234)\nx  <-  rexp(10)\n\nrequire(Deriv)   ### From CRAN\ndrule[[\"sexpmean\"]]   <-  alist(t=sexpmean1(t))  # adding diff rules to \n                                                 # Deriv\ndrule[[\"sexpmean1\"]]  <-  alist(t=sexpmean2(t))\n\n###",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/192380/402101 \n $$\n  \\hat{K}_{\\bar{X}}(t) = n \\log \\hat{M}(t/n) \n$$\n\nwhich we use to construct a saddlepoint approximation. In the following some R code (R version 3.2.3):\n\nset.seed(1234)\nx  <-  rexp(10)\n\nrequire(Deriv)   ### From CRAN\ndrule[[\"sexpmean\"]]   <-  alist(t=sexpmean1(t))  # adding diff rules to \n                                                 # Deriv\ndrule[[\"sexpmean1\"]]  <-  alist(t=sexpmean2(t))\n\n###\n\nmake_ecgf_mean  <-   function(x)   {\n    n  <-  length(x)\n    sexpmean  <-  function(t) mean(exp(t*x))\n    sexpmean1 <-  function(t) mean(x*exp(t*x))\n    sexpmean2 <-  function(t) mean(x*x*exp(t*x))\n    emgf  <-  function(t) sexpmean(t)\n    ecgf  <-   function(t)  n * log( emgf(t/n) )\n    ecgf1 <-   Deriv(ecgf)\n    ecgf2 <-   Deriv(ecgf1)\n    return( list(ecgf=Vectorize(ecgf),\n                 ecgf1=Vectorize(ecgf1),\n                 ecgf2 =Vectorize(ecgf2) )    )\n}\n\n### Now we need a function solving the saddlepoint equation and constructing\n### the approximation:\n###\n\nmake_spa <-  function(cumgenfun_list) {\n    K  <- cumgenfun_list[[1]]\n    K1 <- cumgenfun_list[[2]]\n    K2 <- cumgenfun_list[[3]]\n    # local function for solving the speq:\n    solve_speq  <-  function(x) {\n          # Returns saddle point!\n          uniroot(function(s) K1(s)-x,lower=-100,\n                  upper = 100, \n                  extendInt = \"yes\")$root\n}\n    # Function finding fhat for one specific x:\n    fhat0  <- function(x) {\n        # Solve saddlepoint equation:\n        s  <-  solve_speq(x)\n        # Calculating saddlepoint density value:\n        (1/sqrt(2*pi*K2(s)))*exp(K(s)-s*x)\n    }\n    # Returning a vectorized version:\n    return(Vectorize(fhat0))\n} #end make_fhat\n\nset.seed(1234)\nx  <-  rexp(10)\n\nrequire(Deriv)   ### From CRAN\ndrule[[\"sexpmean\"]]   <-  alist(t=sexpmean1(t))  # adding diff rules to \n                                                 # Deriv\ndrule[[\"sexpmean1\"]]  <-  alist(t=sexpmean2(t))\n\n###\n\nmake_ecgf_mean  <-   function(x)   {\n    n  <-  length(x)\n    sexpmean  <-  function(t) mean(exp(t*x))\n    sexpmean1 <-  function(t) mean(x*exp(t*x))\n    sexpmean2 <-  function(t) mean(x*x*exp(t*x))\n    emgf  <-  function(t) sexpmean(t)\n    ecgf  <-   function(t)  n * log( emgf(t/n) )\n    ecgf1 <-   Deriv(ecgf)\n    ecgf2 <-   Deriv(ecgf1)\n    return( list(ecgf=Vectorize(ecgf),\n                 ecgf1=Vectorize(ecgf1),\n                 ecgf2 =Vectorize(ecgf2) )    )\n}\n\n### Now we need a function solving the saddlepoint equation and constructing\n### the approximation:\n###",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/192380/402101 \n ### Now we need a function solving the saddlepoint equation and constructing\n### the approximation:\n###\n\nmake_spa <-  function(cumgenfun_list) {\n    K  <- cumgenfun_list[[1]]\n    K1 <- cumgenfun_list[[2]]\n    K2 <- cumgenfun_list[[3]]\n    # local function for solving the speq:\n    solve_speq  <-  function(x) {\n          # Returns saddle point!\n          uniroot(function(s) K1(s)-x,lower=-100,\n                  upper = 100, \n                  extendInt = \"yes\")$root\n}\n    # Function finding fhat for one specific x:\n    fhat0  <- function(x) {\n        # Solve saddlepoint equation:\n        s  <-  solve_speq(x)\n        # Calculating saddlepoint density value:\n        (1/sqrt(2*pi*K2(s)))*exp(K(s)-s*x)\n    }\n    # Returning a vectorized version:\n    return(Vectorize(fhat0))\n} #end make_fhat\n\n( I have tried to write this as general code which can be modified easily for other cgfs, but the code is still not very robust ...)\n\nThen we use this for a sample of ten independent observations from a unit exponential distribution. We do the usual nonparametric bootstrapping \"by hand\", plot the resulting bootstrap histogram for the mean, and overplot the saddlepoint approximation:\n\n> ECGF  <- make_ecgf_mean(x)\n> fhat  <-  make_spa(ECGF)\n> fhat\nfunction (x) \n{\n    args <- lapply(as.list(match.call())[-1L], eval, parent.frame())\n    names <- if (is.null(names(args))) \n        character(length(args))\n    else names(args)\n    dovec <- names %in% vectorize.args\n    do.call(\"mapply\", c(FUN = FUN, args[dovec], MoreArgs = list(args[!dovec]), \n        SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES))\n}\n<environment: 0x4e5a598>\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> hist(boots, prob=TRUE)\n> plot(fhat, from=0.001, to=2, col=\"red\", add=TRUE)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/192380/402101 \n > ECGF  <- make_ecgf_mean(x)\n> fhat  <-  make_spa(ECGF)\n> fhat\nfunction (x) \n{\n    args <- lapply(as.list(match.call())[-1L], eval, parent.frame())\n    names <- if (is.null(names(args))) \n        character(length(args))\n    else names(args)\n    dovec <- names %in% vectorize.args\n    do.call(\"mapply\", c(FUN = FUN, args[dovec], MoreArgs = list(args[!dovec]), \n        SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES))\n}\n<environment: 0x4e5a598>\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> boots  <-  replicate(10000, mean(sample(x, length(x), replace=TRUE)), simplify=TRUE)\n> hist(boots, prob=TRUE)\n> plot(fhat, from=0.001, to=2, col=\"red\", add=TRUE)\n\nGiving the resulting plot:\n\n\n\nThe approximation seems to be rather good!\n\nWe could get an even better approximation by integrating the saddlepoint approximation and rescaling:\n\n> integrate(fhat, lower=0.1, upper=2)\n1.026476 with absolute error < 9.7e-07\n\n> integrate(fhat, lower=0.1, upper=2)\n1.026476 with absolute error < 9.7e-07\n\nNow the cumulative distribution function based on this approximation could be found by numerical integration, but it is also possible to make a direct saddlepoint approximation for that. But that is for another post, this is long enough.\n\nFinally, some comments left out of the development above. In \\eqref{**} we did an approximation essentially ignoring the third term. Why can we do that?  One observation is that for the normal density function,  the left-out term contributes nothing, so that approximation is exact.  So, since the saddlepoint-approximation is a refinement on the central limit theorem, so we are somewhat close to the normal, so this should work well. One can also look at specific examples. Looking at the saddlepoint approximation to the Poisson distribution, looking at that left-out third term, in this case that becomes a trigamma function, which indeed is rather flat when the argument is not to close to zero.\n\nFinally, why the name? The name come from an alternative derivation, using complex-analysis techniques.  Later we can look into that, but in another post!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/11110/402101 \n A solution to this is to utilize a form of penalized regression. In fact, this is the original reason some of the penalized regression forms were developed (although they turned out to have other interesting properties.\n\nInstall and load package glmnet in R and you're mostly ready to go. One of the less user-friendly aspects of glmnet is that you can only feed it matrices, not formulas as we're used to. However, you can look at model.matrix and the like to construct this matrix from a data.frame and a formula...\n\nNow, when you expect that this perfect separation is not just a byproduct of your sample, but could be true in the population, you specifically \ndon't\n want to handle this: use this separating variable simply as the sole predictor for your outcome, not employing a model of any kind.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78911/402101 \n Here is some preliminary list of disadvantages I was able to extract from your comments. Criticism and additions are very welcome!\n\nOverall - compared to ARIMA, state-space models allow you to model more complex processes, have interpretable structure and easily handle data irregularities; but for this you pay with increased complexity of a model, harder calibration, less community knowledge.\n\nARIMA is a universal approximator - you don't care what is the true model behind your data and you use universal ARIMA diagnostic and fitting tools to \napproximate\n this model. It is like a polynomial curve fitting - you don't care what is the true function, you always can approximate it with a polynomial of some degree. \n\n\nState-space models naturally require you to write-down \nsome\n reasonable model for your process (which is good - you use your prior knowledge of your process to improve estimates). Of course, if you don't have any idea of your process, you always can use some \nuniversal\n state-space model also - e.g. represent ARIMA in a state-space form. But then ARIMA in its original form has more parsimonious formulation - without introducing unnecessary hidden states.\n\n\nBecause there is such a great variety of state-space models formulations (much richer than class of ARIMA models), behavior of all these potential models is not well studied and if the model you formulated is complicated - it's hard to say how it will behave under different circumstances. Of course, if your state-space model is simple or composed of interpretable components, there is no such problem. But ARIMA is always the same well studied ARIMA so it should be easier to anticipate its behavior even if you use it to approximate some complex process.\n\n\nBecause state-space allows you directly and exactly model complex/nonlinear models, then for these complex/nonlinear models you may have problems with stability of filtering/prediction (EKF/UKF divergence, particle filter degradation). You may also have problems with calibrating complicated-model's parameters - it's a computationally-hard optimization problem. ARIMA is simple, has less parameters (1 noise source instead of 2 noise sources, no hidden variables) so its calibration is simpler. \n\n\nFor state-space there is less community knowledge and software in statistical community than for ARIMA.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/336568/402101 \n What am I missing here?\n\nI don't think you're really missing anything!\n\nAnother observation is that a sum of subsequent linear regression models can be represented as a single regression model as well (adding all intercepts and corresponding coefficients) so I cannot imagine how that could ever improve the model. The last observation is that a linear regression (the most typical approach) is using sum of squared residuals as a loss function - the same one that GB is using.\n\nSeems to me that you nailed it right there, and gave a short sketch of a proof that linear regression just beats boosting linear regressions in this setting.\n\nTo be pedantic, both methods are attempting to solve the following optimization problem\n\n$$ \\hat \\beta = \\text{argmin}_\\beta (y - X \\beta)^t (y - X \\beta) $$\n\nLinear regression just observes that you can solve it directly, by finding the solution to the linear equation\n\n$$ X^t X \\beta = X^t y $$\n\nThis automatically gives you the best possible value of $\\beta$ out of all possibilities.\n\nBoosting, whether your weak classifier is a one variable or multi variable regression, gives you a sequence of coefficient vectors $\\beta_1, \\beta_2, \\ldots$.  The final model prediction is, as you observe, a sum, and has the same functional form as the full linear regressor\n\n$$ X \\beta_1 + X \\beta_2 + \\cdots + X \\beta_n = X (\\beta_1 + \\beta_2 + \\cdots + \\beta_n) $$\n\nEach of these steps is chosen to further decrease the sum of squared errors.  But we could have found the \nminimum possible\n sum of square errors within this functional form by just performing a full linear regression to begin with.\n\nA possible defense of boosting in this situation could be the implicit regularization it provides.  Possibly (I haven't played with this) you could use the early stopping feature of a gradient booster, along with a cross validation, to stop short of the full linear regression. This would provide a regularization to your regression, and possibly help with overfitting.  This is not particularly practical, as one has very efficient and well understood options like ridge regression and the elastic net in this setting.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/336568/402101 \n Boosting shines when there is no terse functional form around.  Boosting decision trees lets the functional form of the regressor/classifier evolve slowly to fit the data, often resulting in complex shapes one could not have dreamed up by hand and eye.  When a simple functional form \nis\n desired, boosting is not going to help you find it (or at least is probably a rather inefficient way to find it).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/70673/402101 \n A footnote in \nPankratz (1983)\n, on page 48, says:\n\nThe label \"moving average\" is technically incorrect since the MA\n  coefficients may be negative and may not sum to unity. This label is\n  used by convention.\n\nBox and Jenkins (1976)\n also says something similar. On page 10:\n\nThe name \"moving average\" is somewhat misleading because the weights\n  $1, -\\theta_{1}, -\\theta_{2}, \\ldots, -\\theta_{q}$, which multiply the\n  $a$'s, need not total unity nor need that be positive. However, this\n  nomenclature is in common use, and therefore we employ it.\n\nI hope this helps.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/141431/402101 \n Suppose you want to find a linear combination of $X_1$ and $X_2$ such that\n\n$$\n\\text{corr}(\\alpha X_1 + \\beta X_2, X_1) = \\rho\n$$\n\nNotice that if you multiply both $\\alpha$ and $\\beta$ by the same (non-zero) constant, the correlation will not change. Thus, we're going to add a condition to preserve variance: $\\text{var}(\\alpha X_1 + \\beta X_2) = \\text{var}(X_1)$\n\nThis is equivalent to\n\n$$\n\\rho\n= \\frac{\\text{cov}(\\alpha X_1 + \\beta X_2, X_1)}{\\sqrt{\\text{var}(\\alpha X_1 + \\beta X_2) \\text{var}(X_1)}}\n= \\frac{\\alpha \\overbrace{\\text{cov}(X_1, X_1)}^{=\\text{var}(X_1)} + \\overbrace{\\beta \\text{cov}(X_2, X_1)}^{=0}}{\\sqrt{\\text{var}(\\alpha X_1 + \\beta X_2) \\text{var}(X_1)}} = \\alpha \\sqrt{\\frac{\\text{var}(X_1)}{\\alpha^2 \\text{var}(X_1) + \\beta^2 \\text{var}(X_2)}}\n$$\n\nAssuming \nboth random variables have the same variance\n (this is a crucial assumption!) ($\\text{var}(X_1) = \\text{var}(X_2)$), we get\n\n$$\n\\rho \\sqrt{\\alpha^2 + \\beta^2} = \\alpha\n$$\n\nThere are many solutions to this equation, so it's time to recall variance-preserving condition:\n\n$$\n\\text{var}(X_1)\n = \\text{var}(\\alpha X_1 + \\beta X_2)\n = \\alpha^2 \\text{var}(X_1) + \\beta^2 \\text{var}(X_2)\n\\Rightarrow \\alpha^2 + \\beta^2 = 1\n$$\n\nAnd this leads us to\n\n$$\n\\alpha = \\rho \\\\\n\\beta = \\pm \\sqrt{1-\\rho^2}\n$$\n\nUPD\n. Regarding the second question: yes, this is known as \nwhitening\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/31038/402101 \n To define the two terms without using too much technical language:\n\nAn estimator is \nconsistent\n if, as the sample size increases, the estimates (produced by the estimator) \"converge\" to the true value of the parameter being estimated. To be slightly more precise - consistency means that, as the sample size increases, the sampling distribution of the estimator becomes increasingly concentrated at the true parameter value.\n\n\n\n\nAn estimator is \nunbiased\n if, on average, it hits the true parameter value. That is, the mean of the sampling distribution of the estimator is equal to the true parameter value.\n\n\n\n\nThe two are not equivalent: \nUnbiasedness\n is a statement about the expected value of the sampling distribution of the estimator. \nConsistency\n is a statement about \"where the sampling distribution of the estimator is going\" as the sample size increases.\n\nAn estimator is \nconsistent\n if, as the sample size increases, the estimates (produced by the estimator) \"converge\" to the true value of the parameter being estimated. To be slightly more precise - consistency means that, as the sample size increases, the sampling distribution of the estimator becomes increasingly concentrated at the true parameter value.\n\nAn estimator is \nunbiased\n if, on average, it hits the true parameter value. That is, the mean of the sampling distribution of the estimator is equal to the true parameter value.\n\nThe two are not equivalent: \nUnbiasedness\n is a statement about the expected value of the sampling distribution of the estimator. \nConsistency\n is a statement about \"where the sampling distribution of the estimator is going\" as the sample size increases.\n\nIt certainly is possible for one condition to be satisfied but not the other - I will give two examples. For both examples consider a sample \n$X_1, ..., X_n$\n from a \n$N(\\mu, \\sigma^2)$\n population.\n\nUnbiased but not consistent:\n Suppose you're estimating \n$\\mu$\n. Then \n$X_1$\n is an unbiased estimator of \n$\\mu$\n since \n$E(X_1) = \\mu$\n. But, \n$X_1$\n is not consistent since its distribution does not become more concentrated around \n$\\mu$\n as the sample size increases - it's always \n$N(\\mu, \\sigma^2)$\n!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/31038/402101 \n Unbiased but not consistent:\n Suppose you're estimating \n$\\mu$\n. Then \n$X_1$\n is an unbiased estimator of \n$\\mu$\n since \n$E(X_1) = \\mu$\n. But, \n$X_1$\n is not consistent since its distribution does not become more concentrated around \n$\\mu$\n as the sample size increases - it's always \n$N(\\mu, \\sigma^2)$\n!\n\n\n\n\nConsistent but not unbiased:\n Suppose you're estimating \n$\\sigma^2$\n. The maximum likelihood estimator is \n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2 $$\n where \n$\\overline{X}$\n is the sample mean. It is a fact that \n$$ E(\\hat{\\sigma}^2) = \\frac{n-1}{n} \\sigma^2 $$\n which can be derived using the information \nhere\n. Therefore \n$\\hat{\\sigma}^2$\n is biased for any finite sample size. We can also easily derive that \n$${\\rm var}(\\hat{\\sigma}^2) = \\frac{ 2\\sigma^4(n-1)}{n^2}$$\n From these facts we can informally see that the distribution of \n$\\hat{\\sigma}^2$\n is becoming more and more concentrated at \n$\\sigma^2$\n as the sample size increases since the mean is converging to \n$\\sigma^2$\n and the variance is converging to \n$0$\n. (\nNote:\n This does constitute a proof of consistency, using the same argument as the one used in the answer \nhere\n)\n\nUnbiased but not consistent:\n Suppose you're estimating \n$\\mu$\n. Then \n$X_1$\n is an unbiased estimator of \n$\\mu$\n since \n$E(X_1) = \\mu$\n. But, \n$X_1$\n is not consistent since its distribution does not become more concentrated around \n$\\mu$\n as the sample size increases - it's always \n$N(\\mu, \\sigma^2)$\n!\n\nConsistent but not unbiased:\n Suppose you're estimating \n$\\sigma^2$\n. The maximum likelihood estimator is \n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2 $$\n where \n$\\overline{X}$\n is the sample mean. It is a fact that \n$$ E(\\hat{\\sigma}^2) = \\frac{n-1}{n} \\sigma^2 $$\n which can be derived using the information \nhere\n. Therefore \n$\\hat{\\sigma}^2$\n is biased for any finite sample size. We can also easily derive that \n$${\\rm var}(\\hat{\\sigma}^2) = \\frac{ 2\\sigma^4(n-1)}{n^2}$$\n From these facts we can informally see that the distribution of \n$\\hat{\\sigma}^2$\n is becoming more and more concentrated at \n$\\sigma^2$\n as the sample size increases since the mean is converging to \n$\\sigma^2$\n and the variance is converging to \n$0$\n. (\nNote:\n This does constitute a proof of consistency, using the same argument as the one used in the answer \nhere\n)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/357224/402101 \n I don't think the formula given in the question can be correct in all cases, it is developed using joint normality.  Without joint normality we can use copulas.  For \n$X,Y$\n random variables with joint distribution with cumulative distribution function \n$F(x,y)$\n and joint density \n$f(x,y)$\n define the transformed random variables (rv) \n$U=F_X(X), V=F_Y(Y)$\n where \n$F_X, F_Y$\n denotes the marginal cumulative distribution functions (cdf). Then the joint distribution of \n$U;V$\n\n\n$$ \\DeclareMathOperator{\\P}{\\mathbb{P}}\n  \\P(U \\le u, V \\le v)=C(u,v)\n$$\n\nis the \ncopula\n of \n$X$\n and \n$Y$\n, with copula density \n$c(u,v)$\n (when it exists).  So, in your setup let us assume that the copula density exists, and for simplicity I will take both \n$X$\n and \n$Y$\n as standard normals.  So what possibility exists for the conditional expectation of \n$X$\n given \n$Y=y$\n ? Using Sklar's theorem we can write the joint density as\n\n$$\n   f(x,y) = c(\\Phi(x),\\Phi(y)) \\phi(x) \\phi(y)\n$$\n\nwhere \n$\\Phi, \\phi$\n are the standard normal cdf, pdf, respectively.  Then the conditional density is given by\n\n$$\n   f(x \\mid y) = \\frac{f(x,y)}{f(y)}=\\frac{c(\\Phi(x),\\Phi(y))\\phi(x)\\phi(y)}{\\phi(y)}= c(\\Phi(x),\\Phi(y))\\phi(x)\n$$\n\nThen we can look at this with various copula functions, see \nhttps://en.wikipedia.org/wiki/Copula_(probability_theory)\n .\n\nThere is a general inequality for copulas\n\n$$\n  W(u,v)=\\max(u+v-1,0) \\le C(u,v) \\le M(u,w)=\\min(u,v)\n$$\n\nwhere both upper and lower limits are copulas (This is the Frechet-Hoeffding bounds). The upper limit isn't very interesting, since it gives \n$\\P(U=V)=1$\n so gives correlation equal 1. The lower limit similarly corresponds to \n$U=1-V$\n with probability one, so correlation is -1.  But for these two extremal copula the conditional expectation function certainly is linear!\n\nLets look at some intermediate cases.  I will use the R package \ncopula\n and some numerical integration to find the conditional expectation function, for the case of the gumbel copula. The code can be simply adapted for other copulas.\n\ncopula\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/357224/402101 \n copula\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )\n\nlibrary(copula)\n    C  <-  gumbelCopula(2)\n    make_cond <-  Vectorize(function(y) {\n        function(x) dCopula( cbind(pnorm(x), pnorm(y)), C)*dnorm(x) \n    }  )\n\nthe last command makes a function representing the conditional density given \n$Y=y$\n. Let us look at how this looks like for three different values of \n$y$\n:\n\ncond_dens <- make_cond(c(-1, 0, 1))\n    plot(cond_dens[[1]],  from=-3,  to=3, col=\"blue\", ylim=c(0, 0.7))\n    plot(cond_dens[[2]],  from=-3,  to=3,  col=\"orange\",  add=TRUE)\n    plot(cond_dens[[3]],  from=-3,  to=3,  col=\"red\",  add=TRUE)\n    title(\"conditional densities for y=-1, 0, 1\")\n\ncond_dens <- make_cond(c(-1, 0, 1))\n    plot(cond_dens[[1]],  from=-3,  to=3, col=\"blue\", ylim=c(0, 0.7))\n    plot(cond_dens[[2]],  from=-3,  to=3,  col=\"orange\",  add=TRUE)\n    plot(cond_dens[[3]],  from=-3,  to=3,  col=\"red\",  add=TRUE)\n    title(\"conditional densities for y=-1, 0, 1\")\n\n\n\nshowing clearly that the conditional distributions now are non-normal. We can also see clearly that the conditional variance is non-constant.\nFor more examples using the \ncopula\n package see \nhttps://www.r-bloggers.com/modelling-dependence-with-copulas-in-r/\n and \nGenerating values from copula using copula package in R\n . Then we can find the conditional expectation function using numerical integration:\n\ncopula\n\nplot(function(y) sapply(make_cond(y), FUN=function(fun)\n                   integrate(function(x) x*fun(x) ,\n                             lower=-Inf,  upper=Inf)$value), \n                              from=-3,  to=3,\n         ylab=\"conditional expectation given y\", xlab=\"y\")\n    title(\"conditional expectation of X given Y=y\")\n\nplot(function(y) sapply(make_cond(y), FUN=function(fun)\n                   integrate(function(x) x*fun(x) ,\n                             lower=-Inf,  upper=Inf)$value), \n                              from=-3,  to=3,\n         ylab=\"conditional expectation given y\", xlab=\"y\")\n    title(\"conditional expectation of X given Y=y\")\n\n\n\nand it is quite clear that the conditional expectation function is not linear!",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/26370/402101 \n As many of the answers above have stated, causation does not imply \nlinear correlation\n. Since a lot of the correlation concepts come from fields that rely heavily on linear statistics, usually correlation is seen as equal to linear correlation. The \nwikipedia article\n is an alright source for this, I really like this image:\n\n\n\nLook at some of the figures in the bottom row, for instance the parabola-ish shape in the 4th example. This is kind of what happens in @StasK answer (with a little bit of noise added). Y can be fully caused by X but if the numeric relationship is not linear and symmetric, you will still have a correlation of 0.\n\nThe word you are looking for is \nmutual information\n: this is sort of the general non-linear version of correlation. In that case, your statement would be true: \ncausation implies high mutual information\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/1150/402101 \n Consider the simplest case where $Y$ is regressed against $X$ and $Z$ and where $X$ and $Z$ are highly positively correlated. Then the effect of $X$ on $Y$ is hard to distinguish from the effect of $Z$ on $Y$ because any increase in $X$ tends to be associated with an increase in $Z$.\n\nAnother way to look at this is to consider the equation. If we write $Y = b_0 + b_1X + b_2Z + e$, then the coefficient $b_1$ is the increase in $Y$ for every unit increase in $X$ while holding $Z$ constant. But in practice, it is often impossible to hold $Z$ constant and the positive correlation between $X$ and $Z$ mean that a unit increase in $X$ is usually accompanied by some increase in $Z$ at the same time.\n\nA similar but more complicated explanation holds for other forms of multicollinearity.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/327040/402101 \n For \n$1 \\leq i < n$\n, denote random vectors \n$(X_1, \\ldots, X_i)$\n and \n$(X_{i + 1}, \\ldots, X_n)$\n by \n$X$\n and \n$Y$\n respectively.  Furthermore, denote product measures \n$\\mu_1 \\times \\cdots \\times \\mu_i$\n (on \n$\\mathcal{X}^i$\n) and \n$\\mu_{i + 1} \\times \\cdots \\times \\mu_n$\n (on \n$\\mathcal{X}^{n - i}$\n) by \n$\\pi_1$\n and \n$\\pi_2$\n respectively. Under these notations, your first equality is then:\n\n\\begin{align*}\nE[f(X, Y)|X] = \\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy). \\tag{1}\n\\end{align*}\n\nAs the right hand side of \n$(1)$\n is a function of \n$X$\n, it is \n$\\sigma(X)$\n-measurable.  To prove \n$(1)$\n, it is therefore sufficient to show that for any \n$G \\in \\sigma(X)$\n, it holds that\n\n\\begin{align}\n\\int_G f(X, Y)dP = \\int_G\\left[\\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy)\\right]dP. \\tag{2}\n\\end{align}\n\nSince every \n$G$\n in \n$\\sigma(X)$\n has the form \n$\\{\\omega \\in \\Omega: X(\\omega) \\in H\\} =: X^{-1}(H)$\n for some \n$H \\in \\mathscr{X}^i$\n (Theorem 20.1, \nProbability and Measure\n by Patrick Billingsley. Here \n$\\mathscr{X}^i$\n denotes the \n$\\sigma$\n-field generated by open sets in \n$\\mathcal{X}^i$\n), \n$(2)$\n is equivalent to:\n\n\n\\begin{align}\n\\int_{X^{-1}(H)}f(X, Y)dP = \\int_{X^{-1}(H)}\\left[\\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy)\\right]dP. \\tag{2}\n\\end{align}\n\nBy the change of variable theorem (Theorem 16.13, \nProbability and Measure\n by Patrick Billingsley. Or refer to \nthis Wikipedia page\n) and the independence of \n$X$\n and \n$Y$\n (hence the distribution of \n$(X, Y)$\n is the product measure \n$\\pi_1 \\times \\pi_2$\n), the left hand side of \n$(2)$\n is:\n\n\\begin{align}\n\\int_{X^{-1}(H)}f(X, Y)dP &= \n\\int_{(X, Y)^{-1}(H \\times \\mathcal{X}^{n - i})}f(X, Y)dP \\\\\n&=\\int_{H \\times \\mathcal{X}^{n - i}} f(x, y) (\\pi_1 \\times \\pi_2)(d(x, y)). \\tag{3}\n\\end{align}\n\nSimilarly, applying the change of variable theorem to the right hand side of \n$(2)$\n yields:\n\n\\begin{align}\n\\int_{X^{-1}(H)}\\left[\\int_{\\mathcal{X}^{n - i}}f(X, y)\\pi_2(dy)\\right]dP\n= \\int_H\\left[\\int_{\\mathcal{X}^{n - i}}f(x, y)\\pi_2(dy)\\right]\\pi_1(dx). \\tag{4}\n\\end{align}\n\nNow the coincidence of the right hand side of \n$(3)$\n and the right hand side of \n$(4)$\n is exactly the consequence of the Fubini's theorem.  This completes the proof.\n\nThe proof to your second equality is completely analogous (with slightly more verbose notations).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/155248/402101 \n I prefer to compute the proportion of explainable log-likelihood that is explained by each variable.  For OLS models the \nrms\n package makes this easy:\n\nrms\n\nf <- ols(y ~ x1 + x2 + pol(x3, 2) + rcs(x4, 5) + ...)\nplot(anova(f), what='proportion chisq')\n# also try what='proportion R2'\n\nf <- ols(y ~ x1 + x2 + pol(x3, 2) + rcs(x4, 5) + ...)\nplot(anova(f), what='proportion chisq')\n# also try what='proportion R2'\n\nThe default for \nplot(anova())\n is to display the Wald $\\chi^2$ statistic minus its degrees of freedom for assessing the partial effect of each variable.  Even though this is not scaled $[0,1]$ it is probably the best method in general because it penalizes a variable requiring a large number of parameters to achieve the $\\chi^2$.  For example, a categorical predictor with 5 levels will have 4 d.f. and a continuous predictor modeled as a restricted cubic spline function with 5 knots will have 4 d.f.\n\nplot(anova())\n\nIf a predictor interacts with any other predictor(s), the $\\chi^2$ and partial $R^2$ measures combine the appropriate interaction effects with main effects.  For example if the model was \ny ~ pol(age,2) * sex\n the statistic for sex is the combined effects of sex as a main effect plus the effect modification that sex provides for the age effect.  This is an assessment of whether there is a difference between the sexes for \nany\n age.\n\ny ~ pol(age,2) * sex\n\nMethods such as random forests, which do not favor additive effects, are not likelihood based, and use multiple trees, require a different notion of variable importance.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/4567/402101 \n Failing to look at (plot) the data.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/385726/402101 \n Xi'an's answer proved (or at least hinted a proof) that there are different distributions with the same mean, variance, skewness and kurtosis. I just want to show an example of three visually distinct discrete distributions with the same moments (mean=skewness=0, variance=1 and kurtosis=2):\n\n\n\nThe code to generate them is:\n\nlibrary(moments)\n\nn <- 1e6\n\nx <- c(-sqrt(2), 0, +sqrt(2))\np <- c(1,2,1)\nmostra1 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.4629338416371, -0.350630832572269, 0.350630832573386, 1.46293384163564)\np <- c(1, 1.3, 1.3, 1)\nmostra2 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.5049621442915, -0.457635862316285, 0.457635862316022, 1.50496214429192)\np <- c(1, 1.6, 1.6, 1)\nmostra3 <- sample(x, size=n, prob=p, replace=TRUE)\n\nmostra <- rbind(data.frame(x=mostra1, grup=\"a\"),\n                data.frame(x=mostra2, grup=\"b\"),\n                data.frame(x=mostra3, grup=\"c\"))\naggregate(x~grup, data=mostra, mean)\naggregate(x~grup, data=mostra, var)\naggregate(x~grup, data=mostra, skewness)\naggregate(x~grup, data=mostra, kurtosis)\n\nlibrary(ggplot2)\nggplot(mostra)+\n  geom_histogram(aes(x, fill=grup), bins=100)\n\nlibrary(moments)\n\nn <- 1e6\n\nx <- c(-sqrt(2), 0, +sqrt(2))\np <- c(1,2,1)\nmostra1 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.4629338416371, -0.350630832572269, 0.350630832573386, 1.46293384163564)\np <- c(1, 1.3, 1.3, 1)\nmostra2 <- sample(x, size=n, prob=p, replace=TRUE)\n\nx <- c(-1.5049621442915, -0.457635862316285, 0.457635862316022, 1.50496214429192)\np <- c(1, 1.6, 1.6, 1)\nmostra3 <- sample(x, size=n, prob=p, replace=TRUE)\n\nmostra <- rbind(data.frame(x=mostra1, grup=\"a\"),\n                data.frame(x=mostra2, grup=\"b\"),\n                data.frame(x=mostra3, grup=\"c\"))\naggregate(x~grup, data=mostra, mean)\naggregate(x~grup, data=mostra, var)\naggregate(x~grup, data=mostra, skewness)\naggregate(x~grup, data=mostra, kurtosis)\n\nlibrary(ggplot2)\nggplot(mostra)+\n  geom_histogram(aes(x, fill=grup), bins=100)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/176803/402101 \n The problem is that there isn't really \na\n $R^2$ for logistic regression.  Instead there are \nmany\n different \"pseudo-$R^2$s\" that may be similar to the $R^2$ from a linear model in different ways.  You can get a list of some at UCLA's statistics help website \nhere\n.\n\nIn addition, the effect (e.g., odds ratio) of the added variable, $x_2$, isn't sufficient to determine your power to detect that effect.  It matters how $x_2$ is distributed:  The more widely spread the values are, the more powerful your test, even if the odds ratio is held constant.  It further matters what the correlation between $x_2$ and $x_1$ is:  The more correlated they are, the more data would be required to achieve the same power.\n\nAs a result of these facts, the way I try to calculate the power in these more complicated situations is to simulate.  In that vein, it may help you to read my answer here:  \nSimulation of logistic regression power analysis - designed experiments\n.\n\nLooking at G*Power's documentation, they use a method based on Hsieh, Bloch, & Larsen (1998).  The idea is that you first regress $x_2$ on $x_1$ (or whatever predictor variables went into the first model) using a linear regression.  You use the regular $R^2$ for that.  (That value should lie in the interval $[0,\\ 1]$.)  It goes in the \nR\u00b2 other X\n field you are referring to.  Then you specify the distribution of $x_2$ in the next couple of fields (\nX distribution\n, \nX parm \u03bc\n, and \nZ parm \u03c3\n).\n\nR\u00b2 other X\n\nX distribution\n\nX parm \u03bc\n\nZ parm \u03c3\n\nHsieh, F.Y., Bloch, D.A., & Larsen, M.D. (1998).  \nA simple method of sample size calculation for linear and logistic regression\n.  \nStatistics in Medicine, 17\n, 1623-1634.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/14109/402101 \n Time-series (or other intrinsically ordered data) can be problematic for cross-validation.  If some pattern emerges in year 3 and stays for years 4-6, then your model can pick up on it, even though it wasn't part of years 1 & 2.\n\nAn approach that's sometimes more principled for time series is forward chaining, where your procedure would be something like this:\n\nfold 1 : training [1], test [2]\n\n\nfold 2 : training [1 2], test [3]\n\n\nfold 3 : training [1 2 3], test [4]\n\n\nfold 4 : training [1 2 3 4], test [5]\n\n\nfold 5 : training [1 2 3 4 5], test [6]\n\nThat more accurately models the situation you'll see at prediction time, where you'll model on past data and predict on forward-looking data.  It also will give you a sense of the dependence of your modeling on data size.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/418184/402101 \n The typical way to estimate a difference in differences model with more than two time periods is your proposed solution b). Keeping your notation you would regress\n$$Y_{ist} = \\alpha +\\gamma_s (\\text{Treatment}_s) + \\lambda (\\text{year dummy}_t) + \\delta D_{st} + \\epsilon_{ist}$$\nwhere $D_t \\equiv \\text{Treatment}_s\\cdot d_t$ is a dummy variable which equals one for treatment units $s$ in the post-treatment period ($d_t = 1$) and is zero otherwise. Note that this is a more general formulation of the difference in differences regression which allows for different timings of the treatment for different treated units.\n\nAs was pointed out correctly in the comments your proposed solution c) does not work out due to collinearity with the time dummies and the dummy for the post-treatment period. However, a slight variant of this turns out to be a robustness check. Let $\\gamma_{s0}$ and $\\gamma_{s1}$ be two sets of dummy variables for each control unit $s0$ and each treated unit $s1$, respectively, then interacting the dummies for the treated units with the time variable $t$ and regressing\n$$Y_{ist} = \\gamma_{s0} + \\gamma_{s1}t + \\lambda (\\text{year dummy}_t) + \\delta D_{st} + \\epsilon_{ist}$$\nincludes a unit specific time trend $\\gamma_{s1}t$. When you include these unit specific time trends and the difference in differences coefficient $\\delta$ does not change significantly you can be more confident about your results. Otherwise you might wonder whether your treatment effect has absorbed differences between treated units due to an underlying time trend (can happen when policies kick in at different points in time).\n\nAn example cited in Angrist and Pischke (2009) Mostly Harmless Econometrics is a labor market policy study by \nBesley and Burgess (2004)\n. In their paper it happens that the inclusion of state-specific time trends kills the estimated treatment effect. Note though that for this robustness check you need more than 3 time periods.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/244023/402101 \n 1) My understanding is that users have a choice of functions to use for $\\phi$, and that the Gaussian function is a very common choice.\n\n2) The density at $x$ is the mean of the different values of $\\phi_h(x_i - x)$ at $x$. For example, you might have $x_1=1$, $x_2 = 2$, and a Gaussian distribution with $\\sigma=1$ for $\\phi_h$. In this case, the density at $x$ would be $\\frac{\\mathcal{N}_{1, 1}(x) + \\mathcal{N}_{2, 1}(x)}{2}$.\n\n3) You can plug in any density function you like as your window function.\n\n4) $h$ determines the width of your chosen window function.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78/402101 \n You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales.\n\nUsing the correlation matrix is equivalent to \nstandardizing\n each of the variables (to mean 0 and standard deviation 1). In general, PCA with and without standardizing will give different results. Especially when the scales are different.\n\nAs an example, take a look at this R \nheptathlon\n data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (run 800m) are around 120.\n\nheptathlon\n\nlibrary(HSAUR)\nheptathlon[,-8]      # look at heptathlon data (excluding 'score' variable)\n\nlibrary(HSAUR)\nheptathlon[,-8]      # look at heptathlon data (excluding 'score' variable)\n\nThis outputs:\n\nhurdles highjump  shot run200m longjump javelin run800m\nJoyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66  128.51\nJohn (GDR)            12.85     1.80 16.23   23.65     6.71   42.56  126.12\nBehmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54  124.20\nSablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78  132.24\nChoubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46  127.90\n...\n\nhurdles highjump  shot run200m longjump javelin run800m\nJoyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66  128.51\nJohn (GDR)            12.85     1.80 16.23   23.65     6.71   42.56  126.12\nBehmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54  124.20\nSablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78  132.24\nChoubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46  127.90\n...\n\nNow let's do PCA on covariance and on correlation:\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/78/402101 \n Now let's do PCA on covariance and on correlation:\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)\n\n# scale=T bases the PCA on the correlation matrix\nhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\nhep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\n\nbiplot(hep.PC.cov)\nbiplot(hep.PC.cor)\n\n\n\nNotice that PCA on covariance is dominated by \nrun800m\n and \njavelin\n: PC1 is almost equal to \nrun800m\n (and explains \n$82\\%$\n of the variance) and PC2 is almost equal to \njavelin\n (together they explain \n$97\\%$\n). \nPCA on correlation is much more informative\n and reveals some structure in the data and relationships between variables (but note that the explained variances drop to \n$64\\%$\n and \n$71\\%$\n).\n\nrun800m\n\njavelin\n\nrun800m\n\njavelin\n\nNotice also that the outlying individuals (in \nthis\n data set) are outliers regardless of whether the covariance or correlation matrix is used.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24529/402101 \n To understand what can go on, it is instructive to generate (and analyze) data that behave in the manner described.\n\nFor simplicity, let's forget about that sixth independent variable.  So, the question describes regressions of one dependent variable $y$ against five independent variables $x_1, x_2, x_3, x_4, x_5$, in which\n\nEach ordinary regression $y \\sim x_i$ is significant at levels from $0.01$ to less than $0.001$.\n\n\nThe multiple regression $y \\sim x_1 + \\cdots + x_5$ yields significant coefficients only for $x_1$ and $x_2$.\n\n\nAll variance inflation factors (VIFs) are low, indicating good conditioning in the design matrix (that is, \nlack\n of collinearity among the $x_i$).\n\nEach ordinary regression $y \\sim x_i$ is significant at levels from $0.01$ to less than $0.001$.\n\nThe multiple regression $y \\sim x_1 + \\cdots + x_5$ yields significant coefficients only for $x_1$ and $x_2$.\n\nAll variance inflation factors (VIFs) are low, indicating good conditioning in the design matrix (that is, \nlack\n of collinearity among the $x_i$).\n\nLet's make this happen as follows:\n\nGenerate $n$ normally distributed values for $x_1$ and $x_2$.  (We will choose $n$ later.)\n\n\nLet $y = x_1 + x_2 + \\varepsilon$ where $\\varepsilon$ is independent normal error of mean $0$.  Some trial and error is needed to find a suitable standard deviation for $\\varepsilon$; $1/100$ works fine (and is rather dramatic: $y$ is \nextremely\n well correlated with $x_1$ and $x_2$, even though it is only moderately correlated with $x_1$ and $x_2$ individually).\n\n\nLet $x_j$ = $x_1/5 + \\delta$, $j=3,4,5$, where $\\delta$ is independent standard normal error.  This makes $x_3,x_4,x_5$ only \nslightly\n dependent on $x_1$.  However, via the tight correlation between $x_1$ and $y$, this induces a \ntiny\n correlation between $y$ and these $x_j$.\n\nGenerate $n$ normally distributed values for $x_1$ and $x_2$.  (We will choose $n$ later.)\n\nLet $y = x_1 + x_2 + \\varepsilon$ where $\\varepsilon$ is independent normal error of mean $0$.  Some trial and error is needed to find a suitable standard deviation for $\\varepsilon$; $1/100$ works fine (and is rather dramatic: $y$ is \nextremely\n well correlated with $x_1$ and $x_2$, even though it is only moderately correlated with $x_1$ and $x_2$ individually).",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24529/402101 \n Generate $n$ normally distributed values for $x_1$ and $x_2$.  (We will choose $n$ later.)\n\nLet $y = x_1 + x_2 + \\varepsilon$ where $\\varepsilon$ is independent normal error of mean $0$.  Some trial and error is needed to find a suitable standard deviation for $\\varepsilon$; $1/100$ works fine (and is rather dramatic: $y$ is \nextremely\n well correlated with $x_1$ and $x_2$, even though it is only moderately correlated with $x_1$ and $x_2$ individually).\n\nLet $x_j$ = $x_1/5 + \\delta$, $j=3,4,5$, where $\\delta$ is independent standard normal error.  This makes $x_3,x_4,x_5$ only \nslightly\n dependent on $x_1$.  However, via the tight correlation between $x_1$ and $y$, this induces a \ntiny\n correlation between $y$ and these $x_j$.\n\nHere's the rub: if we make $n$ large enough, these \nslight\n correlations will result in significant coefficients, even though $y$ is almost entirely \"explained\" by only the first two variables.\n\nI found that $n=500$ works just fine for reproducing the reported p-values.  Here's a scatterplot matrix of all six variables:\n\n\n\nBy inspecting the right column (or the bottom row) you can \nsee\n that $y$ has a good (positive) correlation with $x_1$ and $x_2$ but little apparent correlation with the other variables.  By inspecting the rest of this matrix, you can see that the independent variables $x_1, \\ldots, x_5$ appear to be mutually \nuncorrelated\n (the random $\\delta$ mask the tiny dependencies we know are there.)  There are no exceptional data--nothing terribly outlying or with high leverage.  The histograms show that all six variables are approximately normally distributed, by the way: these data are as ordinary and \"plain vanilla\" as one could possibly want.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24529/402101 \n In the regression of $y$ against $x_1$ and $x_2$, the p-values are essentially 0.  In the individual regressions of $y$ against $x_3$, then $y$ against $x_4$, and $y$ against $x_5$, the p-values are 0.0024, 0.0083, and 0.00064, respectively: that is, they are \"highly significant.\"  But in the full multiple regression, the corresponding p-values inflate to .46, .36, and .52, respectively: not significant at all.  The reason for this is that once $y$ has been regressed against $x_1$ and $x_2$, the only stuff left to \"explain\" is the tiny amount of error in the residuals, which will approximate $\\varepsilon$, and this error is almost completely unrelated to the remaining $x_i$.  (\"Almost\" is correct: there is a really tiny relationship induced from the fact that the residuals were computed in part from the values of $x_1$ and $x_2$ and the $x_i$, $i=3,4,5$, do have some weak relationship to $x_1$ and $x_2$.  This residual relationship is practically undetectable, though, as we saw.)\n\nThe conditioning number of the design matrix is only 2.17: that's very low, showing \nno indication of high multicollinearity whatsoever.\n  (Perfect lack of collinearity would be reflected in a conditioning number of 1, but in practice this is seen only with artificial data and designed experiments.  Conditioning numbers in the range 1-6 (or even higher, with more variables) are unremarkable.)  This completes the simulation: it has successfully reproduced every aspect of the problem.\n\nThe important insights this analysis offers include\n\np-values don't tell us anything directly about collinearity.\n  They depend strongly on the amount of data.\n\n\nRelationships among p-values in multiple regressions and p-values in related regressions (involving subsets of the independent variable) are complex and usually unpredictable.\n\np-values don't tell us anything directly about collinearity.\n  They depend strongly on the amount of data.\n\nRelationships among p-values in multiple regressions and p-values in related regressions (involving subsets of the independent variable) are complex and usually unpredictable.\n\nConsequently, as others have argued, p-values should not be your sole guide (or even your principal guide) to model selection.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24529/402101 \n p-values don't tell us anything directly about collinearity.\n  They depend strongly on the amount of data.\n\nRelationships among p-values in multiple regressions and p-values in related regressions (involving subsets of the independent variable) are complex and usually unpredictable.\n\nConsequently, as others have argued, p-values should not be your sole guide (or even your principal guide) to model selection.\n\nIt is not necessary for $n$ to be as large as $500$ for these phenomena to appear.\n  Inspired by additional information in the question, the following is a dataset constructed in a similar fashion with $n=24$ (in this case $x_j = 0.4 x_1 + 0.4 x_2 + \\delta$ for $j=3,4,5$).  This creates correlations of 0.38 to 0.73 between $x_{1-2}$ and $x_{3-5}$.  The condition number of the design matrix is 9.05: a little high, but not terrible.  (Some \nrules of thumb\n say that condition numbers as high as 10 are ok.)  The p-values of the individual regressions against $x_3, x_4, x_5$ are 0.002, 0.015, and 0.008: significant to highly significant.  Thus, some multicollinearity is involved, but it's not so large that one would work to change it.  \nThe basic insight remains the same\n: significance and multicollinearity are different things; only mild mathematical constraints hold among them; and it is possible for the inclusion or exclusion of even a single variable to have profound effects on all p-values even without severe multicollinearity being an issue.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24529/402101 \n x1 x2 x3 x4 x5 y\n-1.78256    -0.334959   -1.22672    -1.11643    0.233048    -2.12772\n0.796957    -0.282075   1.11182 0.773499    0.954179    0.511363\n0.956733    0.925203    1.65832 0.25006 -0.273526   1.89336\n0.346049    0.0111112   1.57815 0.767076    1.48114 0.365872\n-0.73198    -1.56574    -1.06783    -0.914841   -1.68338    -2.30272\n0.221718    -0.175337   -0.0922871  1.25869 -1.05304    0.0268453\n1.71033 0.0487565   -0.435238   -0.239226   1.08944 1.76248\n0.936259    1.00507 1.56755 0.715845    1.50658 1.93177\n-0.664651   0.531793    -0.150516   -0.577719   2.57178 -0.121927\n-0.0847412  -1.14022    0.577469    0.694189    -1.02427    -1.2199\n-1.30773    1.40016 -1.5949 0.506035    0.539175    0.0955259\n-0.55336    1.93245 1.34462 1.15979 2.25317 1.38259\n1.6934  0.192212    0.965777    0.283766    3.63855 1.86975\n-0.715726   0.259011    -0.674307   0.864498    0.504759    -0.478025\n-0.800315   -0.655506   0.0899015   -2.19869    -0.941662   -1.46332\n-0.169604   -1.08992    -1.80457    -0.350718   0.818985    -1.2727\n0.365721    1.10428 0.33128 -0.0163167  0.295945    1.48115\n0.215779    2.233   0.33428 1.07424 0.815481    2.4511\n1.07042 0.0490205   -0.195314   0.101451    -0.721812   1.11711\n-0.478905   -0.438893   -1.54429    0.798461    -0.774219   -0.90456\n1.2487  1.03267 0.958559    1.26925 1.31709 2.26846\n-0.124634   -0.616711   0.334179    0.404281    0.531215    -0.747697\n-1.82317    1.11467 0.407822    -0.937689   -1.90806    -0.723693\n-1.34046    1.16957 0.271146    1.71505 0.910682    -0.176185",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/24529/402101 \n x1 x2 x3 x4 x5 y\n-1.78256    -0.334959   -1.22672    -1.11643    0.233048    -2.12772\n0.796957    -0.282075   1.11182 0.773499    0.954179    0.511363\n0.956733    0.925203    1.65832 0.25006 -0.273526   1.89336\n0.346049    0.0111112   1.57815 0.767076    1.48114 0.365872\n-0.73198    -1.56574    -1.06783    -0.914841   -1.68338    -2.30272\n0.221718    -0.175337   -0.0922871  1.25869 -1.05304    0.0268453\n1.71033 0.0487565   -0.435238   -0.239226   1.08944 1.76248\n0.936259    1.00507 1.56755 0.715845    1.50658 1.93177\n-0.664651   0.531793    -0.150516   -0.577719   2.57178 -0.121927\n-0.0847412  -1.14022    0.577469    0.694189    -1.02427    -1.2199\n-1.30773    1.40016 -1.5949 0.506035    0.539175    0.0955259\n-0.55336    1.93245 1.34462 1.15979 2.25317 1.38259\n1.6934  0.192212    0.965777    0.283766    3.63855 1.86975\n-0.715726   0.259011    -0.674307   0.864498    0.504759    -0.478025\n-0.800315   -0.655506   0.0899015   -2.19869    -0.941662   -1.46332\n-0.169604   -1.08992    -1.80457    -0.350718   0.818985    -1.2727\n0.365721    1.10428 0.33128 -0.0163167  0.295945    1.48115\n0.215779    2.233   0.33428 1.07424 0.815481    2.4511\n1.07042 0.0490205   -0.195314   0.101451    -0.721812   1.11711\n-0.478905   -0.438893   -1.54429    0.798461    -0.774219   -0.90456\n1.2487  1.03267 0.958559    1.26925 1.31709 2.26846\n-0.124634   -0.616711   0.334179    0.404281    0.531215    -0.747697\n-1.82317    1.11467 0.407822    -0.937689   -1.90806    -0.723693\n-1.34046    1.16957 0.271146    1.71505 0.910682    -0.176185",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32625/402101 \n The process is iterative,\n but there is a natural order:\n\nYou have to worry first about \nconditions that cause outright numerical errors\n. Multicollinearity is one of those, because it can produce unstable systems of equations potentially resulting in outright incorrect answers (to 16 decimal places...)  Any problem here usually means you cannot proceed until it is fixed.  Multicollinearity is usually diagnosed using Variance Inflation Factors and similar examination of the \"hat matrix.\"  Additional checks at this stage can include assessing the influence of any missing values in the dataset and verifying the identifiability of important parameters.  (Missing combinations of discrete independent variables can sometimes cause trouble here.)\n\n\nNext you need to be concerned \nwhether the output reflects most of the data\n or is sensitive to a small subset.  In the latter case, everything else you subsequently do may be misleading, so it is to be avoided.  Procedures include examination of outliers and of \nleverage\n.  (A high-leverage datum might not be an outlier but even so it may unduly influence all the results.) If a robust alternative to the regression procedure exists, this is a good time to apply it: check that it is producing similar results and use it to detect outlying values.\n\n\nFinally, having achieved a situation that is numerically stable (so you can trust the computations) and which reflects the full dataset, you turn to an \nexamination of the statistical assumptions needed for correct interpretation of the output\n.  Primarily these concerns focus--in rough order of importance--on distributions of the residuals (including heteroscedasticity, but also extending to symmetry, distributional shape, possible correlation with predicted values or other variables, and autocorrelation), goodness of fit (including the possible need for interaction terms), whether to re-express the dependent variable, and whether to re-express the independent variables.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/32625/402101 \n You have to worry first about \nconditions that cause outright numerical errors\n. Multicollinearity is one of those, because it can produce unstable systems of equations potentially resulting in outright incorrect answers (to 16 decimal places...)  Any problem here usually means you cannot proceed until it is fixed.  Multicollinearity is usually diagnosed using Variance Inflation Factors and similar examination of the \"hat matrix.\"  Additional checks at this stage can include assessing the influence of any missing values in the dataset and verifying the identifiability of important parameters.  (Missing combinations of discrete independent variables can sometimes cause trouble here.)\n\nNext you need to be concerned \nwhether the output reflects most of the data\n or is sensitive to a small subset.  In the latter case, everything else you subsequently do may be misleading, so it is to be avoided.  Procedures include examination of outliers and of \nleverage\n.  (A high-leverage datum might not be an outlier but even so it may unduly influence all the results.) If a robust alternative to the regression procedure exists, this is a good time to apply it: check that it is producing similar results and use it to detect outlying values.\n\nFinally, having achieved a situation that is numerically stable (so you can trust the computations) and which reflects the full dataset, you turn to an \nexamination of the statistical assumptions needed for correct interpretation of the output\n.  Primarily these concerns focus--in rough order of importance--on distributions of the residuals (including heteroscedasticity, but also extending to symmetry, distributional shape, possible correlation with predicted values or other variables, and autocorrelation), goodness of fit (including the possible need for interaction terms), whether to re-express the dependent variable, and whether to re-express the independent variables.\n\nAt any stage, if something needs to be corrected then it's wise to return to the beginning.  Repeat as many times as necessary.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/6654/402101 \n I found this thought experiment helpful when thinking about confidence intervals. It also answers your question 3.\n\nLet $X\\sim U(0,1)$ and $Y=X+a-\\frac{1}{2}$. Consider two observations of $Y$ taking the values $y_1$ and $y_2$ corresponding to observations $x_1$ and $x_2$ of $X$, and let $y_l=\\min(y_1,y_2)$ and $y_u=\\max(y_1,y_2)$. Then $[y_l,y_u]$ is a 50% confidence interval for $a$ (since the interval includes $a$ if $x_1<\\frac12<x_2$ or $x_1>\\frac12>x_2$, each of which has probability $\\frac14$).\n\nHowever, if $y_u-y_l>\\frac12$ then we know that the probability that the interval contains $a$ is $1$, not $\\frac12$. The subtlety is that a $z\\%$ confidence interval for a parameter means that the endpoints of the interval (which are random variables) lie either side of the parameter with probability $z\\%$ \nbefore you calculate the interval\n, not that the probability of the parameter lying within the interval is $z\\%$ \nafter you have calculated the interval\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/22721/402101 \n The best way to think about this is to imagine a scatterplot of points with $y$ on the vertical axis and $x$ represented by the horizontal axis.  Given this framework, you see a cloud of points, which may be vaguely circular, or may be elongated into an ellipse.  What you are trying to do in regression is find what might be called the 'line of best fit'.  However, while this seems straightforward, we need to figure out what we mean by 'best', and that means we must define what it would be for a line to be good, or for one line to be better than another, etc.  Specifically, we must stipulate a \nloss function\n.  A loss function gives us a way to say how 'bad' something is, and thus, when we minimize that, we make our line as 'good' as possible, or find the 'best' line.\n\nTraditionally, when we conduct a regression analysis, we find estimates of the slope and intercept so as to minimize the \nsum of squared errors\n.  These are defined as follows:\n\n$$\nSSE=\\sum_{i=1}^N(y_i-(\\hat\\beta_0+\\hat\\beta_1x_i))^2\n$$\n\nIn terms of our scatterplot, this means we are minimizing the (sum of the squared) \nvertical distances\n between the observed data points and the line.\n\n\n\nOn the other hand, it is perfectly reasonable to regress $x$ onto $y$, but in that case, we would put $x$ on the vertical axis, and so on.  If we kept our plot as is (with $x$ on the horizontal axis), regressing $x$ onto $y$ (again, using a slightly adapted version of the above equation with $x$ and $y$ switched) means that we would be minimizing the sum of the \nhorizontal distances\n between the observed data points and the line.  This sounds very similar, but is not quite the same thing.  (The way to recognize this is to do it both ways, and then algebraically convert one set of parameter estimates into the terms of the other.  Comparing the first model with the rearranged version of the second model, it becomes easy to see that they are not the same.)\n\n\n\nNote that neither way would produce the same line we would intuitively draw if someone handed us a piece of graph paper with points plotted on it.  In that case, we would draw a line straight through the center, but minimizing the vertical distance yields a line that is slightly \nflatter\n (i.e., with a shallower slope), whereas minimizing the horizontal distance yields a line that is slightly \nsteeper\n.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/22721/402101 \n Note that neither way would produce the same line we would intuitively draw if someone handed us a piece of graph paper with points plotted on it.  In that case, we would draw a line straight through the center, but minimizing the vertical distance yields a line that is slightly \nflatter\n (i.e., with a shallower slope), whereas minimizing the horizontal distance yields a line that is slightly \nsteeper\n.\n\nA correlation is symmetrical; $x$ is as correlated with $y$ as $y$ is with $x$.  The Pearson product-moment correlation can be understood within a regression context, however.  The correlation coefficient, $r$, is the slope of the regression line when both variables have been \nstandardized\n first.  That is, you first subtracted off the mean from each observation, and then divided the differences by the standard deviation.  The cloud of data points will now be centered on the origin, and the slope would be the same whether you regressed $y$ onto $x$, or $x$ onto $y$ (but note the comment by @DilipSarwate below).\n\n\n\nNow, why does this matter?  Using our traditional loss function, we are saying that all of the error is in only \none\n of the variables (viz., $y$).  That is, we are saying that $x$ is measured without error and constitutes the set of values we care about, but that $y$ has \nsampling error\n.  This is very different from saying the converse.  This was important in an interesting historical episode:  In the late 70's and early 80's in the US, the case was made that there was discrimination against women in the workplace, and this was backed up with regression analyses showing that women with equal backgrounds (e.g., qualifications, experience, etc.) were paid, on average, less than men.  Critics (or just people who were extra thorough) reasoned that if this was true, women who were paid equally with men would have to be more highly qualified, but when this was checked, it was found that although the results were 'significant' when assessed the one way, they were not 'significant' when checked the other way, which threw everyone involved into a tizzy.  See \nhere\n for a famous paper that tried to clear the issue up.\n\n(Updated much later)\n  Here's another way to think about this that approaches the topic through the formulas instead of visually:",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/22721/402101 \n (Updated much later)\n  Here's another way to think about this that approaches the topic through the formulas instead of visually:\n\nThe formula for the slope of a simple regression line is a consequence of the loss function that has been adopted.  If you are using the standard \nOrdinary Least Squares\n loss function (noted above), you can derive the formula for the slope that you see in every intro textbook.  This formula can be presented in various forms; one of which I call the 'intuitive' formula for the slope.  Consider this form for both the situation where you are regressing $y$ on $x$, and where you are regressing $x$ on $y$:\n$$\n\\overbrace{\\hat\\beta_1=\\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}}^{y\\text{ on } x}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\overbrace{\\hat\\beta_1=\\frac{\\text{Cov}(y,x)}{\\text{Var}(y)}}^{x\\text{ on }y}\n$$\nNow, I hope it's obvious that these would not be the same unless $\\text{Var}(x)$ equals $\\text{Var}(y)$.  If the variances \nare\n equal (e.g., because you standardized the variables first), then so are the standard deviations, and thus the variances would both also equal $\\text{SD}(x)\\text{SD}(y)$.  In this case, $\\hat\\beta_1$ would equal Pearson's $r$, which is the same either way by virtue of \nthe principle of commutativity\n: \n$$\n\\overbrace{r=\\frac{\\text{Cov}(x,y)}{\\text{SD}(x)\\text{SD}(y)}}^{\\text{correlating }x\\text{ with }y}~~~~~~~~~~~~~~~~~~~~~~~~~~~\\overbrace{r=\\frac{\\text{Cov}(y,x)}{\\text{SD}(y)\\text{SD}(x)}}^{\\text{correlating }y\\text{ with }x}\n$$",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/168631/402101 \n Considering multicollineariy is important in regression analysis because, \nin extrema\n, it directly bears on whether or not your coefficients are uniquely identified in the data. In less severe cases, it can still mess with your coefficient estimates; small changes in the data used for estimation may cause wild swings in estimated coefficients. These can be problematic from an inferential standpoint: If two variables are highly correlated, increases in one may be offset by decreases in another so the combined effect is to negate each other. With more than two variables, the effect can be even more subtle, but if the \npredictions\n are stable, that is often enough for machine learning applications.\n\nConsider why we regularize in a regression context: We need to constrict the model from being \ntoo\n flexible. Applying the correct amount of regularization will slightly increase the bias for a larger reduction in variance. The classic example of this is adding polynomial terms and interaction effects to a regression: In the degenerate case, the prediction equation will interpolate data points, but probably be terrible when attempting to predict the values of unseen data points. Shrinking those coefficients will likely minimize or entirely eliminate some of those coefficients and improve generalization.\n\nA random forest, however, could be seen to have a regularization parameter through the number of variables sampled at each split: you get better splits the larger the \nmtry\n (more features to choose from; some of them are better than others), but that also makes each tree more highly correlated with each other tree, somewhat mitigating the diversifying effect of estimating multiple trees in the first place. This dilemma compels one to find the right balance, usually achieved using cross-validation. Importantly, and in contrast to a regression analysis, the predictions of the random forest model are not harmed by highly collinear variables: even if two of the variables provide the same child node purity, you can just pick one.\n\nmtry",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/168631/402101 \n mtry\n\nLikewise, for something like an SVM, you can include more predictors than features because the kernel trick lets you  operate solely on the inner product of those feature vectors. Having more features than observations would be a problem in regressions, but the kernel trick means we only estimate a coefficient for each exemplar, while the regularization parameter \n$C$\n reduces the flexibility of the solution -- which is decidedly a good thing, since estimating \n$N$\n parameters for \n$N$\n observations in an unrestricted way will always produce a perfect model on test data -- and we come full circle, back to the ridge/LASSO/elastic net regression scenario where we have the model flexibility constrained as a check against an overly optimistic model. A review of the KKT conditions of the SVM problem reveals that the SVM solution is unique, so we don't have to worry about the identification problems which arose in the regression case.\n\nFinally, consider the actual \nimpact\n of multicollinearity. It doesn't change the predictive power of the model (at least, on the training data) but it does screw with our coefficient estimates. In most ML applications, we don't care about coefficients \nthemselves\n, just the loss of our model predictions, so in that sense, checking VIF doesn't actually answer a consequential question. (But if a slight change in the data causes a huge fluctuation in coefficients [a classic symptom of multicollinearity], it may also change predictions, in which case we do care -- but all of this [we hope!] is characterized when we perform cross-validation, which is a part of the modeling process anyway.) A regression is more easily interpreted, but interpretation might not be the most important goal for some tasks.",
    "metadata": {
      "source": "StackExchange"
    }
  },
  {
    "content": "Stack Exchange Link: https://stats.stackexchange.com/a/26318/402101 \n As many of the answers above have stated, causation does not imply \nlinear correlation\n. Since a lot of the correlation concepts come from fields that rely heavily on linear statistics, usually correlation is seen as equal to linear correlation. The \nwikipedia article\n is an alright source for this, I really like this image:\n\n\n\nLook at some of the figures in the bottom row, for instance the parabola-ish shape in the 4th example. This is kind of what happens in @StasK answer (with a little bit of noise added). Y can be fully caused by X but if the numeric relationship is not linear and symmetric, you will still have a correlation of 0.\n\nThe word you are looking for is \nmutual information\n: this is sort of the general non-linear version of correlation. In that case, your statement would be true: \ncausation implies high mutual information\n.",
    "metadata": {
      "source": "StackExchange"
    }
  }
]