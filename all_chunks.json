[
  {
    "content": "LinkedIn Link: https://bit.ly/3EHlb1m \n Things that can get you rejected in a Data Science Interview * \n\n1) Saying \"Logistic Regression is not Regression but a classification algorithm\"\n\n2) Saying \" p-value is the chance of obtaining a result by sheer chance\"\n\n3) Saying \"We accept the Null hypothesis\"\n\n4) Saying \"In linear regression the dependent and independent variables needs to follow normal distribution, if not they need to be log transformed\"\n\n5) Saying \"Stepwise regression is a type of regression\"\n\n6) Saying \"your modeling strategy is to try all the models on the data and choose one based on accuracy metric via some low code library\"\n \n7) Saying \"Central limit theorem kicks in at n=30\"\n\n8) Saying \"Confidence Interval is the probability that the parameter of interest lying between the interval is 90 % or 95%\"\n\n9) Saying \"your strategy to deal with outliers is to throw them out just to fit the curve more smoothly\"\n\n10) Saying \"your strategy for imputation of missing values is to fill it by average(mean) values\".\n\nNote: * If interviewed by a knowledgeable Data Scientist/statistician\n\nAlso, the natural question you might have is, \"what is the correct answer to all of the above questions?\"\n\nPls check the comments section for the right answers to some of these questions.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientist \nhashtag\n#datasciencecareers \nhashtag\n#machinelearning \nhashtag\n#interviewquestions",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3eysVI5 \n What do recruiters look for while hiring Data Science Interns?\n\nRecently, this question was asked of us by many Internship applicants. Also a popular Data Science publication asked me questions related to Intern hiring (link to the article in comments).\n\nWhile not all my answers could be featured in the article, I thought it will be helpful to share my answers for the benefit of aspiring data scientists.\n\nHere are my answers to some of these questions. \n1) You hire for ML internships. What do you look for while selecting someone for such a role?\n\nWell On the skills side, an ML intern should have a base level knowledge of Statistical/Mathematical concepts. Not to mention a bit of coding experience in R or python and MS Office skills.\n\nThe other major thing I look for in a candidate is whether they have a good attitude as well as aptitude for learning new things.\n\nData Science is an ever evolving field and in order to keep up, one needs to be a constant learner and be curious.\n\n2) How can one convert an ML internship to a job? What should one do during that internship period to make this conversion?\n\nAn intern can improve their odds of converting internship to a job by doing the following:\n\na. Demonstrate that he/she is a quick learner.\nb. Be Inquisitive.\nc. Make some significant contribution in the project that results in some tangible results for the company.\n\n3)How important is having published papers to bag an ML internship?\n\nIt depends. For a research intern role in Academia or research centers, having published papers will really help. But in general for applied data science roles in Industry, publishing papers is not mandatory but just a 'good to have'.\n\n4) Is it better to intern at a small firm or a big company to learn?\n\nI would say Smaller firms or startups. Generally things move very quickly at a smaller firm or startups and the learning curve is steeper too. An intern at a smaller firm thus has an opportunity to learn diverse things at a quicker rate. \n\nAlso one advantage of interning at smaller firms or startups is that the intern will have closer proximity to the founder or CEO, thus he/she can readily see how decisions are made, how their work impacts the decisions the management takes. An opportunity to interact with the founder or CEO directly could foster an entrepreneurial spirit in the interns.\n\nHope these answers were useful.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3eysVI5 \n Also one advantage of interning at smaller firms or startups is that the intern will have closer proximity to the founder or CEO, thus he/she can readily see how decisions are made, how their work impacts the decisions the management takes. An opportunity to interact with the founder or CEO directly could foster an entrepreneurial spirit in the interns.\n\nHope these answers were useful. \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#internships \nhashtag\n#datasciencejobs \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#artificialintelligence \nhashtag\n#hiringinterns",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3qqxbiu \n Why saying \"We accept the Null Hypothesis\" is wrong. - An Intuitive explanation\n\nWe often come across YouTube videos , posts, blogs and private courses wherein they say \"We accept the Null Hypothesis\" instead of saying We fail to reject the Null hypothesis.\n\nIf you correct them, they would say what s the big difference? The opposite of Rejecting the Null is Accepting isnt it ?\".\n\nWell, it is not so simple as it is construed. We need to rise above antonyms and understand one crucial concept. That crucial concept is Popperian falsification.\n\nThis concept or philosophy also holds key to why we use the language Fail to reject the Null.\n\nBasically, the Popperian falsification implies that Science is never settled. It keeps changing or evolving. Theories held sacrosanct today could be refuted tomorrow.\n\nSo under this principle, scientists never proclaim X theory is true. Instead what they try to do is, they try to prove that the theory X is wrong. This is called the principle of falsification.\n\nNow having tried your best and you still could not prove the theory X is wrong, what would you say ? You would say I failed to prove theory X is wrong. Ah.. now can you see the parallels between I failed to prove theory X is wrong and We fail to reject the Null .\n\nNow lets come to why you cant say we accept the Null hypothesis.\n\nWe could not prove theory X is wrong. But does that really mean theory X is correct ? No, somebody more smarter in the future could prove theory x is wrong. There always exists that possibility. Remember above that we said science is never settled.\n\nA more classic example is that of the Black Swan. \nSuppose a theory proposes that all swans are white. The obvious way to prove the theory is to check that every swan really is white  but theres a problem. No matter how many white swans you find, you can never be sure there isnt a black swan lurking somewhere. So, you can never prove the theory is true. In contrast, finding one solitary black swan guarantees that the theory is false.\n\nNote : The post is merely to drive home the point how the language we fail to reject\" came about. It is not a post favoring inductive reasoning over deductive reasoning or vice versa. Neither it is an effort to prove or disprove Karl Popper's falsification principle.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#analytics \nhashtag\n#datascientist \nhashtag\n#ai \nhashtag\n#hypothesistesting \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3qsHhiV \n Logistic Regression is often one of the first algorithm introduced in many Machine Learning courses. Yet there are lot of misconceptions about it.\n\nIf a meme can become a cryptocurrency, why not use a meme to drive home correct data science concepts . \n\n\nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#artificialintelligence \nhashtag\n#bigdata \nhashtag\n#python \nhashtag\n#deeplearning \nhashtag\n#datascientists \nhashtag\n#logisticregression \nhashtag\n#analytics \nhashtag\n#statistics \nhashtag\n#statisticians \nhashtag\n#statisticalmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/33Uwfv6 \n The many names of Dependent and Independent variable\n\nWhile interviewing candidates for data science roles I noticed that some used different names for Independent variable and Dependent variable.\n\nOne pattern I observed was, people without statistical or related background predominantly used the term 'Target' and 'Feature'. \n\nPeople with statistical and related background used the following for Dependent and Independent variable:\n\n Dependent variable : Regressand, Predicted, Endogenous, Target, Response, Explained Variable\n\n Independent variable: Regressor, Predictor, Exogenous, Feature, Covariate, Explanatory Variable\n\nNow I don't want to be pedantic about this and I am not advocating that there is only one ideal term to use.\n\nI am just sharing this knowledge only on a 'good to know' basis and perhaps to allay some misconceptions. \n\nAlso I believe knowing the etymology and background behind this naming convention will perhaps help you appreciate the subject better.\n\nI will talk about four terms in this post (any more will perhaps warrant an article . If need be, I will cover any related concepts in future posts).\n\n Independent variable and Dependent variable: \n\nMisconception: They are named so because the dependent variable is 'dependent' on a set of variables - 'the independent variables'. And the independent variables are 'independent' of each other and the dependent variable.\n\nExplanation: The names 'independent' and 'dependent' variable came about in context of controlled experimental studies. If one read RA Fischer's work (Arrangement of Field Experiments or Statistical methods for research workers), we can see that there were studies on crop growth pattern vis a vis the treatment of fertilizers. \n\nIn these cases, the choice of fertilizers or say type of soil were chosen independently. This is how the name 'independent' came about.\n\nMany say the Independent and Dependent variable is poor terminology. The relationship is never unidirectional. In a statistical sense, the independent variables are not 'independent' of each other or the dependent variable. Also, one should not make the mistake of assuming causality because of the terms IV and DV.\n\nIn the context of linear regression, perhaps it best to think of it as Expected value of Y (mean) being conditioned on X. \n\nThe formulaic structure is given as\nE(Y|X)=0+1X.\n\n Endogenous and Exogeneous variables:",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/33Uwfv6 \n In the context of linear regression, perhaps it best to think of it as Expected value of Y (mean) being conditioned on X. \n\nThe formulaic structure is given as\nE(Y|X)=0+1X.\n\n Endogenous and Exogeneous variables: \n\nThe term 'Endogenous and Exogeneous' is predominantly used in the context of Structural Equation Modeling (SEM). \n\nIt must be kept in mind that, these terms are rarely used in context of linear regression. The reason being, exogenous by definition means variables that are not affected by any other variable. However they do affect the Endogenous variable. Also, Endogenous variable in no way affect exogenous variables.\n\nHope this was useful.\n\nEdit: The subscript '0' under Error is a typo.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#econometrics \nhashtag\n#datascientists \nhashtag\n#linearregression \nhashtag\n#ml \nhashtag\n#statistician",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3eu47ky \n Most Important Assumption Checks of Linear Regression\n\nFollowing my post on \"The many names of Dependent and Independent Variables\" last week, I was asked a very interesting question by one aspiring Data Scientist. \n\nHis question was, \"Is there an order of importance to check the assumptions of Linear Regression\"?\n\nWhat a good question.\n\nI have seen many data scientists blindly run Q-Q plots or run tests like Shapiro-Wilk test because they think \"the dependent variable and the independent variables\" needs to follow Normal Distribution. \n\nFirstly, let me dispel the myth. There is absolutely no requirement that the DV and IV be normally distributed. The assumption is for the Errors. \n\nNow coming to question, I have been practicing the following order :\n\n1. Linearity\n2. Independence of Errors\n3. Homoscedasticity \n4. Normality of errors\n\nYou see, the normality of error assumption check is the least one should be worried about (related post link in comments). Linearity and Independence of errors are much more important assumptions to check.\n\nI would suggest the readers to also read Andrew Gelman's book \"Regression and other stories\". In it, Andrew Gelman prioritizes two more assumptions ahead of the assumption of Linearity.\n\nAn excerpt from the book: \n\n Validity : \"Most important is that the data you are analyzing should map to the research question you are trying to answer. \n\nThis sounds obvious but is often overlooked or ignored because it can be inconvenient. \n\nOptimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to the cases to which it will be applied.\"\n\n Representativeness: \"A regression model is fit to data and is used to make inferences about a larger population, hence the implicit assumption in interpreting regression coefficients is that the sample is representative of the population. \n\nTo be more precise, the key assumption is that the data are representative of the distribution of the outcome y given the predictors x1, x2,..., that are included in the model.\"\n\nI really like the 'Representativeness' point because very rarely do aspiring data scientists think of Regression in terms of sampling, population, estimates, parameters and statistic .\n\nSo to summarize the order of importance for Assumption checks would be:",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3eu47ky \n To be more precise, the key assumption is that the data are representative of the distribution of the outcome y given the predictors x1, x2,..., that are included in the model.\"\n\nI really like the 'Representativeness' point because very rarely do aspiring data scientists think of Regression in terms of sampling, population, estimates, parameters and statistic .\n\nSo to summarize the order of importance for Assumption checks would be:\n\n1) Validity\n2) Representativeness\n3) Linearity\n4) Independence of errors\n5) Homoscedasticity\n6) Normality of errors\n\nHappy Monday \n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#econometrics \nhashtag\n#datascientists \nhashtag\n#linearregression \nhashtag\n#ml \nhashtag\n#statistician",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Fx4Ey2 \n First principles thinking in Data Science.\n\nFirst principles thinking is defined as \"boiling problems down to their most fundamental truths\".\n\nSo when it comes to Data Science, what are the first principles ?\n\nIn my opinion they are :\n\nMeasures of central tendency - mean, median mode.\nMeasures of dispersion - variance, Standard deviation, Interquartile range.\n\nMost of the topics in Data Science somehow boil down to central tendency or dispersion . Let me explain through some examples.\n\n1. Linear regression : \nGenerally, One models the expected value (the mean) not the raw value of dependent variable.\nPls note one can model any quantile in linear regression.\n\n2. Probability distributions : \nThe famous normal distribution is characterized by location parameter (mean) and scale parameter (standard deviation).\nSimilarly other distributions too are characterized by location and scale parameter. \n\n3. Machine learning :\nModel Drift: When we say the model has drifted, it actually means that the existing model has drifted in terms of the location or scale parameter or both from the real model.\n\nAccuracy metrics: accuracy metrics like F1 is nothing but harmonic mean.\n\n4. Outlier detection or Anomaly detection : We classify something as an outlier if some data point is 2SD or 3SD or even 6SD.\n\n5. Time series forecasting : \nWell one of the key concepts in Time series forecasting is stationarity. A stationary time series is the one whose properties such as mean, variance and autocorrelation structure stays constant over time. Stationarity is important because it is easier and more accurate to estimate parameters of a series whose properties do not change over time. If the mean and variance of the series keep on changing over time, the accuracy of the estimates will vary over time.\n\n6. Hypothesis testing :\nWe have hypothesis testing of mean and difference in means. For e.g. t-test and ANOVA.\n\n7. Information theory :\nMany algorithms like Decision trees, model comparison techniques like AIC use information theory at its core. Even probability distribution comparisons techniques KL Divergence uses Information theory concepts like Entropy, Information gain etc.\n\nWell Entropy again is the expected value (average) of the self-information of a variable\nor\nthe Entropy is the smallest possible average size of lossless encoding of the messages sent from the source to the destination.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Fx4Ey2 \n 7. Information theory :\nMany algorithms like Decision trees, model comparison techniques like AIC use information theory at its core. Even probability distribution comparisons techniques KL Divergence uses Information theory concepts like Entropy, Information gain etc.\n\nWell Entropy again is the expected value (average) of the self-information of a variable\nor\nthe Entropy is the smallest possible average size of lossless encoding of the messages sent from the source to the destination.\n\nWhat other Data Science concepts would you add that boil down to measures of central tendency and dispersion?\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32Na3CQ \n Entropy and Getting Hired. \n\nIn information theory, there is a concept called Entropy. If you want a good refresher, I would highly recommend the article by Naoki (Link in comments).\n\nBasically, Low Entropy means receiving very predictable information.\nHigh entropy means receiving very unpredictable information (some also define it as an element of surprise).\n\nI have been interviewing data scientists for past 3-4 yrs and I must say most data science aspirants' resumes have low entropy.\n\nEveryone has the same projects :\n\n Titanic survivors prediction\n Heart disease prediction \n Home loan prediction\n Credit default prediction\n Same Kaggle exercises\n Some object detection exercise\n Predicting stock market via forecasting (LSTM)\n\nIf you want to improve your odds of getting hired for Internship or full time positions, I would highly recommend increasing the entropy of your resume.\n\nI know your natural question would be, \"How do I do that?\"\n\nWell for starters, don't do what everyone does. \n\nInstead of putting the run-of-the-mill examples cited above:\n\n Perhaps do your own pet project\n Collect your own data (need not be huge), \n Study the data thoroughly (talk about the data generating distribution)\n Present the insights (talk about the outliers and if they really contain some signal)\n Apply the appropriate statistical technique or ML algorithm.\n\nFinally,\n Don't just talk abt the accuracy metrics . \n Talk about how your solution helped you solve the problem and the tangible benefits you gained. \n\nAnd just like that you have increased the entropy of your resume. \n\nYour resume now has an \"element of surprise\" for the recruiter and not to mention you also have improved your odds of getting hired !!\n\nHappy Monday.\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#internships \nhashtag\n#interviewtips \nhashtag\n#resumetips \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#datasciencejobs",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/33Y84ff \n Abstraction and Data Science - Not a great combination !!\n\n How 'Too Much Abstraction' is dangerous in Data Science.\n\n How certain libraries and low code solutions make Data Science unintuitive for Data Scientist through 'too much abstraction'.\n\n Data Science is just not about predictions. How these predictions are made and what ingredients led to those predictions also matter a lot.\n\n The more one abstracts away, the more is the chance of doing data science wrongly.\n\nRead on to know more. \n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#bigdata \nhashtag\n#datascientist \nhashtag\n#statistics \nhashtag\n#python \nhashtag\n#rprogramming \nhashtag\n#deeplearning \nhashtag\n#lowcode \nhashtag\n#sklearn \nhashtag\n#abstraction \nhashtag\n#programming \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3sK5W5c \n Fallacy of \"Modeling will take only 20% time\"\n\nIt is said that, in a Data science project, roughly 80% of the time goes into Data cleaning, aggregation, transformation activities.\n\nSure. No body is denying the fact that data cleaning, aggregation and transformation activities is tough.\n\nBut because one has gone through this ordeal, one falsely believes that the worst is behind them.\n\nThey think modeling is a relatively smaller hill to climb.\n\nDuring my consulting with various companies this year, I noticed the following\n\n1. Wrong models applied to the problem\n2. Off-the-shelf models from various libraries applied as it is.\n3. Models not tuned properly (e.g. ppl not even knowing defaults can be changed !!)\n\nIMHO, majority of the Data science projects fail because of the above mentioned three reasons. \n\nPeople think modeling will take only 20% of time or lesser. \nHence they falsely assume that modeling is the easy part.\n\nActually what I have observed is that, Modeling takes only 20% of time because the project is structured to allow only that much time !!\n\nI believe we need to overcome two fallacies in order to make our Data science projects more successful. \n\nThose two Fallacies are:\n\n \"Modeling is the easiest part in Data science project\"\n\n\"Modeling will take only 20% of the time\"\n\nWhat do you think?\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#dataengineering \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3ENhsPT \n Of late there is a greater impetus to Automate everything in Data Science. One such area is Feature Selection. I am of the firm opinion that Feature Selection is best done by data scientist (with domain knowledge) and not through some automation. \n\nHowever, some dont agree.\n\nI had been thinking about writing on this topic for a long time.\n\nThanks to Awbath AlJaberi recent post (link in comments), I got enough fodder to prove How and Why Automated Feature Selection can be dangerous.\n\nThe case in point example is Chi Square Test of Independence. Many Data Scientists blindly use it for feature selection without thinking how and in which context it is supposed to be used. \n\nWhen Data Scientists use chi square test for feature selection, they just merely go by the ritualistic If your p-value is low, the null hypothesis must go. \n\nThe automated function they use behaves no differently. We shall see how shortly.\n\nComing back to chi square, the typical hypothesis set up is:\n\nHo: There is no association between the variables\n\nH1: There is association between the variables.\n\nSo, when data scientists use the automated chi square selection of sklearn , it just selects the best features purely based on Select variables where p-value less than alpha (often 0.05). \n\nHere starts the trouble.\n\nWhat many data scientists (without stats background) dont realize is that p-value is not indicative of strength of the association. One must understand the difference between Statistical significance and practical significance. \n\nStatistical significance does not always imply practical significance.\n\nAlso, P-values can easily be hacked. All you need to do is increase your sample size!. At large sample sizes even small effects can become significant while for small sample sizes, even large effects may not be significant.\n\nTo gauge practical significance, statisticians use Effect Size. \n\nThe automatic feature selection function (e.g. sklearn) does not perform Effect Size tests like Cramer's V.\n\nSo what does all these mean?\n\n1) If your data set (training + test) is huge. It becomes that much easy to show some association between the variable and the target variable. \n\n2) In absence of domain knowledge you might be picking variables thinking they have some association to target variable but in reality that association might be very weak.\n\nAutomated feature selection exposes you to certain risks that you may not even be aware of!\n\nThe Takeaways:",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3ENhsPT \n So what does all these mean?\n\n1) If your data set (training + test) is huge. It becomes that much easy to show some association between the variable and the target variable. \n\n2) In absence of domain knowledge you might be picking variables thinking they have some association to target variable but in reality that association might be very weak.\n\nAutomated feature selection exposes you to certain risks that you may not even be aware of!\n\nThe Takeaways:\n\nPls remember, everything in Data Science cant be automated. When it comes to feature selection, domain knowledge is your biggest safety net. And in case you use auto feature selection like chi square, pls do the Effect size test before choosing variables.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#featureengineering \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#sklearn \nhashtag\n#python",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3pyYVCa \n Another Day and Another Myth Busted.\n\nThe central limit theorem does not kick in at n=30.\n\nA quick recap on what is CLT : The sample mean will be approximately normally distributed for larger sample sizes (n), regardless of the distribution from which we are sampling.\n\nMyths:\n\n It is a myth that \"sample mean tends to be approximately normally distributed if the sample size is at least 30.\"\n\n The rule of thumb that if n>30 use z test and if n< 30 use t test is not entirely correct. (You can use t test for larger samples too.)\n\nWhy did n=30 come about ?\n\nThe n=30 heuristic was not devised out of purely mathematical or statistical reasons. Back in the day (when computers were in its infancy), people used tables much like a logarithm table to see the values of any distribution for any combination of DF and significance level. \n\nAnd because such a table had to fit many distributions (t, normal, chi square etc.), the t distribution page was restricted to only 30 entries. Also at n=30, normal distribution and t distribution were deemed 'approximately' same.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3px3BbO \n The more I consult for various companies, the more my belief that 'Data Science is a team effort' gets reaffirmed.\n\nYou need a team to focus on certain aspects in a Data Science project. One person can't do it all.\n\nThere will always be a trade-off in 'One person doing it all'.\n\nEither your data/software engineering gets compromised or your data science.\n\nWith many off the shelf Data Science libraries available in Data Science, one could be tricked in to believing doing data science is 'easy'. However the reality is far from it.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#ai \nhashtag\n#deeplearning \nhashtag\n#datascientists \nhashtag\n#analytics \nhashtag\n#nlproc \nhashtag\n#statistics \nhashtag\n#softwareengineering \nhashtag\n#team",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3z8HDiK \n Many are aware of the concept 'Feature Selection' but a relatively less known concept is 'Feature Augmentation'.\n\nI believe one of the underrated skill in Data Science is Feature Augmentation.\n\nNot many know how to create new features from existing ones, to do that domain experience is key.\n\nSometimes feature augmentation can be the difference between a robust model and a mediocre one (both from a machine learning perspective and business wise).\n\nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#ai \nhashtag\n#analytics \nhashtag\n#datascientists \nhashtag\n#data \nhashtag\n#statistics \nhashtag\n#deeplearning \nhashtag\n#bigdata",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3pxrVKA \n Lost in Translation\n\nThe first divergence that happens in any Data Science project is when the Business problem is translated into a Data Science problem.\n\nGet this step wrong and no matter what robust Data pipeline or engineering you do downstream, you are bound to get disappointing results.\n\nIt is for this reason Business acumen and subject matter expertise is stressed upon. \n\nHow to frame a Business problem correctly into a Data Science problem is a skill in itself. \n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#deeplearning \nhashtag\n#analytics \nhashtag\n#bigdata \nhashtag\n#datascientists \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FuIEE1 \n Lack of not knowing how the ML algorithms works under the hood leads to acceptance of 'default'.\n\nHave seen many Data Scientists stick to threshold of 0.5 in Logistic Regression because they believe it is the 'convention' one must stick to.\n\nSame applies to p-value of 0.05. \n\nSticking to 'default' could also prevent one from solving the business problem effectively. \n\nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#artificialintelligence \nhashtag\n#mathematics \nhashtag\n#bigdata \nhashtag\n#datascientists \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Jnmo14 \n At what stage should a Data Scientist be roped in ?\n\nTypically, a Data Science project lifecycle can be broken down into 7 stages (as depicted in the image).\n\nMany companies start hiring / involving \nhashtag\n#datascientist at the Data cleaning stage. I believe it is too late a stage to hire a Data Scientist.\n\nI believe a Data Scientist should be hired right at the 'Business use case definition stage' or no later than 'Data Collection stage'. \n\nBusiness Use Case Stage - Hiring at the use case stage will allow the Data Scientist to give feedback on the feasibility of applying Data Science in the project. Many companies lament the sunken cost when they come to realize that perhaps a not so ideal \nhashtag\n#datascience method has been applied.\n\nData Collection Stage- Many companies lament that 80% of their time goes in data cleaning. It is not just data cleaning but weeding out poorly collected data. \n\nData Scientist hired at this stage would be able to tell \n1) What data to collect \n2) How to collect \n3) From where to collect \n\nThus the '80% of time' can be mitigated to a large extent.\n\nAlso, a data scientist with a strong background / training in statistics (more specifically design of experiments and market research) would be very handy at this stage.\n\nhashtag\n#machinelearning \nhashtag\n#bigdata \nhashtag\n#statistics \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FC6TQC \n Descriptive statistics and good data visualizations are your two best friends when you are trying to prove or disprove application of a particular ML algorithm. Especially to the non data science background stakeholders/management. \n\nGood descriptive statistics and visualizations can clearly bring out the anomalies or incompleteness in the data or show how the data generating process looks like (PDF). \n\nThis in turn can give you strong talking points on what algorithm can be applied or perhaps that no algorithm can be applied ! \n\nThe adage of 'seeing is believing' holds true in these contexts.\n\nhashtag\n#datascience \nhashtag\n#bigdata \nhashtag\n#analytics \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#ai \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32JdfPu \n In my last post or should I say the meme, I tried to dispel some of the misconceptions about Logistic Regression.\n \nFor few years, I had been wondering what is it that makes people to learn it wrongly. Why do people still say Logistic Regression is not a Regression, but a classification algorithm.\n \nThe obvious answers are the resources - Modern ML books (often 100 pages or lesser), blogs, private courses, pop data scientists on YouTube etc. \n \nBut the next question is, when one is implementing logistic regression, why is it that they are not aware about the various types of link functions, GLM families etc. Why doesn't the intuition of Logistic Regression being Regression set in?\n \nAnecdotally, I have noticed two things. \n1. People who transition to Data Science from a statistical or related backgrounds rarely make this mistake.\n2. People who learn R first and then move to Python too don't make this mistake.\n \nw.r.t to point 1, Their statistical training teaches the Logistic Regression the right way. This brings us to point 2. \n \nAround 4-5 years ago, I predominantly used R. However, in the last few years I moved to python as part of NLP and ML work.\n \nBut during my time using R, I vividly remember the syntax for implementing LR being intuitive. It kind of gave a peek into what the algorithm is all about.\n \nIf you notice in the image, the R syntax clearly tells the user that the model is Generalized linear model. Additionally, it also tells you that we are using Binomial regression with a logit link. Now people who use this syntax will never say \"Logistic regression is not regression\".\n \nOn the other hand, let us take the python syntax (specifically sklearn, since many data scientists use that).\n \nAs you can see, the syntax does not tell you much. It is just instantiation of the class. Of course, one needs to take a peek inside this class. But many don't because they have been sold on 'Implement machine learning in 2 lines of code'.\n \nAny person without a statistical training, implementing logistic regression code in python (sklearn) will not even have a hint whether it is indeed regression! Again, they have been educated on 'Supervised' and 'Unsupervised' algorithm. To them logistic regression is a 'supervised classification algorithm' because they have heard and seen it being used only for *classification* purposes.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32JdfPu \n Any person without a statistical training, implementing logistic regression code in python (sklearn) will not even have a hint whether it is indeed regression! Again, they have been educated on 'Supervised' and 'Unsupervised' algorithm. To them logistic regression is a 'supervised classification algorithm' because they have heard and seen it being used only for *classification* purposes.\n \nBut because you start calling logistic regression as classification algorithm does not make it one. At best using logistic regression for 'classification' is one of the clever hacks. \n \nA hack should not be misconstrued for the original. This is not about semantics but about learning the algorithm for what they are.\n \nP.S. The goal here is not to trigger R vs Python debate. Each of their merits far outweigh their miniscule demerits.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32JdfPu \n hashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#artificialintelligence \nhashtag\n#bigdata \nhashtag\n#python \nhashtag\n#deeplearning \nhashtag\n#datascientists \nhashtag\n#logisticregression \nhashtag\n#analytics \nhashtag\n#statistics \nhashtag\n#statisticians \nhashtag\n#statisticalmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3JjTcbz \n People often use the terms 'Errors' and 'Residuals' Synonymously. Especially in context of linear regression. However in a strict statistical sense, they are not the same !!\n\n :\nError pertains to the Data Generating Process. It is actually the difference between the observed values and the '  ' of the quantity of interest (such as population mean).\n\nAn example: Imagine we have a theoretical value of 'ideal circumference of a tennis ball'. Lets say it is 8 cm. Now a factory manufacturing the tennis ball does not always manufacture it to perfection of 8cm. it could be 7.95, 7.83 cm or 8.01, 8.09 etc. This difference from theoretical value of 8cm is the Error.\n\n :\nResiduals happen because of estimation i.e. us 'fitting a model' . Basically it is the difference between observed and predicted values. Note that the    .\n\nAn example would be sample mean . We hope that the sample mean is a good estimator of the 'theoretical and unobservable' population mean.\n\nhashtag\n#datascience \nhashtag\n#analytics \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#data \nhashtag\n#bigdata \nhashtag\n#ai \nhashtag\n#statisticalmodeling \nhashtag\n#linearregression \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3EySvYa \n When a Data Scientist transitions from theory to practice, One of the realization to dawn upon him/her is that :\n\nNormal Distribution may be the most popular distribution but it is not the most prevalent.\n\nThe sooner a Data Scientist realizes this, the better.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#bigdata \nhashtag\n#artificialintelligence \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Fz5dHC \n Me : X is strongly correlated with Y.\nSmarty pants : Hey, but correlation is not causation.\n\nOvergeneralization of some phrases like \"correlation is not causation\" has lead to more harm than good.\n\nSome folks use this phrase to sound cool or use it in a way to dismiss arguments without thinking about them thoroughly.\n\nSuch callous usage of the phrase leads to outright rejection of any further discussion into causality. \n\nNotice how in my conversation above, I had not even used to word causation. Yet, Mr. Smarty pants blurts out \"correlation is not causation\" like a parrot on cue.\n\nNo body is saying \"correlation is causation\" (at least not right off the bat). But a strong correlation does lead to the question of causality. \n\nCorrelation at the least expresses a hypothesis that the two variables or phenomenon might be related. It also leads us to explore more robust methodology like Hill's criteria to establish causality .\n\nOne of my friends suggested to me that one should use \"X has a strong normalized covariance with Y\" instead of \"X is strongly correlated with y\".  This really perplexes non statisticians/data scientists and they stop quipping \"correlation is not causation\".\n\nI think the best summary to all this is by Randall Munroe XKCD \"Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing 'look over there'\"\n\nhashtag\n#datascientists \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#correlation \nhashtag\n#analytics \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3ewMa4W \n When it comes to linear regression, one of the biggest misconception is that the dependent variable and independent variables must follow Normal Distribution.\n\nMany folks under this misconception start to log transform their dependent and independent variables.\n\nThe normality assumption is never about the dependent and independent variables, rather it is about the error (unobservable) and the residuals (observable).\n\nFor more on error and residuals pls see this link (https://lnkd.in/dNY8Gn2)\n\nComing back to the normality assumption, the log transformation makes the interpretation difficult and many are not trained correctly enough to interpret them.\n\nAlso, log transformation is not really advisable unless you really know the nature of the dependent and independent variables i.e. do you see a multiplicative relationship?\n\nThe normality assumption is there to facilitate inference (CI, t test, f test etc.). \n\nIf you are in a non mission critical field, often the normality assumption is not the strictest assumption which is followed (not advising that one should turn a blind eye towards it either). \n\nYou will never get a perfect residual plot that follows normal distribution. Neither is the the assumption of errors being perfectly normal ever true.\n\nIf you suspect that your errors are not going to be normally distributed and there might be some non linear relationship between your dependent and independent variables, there is always a family of GLMs (Generalized linear models) to try.\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#dataanalytics \nhashtag\n#statistics \nhashtag\n#statisticalmodeling \nhashtag\n#linearregression \nhashtag\n#bigdata \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3mGmHKN \n It is surprising to know that many have the misconception that linear regression models the raw values of Y(Dependent Variable) !!\n\nIn linear regression, we estimate the 'expected value' of the dependent variable. This expected value could be mean, median or any other quantile.\n\nHowever more popularly in linear regression, we model the 'conditional mean'.\n\nThere are tell tale signs that we are modeling the 'conditional mean' if one looks closely at the Total sum of squares formula (see image)\n\nAlso if one notices in text books, the 'how to interpret regression output' section has something like below\"\n\n\"If the height increases by 1 meter, then the  weight increases by 10.2 kg, all other variables held constant\".\n\nNotice the word 'average' in the statement. This is very important to keep in mind.\n\nAlso, the intuition that we are modeling the 'expected value' is very important as it makes us think twice before performing any transformation (log, square, inverse etc.) on the dependent variable.\n\nFor consequences of transforming dependent and independent variables, pls check this excellent Quora post by Adrian Olszewski\n(https://qr.ae/pGPhu5)\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#analytics \nhashtag\n#linearregression \nhashtag\n#bigdata \nhashtag\n#statisticalmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3euOV6C \n How do you go about a Data Science project ?\n\nI will \"Try all models\" and \"Let some Statistical/ML technique do feature selection for me\" is the wrong answer.\n\nYet, this answer was doing the rounds on LinkedIn of late. Mainly because of the article titled \"You Don't need Math for Machine Learning\" (link of the infamous article in comments).\n\nI can't help but see the funny side of the video and draw parallels with the approach of 'Try all models'.\n\nIf you \"Try all models\" blindly and choose one based on some accuracy metric, sorry you are not doing data science. You are just gaming the system. \n\nSimilarly if you were clueless on which variables to select for your model, you will be clueless even after getting some variables through some ML/Stat feature selection technique.\n\nJust that you would falsely convince yourself that 'these features that you have gotten are important'.\n\nApplied Data Science is a serious business. We really need to know what we are doing and knowing Maths and statistics helps in that regard.\n\nWe may not be 'Scientists' in the strictest sense. But at least to do justice to the word 'Science' in Data Science :\n\n One needs to Ideate and think through the problem thoroughly.\n Choose features guided by Domain Knowledge.(or talk to domain experts before choosing features)\n Know and understand how the models work under the hood.\n Weigh the pros and cons of applying an algorithm/technique to data.\n\nAI could be a Multi Billion Dollar Industry or even a Trillion Dollar Industry but Only if Data Science is done right. Doing it wrongly will only cause disenchantment and lower adoption.\n\nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#artificialintelligence \nhashtag\n#deeplearning \nhashtag\n#algorithms \nhashtag\n#bigdata \nhashtag\n#datascientists \nhashtag\n#maths \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3HgOOIn \n Clearing Misconceptions around Confidence Interval.\n\nOne of the most confusing topics in statistics is Confidence Interval. The internet is abound with incorrect explanations. Even Wikipedia definition is so ambiguous that it has put a blaring sign of caution !\n\nI guess Just for this reason many aspiring data scientist / statisticians can't be faulted for learning it wrongly. But at the same time it is important to learn it correctly as it plays a major role in NHST inference. \n\nMany introductory courses also have Linear Regression as the first topic in ML algorithms and interpreting the CI correctly in the output table is crucial. \n\nHere are some ways Confidence Intervals is wrongly defined:\n\nThe probability that the parameter of interest lying between the interval is 90 % or 95%'. \n\nThere is 95% probability that the mean weight is between 50 kg and 70 kg. \n\nThe probability that mean will *range* between 50kg and 70kg is 95%. \n\nFrom a Frequentist perspective, it does not make any sense.\n\nThe parameter of interest either lies within the interval or does not. There is no probability or chance (represented as %) associated with it.\n\nSo here is the correct definition of Confidence Interval:\n\nIf one ran the same statistical test taking different samples and constructed a confidence interval each time, then in 95% of the cases, the confidence interval so constructed for that sample will contain the true parameter. \n\nNote: of course there are other nuances like realized sample and unrealized sample. Inclusion of those in the above definition might be more confusing to the reader. Hence not included.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#analytics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#hypothesistesting \nhashtag\n#datascientist \nhashtag\n#statisticalanalysis",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3mEmQOG \n Good to see ML Ops engineers finally realizing that the work of a Data Scientist is not easy either. \n\nThe ML algorithm is the nucleus of a Data Science project. \n\nAn experienced Data Scientist might be able to conjure up a great working algorithm in lesser time than it takes to do data cleaning and data pipeline engineering /architecting.\n\nBut lesser time taken does not mean it is easy or any less important compared to other processes in a data science lifecycle. \n\nGet this 'nucleus' wrong and the whole process becomes unstable.\n\nAlso this is one of the reasons why too much abstracted libraries or low code solutions are not a good idea. You simply don't know which knobs and levers to tweak to get the model right !!\n\nFor those interested on why too much abstraction is a bad idea, check out my article -> https://lnkd.in/g6KKPEe\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientist \nhashtag\n#deeplearning \nhashtag\n#bigdata \nhashtag\n#python \nhashtag\n#artificialintelligence \nhashtag\n#lowcode \nhashtag\n#programming \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/316TQrB \n Both the tweet and reply are Epic. What do you think ? \n\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#deeplearning \nhashtag\n#datascientists \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3pzlert \n Yup, Real Statisticians / Data Scientists don't test for normality. \n\nEspecially normality of data. The normality assumptions are often about Errors and residuals not the data itself.\n\nTesting for normality has become ritualistic thanks to many courses, books advocating the same. Many aspiring statisticians/data scientists and sometimes even seasoned ones perform normality test just because 'they have been told so'. \n\nI would urge the readers to check out Adrian Olszewski's latest series \"statistical sins\" (very aptly named I must say) where he specifically talks about the consequences of transforming variables because one thinks \"everything should follow normal distribution\".\n\nAlso you may check out other posts where I talk about difference between Errors and Residuals and about normality assumption in regression.\n\nThese links are provided in the comments. Since for some reason LI algorithm penalizes posts with multiple links the same way L1 regularization penalizes overfitting - \n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3z6E9wT \n \"All models are wrong, some are useful\"  \"Modeling is a futile exercise\".\n\nThe phrase \"All models are wrong, some are useful\" is quite loosely used. Some take it in a very literal sense to imply that \"Modeling is a futile exercise\". \n\nThis is a terrible misunderstanding and we shall see why shortly.\n\n\"All models are wrong, some are useful\" is an aphorism (meaning it is a concise expression of general truth). But the aphorism in this case leads to misinterpretation.\n\nFirstly, it is important to understand what modeling is.\nThe purpose of modeling is to provide an abstraction of real process. Basically, a good approximation of reality. \n\nAnybody who mistakes the abstraction for the real, commits the Fallacy of Reification (yup, one more fallacy to add to the list of all fallacies which we data scientists/statisticians commit).\n\nComing to why the phrase should not be taken literally, a good analogy would be that of an example of a Map.\n\nIn an exact sense, a map is also wrong because it does not provide 1:1 mapping of the real world.\n\nSo, George Boxs phrase should be construed the same way as a map is considered wrong because it does not represent the real world.\n\nDoes a map provide 1:1 mapping to the real world? \nNo. \nBut is it mighty useful? \nHell yeah.\n\nAlso, talking about utility of the models, John Tukey's words conveys the essence clearly. \n\n\"Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question\".\n\nSo, to summarize\n\"All models are wrong, some are useful\"  \"Modeling is a futile exercise\".\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#artificialintelligence \nhashtag\n#bigdata \nhashtag\n#analytics \nhashtag\n#deeplearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3sDDr9c \n Agree. Hence it is not correct to use the word 'Full stack Data Scientist' too. The skillsets are unstackable !!\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientist",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3z7kCMV \n The problem of learning \"Formula First\" in Data Science\n\nIn Data Science and Statistics, one encounters a lot of formulas (Probability distribution functions (PDFs), laws, rules etc.).\n\nMost of us start learning the formula first (often by rote), instead of learning the intuition.\n\nFormulas are crystallized information. Very hard to break and unpack. \n\nInstead the learning should always be from first principles which then eventually can be condensed into crystallized form - The Formula. \n\nThus also encapsulating the knowledge you gained.\n\nSo the learning path should be:\n\nIntuition -> Formula\nRather than \nFormula -> Intuition.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#bigdata \nhashtag\n#datascientists \nhashtag\n#analytics \nhashtag\n#artificialintelligence \nhashtag\n#deeplearning \nhashtag\n#mathematics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FB4xS1 \n A Serious Problem with Data Science Learning Ecosystem.\n\nGenuinely knowledgeable Data Scientists / Statisticians have 'Impostor Syndrome' and hence refrain from teaching or sharing their knowledge.\n\nPeople with half baked knowledge have 'Dunning-Kruger Effect' and teach incorrect things with confidence.\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3HmgsUa \n Could not find the main content of the post.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3sBJ6wF \n Could not find the main content of the post.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FGrwLB \n Data Science is one field where many want to teach before learning it thoroughly themselves.\n\nAlso, there is an effort to make things simpler for the 'layman'. Certain things should not be made simpler. In doing so, the essence and correctness is lost.\n\nIn Statistics especially, there is no such thing called \"Poetic License\" . One simply shouldn't add adjectives to amplify things. One should not use words synonymously. \n\nOne should not use likelihood and probability synonymously.\n\nOne should not use 'insignificant' in place of 'significant'.\n\nThere is a reason why we say \"We fail to reject the Null Hypothesis\". (Link in comments)\n\nThere is a reason when we speak about confidence intervals, we say \"The parameter of interest either lies within the interval or does not. There is no probability or chance (represented as %) associated with it.\" (Link in comments)\n\nI guess making things simple is fine but things should not be made 'simpler' at the cost of statistical correctness.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3mFqdFg \n In one of my article, I had written about how over abstraction can be dangerous in Data Science (Link in comments). \n\nI particularly referred to low code libraries that abstract away too much detail out of the algorithms. This leads to Data Scientists developing no intuition and how it leads to a typical XY problem (Link in comments).\n\nThe XY problem here being \n\nSolution Y -'How do I improve the accuracy metrics of the model?'\n\nWhile the real X problem was - 'Are these even the right algorithms to solve the problem ?'.\n\nNow some people privately asked me, \"Does low code or AutoML or Auto NLP have no use at all ?\"\n\nTo answer this, I would like to first define two types of ML algorithms. \n\n1) Inference Type\n2) Prediction Type\n\nI recently stumbled upon an article by W.D where he uses the words  - hat and Y-hat for Inference and Prediction type respectively.\n\nThe link to his excellent article is in the comments. So because  -hat and Y-hat are more catchy, I will loan these words from him.\n\nIn -hat type problems, We care about what variables go into the model. What are the parameter values and How much each of the Independent variables affect the Dependent variables. Here the goal is not just prediction but how that prediction was made. \n\nE.g. Loan default : A person would ideally demand to know why his/her loan is being denied . The algorithm needs to have that explanation.\n\nIn Y-hat type problems, One does not care about what variables go into the model. One does not care about the parameter values. All one cares about is whether the prediction is accurate or not.\n\nNow lets come to the question of 'Does Low code or AutoML or Auto NLP have no use at all'?\n\nLow code, AutoML or AutoNLP solution can be useful in Y-hat type problems. This is because the feedback loop on whether the algorithm is giving the correct output, is quicker. I mean if the algorithm output the label 'Cat' on a Dog image, you would immediately catch it.\n\nSimilarly in NLP, if you had developed a semantic search algorithm, Upon keying in a search word, if you were to be provided with irrelevant words or searches then you would immediately catch that as well. Ditto with multi label classification tasks.\n\nSo, it all boils down to speed of feedback.\n\nIn Y-hat models, generally the feedback is quicker. They are faster to judge, on whether the output is correct or not. This is a big safety net.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3mFqdFg \n Similarly in NLP, if you had developed a semantic search algorithm, Upon keying in a search word, if you were to be provided with irrelevant words or searches then you would immediately catch that as well. Ditto with multi label classification tasks.\n\nSo, it all boils down to speed of feedback.\n\nIn Y-hat models, generally the feedback is quicker. They are faster to judge, on whether the output is correct or not. This is a big safety net.\n\nSo in these cases AutoML and AutoNLP could be useful. However for  -hat type problems, low code or AutoML may not be useful.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3sCqY5O \n Why the word AI?\n\nOne of the biggest concerns Data Scientists have is that, they are not able to make the stakeholders or fellow non-data scientist colleagues realize :\n\nHow innovative their data science solution is. \n\nHow they solved a seemingly unsolvable problem.\n \nHow beautifully they have built a parsimonious model that not generalizes on test data but also in real life.\n\nHow they applied a concept prevalent in one field to their own field (True transfer learning).\n\nData Scientists are like Chess players. Chess players are popular but not as popular as Michael Jordan, Federer, Nadal, Messi, Serena, Ronaldo, Sachin.\n\nThe non-chess sports personalities are popular because they evoke emotions in the people, they show the 'grunt' .\n\nChess does not evoke emotions (in the audience) and neither does it show the 'grunt'.\n\nDoing Data Science is like playing chess. There is no 'Grunt' in Data Science.\n\nData Engineers on the other hand can show the 'Grunt' in the form 'amount of data streaming in', 'Ever increasing n.o of servers'. 'No of cores being utilized' etc.\n\nThe question then arises how does a Data Scientist show he/she is doing something remarkable. I think one way is to show how magical it is. And how do you show it ?\n\nYou call it AI. \n\nFor years, I used to wonder why do we need this Hype word. But I have started to realize the rationale behind usage of the word.\n\nIf you notice in the popular Netflix series 'Queen's Gambit', the director never overtly dwells on how brilliant a particular chess move was. \n\nThe reason being not many would even get 'How brilliant the move was'. Instead the director resorts to showcasing that 'Beth' is a very intelligent chess player through other ways.\n\nThose methods include showcasing to audience that Beth has this almost supernatural power to visualize the chess moves on a ceiling.\n\nAI provides the 'Grunt' to Data Scientists. In just two letters, it encompasses all the great things a Data Scientist does.\n\nImage credits : Wikipedia (Netflix)\n\nhashtag\n#datascience \nhashtag\n#artificialintelligence \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3EBIOZ9 \n Everyone talks about Feature Engineering / Feature Selection. However not many pay attention to 'Assumption Selection'. \n\nEvery model has some assumption. It is the skill of a Data Scientist to decide, which ones to strictly enforce and which ones to relax.\n\nhashtag\n#datascience \nhashtag\n#datascientist \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/345BHvm \n The German Tank Problem\n\nWhen we talk about application of statistics during world war 2, somehow the image of the airplane with red dots (survivorship bias) comes to mind. \n\nDon't worry I am not going to stick that image here again \n\nThere are however other lesser known statistical applications during world war 2 which had a huge impact in the outcome of the war. \n\nOne such application is in the 'German Tank problem'.\n\nThe 'German Tank Problem' is basically about statistical theory of estimation. Estimating population based on sample is nothin new, but the innovative application in the German tank problem is something to really learn about.\n\nThe problem is about estimating the number of tanks produced in a month by the German army just based on the serial numbers of the the captured tanks. The assumption of course here was that the tanks were numbered sequentially as they were manufactured. Also the sampling is done without replacement. \n\nThis humble technique of estimation even beat the Allies Intelligence agencies' prediction. \n\nWhile the Intelligence agencies' numbers were way off (around 1000-1500), the statistical technique had estimated the n.o of tanks to be 246 a month. This was later proven to be correct as the post war actual number showed it to be 245 tanks a month !!\n\nStatistics is an art of 'Doing More With Less'. \n\nP.S : Check out the link in comments for detailed statistical/mathematical details.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32EuNwj \n Confusion around Skewness of Probability Distribution \n\nWhile learning about skewness of probability distributions many aspiring data scientists / statisticians get confused between left and right skewed distributions. \n\nThis confusion stems from the fact that the head of the distribution points/leans right yet we call it left skewed or the head leans to left and we call it right skewed.\n\nWell the heuristic to quell this confusion is to observe the 'Tapering off' in a probability distribution. \n\nIf the 'Tapering off' is on the left then it is left skewed, if on the right it is right skewed.\n\nhashtag\n#statistics \nhashtag\n#probability \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Jo0zP2 \n Some people say \"What is the harm in calling Logistic Regression a classification algorithm?\" \n\nWell the issue is so grave that many data scientists believe Regression in Logistic Regression is actually a misnomer !!\n\nMany don't even realize that Logistic Regression could be used for Regression or for that matter one could use it without having to set arbitrary cut-off points !!\n\nCalling Logistic Regression a Classification algorithm and believing it to be for 'Classification purposes only' is akin to a person carrying a wheelbarrow on his head without even realizing that it can be rolled. \n\nLR as classification is actually a hack not the original intended usage. LR can be used for non classification (regression) tasks like Decile Analysis too (Link in comments).\n \nP.S : I have written a series of posts and even a meme on why Logistic Regression is Regression. The link to all is in the comments.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#logisticregression \nhashtag\n#statistics \nhashtag\n#ai \nhashtag\n#regression",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32IWLH7 \n Thanks for asking this Question Prasannaw Maddipati. Firstly, 'Confusion' in Confusion matrix is not because one can't learn what is TP, FP, TN, FN by rote or memorize. It is because the classifier 'mislabels' or 'confuses' the true label for something else.\n\nThe 'Confusion Matrix' in its truest form was invented by Karl Pearson (around 1904) and was then called 'Contingency Table'.\n\nThe word 'Confusion Matrix' came about (an educated guess) as it was adapted by the field of Psychology. There, the word 'Confusion' connotes to mislabeling the labels.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#ai \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#confusionmatrix",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32EvCVV \n The curious case of statisticians recommending Cross Validation.\n\nGenerally, Statistics is about parsimonious models, drawing inferences about population from sample. In a nutshell statistics is \"the art of doing more with less.\"\n\nWhereas In Machine Learning, it is about getting as big a dataset as possible for training, Parsimony of models is rarely the goal. In a nutshell \"ML is about doing more with more.\"\n\nHowever, one area where this paradigm flips on its head is in the area of model validation.\n\nStatisticians advocate for the usage of Cross Validation (a more compute intensive method of validation) while the Machine learning folks advocate for Train-Test split (which is not that compute intensive method relative to CV).\n\nOne would wonder, why this flip?\n\nWell, Statisticians believe that if you are doing Train-Test split, you are essentially not utilizing the full data at hand for model training. \n\nYou are kind of throwing away 25% or 30% of data.\n\nFor them, full usage of data kind of gets you closer to estimating the population parameters 'relatively more accurately'. Not utilizing the full data is kind of leaving money on the table.\n\nComing to why they advocate cross validation. \n\nIn an essence Cross Validation tests the model on the entire dataset although in subsequent sub samples or folds.\n\nOne kind of gets an 'average' measure of model consistency via CV.\n\nWhere as in Train-Test, not much can be known about model consistency merely based on one test set.\n\nHence I think Statisticians are right in advocating for CV over Train-Test split method.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#bigdata \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FvVulr \n What was your 'Red Pill' moment in Data Science ?\n\nMine was knowing 'Normal Distribution may be the most popular distribution but it is not the most prevalent'.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#datascientists \nhashtag\n#redpill \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3sEwbtO \n Things that surprise New Data Scientists when they first step into the corporate world.\n\n1) p<0.05 : 0.05 is not sacrosanct. Industries also use 0.01 and even 0.10.\n\n2) A highly accurate model might not be useful. A useful model might not be highly accurate.\n\n3) Logistic regression threshold of 0.5 is not sacrosanct. Again Industries use different threshold values based on their business problem.\n\n4) There is no perfect Normal distribution. Stop expecting to see perfect normal distribution be it the errors or the data generating process.\n\n5) There is no perfect dataset. Sometimes in the Industry one has to make do with what they have.\n\n6) N<30, use t test; N>30 use z test. (you can use t test for larger samples too. check out the links in comment).\n\n7) Sometimes Practical significance overrides statistical significance.\n\n8) R squared value of 0.9 or greater is rarely advocated upon. In fact some domains would be more than happy if they got R squared value of 0.4 or 0.5 !!\n\nPlease feel free to add any other points in comments section that may help aspiring data scientist or new data scientists. \n\nhashtag\n#datascience \nhashtag\n#datascientist \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3eMGG6p \n Intuitive Explanations to Data Science Questions\n\nStack Exchange and Cross validated are one of the most underrated resources to learn Data Science concepts.\n\nOver the years, asking some curious questions led me to interesting rabbit holes. I then used to bookmark these questions.\n\nJust thought I should share these wonderful questions and answers with the Data science learning community.\n\nSo here it is. Pls feel free to download this PDF. \n\nHappy Learning !!\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#artificialintelligence \nhashtag\n#datascienceeducation",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Hns0qx \n Could not find the main content of the post.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3zevvwE \n Heuristics to judge Data Science courses \n\nI am often asked by many aspiring data scientists, \"How to know whether a course is a good one?\". Since I will be unable to reply to all the inmails or messages, I decided to rather make a post. \n\nSo here are the heuristics:\n\n1) Time: \nIf the course proclaims to make you data scientist in 1,3,6 months. It is a NO.\nIf the course devotes only 1 week for a topic that actually has a huge body of work (e.g., Hypothesis testing, Probability & Probability distributions, GLMs, Machine learning algorithms) and many nuances in application. It is a NO. \n\n2) Orthogonal Stuff: \nIf the course teaches you totally orthogonal stuff in two consecutive weeks, then that course is not worthwhile subscribing to.\nE.g., Week 1: Hypothesis testing, Week 2: Advanced Machine learning (random forest, xgboost etc)\nGood courses are structured in such a way that they build upon the previous module.\n\n3) Unstackable skill set: \nMany courses would proclaim to make you Data Scientist + Data Engineer + Devops specialist + Data visualization expert + Domain expert + etc.. \nYes, a Data Scientist ought to know basic sql and enough programming to build MVP. However, when you vest your time developing distinct skill sets such as above, you dont become full stack or 10x Data Scientist or 10x Data engineer. Rather you spread yourself thin on all these distinct skill sets.\nThat is why Data Science is a team game not one hypothetical unicorn calling himself/herself jack of all trades and master of all. \n\n4) Data Science program being titled as Internship:\nOf late some institutes have started to call their programs as internship while in reality it is a data science training program. It is only an internship if you work on real industry or academic problem under the guidance of researchers, mentor or group of mentors. \n\nIf you pursue such courses and put the title of intern at xyz, chances are that you will be overlooked for real internship or Job opportunities since it is a deceitful practice to call your experience as internship' while in reality it is a data science learning bootcamp\n\n5) Teaching run-of-the-mill-toy datasets:\nIf a course exposes you only to the standard toy dataset problems like Titanic passenger survival prediction, Iris flower classification, Boston house prediction or heart disease prediction then chances are they are just picking up examples from the internet and have nothing new. So, it is a No.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3zevvwE \n 5) Teaching run-of-the-mill-toy datasets:\nIf a course exposes you only to the standard toy dataset problems like Titanic passenger survival prediction, Iris flower classification, Boston house prediction or heart disease prediction then chances are they are just picking up examples from the internet and have nothing new. So, it is a No.\n\nWhen you pursue Become Data Scientist in 1, 3, 6 months courses, you dont learn. You just become familiar with names of algorithms, statistical techniques. \n\nOften people completing these courses face dejection on not getting a DS job. They conflate knowing the name of algorithms vs knowing the algorithm.\n\nHope these heuristics were helpful. \n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientist \nhashtag\n#datasciencejobs \nhashtag\n#artificialintelligence \nhashtag\n#datasciencetraining",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3mE1PnB \n Formula 1, Kaggle and Applied Data Science\n\nIn Formula 1 racing, the drivers prepare for a race in 3 ways.\n\n1. Simulator \n2. Walk on the race track\n3. Drive car on the race track during practice season.\n\nDrivers use simulator to develop a mental picture of the track and to acclimatize themselves to turns and straights of the race track. But they don't stop at this because Simulator driving is never equal to driving on the real race track.\n\nSimulator driving does not give the real feeling for track undulations (banks and angles, ups and downs). It also never gives the drivers the feeling of gust of wind, rain, track heat. \n\nSo they literally take a walk on the race track .\n\nBut these are F1 drivers not marathon runners so they need to practice driving an actual car in real. So here is where the practice session comes in.\n\nSo you see there are 3 levels of information gathering and each level gets them that much closer to be well prepared for the actual race.\n\nWhy did I talk about all these ? Well recently a meme (link in comments) on twitter really ruffled the feathers of Kaggle supporters. \n\nI don't have anything against Kaggle competition but I only disagree with them when they say Kaggle competition is equal to real life ML experience. \n\nKaggle competition is like driving in a simulator. You don't get to experience the many curveballs that a real ML project throws at you. \n\nLets see some real curveball examples:\n\n1. After weeks of work (data cleaning, data transformation, feature studying and selection), you almost finish modeling but then the stakeholder/client tells you \"Hey there was an error in the data. Here, use this rectified data\". \n\n2. The client was promised the models in 2 weeks but you know it will take at least 3 weeks. Your manager is breathing down on your neck for faster delivery.\n\n3. Your team is informed that the scope has 'evolved' and that means taking into account new data which in turn means rebuilding of models.\n\n4. Your model is telling a story counterintuitive to the belief held in the domain. Now you are tasked with communicating this to the stakeholder.\n\nHence you see Kaggle is not enough. You need to experience the many curveballs.\n\nYou need to wade through real data, study the patterns and anomalies . And then build your model on training data much like practice session in F1.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3mE1PnB \n 4. Your model is telling a story counterintuitive to the belief held in the domain. Now you are tasked with communicating this to the stakeholder.\n\nHence you see Kaggle is not enough. You need to experience the many curveballs.\n\nYou need to wade through real data, study the patterns and anomalies . And then build your model on training data much like practice session in F1.\n\nAlso one hundredth or one thousandth of a second might separate two drivers in F1 qualifying but an improvement of one thousandth in accuracy in models in real life is rarely worth the effort.\n\nRemember, in Kaggle or other competitive hackathons, the task ends at obtaining some level of accuracy. However in real ML application, the real work only begins after obtaining accuracy metrics. \n\nhashtag\n#datascience \nhashtag\n#kaggle \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#hackathons \nhashtag\n#artificialintelligence \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3JmFeFH \n Know Your Data First - Time series, Cross Sectional & Panel Data\n\nAspiring Data Scientists believe that if they perform EDA i.e. draw up charts, study summary statistics and check for outliers, then their data science process is robust.\n\nBut most don't question the following:\n\nHow the data came about? \nShould the data have been collected in that particular format?\n\nA big chunk of the projects in the industry involves dealing with tabular data. However, not many have the knowledge of Time Series Data, Cross Sectional Data and Panel Data. \n\nA quick refresher :\n\n1.Time series\nWe are collecting data about Laptop Brand 'XY1' sales over the years\n\nYear  sales\n2017 50,000\n2018 54,000\n2019 65,000\n2020 45,000\n\nNotice above that the data is collected for the same entity 'Brand XY1' but at different intervals of time. \n\n2. Cross section data\nNow we are collecting not only Brand XY1 data but also its rivals. However we are just interested in the year 2020.\n\nBrand  Year   Sales\nXY1   2020  45,000\nEL3    2020  23,000\nFT3    2020  56,000\nTY4    2020  24,000\n\nNotice, we are interested in data at a particular point in time (i.e. yr 2020). This interest in particular point in time distinguishes Cross section from Time series.\n\n3. Panel Data:\nPanel data is what you get if you mix both the Time series and Cross Sectional data. Lets say that now you want to track brand XY1 and its rivals sales over the years. This results in data like below:\n\nBrand Year Sales\nXY1  2017 50,000\nXY1  2018 54,000\nEL3  2017 12,000\nEL3  2018 23,000\nFT3  2017 32,000\nFT3  2018 35,000\n\nNow lets return to the crux of the matter. \n\nTake a look at the heart disease dataset available at Kaggle (refer image). Many blindly apply ML algorithms on this data set.\n\nThere is a fundamental problem in the dataset. \nIf you notice, there is only one data point under each feature for a patient. Features like BP, cholesterol, heart beat are not static. They range. \n\nBlood pressure of a person varies hour to hour and on a daily basis, so does heart beat. So when it comes to prediction problem there is no telling weather 135 mm hg blood pressure was one of the factors to cause the heart disease or was it 140, all while the data set might be reporting 130 mm hg. \n\nIdeally, multiple measurements need to be had for each feature for a patient. And that multiple measurements leads to Panel data.\n\nExample \n\n Id       Date    Time    age  sex  CP  trestbps chol.....\n\n123abc  2/2/19 11.00am  45   M    3    145     233",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3JmFeFH \n Ideally, multiple measurements need to be had for each feature for a patient. And that multiple measurements leads to Panel data.\n\nExample \n\n Id       Date    Time    age  sex  CP  trestbps chol.....\n\n123abc  2/2/19 11.00am  45   M    3    145     233\n\n123abc  2/2/19 12.00pm  45   M    3    144     232\n.\n..\n345tyu  2/2/19  11.00am  32   F    1    126     204\n345tyu  2/2/19  12.00pm 32   F    1    124     203\n\nIn summary: Pls know your data before applying ML Algorithms.\n\nP.S: I have written more on why one shouldn't apply ML Algos on Heart disease dataset (Link in comments).\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#econometrics \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Ji8U6O \n Lessons for Data Scientists from Zillow fallout.\n\n  : One of the anonymous worker had this to say on Reddit \"Zillow has almost zero institutional knowledge in quantitative methods, and pretty much no one in Zillow AI had that kind of background.\" . Knowing how things work under the hood is not merely a 'good to have' skill but a mandatory one. \n\n       : Earlier in some of my posts I had opined that perhaps for y hat problems, low code libraries might be handy. But the Zillow saga has made me to change my mind. \n\nAn excellent article on this Zillow saga was penned by W.D (link in comments). While I don't want to unduly bash FB's Prophet library, I do agree with W.D sentiment on low code libraries. I would perhaps paraphrase and say this \"Low code libraries are only useful if you know what youre doing or are doing something unimportant, and it is possibly   if you dont know what youre doing and are using it to make an actual decision.\"\n\n'        : Following from last point, low code libraries always purport that they \"are easy to use\". That anyone or say any citizen data scientist (whatever that term means) can use it to build models. \n\nHere in lies the problem. Ease of use should not be at the cost of achieving the desired functionality. Unfortunately because many don't have the adequate statistical / mathematical knowledge, they will not be in a position to even discern if what they are doing is wrong. Low code libraries makes things 'easy to use' by abstracting a lot of things from the user's view. But the decision of 'how much to abstract' is often a very tricky one and I am afraid the creators of such library rarely consult statisticians/data scientists in this regard. (For more on how over abstraction can be dangerous, you can check out my article in comments.)\n\n : I know this might sound like a fluffy influencer talk but bear with me for a while. One of the important quality a data scientist must have or inculcate is 'courage'. \n\nWhy courage you ask? \n\nWell, often a data scientist needs to put his/her foot down and say to the management \"Sorry, I don't think we should really be applying this technique or ML algorithm to this problem. It is a bad idea\". Of course such strong conviction can only come from deep technical and domain (business) knowledge. But the point is, a data scientist should have the courage to say 'No'.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Ji8U6O \n Why courage you ask? \n\nWell, often a data scientist needs to put his/her foot down and say to the management \"Sorry, I don't think we should really be applying this technique or ML algorithm to this problem. It is a bad idea\". Of course such strong conviction can only come from deep technical and domain (business) knowledge. But the point is, a data scientist should have the courage to say 'No'. \n\nSometimes decisions would be made despite your suggestions/advice but at least you will not be completely blamed like Zillow Data Scientists are being blamed now.\n\nhashtag\n#timeseries \nhashtag\n#forecasting \nhashtag\n#datascience \nhashtag\n#datascientist \nhashtag\n#zillow \nhashtag\n#lowcode \nhashtag\n#statistics \nhashtag\n#econometrics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/346fRYD \n Market Research - A unique but efficient path to become a Data Scientist.\n\nThere are many paths to become a Data Scientist.\n\nOne transformative experience which really built the rigor in me towards applying data science was Market Research.\n\nLet me illustrate how:\n\n1. Data collection: \nAs part of my MR internship, I along with a fellow intern(now a good friend) had to design a questionnaire. It wasn't a simple 1 pager but a lengthy 40 pager !! We had to carefully consider the data measurement scales (nominal, ordinal, interval ratio) and Likert scales. After this, we went house to house to collect responses. \n\nNow we were cautioned not to make any errors. Because if we made an error, we would have to go back and record the response again. And there was no guarantee that the person would entertain us again.\n\nLesson relevant to \nhashtag\n#datascience: How and what data you collect is very important. Any mistake during data collection would not only be costly but very hard to rectify. \n\n2. Data Randomization: \nMarket research domain has a robust way to ensure the data one collects is randomized and is free from bias. One such approach we were taught was 'right hand rule'. We would go to the starting point in a locality and take the households that fell on the right hand side. If we had a successful response then we would skip 5 houses !!. Not to mention we also had a mechanism to randomly select a member of the house if the house had >1 member. It was called a 'kish matrix'.\n\nLesson relevant to DS: Pay attention to bias in your data collection.\n\n3. Encoding Data: \nOnce the hard work of data collection was done, it was time to enter the data on paper into a computer, for analysis and applying statistical techniques. \n\nLesson Relevant to DS: Data collection, cleaning, data entry is all drudge work. But one has to cross this bridge to get to the beautiful land of modeling. There is no way around it. Oh, you see all the above took up 60-70% of the project time!!\n\n4. Applying the right statistical technique: \nNow comes the fun part Or is it ? All your efforts till now will go in vain if you don't choose the right statistical techniques. E.g. Likert type questions require ordinal regression. These nuances must be learnt.\n\nLesson relevant to DS: Don't go and pick off the shelf libraries and apply it to your data. Understand the data, hypothesis before applying the relevant technique.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/346fRYD \n 4. Applying the right statistical technique: \nNow comes the fun part Or is it ? All your efforts till now will go in vain if you don't choose the right statistical techniques. E.g. Likert type questions require ordinal regression. These nuances must be learnt.\n\nLesson relevant to DS: Don't go and pick off the shelf libraries and apply it to your data. Understand the data, hypothesis before applying the relevant technique. \n\n5. Soft Skills: \nI was a geeky introvert, but the internship required me to chat up strangers, somehow convince them to answer a 40 page questionnaire!!. At the end, I didn't become a great orator but it did fill me up with confidence and skill to persuade.\n\nLesson relevant to DS: At the end of the day, much of applied DS projects is about convincing the stakeholder, making them believe in your model, helping them see the insights sans the technical mumbo jumbo. Soft skills are crucial in this regard. \n\nhashtag\n#marketresearch \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#experience \nhashtag\n#marketinganalytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Ji9hOK \n At Aryma Labs, our Internship process is a little different. Before we let the interns work on real projects. We do three things.\n\n Ascertain Intern's current understanding of statistical concepts.\n If their understanding is wrong or incomplete, make them unlearn it.\n Make them learn the concepts correctly from good resources.\n\nWe do these three things because there is a lot of misinformation out there in the internet with regards to statistical concepts. \n\nOnce the interns have learnt the concept correctly they kind of have an inbuilt radar to detect incorrect explanations.\n\nMy day is made when my interns come to me and say \"I think the confidence Interval explanation in this popular video or book is wrong\". \n\nInternship is just not about getting a certificate or earning stipend. \n\nIt is about learning things from first principles, developing a radar to detect incorrect explanations. \n\nOnly then we can apply data science correctly. \n\nhashtag\n#datascience \nhashtag\n#internship \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#learning \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/33Yl1Wp \n Putting ML models in production is tough. Monitoring and Maintaining it in production is even tougher.\n\nSome organizations think that their MLOps engineers can easily deploy these ML algorithms. What they and MLOps engineers don't get is, there is a difference between putting a set of rules (as in normal software programming) in production vs putting a mathematical function in production.\n\nPutting a mathematical function in production is akin to putting a jelly on a conveyor belt with heavy shaking. The mathematical function much like a jelly expands, squishes, moves around and sometimes totally collapses. \n\nCharles Martin has aptly summarized about this and I concur \"Data Science is not IT. And it is not software engineering. And while Data Science needs IT and software support, it doesn't deliver when all the time is spent building instead of learning and applying. And, today, this is the biggest challenge I see with clients.\"\n\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#statistics \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32J20Xj \n In context of Linear Regression.\n\nMuch like we can't observe a real black hole, we can't observe true error. However, the residuals give us a peak of what the errors look like (not necessarily an absolute view of it).\n\nIn a way, residuals are hazy reflection of the errors. \n\nYou can read more about Errors and Residuals from my post (Link in 1st comment).\n\nP.S The image on the left is the famous Black hole image released in 2019 by Katie Bouman and team.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#linearregression \nhashtag\n#machinelearning \nhashtag\n#statistician",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/32Bk3yN \n Are Estimation and Prediction the same ?\n\nI know many of us use the words Estimation and Prediction synonymously. \n\nHowever, if you are a hands-on statistician or Data Scientist, it is good to know the distinction between the two in a statistical sense.\n\nFirstly, let us unpack few terms: Statistic and Parameter\n\n Statistic describes the sample whereas a Parameter describes about the population.\n\n An estimator is a statistic that helps us infer the value of an unknown parameter.\n\nFor example:\n\nSample mean is an estimator of the Population mean (a parameter).\n\n In other words, an estimate is a guess about the true state of nature (the parameter).\n\nIn my last post \"Most Important Assumption Checks of Linear Regression\", I covered Andrew Gelman's concept of 'Representativeness'.\n\nJust to recap the concept: \"A regression model is fit to data and is used to make inferences about a larger population, hence the implicit assumption in interpreting regression coefficients is that the sample is representative of the population.\"\n\nSo, you see whenever we write the Regression equation, we ideally need to use  instead of Y. Because we are actually estimating the true value of Y through .\n\nThis is illustrated in the image below. Please note, I have put a hat on top of Error (which is to be read as residual) in first equation because in a way residuals are estimates of error.\n\nSo, now coming to what is predictor and prediction.\n\n Predictor uses the data to guess at some random value that is not part of the dataset.\n\n A prediction in an essence is a guess about another random value.\n\nSo lets say we take the same regression model as an example. The regression model has dependent variable (Y) - weight of people and X (independent variable) - height\n\nNow we have the data as follows:\n\nY (in pounds): 110, 130, 145, 160, 175\nX (in inches): 50, 60, 70, 80 , 90\n\nNow if were to use our regression model to predict the weight of a person say at 100 inches height, then it would be a prediction.\n\nYou see the height of 100 inches is not part of the dataset. However we did use the data at hand.\n\nP.S: I would urge the readers to also read the excellent stack exchange answer on 'Difference between estimation and prediction'. (link in first comment)\n\n#statistics #datascience #econometrics #datascientists #linearregression #machinelearning #statistician #prediction #estimation",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3mExipC \n Importance of knowing the history of an Algorithm / Statistical Technique\n\nI think it is important for every data scientists to know the history and context in which a particular algorithm or statistical technique was first used.\n\nThe reason being: \n Knowing that background tells you how far off your application is from the original intended usage. \n\nNow, Why is that important?\n\nIt is important because, blindly applying a technique from one field to another can result in unforeseen problems. \n\nFor example:\n\n P values\n\nMany use p values for Feature selection in Machine Learning (see the link in comments) \n\nWhat many don't realize is that p values were initially used for experimental trials that involved replication and randomization. \n\nA thorough understanding and background on p values is necessary to correctly apply p values in Machine learning.\n\n Setting a threshold in Logistic Regression (e.g. 0.5, 0.6 etc.).\n\nDespite writing many posts and articles that Logistic Regression is indeed a Regression, people do share misinformation on this time and again on Linkedin and other platforms.\n(you can read popular articles and posts on why LR is regression : link in comments).\n\nBut the larger point is, knowing that LR outputs probabilities and not magic 0 or 1 will help one realize that setting a threshold is only artificial and in a way a forced choice (pls do check Frank Harrel's article in comments).\n\n Statistics is only about Inference and not prediction\n\nRecently I came across posts and articles where people say \"Statistics is only about Inference and not prediction\". They go on to give example of OLS regression and how it is used in inference. They also seem to be under the misconception that OLS was solved only in 1900s after invention of calculators.\n\nFactually, this is wrong. Both Legendre and Gauss independently solved OLS around 1805-1809. Good 100 yrs before the author thinks OLS were solved.\n\nAlso, the earliest use cases of OLS was in the field of Astronomy and Geodesy. \nAnd guess what? \nThey used it for prediction not for inference.\n\nSo much for \"statistics primary goal is inference, its purpose is descriptive\" . \n\nSo, you see knowing the history and background in which the algorithm or technique was first used, will only be beneficial and help us avoid a lot of pitfalls.\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#datascience \nhashtag\n#artificialintelligence \nhashtag\n#statistician",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3HkdDTS \n The pitfalls of using SMOTE\n\nI was recently asked by an aspiring data scientist, \"what should be done when there is huge class imbalance?\"\n\nSomehow the word on the Data Science street is that we must always try to correct the class imbalance before applying any ML algorithm.\n\nWe are such stickler for perfection that we always want the ideal perfectly balanced dataset. \n\nLet me caution that, correcting class imbalance in never a good option. Especially with techniques like SMOTE.\n\nMany try to over sample the minority class by techniques like SMOTE (Synthetic Minority Over sampling technique).\n\nNow the first word that should ideally make you sit up and take notice is the word 'Synthetic'.\n\nYou see in SMOTE, you are artificially creating samples. \nNow why is this wrong.\n\nWhen you take a certain Data generating process or phenomenon, the class imbalances are what they are naturally. \n\nFor e.g. number of ppl defaulting on loans will always be lesser than ppl who don't.\n\nYou may artificially over sample the minority class and inflate the class frequencies in training set. But guess what, when it comes to the test set or validation set, your algorithm will perform miserably.\n\nMany are left scratching their heads at this point and question why their classifier algorithms performed so badly.\n\nThe answer is simple, your ML classifier 'learnt' a pattern in your training set and expected the same in validation set. However, the validation set class frequencies will be what they are naturally.\n\n You can control your training set but you can't control the real world or real data generating process. \n\nAnother problem that I have seen is that people use LIME or SHAP on top of using SMOTE. \n\nAgain two wrongs don't make a right. More on LIME and SHAP in future posts.\n\nSo to summarize, here are the pitfalls of using SMOTE\n\n Wrong calibration. \n\n SMOTE will make interpretation of predicted probabilities difficult.\n The predicted probabilities won't be reflective of the true probability prevalent in the natural data generating process /phenomena.\n\nWhat you should do.\n\n Ideally, never have the urge to correct class imbalances. \n Try to work with the class imbalance at hand.\n Never use SMOTE to correct the class imbalance\n Don't use LIME or SHAP on top of SMOTE. \n\nhashtag\n#datascience \nhashtag\n#datascientist \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3sFjzT7 \n Compelling reasons to not use Low code Data Science libraries.\n\n Some Low code Data science libraries are advertised as \"For Beginners & Experts\".\n\nSorry, it is for neither. In fact they do huge disservice to both Beginners and Experts. \n\nAsk how ?\n\nThey make Data science look unreasonably easy to beginners while making it totally unintuitive to the experienced. \n\n Now why do low code DS library creators hate statisticians? \n\nStatisticians bring with them epistemology and teleology. When statisticians measure low code DS libraries through these yard sticks, low code DS libraries comes up short on various levels. Basically, the statistical rigor exposes the many inadequacies of low code DS libraries. \nYou can read more about this in the articles/posts I wrote few months ago (Link in comments).\n\n Will Low code DS library creators ever listen to statisticians ?\n\nIf you see any profession, say carpentry, hard hat jobs, dentistry. The manufacturers of the tools for theses professions always listen and design the tools based on feedback from the professionals. But Low code DS library creators will not extend that courtesy to statisticians.\n\nOne reason being, if they listened, their low code will no longer remain low code !! \n\nYou see statisticians will want statistical rigor. Statistical rigor can't \nbe abstracted too much. Just like bias-variance tradeoff, there is a tradeoff here. \n\nThe more you abstract statistical concepts / machine learning algorithms, they tend to become that much less intuitive. The more unintuitive it becomes, more is the chance of such tool being used haphazardly and lo and behold you have \"80 % of the Data Science projects failing\".\n\n The low code DS library output looks good and has no bugs/errors.\n\nThe thing with low code DS libraries is that it should not only be judged by software engineering parameters (e.g. bug/error free code). \nOften the output from these libraries might be presented in visually cool fashion and also one may not encounter any errors/bugs.\n\nActually the low code DS libraries should be judged from a statistical perspective. Often wrong statistical assumption or no assumptions being made in the low code library is a huge error that does not show if it is gauged through software engineering lens. \n\nThese errors will show up in rather unpleasant ways like\n\"you finding out that the model did not work well on the ground\".",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3sFjzT7 \n Actually the low code DS libraries should be judged from a statistical perspective. Often wrong statistical assumption or no assumptions being made in the low code library is a huge error that does not show if it is gauged through software engineering lens. \n\nThese errors will show up in rather unpleasant ways like\n\"you finding out that the model did not work well on the ground\".\n\nBottom line : If you are in the business of applied Data Science, be wary of these low code DS libraries.\n\nhashtag\n#datascience \nhashtag\n#lowcodenocode \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#statistician \nhashtag\n#lowcode \nhashtag\n#programming",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3EyBcGD \n Parallels Between Data Dredging and Algorithm Dredging.\n\nMost of us would have heard of Data Dredging. \nIf not, let me provide a quick refresher on Data Dredging. This will provide a good Segway into explaining Algorithm Dredging.\n\nSo Data Dredging involves the following:\n\n No specific hypothesis is formulated prior.\n\n One analyses the data over and over again until some statistically significant result (denoted by low p-value) is found.\n\n Reports only the statistical significant result while failing to report any of the not significant results.\n\nNow the above is wrong because there are always accidental patterns in the data and data dredging (p-hacking) has a good chance of finding it.\n\n(Note : I am not saying p-value is \"something happening out of chance or randomness\". For a more detailed info on p-value and p-hacking, check the link in comment.)\n\nTo summarize, Data dredging is an unplanned way of analysis that cherry picks only statistically significant results. Many statisticians advise against data dredging because it leads to increase in false positive cases. One might find an effect where there truly was none. \n\nSo how does all these relate to Algorithm Dredging.\n\nBut firstly, what is Algorithm Dredging? and Where does it happen?\n\nAlgorithm dredging involves the following:\n\n No prior idea of which ML algorithm to apply to the data.\n\n Apply a dozen or more ML algorithms at the data.\n\n Report accuracy metrics like Accuracy, AUC, Recall, Precision, F1, Kappa etc. for each of the ML algorithm.\n\n Choose only the ML algorithm that has the best metrics (not to mention, much like F1 cars these models are separable only in the 3rd or 4th decimal point of accuracy metrics :P) \n\nBy now you would have guessed where this Algorithm dredging happens frequently.\n\nIf you guessed Low code Data science libraries, Bingo.\n\nLow code Data Science libraries perpetuate Algorithm dredging. \n\nAs much as data dredging is wrong, so is algorithm dredging.\n\nYou see, Algorithm dredging is mindless or unplanned way of choosing an algorithm to apply to the data.\n\nIf you are selecting ML models this way then I am sorry to say you are not doing Data Science. \n\n In data dredging you end up with false positive results. In algorithm dredging you end up with wrong models !!\n\nAlso the problem with algorithm dredging leads us to an XY problem (Link in comment). XY problem is actually asking about your attempted solution rather than asking about your actual problem.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3EyBcGD \n If you are selecting ML models this way then I am sorry to say you are not doing Data Science. \n\n In data dredging you end up with false positive results. In algorithm dredging you end up with wrong models !!\n\nAlso the problem with algorithm dredging leads us to an XY problem (Link in comment). XY problem is actually asking about your attempted solution rather than asking about your actual problem.\n\nPeople don't realize that they may have applied a wrong ML algorithm.(X)\n\nRather they ask the question what should be done to improve the accuracy metric further.(Y)\n\nIn my last post I highlighted the importance of asking the right question. \n\nAlgorithm dredging makes you believe that a right answer has been provided while blind sighting you from asking the right question.\n\nP.S : Image adapted from XKCD. \n\nhashtag\n#datascience \nhashtag\n#phacking \nhashtag\n#lowcode \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3quDBNt \n Glad that finally somebody said it.\n\nI have seen various instances of browbeating by Data engineers/ software engineers towards Data Scientists/statisticians. \n\nOften the tone is \"Hey Data scientists why don't you learn production grade coding?\"\n\nHowever, never does the polite data scientist ever retort and say \"Hey software /data engineers, perhaps you should also learn statistical concepts, probability, probability density functions, calculus, linear algebra and measure theory?\" \n\nI am not saying Data scientists should not know good coding. of course they should learn and know coding at a reasonable level. The point is, expecting Data scientists to know 'production grade' coding is as unreasonable as expecting software /data engineers to know deep statistics/ mathematics concepts.\n\nCoding problems predominantly are 'known-unknown' type. Meaning there are solutions to your problem but just that you don't know they exist. With good googling skills one can definitely find this 'available' solution.\n\nData Science problems however falls under two categories :\n\n1) Known-unknown type: These are your typical Linear / logistic regression problems or xgboost type problems.\n\n2) Unknown-unknowns type: These are problems you don't even know if there is a solution.\n\nThe biggest catch is that many believe that they are dealing with 'known-unknown' type problems in data science while in reality they are dealing with 'Unknown-unknowns' type.\n\nOverall, I agree with Ben Doremus that if someone is good at stats but average at coding, they can still 'get by' by googling. However, if some one is bad at stats and good at coding, they can't 'get by' by googling. In Data science projects, the nucleus is the algorithm/statistical technique. Often this nucleus is hard to figure out. It is an 'unknown-unknowns' problem.\n\nOne can't 'google away' through 'unknow-unknowns' type problems. Good understanding of statistics / Maths and a bit of creativeness is key to solving these problems.\n\nBottomline: It takes a team of Data engineers, Data scientists, software engineers and business stakeholders to make Data Science project a success. Data science community will do better if the browbeating attitude is done away with.\n\nP.S: Ben Doremus thanks for your articulate post..\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#coding \nhashtag\n#artificialintelligence \nhashtag\n#mathematics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3YZEzC3 \n Why people go wrong with Normal Distribution.\n\nWhat is in a Name?\nWell when it comes to statistics, Everything !!\n\nI have come across many posts and articles where people think 'Normal Distribution' is named so because it is the most prevalent in nature or that most natural phenomenon follow normal distribution. \n\nIt is this 'false belief' which makes people go wrong with Normal Distribution applications.\n\nFor Example: \n\n Use of Normal distribution (Bell Curve) in performance appraisals.\n\n People applying various transformation to dependent variable and Independent variables because their variables don't look 'Normal'. In a way they think it is 'abnormal' to not have a 'normal distributed' data !! (Oh Karl Pearson, your worst fears indeed has come true. Only if you had stuck to 'Gaussian-Laplace' curve ) \n\n People thinking that Central limit theorem kicks in at n =30 because the data starts to look 'normal'.\n\n People thinking that, under the null hypothesis, the distribution of p value is Normal !! (Correct answer is Uniform distribution. check the link in comments.)\n\nHopefully this post will help people realize that: \n\n The 'Normal' in Normal Distribution does not mean it is the most common distribution in nature. \n\n In fact it is abnormal to expect your data or data generating process to be Normal.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#normaldistribution \nhashtag\n#data",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WTJPVF \n Be Wary of Low Code Feature Importance / Selection Techniques.\n\nI recently came across a post about one low code feature importance library.\n\nI am always wary of these Feature Importance /Feature selection techniques and you should be too. \n\nAs mentioned in my article \"Abstraction and Data Science - Not a great combination\" (Link in comments), over abstraction in Data Science can make things unintuitive. \n\nBecause of over abstraction in these feature importance libraries, it isn't clear what statistical techniques are used to arrive at feature importance !!\n\nTherefore the users must always question two things:\n\n1) What statistical technique is being used to arrive at Feature importance / Feature selection\n2) Is the statistical technique employed, correct ?\n\nAlso, there are other concerns.\n\n Relative comparison problem\n\nWhen we do relative comparisons, there is an inherent problem. If we had poor variables in the first place, then relative selection will only pick the 'best out of the worst' or rank them in the order of 'least worst to worst'.\n\nThis is why every Feature selection must be guided by domain knowledge.\n\n Too close to call (Subjective deletion)\n\nFrom what I see on the chart, temperature, relative humidity and humidity all differ by each other only slightly on 'relative importance' scale. For e.g. it could be temperature 1.20, relative humidity 0.92, and humidity 0.73. So a decision to get rid of a variable based on these close margins becomes highly subjective. Here again, domain knowledge could be the only saving grace.\n\nWhat about Regression problems?\n\nOver and above the problems stated above, If users apply feature importance techniques to regression problems, they must also be wary of two problems:\n\n Interaction effects.\n\nIf one blindly goes by low code feature importance technique and removes a feature which had some interaction effect, then it could lead to model misspecification and bad inferences.\n\nAgain, domain knowledge can be a guiding light to discern interaction effects apart from interaction plots.\n\n Omitted variable bias.\n\nThe low code feature importance libraries tell you which feature is of low importance and hence should be removed. Remember most of these techniques are performed pre-modeling.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WTJPVF \n If one blindly goes by low code feature importance technique and removes a feature which had some interaction effect, then it could lead to model misspecification and bad inferences.\n\nAgain, domain knowledge can be a guiding light to discern interaction effects apart from interaction plots.\n\n Omitted variable bias.\n\nThe low code feature importance libraries tell you which feature is of low importance and hence should be removed. Remember most of these techniques are performed pre-modeling.\n\nSo there is a big chance that you are mistakenly removing a variable which could be significantly determining the dependent variable. This omission of a relevant variable is called Omitted Variable bias. As a result of removing relevant variables, your coefficients could be biased.\n\nSo, before you use any shiny new low code feature importance libraries, pls keep all the above points in mind.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#featureengineering \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#lowcode \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3PYzBRU \n Domain Expertise\n\nFew years ago I read the book 'Blink' by Malcolm Gladwell. In it, he narrates a very intriguing story about a Fire fighter.\n\nThe Fire Fighter (a lieutenant) along with his team answer a regular call about a house on fire.\n\nThey reach the house and notice that the fire is in the kitchen. They start to extinguish the fire with a fire hose. But to their surprise the fire does not abate at all. The men continue to spray water but the situation does not improve.\n\nThe Lieutenant feels something is off. He immediately instructs his team to evacuate the house.\n\nJust moments after the firemen make their way out, the floor on which they were standing collapses !!\n\nThe fire was actually in the basement and not in the kitchen.\n\nThe lieutenant did not immediately know what made him to order an evacuation. \n\nHowever, reflecting on the incident, the lieutenant realized that there were three anomalies.\n\n Ideally a normal kitchen fires respond to water but this one did not.\n\n Second, this fire was unusually more hotter than any kitchen fire he had experienced before.\n\n Third, the fire was quieter for a typical kitchen fire.\n\nThrough years of experience, the fireman had accumulated knowledge of what happens in a typical kitchen fire. It was this knowledge which allowed him to order an timely evacuation of his team.\n\n As they say, true sign of expertise is in knowing \"What doesn't happen\".\n\nYou see, this is in true essence 'Domain Expertise'.\n\nWhy this short story ?\n\nWell, I noticed in some posts that people think domain expertise in data science can be obtained in as simple steps such as:\n\n Googling the words / KPIs of a particular domain\n Writing a blog about the topics\n Practicing on Kaggle dataset\n And then making recommendations.\n\nI understand such posts are 'feel good' posts. But sometimes we need to tell the aspiring data scientists the stark reality.\n\nSometimes one has to choose bitter truth over a convenient and soothing lie.\n\nI am sorry to break it to you but I really wish domain expertise was this easy to gain.\n\nThere is a difference between knowing the name of something and knowing something.\n\n Domain Expertise is not some technique that can be picked up in such short practice. It is an accrued experience.\n\nAn accrued experience of knowing what happens and what does not happen in the domain. \n\nHence it takes years to build domain expertise.\n\nHappy Monday.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3PYzBRU \n I am sorry to break it to you but I really wish domain expertise was this easy to gain.\n\nThere is a difference between knowing the name of something and knowing something.\n\n Domain Expertise is not some technique that can be picked up in such short practice. It is an accrued experience.\n\nAn accrued experience of knowing what happens and what does not happen in the domain. \n\nHence it takes years to build domain expertise.\n\nHappy Monday. \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#domainexpertise \nhashtag\n#experience \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3I92SXY \n Many have asked us the motivation behind starting Aryma Labs.\n\nThe motivation behind starting Aryma Labs came from the realization that many companies have huge troves of data but dont know how to extract actionable insights from it. There is also a shortage of talent who know how to apply data science to business problems correctly. This void led to the creation of Aryma Labs.\n\nOf course, the journey so far has not been easy. \nThanks to Ridhima Kumar's initial grit and resilience. \n\nRunning a bootstrapped startup is never easy. However, we are glad that we have made a good beginning. \n\nThanks AIM and Sreejani Bhattacharyya for covering our story. \n\nhttps://lnkd.in/ggGJvJ-c\n\nhashtag\n#datascience \nhashtag\n#entrepreneurship \nhashtag\n#datascientists \nhashtag\n#artificialintelligence \nhashtag\n#machinelearning \nhashtag\n#bootstrapped \nhashtag\n#startup \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3C7eS8w \n Machine Learning  Software Engineering\n\nThey say 80% of the Data Science projects fail. One way to quickly hasten the demise of your data science project is to treat it like a software engineering project.\n\nI have seen Data scientists quitting organizations for the reason that their projects are headed by a person purely from software engineering background who does not understand the nuances of Machine Learning algorithm development lifecycle.\n\nYes, ML algorithms are expressed in codes but that does not mean that ML algorithm development be treated the same as software development.\n\nSome organizations think that their MLOps engineers can easily deploy these ML algorithms. What they and MLOps engineers don't get is, there is a difference between putting a set of rules (as in normal software programming) in production vs putting a mathematical function in production.\n\nPutting a mathematical function in production is akin to putting a jelly on a conveyor belt with heavy shaking. The mathematical function much like a jelly expands, squishes, moves around and sometimes totally collapses.\n\nThe reason machine learning models behave like a jelly is because they are dependent on external data. The probability distribution of external data keeps changing.\n\nThis is also one of the reasons why software design patterns of encapsulation, modularity is not that effective when it comes to Machine Learning.\n\nUnlike traditional software development, we can't just make a change in the property file and expect the program to run smoothly.\n\nThe article covers this topic excellently through the concept of CACE \"Change anything Changes Everything\".\n\n\"If we change the input distribution of values in x1, the importance, weights, or use of the remaining n  1 features may all changethis is true whether the model is retrained fully in a batch style or allowed to adapt in an online fashion. Adding a new feature xn+1 can cause similar changes, as can removing any feature xj . \n\nNo inputs are ever really independent. We refer to this here as the CACE principle:\nChanging Anything Changes Everything.\n\nThe net result of such changes is that prediction behavior may alter, either subtly or dramatically, on various slices of the distribution. The same principle applies to hyper-parameters. \n\nChanges in regularization strength, learning settings, sampling methods in training, convergence thresholds, and essentially every other possible tweak can have similarly wide ranging effects\"",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3C7eS8w \n The net result of such changes is that prediction behavior may alter, either subtly or dramatically, on various slices of the distribution. The same principle applies to hyper-parameters. \n\nChanges in regularization strength, learning settings, sampling methods in training, convergence thresholds, and essentially every other possible tweak can have similarly wide ranging effects\"\n\nIf there is one article that you would want to read over the weekend, this is it. I can't recommend this article enough.\n\nThe path to better MLOps is not through good software design patterns alone but through better understanding of statistics and maths behind the algorithms.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#mlops \nhashtag\n#softwareengineering \nhashtag\n#artificialintelligence \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3GnPFcH \n Data Science Talent Pool \n\nRecently I interviewed a candidate for Data Scientist role. He had listed proficiency in statistics and ML. So I began to ask questions - like explain P-value, should the DV and IDV be normally distributed in Linear regression, Is Logistic regression a classification or regression etc.\n\nTo my surprise, he got all the answers wrong.\n\nWhen I corrected his answers ,he became defensive and started pointing to resources to prove me wrong. Unsurprisingly one of his resources included a link to a popular Data science guru's YouTube channel.\n\nMy suspicions became clear on where he was getting is (mis)information from. \n\nFor the next 30 mins, I shared articles, excerpts from good books. Only then was he convinced. The cognitive dissonance was clearly visible on his face. \n\nThe interview became a tutorial class. But I didn't mind, as long as I could help the candidate see where he was wrong.\n\nThe level of misinformation is so much that those correcting this misinformation are looked at with suspicion and their know-how questioned. \n\nThis was not a one-off instance, I have noticed the drop in quality of candidates for the past year or two.\n\nHaving talked to fellow data science founders, industry veterans and recruiters, I realized that I am not alone in this sentiment.\n\nOf course candidates are not be fully blamed for this. Pop data scientists and shady training institutes are the major culprits. Not to mention 'Auto ML-ization' of data science.\n\nDear aspiring data scientists, huge number of followers does not mean that your fav DS guru is immune to mistakes.\n\nTruth be told, I am always wary of DS guru if their majority followers comprise of newbies. Of course there are exceptions, you have genuinely good DS guru's like Andrew NG, Statquest's Josh Starmer, Brandon Foltz and Ritivik math to name a few. \n\nThe reason I am wary is because as a newbie you would not know if your DS guru is teaching you correctly or not. \n\nOne heuristic that I follow to gauge DS guru's is whether they make their content the hero or themselves. \n\nWhen I or other data scientists correct wrong Data Science posts on LinkedIn or on YouTube, we are branded as stats police/ gate keepers.\n\nBut pls understand that we are not gate keepers. We want more people to come into data science. \n\nThe real gatekeepers are pop data scientists and shady training institutes who teach you wrongly.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3GnPFcH \n One heuristic that I follow to gauge DS guru's is whether they make their content the hero or themselves. \n\nWhen I or other data scientists correct wrong Data Science posts on LinkedIn or on YouTube, we are branded as stats police/ gate keepers.\n\nBut pls understand that we are not gate keepers. We want more people to come into data science. \n\nThe real gatekeepers are pop data scientists and shady training institutes who teach you wrongly. \n\nAsk how? Well because you learn things wrongly, you fail in your interviews. You are kept waiting from achieving your dream data science job. \n\nAt the end of the day, we are all interconnected in this DS eco-system. Students get trained in statistics/data science so that they can apply it in the industry.\n\nIf the training is bad, the industry doesn't get good candidates. Or if some do make it to industry with poor training, then they would be unable to solve real problems.\n\nNone of these scenarios augurs well for DS community. \n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#hiring",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3I8BxoU \n Yup, the unfortunate level of intellectual discourse \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WPK1p1 \n When it comes to coding, we can always look up stack overflow for some ready made code snippets to our problem. However, when it comes to Data science, there is no ready made answers. One reason is that the data, domain and the context varies. There is no 'one size fits all' solutions.\n\nThe closest we can come to stack overflow like solution is stackexchange and cross validated. Many intuitive answers are provided to statistical and data science questions.\n\nOver the years, I have personally learnt a lot from stackexchange and cross validated answers.\n\nThe PDF below comprises of list of people to follow on stackexchange and crossvalidated. Just go to the respective profiles and see their top questions and answers.\n\nHappy Learning \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#econometrics \nhashtag\n#timeseriesanalysis \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3CwZdzV \n A gentle reminder : The standard error is the estimate of the standard deviation.\n\nStandard error is based on samples. Standard deviation is based on the population.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#econometrics \nhashtag\n#statistician",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3C8V6d8 \n Why the Assumptions ?\n\nWhen it comes to statistical tests or ML algorithms, we make many assumptions.\n\nFor e.g. in Linear regression we make assumptions like:\n\n\"Errors need to be normally distributed\"\n\n\"Independence of errors\"\n\n\"Linearity\"\n\n\"Homoscedasticity\"\n\nBut why do we make these assumptions ? What purpose do they serve ?\n\nOne might be right in thinking that it is for mathematical / statistical convenience. \n\nThe deeper answer is that:\n\nWe make assumptions because in a way it means that we have less parameters to estimate. \n\nThe more assumptions we make, the lesser parameters needs to be estimated.\n\nImportant caveat : Assumptions may not always be helpful. \nFor e.g. in Student t -test we do assume equal variance. But the Welch's test (which makes no assumption of equal variance) is generally advocated by statisticians over student t test because of better statistical power.\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3GJzQ0p \n Whenever you use any low code data science library, ask yourself this:\n\n How over abstracted is the library ?\n\n What details are being hidden as a result of this over abstraction?\n\n What collection of algorithms is preset and given to you?\n\n Who made the decision to have only a set of algorithms for you to compare? Were they trained statisticians?\n\n Why is it that when the algorithms are compared against each other by accuracy metrics, appear like lap timings of F1 cars and are separable only in the 4th or 5th decimal? (hint because the collection of algorithms belong to the same family ).\n\n Is it even ideal to choose one algorithm out of the rest merely based on 4th or 5th decimal points ?\n\n Last but not least, what dependencies does your low code data science library have? What other open source libraries does it utilize under the hood?\n\nLow code data science libraries may appear squeaky clean and fit like Homer Simpson at the surface. But behind the scenes a lot is hidden. One must be wary of these. \n\nFor Y-hat problems, Low code libraries may be ok to use because the user gets to know whether the output is right or wrong immediately. The feedback lag is negligible.\n\nHowever for  -hat problems, Low code libraries should be seldom used and if used, one should keep in mind all the points above.\n\nI have written in detail about the many perils of low code data science libraries. Link in comments.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#lowcode \nhashtag\n#artificialintelligence \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WyUKEz \n If you are an aspiring data scientist, two skills will keep you in good stead\n\n1) Be an Autodidact\n2) Know how to Google\n\nThe combination of these two skills have tremendous benefits at three stages:\n\n Learning stage\n Job hunting stage\n At the job stage\n\nIn this post however, I would like to delve deeper on how the combination of these two skills will help you at the learning stage.\n\nThere is plenty of free material on the internet if you want to learn Statistics, Linear Algebra, Calculus, Machine Learning, NLP, Deep Learning and Time Series Forecasting.\nAll you need to know is how to search for these free materials and once you have discovered them, learn it on your own.\n\nMost data science guru's are to some degree Autodidact and they have the ability to search for free content on the internet.\n\nThey then monetize it by selling you their courses (which essentially is a repackaging of the many freely available content on the internet) .\n\nYou can save your hard earned Rupees /Dollars by being an Autodidact and knowing how to google things.\n\nFrom what I see on the Internet, some of the data science Guru's content is a straight lift from freely available contents such as Andrew NG's ML and DL course, Khan Academy's stats playlist, Gilbert Strang and 3blue1 brown YouTube videos etc. \n\nI don't have a problem when some data science guru's refer these resources and base their content on them.\n\nBut personally I feel they are doing disservice to the audience by not pointing them to the original resource from which they drew inspiration or learnt things.\n\nSo before you decide to follow some data science guru's videos or enroll in their courses, pls read and learn things from the Gangotri (the origin).\n\nAccording to me, the below is the 'Gangotri of things':\n\n Statistics - Khan Academy, Zed Statistics, Josh Starmer, Brandon Foltz, ISLR course videos, MIT Open course and NPTEL\n\n Linear Algebra - Gilbert Strang, 3blue1brown.\n\n Machine Learning/ Deep Learning - Andrew NG courses.\n\n NLP - Stanford CS229 (Prof Christopher Manning), Dan Jurafsky's NLP series.\n\n Probability - Joe Blitzstein videos.\n\nP.S: Of course this is not an exhaustive list. But by going through these resources you will be in better position to decide whether it is worth pursuing courses offered by some data science guru or institutes.\n\nHappy Learning. Happy Monday \n\n#datascience #statistics #machinelearning #datascientist #deeplearning #datasciencejobs #hiring",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3YVLYCg \n What to Expect as a Data Scientist in the Industry - A Note to Aspiring Data Scientists\n\nI have worked in different capacities in my career. \n1) Individual contributor \n2) As part of a team. \n3) As a data science team lead\n\nI thought I should share some of my experiences with aspiring data scientists.\n\n Never enough data - Often the data scientist finds himself/herself in a position where the data needed for modeling is inadequate. Always be prepared to model with the data at hand.\n\n Noisy Data - The data that you requested has arrived but it is full of noise. Begin the dreaded data cleaning process with a smile because you will never get perfectly clean data in real life. \n\n Scope Change - You got the data, you know the business problem and have just started modeling. But just then the stakeholder informs of you a change in scope. Now this small change in scope changes everything. Re do everything from data cleaning, data preparation and modeling (again with a smile because this movie will keep repeating ).\n\n Less is more - Deep learning et all are powerful but you don't want to kill a mosquito with an anvil. Most enterprise problems have tabular data. Always try to implement a parsimonious model or technique first. 'Less is more' should be the mantra.\n\n People to champion your cause: As a Data Scientist, you will need somebody to champion your cause. Now championing your cause here means shielding you from unwanted politics, making available the required resources (data, GPUs, training) and finally requesting the higher management to give you a longer rope especially when building Data Science products.\n\n Be a good explainer: As a child I was fascinated by magicians. I even once mustered up courage and asked one \"How did you do the trick\".\nTo non data science folks, your demo or Algo will appear like magic trick. But unlike magicians, you can't say \"it is magic\" and get away. You need to be a good explainer.\n\n Blow your own trumpet : As a data scientist, only you know what ordeals you faced in developing an innovative solution. \nYou should effectively narrate how difficult a problem you have solved, how your solution has yielded or will yield millions of dollars in revenue.\nThe reason you have to blow your own trumpet is because no one will do it for you and your promotion / hikes/ job depends exactly on how you and your work are perceived.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3YVLYCg \n Blow your own trumpet : As a data scientist, only you know what ordeals you faced in developing an innovative solution. \nYou should effectively narrate how difficult a problem you have solved, how your solution has yielded or will yield millions of dollars in revenue.\nThe reason you have to blow your own trumpet is because no one will do it for you and your promotion / hikes/ job depends exactly on how you and your work are perceived. \n\n Expectation management : Every Data Science project is about expectation management. Unless you set the right expectations, stakeholders will expect the moon from you. \nAlways remember: under promise but over deliver; never the reverse. \n\nHope this was useful. \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#datasciencejobs \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#hiring",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vntKvR \n Rules of Thumb in Statistics\n\nWe come across 'Rules of Thumb' in: \n\n Experimental designs \n Statistical tests to apply\n Thresholds/cutoff to set in ML algorithms \n\nRules of thumb arise out of practical experience rather than proven theory. The problem is that, it may not be generalizable everywhere or in every situation.\n\nHere are some Interesting examples.\n\n N>30 \nThe two common issues with N>30 is that a) It is believed that at N=30, CLT kicks in. b) It is believed that one could should use t test when N<30 and when N>30, Z test.\n\nReason: Back in the day (when computers were in its infancy), people used tables much like a logarithm table to see the values of any distribution for any combination of DF and significance level.\n\nAnd because such a table had to fit many distributions (t, normal, chi square etc.), the t distribution page was restricted to only 30 entries. Also at n=30, normal distribution and t distribution were deemed 'approximately' same.\n\n P < 0.05 \nMany believe that significance level of 0.05 is sacrosanct. \nBut how did this p<0.05 come about. It is from misinterpreting RA Fisher's words.\nRA Fisher's these words try to dispel the myth:\n\"If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance\"\n\n 1 in 10 rule \nThis particular rule applies in Logistic regression.\nIt basically says that you could include 1 predictor per 10 cases. Many statisticians have advocated against this citing reasons of 'accuracy of logit coefficients' and 'separation'. (Link of paper in comments.)\n\n 10 % Sample Size \nThe rule states that the sample size should not be more than 10% of the population. This rule is generally cited when dealing with case of CLT where sampling is without replacement.\n\nSimilarly, this rule is also cited with independence criteria of Bernoulli trials. Ideally, Bernoulli trials must be independent but this condition is relaxed if sample size is 10 % of population or lesser.\nThis rule of thumb is not much wrong but just that it is still arbitrary.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vntKvR \n 10 % Sample Size \nThe rule states that the sample size should not be more than 10% of the population. This rule is generally cited when dealing with case of CLT where sampling is without replacement.\n\nSimilarly, this rule is also cited with independence criteria of Bernoulli trials. Ideally, Bernoulli trials must be independent but this condition is relaxed if sample size is 10 % of population or lesser.\nThis rule of thumb is not much wrong but just that it is still arbitrary.\n\n Chi square test 20 % of the cell rule.\nFirstly, many think the rule of if 20% of the cells are less than 5, one can't perform chi square' applies to observed value. In fact it actually refers to expected value.\n\nSecondly, this rule of thumb is considered too stringent by some statisticians and have advocated alternatives like Fisher's exact test.\n\n: It is always better to check how these rules came about. If it is due to reasons like Lack of technology & misunderstanding of concepts, then one must be careful in using them.\n\nHappy Monday. \n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3jCiXeK \n Tukey and Mosteller's bulging rule (a.k.a Ladder of powers rule)\n\nIn one of my posts (Link in comments), I had written about the order of importance of assumptions of Linear Regression. In it, I had emphasized that the assumption of Linearity is most important.\n\nThere might be cases where one could encounter non linearity. \n\nTukey and Mosteller devised their famous \"bulging rule\" for scenarios where one may have to re-express the dependent variable or independent variables or both to bring about linearity. \n\nHere is how the rule works:\n\n Plot the dependent and independent variable\n\n Check if your plot resembles any of the bulges with any of the four quadrants (pls refer fig 1.a).\n\n Apply the suggested transformation of that quadrant to either the DV or IDV or both. \n\nIllustration:\n\nLets say, your plot looks like fig 1.b. This plot resembles the bulge in quadrant 4 of fig 1.a, then the suggested rule is to increase X (the independent variable) to the power of 2, 3...etc. or decrease y (the dependent variable) by power of 1/2, log (y) etc., or perform the transformation on both x and y simultaneously.\n\nSimilarly, the adequate transformations can be applied based on the shape of the plot. \n\n Caution:\n\n The ladder of powers rule should only be applied if the underlying relationship between the DV and IDVs happens to be of the form of powers (or logs).\n\n Generally the power transformation like the ladder of powers rule work only if the relationship between the DV and IDVs is monotonic. if there are up and down undulations (non monotonic) then these rules don't work.\n\n With any transformations, one has to be careful in interpreting the results. The interpretation of y= B0 + B1X1 + B2X2 + e will not be the same as say log(y) = B0 + B1X1 + B2X2 + e\n\n Interesting fact: \n\nIt was Tukey and Mosteller's bulging rule that motivated the development of Box Cox transformation. The Box Cox transformation is used to stabilize variance and also to transform non-normal dependent variable into normally shaped ones.\n\nHowever there are many disadvantages of using Box Cox (more on that in future posts). \n\nP.S: Credits and References of interesting articles on the bulging rule are in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#linearregression \nhashtag\n#artificialintelligence \nhashtag\n#econometrics \nhashtag\n#exploratorydataanalysis",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3hTgcoZ \n Log transformation for the wrong reasons\n\nIn my last post, I had talked about various power transformations and log transformations (ICYMI, link in comments).\n\nOne of the most popular transformation method among both aspiring and seasoned data scientists is the log transformation.\n\nAt the drop of the hat, people say \"let's log transform the data\".\n\nHere are scenario's where Log Transformation is applied for wrong reasons.\n\n1. To make data \"Normal\" \n\nMany have the \"Normal distribution\" fixation, i.e. they think Normal distribution is the most prevalent distribution in nature. \nBecause of this fixation, people try to make their data look \"Normal\" by applying transformation like log transformation and Box-cox transformations.\n\nNow one key thing to keep in mind is that, log transformation will make your data near \"Normal\" only if your original data distribution was Log Normal.\n\nIf your original data distribution was not log normal, then log transformation could exacerbate skewness. For e.g. An already right skewed data may become more right skewed or left skewed.\n\nI would urge the readers to read this excellent paper \"Log-transformation and its implications for data analysis by Changyong Feng et all\" (Link in comments).\n\n2. To stabilize variance and cover up outliers.\n\nWhen the data are plotted, often the outlier sticks out like a thorn. Now outliers may have some useful information to tell. But many either delete outliers or do log transform to make the outlier not look like an outlier. An added problem is that, if outliers were present and we log transform the data, then the variance will get inflated rather than being stabilized.\n\n3. Aesthetic reasons \n\nAgain a data scientist might find the data too \"lumped up together\". To spread out the data points, they may log transform the data.\n\nLog transformations also have the following repercussions.\n\n Making Interpretations tricky - When log transformations are applied say in linear regression, the interpretations changes. Now we have introduced a multiplicative relationship and the interpretation becomes percentage change rather than the plain vanilla \"the mean change in the dependent variable for one unit of change in the independent variables\".\n\n Deviating from original hypothesis\nOnce you log transform the data, you are no longer testing the original hypothesis. (Check the section 3.2 in the paper by Changyong Feng et all (Link in comments)).\n\nThe Remedies:",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3hTgcoZ \n Deviating from original hypothesis\nOnce you log transform the data, you are no longer testing the original hypothesis. (Check the section 3.2 in the paper by Changyong Feng et all (Link in comments)).\n\nThe Remedies:\n\n Think before you transform: Given the above, it is always better to be cautious in applying log transformation to the data. Remember, the cure should not be worse than the ailment.\n\n Adopt GLM: In context of regression, if you suspect error distribution is not 'Normal', then there are plethora of GLM families that one could apply. Most importantly, GLM families of algorithm help you circumvent the \"error have to be normal\" problem.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3IafIVT \n Data Science Ritualism\n\nCouple of months ago, a data science training institute asked my opinion on their curriculum. \n\nMy straightforward feedback was that, they were either covering unnecessary things (from an Industry application point of view) or \nthey were covering topics just superficially.\n\nMany such institutes knowingly or unknowingly propagate what I call 'Data Science Ritualism'. \n\nWhen it comes to Linear Regression, they propagate the following rituals \n\n1) Check for normality of Dependent and Independent variable (this is wrong)\n\n2) Check for normality of residuals (This is ok, but normality assumptions is the least one should be worried about. Check the link in comments on my post on the same)\n\n3) Strictly adhering to 0.05 p-value and removing variables that don't meet this criteria. (In reality, p value of 0.05 is not cast in stone. Many Industries, use either more stringent p-value (0.01) or use a little \nlax p-value of up to 0.10. At the end of the day, one should be wary of Rule of thumb - related post link in comments)\n\n4) Using R squared as a guidance for 'Goodness of fit' (This is again wrong)\n\n EDA Ritualism\n\nComing to Data Science projects, Some Institutes propagate 'EDA Ritualism'.\n\nEDA is important phase of a Data Science project. But I often notice that it is followed more like a ritual. Pls note that before you do any EDA, you must have a hypothesis (guided by domain knowledge of course).\n\nThe reason you need to have a (hypothesis + domain knowledge) because standard EDA function will render plots showing data points as outliers. But whether they are really an outlier can only be informed by your hypothesis+ domain understanding.\n\nOne classic example of EDA Ritualism that I keep encountering is, from the jupyter notebook of aspirants analyzing Kaggle's heart disease prediction dataset.\n\nThey flag many many data points as 'outliers' which are perhaps not outliers if one takes into account a patient's BMI or age !!\n\nOverall my suggestion (much along the lines of what Maarten has suggested) would be that:\n\n We need to teach aspirants the difference between errors and residuals. How measurement error can impact a model. What technique to apply when we have measurement errors.\n\n Many courses stop at Linear Regression and Logistic Regression. There should be more emphasis on GLM family of models. Mixed Effects models.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3IafIVT \n Overall my suggestion (much along the lines of what Maarten has suggested) would be that:\n\n We need to teach aspirants the difference between errors and residuals. How measurement error can impact a model. What technique to apply when we have measurement errors.\n\n Many courses stop at Linear Regression and Logistic Regression. There should be more emphasis on GLM family of models. Mixed Effects models. \n\n Also GEE, GLS and GAM techniques needs to be taught( I know these sound complex but once your understand what they are, they become intuitive - some useful links on this subject in comments).\n\nWith respect to EDA:\n\n One should focus more on asking good questions and generating good hypothesis before clicking on a button that generates a multitude of EDA charts.\n\n Always try to play the devil's advocate when you see something counterintuitive in the EDA. \n\nHappy Monday \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#statistics \nhashtag\n#datasciencetraining",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3I6Xbd5 \n A really intuitive illustration of why Standard Deviation is called the \"Measure of Dispersion\".\n\nCouple of points to notice:\n\n Notice how the spread of the data points away from the mean increases the Standard Deviation (indicated by widening of pink arrows).\n\n The mean is susceptible to outliers. Notice how the extreme outlier pulls the mean in its direction.\n\n Talking about susceptibility to outliers. While mean is susceptible to outliers, the median is susceptible to length of sequence !!\n\nAn excellent code illustration of how median is susceptible to 'length of sequence' is provided by Adrian Olszewski (Link in comments).\n\nStandard Deviation Video Credits - Twitter (@ TimBrzezinski)\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3YRi6Hj \n There are two types of Data Scientist. \n\nType 1 : Immediately deletes an outlier upon encountering it.\n\nType 2 : Ponders deeply about the outlier and thinks it could really have some good insights. \n\nDon't be Type 1. \n\nhashtag\n#datascientist \nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#outliers",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3jEqWIg \n ISLR vs ESLR\n\nOne of the common mistakes I find aspiring data scientists make is that, they pick up ESLR book first. \n\nThe result of this choice is that the aspirants feel \"they are not smart enough\". Some even quit pursuing Data Science thinking, \"that's how tough all of data science is gonna be\". \n\nLet me tell you an honest truth. ESLR is a tough book to assimilate even for people with good mathematical or statistical foundations.\n\nFew years ago, due to a bad advice I picked ESLR first. On taking a look at the index of the book, I thought it would be 'basic' (just as it was told by the person who advised me).\n\nBut once I started going deep into the chapters, I noticed some knowledge gaps in me. I had to learn and re learn some of the calculus, Algebra and probability theories to understand\nthe mathematical derivations and theories. Not to mention, my confidence in my ability to learn things fast was shaken too.\n\nI initially thought, perhaps it is just me who is finding the contents tough. But upon speaking to fellow data science practitioners I realized that I was not alone.\n\nIt took me nearly 8 months to finally go cover to cover on ESLR. \n\nSo do I recommend not reading ESLR ?\n\nAbsolutely not. \n\nI would 100% recommend all aspiring data scientist to read ESLR but I would suggest so only after they have read ISLR. \n\nIn barbell lifting, we start with lighter weights first and then progressively add heavier weights. Similarly it is better to start with ISLR first and then move to ESLR.\n\nHappy Learning. Happy Monday.\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#datascienceeducation \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#learning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3vqbG4i \n Data Science Toolkit \n\nEvery Data Scientist's biggest dilemma is \"Which algorithm to apply to solve the problem at hand\". \n\nFrom my experience, I feel there are 4 \nhashtag\n#datascience toolkit scenarios\n\n Not knowing the tool's full use cases:\n\nI have seen many courses that stop at Linear Regression and Logistic Regression. They don't cover other types of regression or for that matter the GLM family.\n\nAs a result many data scientists don't know how to fully exploit the tools (algorithms) at their disposal. \n\nThe other major problem is the thinking \"this algorithm can only do x\". The best example is \"Logistic Regression is for classification only\". \nLogistic Regression has been used as non classifier (in the form of Decile analysis) for ages in domains like Marketing and Credit risk rating.\n\nFurthermore few days back Adrian Olszewski penned an excellent post highlighting the very many non classification uses of Logistic Regression (link in comments)\n\n Using the same tool again and again:\n\nIn few of my consulting projects, I noticed that the teams had got habituated to using XGBoost and Catboost. \n\nThe reason - they found good success using them. \n\nSimilarly Linear Regression too is heavily used by many in the industry owing to interpretability. \n\n Using an Anvil to kill a mosquito: \n\nDeep learning for sure has enabled us to achieve great things in NLP and CV. However, for tabular data, DL seems like an overkill to me.\n\nDitto goes for time series prediction tasks. From my experience, I find that simpler algorithms like ARIMA et all do outperform DL methods on time series problems. \n\n Choosing not to use any tool:\n\nOften , we \nhashtag\n#datascientists forget to ask an important question. \"Is Machine Learning algorithm required for this business problem?\n\nI know the last statement would have set a cat among the pigeons. Many of you would be alarmed and probably might ask Are you trying to put us out of our job.\n\nOn the contrary, No.\n\nThere are perhaps thousands of genuine problems which do require Machine Learning approaches. However, there are some problems which could be solved through simple analytics rather than ML.\n\nI have seen implementation of ML algorithms to very frivolous problems or problems where \nhashtag\n#ml simply can't be applied.\n\nWhen a Data Scientist says  sorry, your business problem does not warrant a ML algorithm, it enormously raises the stature of the Data Scientist in the eyes of the client/company.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3vqbG4i \n There are perhaps thousands of genuine problems which do require Machine Learning approaches. However, there are some problems which could be solved through simple analytics rather than ML.\n\nI have seen implementation of ML algorithms to very frivolous problems or problems where \nhashtag\n#ml simply can't be applied.\n\nWhen a Data Scientist says  sorry, your business problem does not warrant a ML algorithm, it enormously raises the stature of the Data Scientist in the eyes of the client/company.\n\nNow saying the above requires honesty and courage. It also does two things:\n\n It showcases that the Data Scientist has vast knowledge and hence also knows the limitations.\n\n It showcases that the Data Scientist not only understands the business but also cares about client well being. \n\nIt is easier to bill a client for a project that really requires no ML . But in the long run, one would not have repeat customers. \n\nMuch like any other business, success in data science consulting depends on Deep know-how and honesty. \n\nhashtag\n#statistics \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Gst2E6 \n I never take a Data Scientist or their opinion seriously, once they say \"Statistics is irrelevant\" , \"Statistics is rarely used in Data Science\" or \"You don't need statistics in Data Science\". \n\nIn the past, many have asked me and other Statisticians- why do we get riled up when someone says \"Logisitic regression is not regression\".\n\nBecause we knew (feared) opinions like this would be shared in the internet and unfortunately would become common.\n\nOpinions like these are formed because of following reasons:\n\n1. First they rename statistics concepts and techniques \n\n2.They think or try to brand statistics as \"hypothesis testing\" only.\n \n3. Then they discredit statistics and it's contributions.\n\n4. They eventually forget that most of the present day ML and DL algorithms arose out of statistics.\n\n5. They use over abstracted (software term) Data Science low code libraries which further makes them unsee the statistics at work under the hood.\n\n6. Overt focus only on y hat problems (kaggle competitions) makes them think ML = prediction only. \n\nSo dear aspiring data scientists, don't fall for such lazy opinions. Statistics is fundamental to Data Science. \n\nJust because you never lifted the hood of the car doesn't mean the engine does not exist. The car runs because of the engine. \n\nSimilarly much of modern day ML and DL is powered by statistics (whether you acknowledge it or not). \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3CbwiRs \n Should Data Scientists know what's under the hood?\n\nIn my last post, some opined that a data scientist need not know statistics or how the ML algorithms worked under the hood.\n\nOften the proponents of this thinking give the analogy that \"Driver of a car need not know how the engine work\".\n\nMany low code Data science library creators will also have you believe that being a operator of a tool is sufficient. No ulterior motive on their part (wink, wink) \n\nYou may ask what is the harm in not knowing what's under the hood. While there are many problems, the most common problem that I have seen is acceptance of defaults.\n\nHave seen many Data Scientists stick to threshold of 0.5 in Logistic Regression because they believe it is the 'convention' one must stick to.\n\nSame applies to p-value of 0.05.\n\nSticking to 'default' could also prevent one from getting an optimal solution for the problem.\n\nComing back to the analogy \"Driver of a car need not know how the engine works\", I must say it is flawed.\n\nA Data scientist is not just the operator of a car but in an essence the builder and maintainer of the engine too.\n\nNow lets say you are just a 'Button-Pusher' Data Scientist. You don't know how things work under the hood and the model breaks down. Whom will you approach now to fix the model? Another Data Scientist who knows the ins and outs ? Then why not hire this data scientist instead from the beginning. You see, this is how organizations think.\n\nTherefore if you are not the person knowing how things work under the hood, your job could be in jeopardy.\n\nWhen a model breaks down, the data scientist should know the reasons for the break down and most importantly the ways to fix it.\n\nSo, what is the correct analogy ?\n\nWell, IMHO the data scientist is like a mechanic and a test driver. Once he/she finds the model to be ok, they then can deliver the model to the customer (client).\n\nThe client is the operator not the data scientist. And after some usage if the client finds that the engine (model) is sputtering, they can go back to the data scientist for the fix or maintenance.\n\nLet me know your thoughts in the comments.\n\nHappy Monday \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Wpdgzf \n How much does a Candidate's Kaggle profile matter for a Data Science role? \n\nTo be honest, very little. \n\nLet me elaborate. \n\nI would prefer to hire a candidate with one good real project experience than a person with some grand master title but having zero real project experience.\n\nWhen I say one good real project experience, I mean the following experience:\n\n Been involved in problem formulation stage.\n\n Data collection and cleaning (liaising with people or departments to get data).\n\n Model building (Experience of not just XGBoost but perhaps a variety of other algorithms too).\n\n Presenting models to stakeholders or clients.\n\n Getting feedback on the model working from stakeholders / clients\n\n Ironing out the deficiencies in the model or at least some closure that the model failed or succeeded.\n\nYou see a real data science project is just not about fitting a curve to the data or tuning some hyperparameters blindly. \n\nIt is also about:\n\n Correct problem formulation\n\n Collecting the right data and cleaning it.\n\n Applying algorithm in a scientific /knowledge driven way rather than \"throw any model at the data and see what sticks\".\n\n Expectation management. \n\nA person who has experienced the above will also have humility. \nI bring the topic of humility because of late I have seen some Kaggle GMs and Masters pass really disparaging remarks like \"Statistics in not required in MOST Data Science projects\".\n\nData science projects are tough. In Kaggle competitions, accountability ends after you submit the model. In real life Data Science projects, the accountability only begins.\n\nP.S This post was inspired by a Reddit post (link in comments). I would urge the readers to read the many insightful comments under the Reddit post.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#datasciencejobs \nhashtag\n#ai \nhashtag\n#kaggle",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3C58EWH \n What should be done when there is huge class imbalance?\n\n\"We will correct the class imbalance\" is often the wrong answer.\n\nFew months ago, I wrote about the pitfalls of using SMOTE.\n\nSome had asked, on what basis I was saying so. Well it was purely from my experience of building many ML models and also from speaking to data scientists smarter than me.\n\nStill many were not convinced. Few weeks ago an interesting paper came out \"The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression\" by Maarten van smeden et al. \n\nIn it, they come to the same conclusions that I had talked about (link of the paper in comments).\n\nJust to recap.\n\nWhen you use SMOTE, you are creating samples synthetically.\n\nWhen you take a certain Data generating process or phenomenon, the class imbalances are what they are naturally. \n\nFor e.g. number of ppl defaulting on loans will always be lesser than ppl who don't.\n\nYou may artificially over sample the minority class and inflate the class frequencies in training set. But guess what, when it comes to the test set or validation set, your algorithm will perform miserably.\n\nMany are bewildered on why their classifier algorithms performed so badly.\n\nThe answer is simple, your ML classifier 'learnt' a pattern in your training set and expected the same in validation set. \nHowever, the validation set class frequencies will be what they are naturally.\n\nYou can control your training set but you can't control the real world or real data generating process. \n\n Wrong calibration. \n\n SMOTE will make interpretation of predicted probabilities difficult.\n The predicted probabilities won't be reflective of the true probability prevalent in the natural data generating process /phenomena\n\nhashtag\n#datascience \nhashtag\n#datascientist \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3jBjIVt \n Tips for Data Science Internships\n\nDear Data Science aspirants, \n\nThere are certain Do's and Don't when it comes to applying for Internships.\n\nDon't start with the question \"How much is the stipend ?\".\n\nEvery real industry Internship should be looked at as a learning opportunity and how it could further your career.\n\nThere is nothing wrong in asking about the stipend but that should not be the first question. \n\nInstead, if you ask \"What skill set should I develop to be useful to the organization during my internship?\", this will definitely make the interviewer see you in a very positive light. \n\n10 yrs ago, When I was hunting for Internships I used to send connection requests to Data Scientists on Linkedin shamelessly and I use to ask them \"What skill set will help me land an Internship in your org?\"\n\nMany were quite helpful and responded with answers like :\n\n- Be proficient in MS Excel\n- Have decent level of programming in R (R was very popular in data science circles then)\n- Have some basic SQL knowledge\n- Have good grasp of statistical concepts\n- Have good power point presentation skills\n\nHaving received these feedbacks, I got to work in improving skills in which I was lacking. \n\nI used to watch Excel tutorials and SQL tutorials. Took some courses in R.\n\nAfter some preparations, I applied to many companies again. Pretty soon I had 2-3 offers. Even companies that said they could offer only free internships later agreed to pay stipend after evaluating my skills. \n\nSo Dear Data Science aspirants, develop skill sets and you will have a higher negotiating power to command a paid internship. \n\nAt the end of the day, companies do look for some bare minimum skills in the candidate so that they could teach them and use them in real projects.\n\nTalking about Internships, We don't condone \"Free Internships\". All our Internships are paid. \n\nWe are currently hiring interns to work on exciting real projects. Check the link in first comment. \n\nhashtag\n#datascience \nhashtag\n#hiringpost \nhashtag\n#internship \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3C590wv \n If - else statements - The unsung hero in Data Science projects.\n\nOver the years, I have built many ML and Statistical models in various domains. One communality in all domains were - Edge cases.\n\nIn every domain, there are edge cases.\n\nSometimes, Your ML model can only take you so far. \n\nLast year I built a semantic search engine for a client. The domain of the client was quite niche and they had acronyms that were quite rare to find on the internet. \n\nThere were also acronyms that had same but different expansions. Something like WWF - World Wide Fund and WWF- World Wrestling Federation.\n\nWe decided to use a pretrained NLP model but because the domain was niche, the model did not have the vectors for those words.\n\nThe choice was either to custom train the model or use if -else statements with a dictionary lookup for edge cases.\n\nSince we were pressed for time and the edge cases were limited, if else statements did the trick for us.\n\nIf else cases are in a way an insurance policy. \n\nIf else statements effect may not show up in accuracy metrics like F1, Precision, Recall etc. But they do boost them indirectly.\n\nIf else statements are also a quick and non ML / Stat way to encode your domain knowledge in to the model.\n\nhashtag\n#datascience \nhashtag\n#nlproc \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#coding",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WLJV1y \n Linear Regression is perhaps the most written topic in Data Science.\n\nYet it is still the most misunderstood. \n\nEvery data science aspirant writes a blog on Linear Regression to showcase his/her learning and to announce their initiation into the data science world.\n\nBut the unfortunate truth is that many of these blogs have lot of errors. Perhaps the data science aspirants are picking these up from shady data science gurus or from their peers.\n\nI have been interviewing candidates for past 3-4 yrs, and by and large most had displayed very little understanding of the fundamentals of Linear Regression.\n\nSome popular misconceptions include:\n\n The Data should be normal\n The Dependent variable and Independent variable should be normal\n Not having high R squared means model is bad.\n Believing we model the raw values and not the expected value (mean)\n\nThe first link in comments contains some of my articles on Linear Regression wherein I try to dispel some of these misconceptions.\n\n(Image adapted from the original by @ ash_lmb, original link in comments) \n\nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3jqvCRJ \n \"Math behind ML algorithms is easy\" - is a popular opinion floating on Linkedin of late. \n\nThis notion is becoming popular perhaps because we fit so many algorithms by just calling a .fit () function. \n\nEverything looks easy once it is done and established. Imagine a situation where OLS was not discovered by Legendre, I am sure the dawn of AI age would have been delayed by centuries.\n\nMost of us don't even care how the statisticians came up with clever ideas like Kernel trick in SVM, sigmoid function in Logistic Regression, or let's say idea of using t-distribution in T-SNE.\n \nIn this post, I would like to take the example of t-SNE and hopefully make you the readers see the ingenuity behind this technique. I guess once we realize the ingenuity behind these ML techniques,\nwe will no longer feel \"Math behind ML algorithm is easy'. \n\nt-SNE stands for t-distributed stochastic neighbor embedding. t-SNE is a technique to visualize high dimensional data to low dimensional space. The original idea of stochastic neighbor embedding\nwas developed by Geoffrey Hinton. The t-variant was proposed by Laurens van der Maaten.\n\nBut why t -variant ?\n\nWell, in many of the dimensionality reduction techniques (e.g. t-sne), we try to map the data points from a higher dimension to a lower dimension.\n\nWhile doing so a fundamental problem occurs, the distance between the points at a higher dimension can't be preserved at a lower dimension.\n\nThis results in a overlapping or erroneous distance between the points at a lower dimension. This problem is called the 'Crowding Problem'.\n\nHow does t-distribution solve this ?\n\nThis utility of the t - distribution is that it helps prevent overcome the the 'Crowding problem'. \n\nGenerally when points are projected from higher dimensional space to lower dimensions, the points get squished and tend to get crowded.\n\nThe t-distribution being heavier tailed helps in this regard. \n\nA layman explanation - In a way you could think of t-distribution of being a wider net relative to normal distribution. So if you were to drop a ball from say the 20th floor of a building. You would have better chances of catching it on the ground with t-distribution than normal distribution.\n\nFor me the idea to use t-distribution is simply ingenious. Every ML algorithm that we use is filled with such ingenious ideas, only that we fail to appreciate it.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3jqvCRJ \n A layman explanation - In a way you could think of t-distribution of being a wider net relative to normal distribution. So if you were to drop a ball from say the 20th floor of a building. You would have better chances of catching it on the ground with t-distribution than normal distribution.\n\nFor me the idea to use t-distribution is simply ingenious. Every ML algorithm that we use is filled with such ingenious ideas, only that we fail to appreciate it.\n\nAnd knowing these intricacies behind the ML algorithm is not just for the ML researchers. Data Scientists who apply various ML algorithms to business problems should also know these intricacies. As it will help in devising innovative solutions.\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datavisualization \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WxjTj4 \n Tips for Kagglers transitioning to Real Life Data Science\n\n1) Overfitting\n\nThe biggest instance of overfitting is - believing what worked on Kaggle competitions will always work on real life problems too.\n\nXGBoost might work on Kaggle datasets and help you top the table. But in real life, prediction (Y-hat) is not the sole motive. Inference (-hat) is important too. (Link about Y-hat and -hat problems in comments).\n\nSo rather than being a one trick pony, it is better to know other algorithms too. A swiss knife is better than just a single blade. \n\n2) Overt focus on Validation\n\n\"THE most important lesson you should get out of competing on Kaggle is that coming up with a robust validation scheme trumps almost everything else in terms of how well your models will perform in the end\"\n\nA connection on Linkedin forwarded me this tweet of a famous Kaggle GM and asked my opinion. \n\nWell I see this fallacy in most Kagglers. Validation techniques are diagnostics. They will not prevent underfitting or overfitting. In a way, validation techniques are like thermometer, they will tell you whether you have fever or not. But won't help you cure your fever.\n\nWhat will improve your model is having good data, choosing the right variables (guided by domain knowledge), choosing the right model and then finally having good validation mechanisms.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#artificialintelligence \nhashtag\n#kaggle",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Q2novt \n XGBoost is not immune to Multicollinearity and Missing values.\n\nXGBoost is a very useful algorithm when it comes to prediction tasks on tabular data. \n\nHowever, some assertions about the algorithm is not correct.\n\nSome use the phrase 'immune' rather nonchalantly. \n\nLet's come to the first assertion \n\n1) XGBoost is immune to Multicollinearity\n\nFirstly, it is important to understand what is Multicollinearity and how it affects the model.\n\nMulticollinearity generally occurs when there is a high correlation between independent variables. This also means lot of redundant information.\n\nRedundant information is normally not an issue when your goal is just to predict things well. It becomes an issue when you have to do inference. \n\nIn inference tasks you have to uniquely identify the coefficients i.e we need to infer or attribute the changes in dependent variable to the independent variables.\n\nIn the domain of Marketing Mix Modeling (MMM), statisticians detest the presence of multicollinearity because tasks like MMM is more inferential than predictive. \n\nSo what does Multicollinearity do exactly?\n\nWell it inflates the variance of the estimated parameters. Hence the tests like VIF (Variance Inflation factor) for Multicollinearity.\n\nXGBoost is generally used for y hat tasks (predictions). It is seldom used for inferential tasks. So the users generally don't care about the multicollinearity.\n\nBut to say XGBoost is 'immune' to multicollinearity might be stretching it. XGBoost users perhaps mean to say presence of Multicollinearity does not affect prediction. \n\nMulticollinearity is not an issue even in linear regression if the goal is only prediction. Some XGBoost users conflate Non effect of Multicollinearity on prediction tasks to Non effect of it on Inference tasks. As they say, Absence of evidence is not the evidence of absence. Just because one chooses to ignore multicollinearity, it does not mean it doesn't exist.\n\n2) XGBoost is immune to missing values.\n\nAgain, XGBoost does impute missing values. But the question is does it impute it well. As Alice SH Wong rightly mentioned in one of her recent posts, XGBoost does not have the mechanism to deal with MCAR, MAR, MNAR type missing values. (Link to the explanation on MCAR, MAR, MNAR is in first comments).\n\nAt the end of the day, no body disputes the utility of XGBoost when it comes to tabular data. But as with any algorithm, it is important to know the short comings too.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Q2novt \n Again, XGBoost does impute missing values. But the question is does it impute it well. As Alice SH Wong rightly mentioned in one of her recent posts, XGBoost does not have the mechanism to deal with MCAR, MAR, MNAR type missing values. (Link to the explanation on MCAR, MAR, MNAR is in first comments).\n\nAt the end of the day, no body disputes the utility of XGBoost when it comes to tabular data. But as with any algorithm, it is important to know the short comings too.\n\nhashtag\n#datascience \nhashtag\n#xgboost \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Wwjmhl \n If your goal is to be an applied data scientist, the best set of people to learn Statistics / ML from are :\n\n1) Biostatisticians\n\n2) Statisticians/ Data Scientists who make a living by applying these techniques to business problems\n\nAsk Why ?\n\nWell both these set of people have tremendous skin in the game. \n\nThey know that the consequence of getting things wrong could be terrible.\n\nHence they take utmost care and apply statistical / machine learning techniques with a lot of rigor. They know the ins and outs of the techniques.\n\nMost importantly, they have the knowledge of what works or doesn't on the ground.\n\nP.S : A good resource on statistics is this book. The book is titled \"Biostatistics for Biomedical Research\". However, it is a must read for any aspiring data scientist. The concepts are covered with great detail and also highlights the repercussions of applying techniques wrongly. I often refer this book. \n\n(Link of the book in first comment)\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#biostatistician \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Q2nuDl \n Yesterday, I was asked \"Is there really a need to learn Linear Algebra when it comes to Linear Regression?\".\n\nWell, I would err on the side of caution and suggest every data science aspirant to learn linear algebra. \n\nIt is true that some of the modern day ML/Statistics libraries solve the system of equations for you automatically and hence you need not manually solve these equations yourself.\n\nSo why learn linear algebra ?\n\nLearning linear algebra will make you get a better geometric intuition. \n\nLearning linear algebra helped me better understand the concept of orthogonality when it comes to PCA. It helped me get intuition of how the explained variance changes due to rotation. \n\nLinear algebra helped me better understand various orthogonal rotation methods like varimax, equimax and oblique rotation methods in Factor analysis.\n\nEven in NLP, it helped me better understand why the cosine similarity score between two words are high.\n\nAnd examples like these are many.\n\nI am ever so grateful to Gilbert Strang's Video Lectures and 3blue 1brown videos. They helped me to really appreciate Linear Algebra.\n\nOnce you learn and understand Linear Algebra a whole new world opens up. I always used to wonder why mathematicians describe Mathematical proofs, equations and conjectures as \"Beautiful\".\n\nOnce I learnt Linear Algebra, I could see the same beauty.\n\nP.S Gilbert Strang's video lectures and 3blue 1 brown video links in first comment.\n\nhashtag\n#datascience \nhashtag\n#linearalgebra \nhashtag\n#statistics \nhashtag\n#mathematics \nhashtag\n#datascientists \nhashtag\n#nlp \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3C7myaQ \n Most Data Science aspirants lament that there isn't enough real datasets to practice on.\n\nWhile most start out with Titanic, Boston house rent or IRIS datasets, these data sets rarely prepare the aspirants for the real life data science problems.\n\nI mean just take example of Titanic dataset. Imagine, you are transported back in time (armed with your Logistic regression, RF or XGboost). Do you think people would calmly wait for you to run your algorithm, while the ship is sinking and ice cold waters raising up ? \n\nRegardless of whether people were from first class or second, male or female, everybody would like to make an attempt to survive. Telling people that their odds of survival is low because they are male or from second class will simply not cut it.\n\nHence I always wondered the usefulness of these datasets.\n\nSo what is the solution ?\n\nDuring my Bsc Statistics and MBA days, we used to access MOSPI (Ministry of Statistics and Programme Implementation) datasets. The data are real and one could use it to analyze and answer real problems.\n\nRecently, Niti Ayog launched National Data & Analytics Platform (NDAP). I feel it is an improvement on the MOSPI data with much better search feature. \n\nThe data is available by :\nSectors\nGranularity \nTime period\n\nAlso, given the size of India and the amount of data generated, these datasets are truly 'Big Data'.\n\nAs an aspiring data scientist, what more can you ask for.\n\nLink to the NDAP platform in comments.\n\nP.S: This post might be relevant only to data science aspirants based in India. I am sure other countries too would have such public data repositories and perhaps data science aspirants world wide could access them in their respective countries. I feel these public datasets are more useful than Titanic or house rent datasets.\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#nitiaayog \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3jEvM8p \n I don't know whether it is just me or other data science managers / recruiters have noticed one interesting development. \n\nI have noticed in the past 2-3 years that resumes of aspiring data scientists are increasingly filled with NLP and CV projects while there is no mention of any statistical or classic machine learning \nwords !!\n\nIf one were to plot the Term Frequency (TF) terms like NLP, CV, BERT, Transformers, chat bot, sentiment analysis, CNN, LSTM, RNN, etc. over the years of DS aspirants resumes, I am sure it would be showing a hockey stick like growth.\n\nWhile I am happy that students are pursuing NLP and CV more, I believe gaining fundamental knowledge in Statistics, Probability and classic ML will hold them in good stead.\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#statistics \nhashtag\n#nlproc",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WQl8tj \n A big chunk of the projects in the industry involves dealing with tabular data. However, not many have the knowledge of Time Series Data, Cross Sectional Data and Panel Data. \n\nA quick refresher :\n\n1.Time series\nWe are collecting data about Laptop Brand 'XY1' sales over the years\n\nYear  sales\n2017 50,000\n2018 54,000\n2019 65,000\n2020 45,000\n\nNotice above that the data is collected for the same entity 'Brand XY1' but at different intervals of time. \n\n2. Cross section data\nNow we are collecting not only Brand XY1 data but also its rivals. However we are just interested in the year 2020.\n\nBrand  Year   Sales\nXY1   2020  45,000\nEL3    2020  23,000\nFT3    2020  56,000\nTY4    2020  24,000\n\nNotice, we are interested in data at a particular point in time (i.e. yr 2020). This interest in particular point in time distinguishes Cross section from Time series.\n\n3. Panel Data:\nPanel data is what you get if you mix both the Time series and Cross Sectional data. Lets say that now you want to track brand XY1 and its rivals sales over the years. This results in data like below:\n\nBrand Year Sales\nXY1 2017 50,000\nXY1 2018 54,000\nEL3 2017 12,000\nEL3 2018 23,000\nFT3 2017 32,000\nFT3 2018 35,000\n\nI have provided this quick refresher because, despite writing about this many times, I still see data science aspirants pick up the infamous \"Heart Disease Prediction\" dataset. \n\nTake a look at the heart disease dataset available at Kaggle (refer image). Many blindly apply ML algorithms on this data set.\n\nThere is a fundamental problem in the dataset. \nIf you notice, there is only one data point under each feature for a patient. Features like BP, cholesterol, heart beat are not static. They range. \n\nBlood pressure of a person varies hour to hour and on a daily basis, so does heart beat. So when it comes to prediction problem there is no telling weather 135 mm hg blood pressure was one of the factors to cause the heart disease or was it 140, all while the data set might be reporting 130 mm hg. \n\nIdeally, multiple measurements need to be had for each feature for a patient. And that multiple measurements leads to Panel data.\n\nExample \n\nId      Date    Time   age sex CP trestbps chol.....\n\n123abc  2/2/19 11.00am 45  M   3   145    233\n\n123abc  2/2/19 12.00pm 45  M   3   144    232\n.\n..\n345tyu  2/2/19  11.00am 32  F   1   126    204\n345tyu  2/2/19  12.00pm 32  F   1   124    203\n\nIn summary: Pls know your data before applying ML Algorithms.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WQl8tj \n Ideally, multiple measurements need to be had for each feature for a patient. And that multiple measurements leads to Panel data.\n\nExample \n\nId      Date    Time   age sex CP trestbps chol.....\n\n123abc  2/2/19 11.00am 45  M   3   145    233\n\n123abc  2/2/19 12.00pm 45  M   3   144    232\n.\n..\n345tyu  2/2/19  11.00am 32  F   1   126    204\n345tyu  2/2/19  12.00pm 32  F   1   124    203\n\nIn summary: Pls know your data before applying ML Algorithms.\n\nP.S: I have written more on why one shouldn't apply ML Algos on Heart disease dataset (Link in comments).\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#econometrics \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3YVlNvt \n Democratizing Data Science - The Wrong Way\n\n1) You don't need to know advance math (calculus, algebra) if you are a beginner.\n\n2) You don't need to know how ML algorithm works under the hood.\n\n3) You are a user of ML. So you don't need to know how ML works.\n\n4) You don't need to know statistics because \"it is very rarely used in most DS roles\"\n\n5) \"You don't need to know math to train models\".\n\n6) \"No company cares about how much data science theory you know. They just want builders\".\n\n7) Kaggle = Real life Data Science experience\n\n8) You can become a Data Scientist in 50 days.\n\n9) AutoML will take care of everything\n\nNote: I am all for democratizing data science. But the above list of advice will do more harm to the data science aspirants than good. Unfortunately some popular data science influencers do propagate the same. \n\nI have seen many aspirants quit the data science journey because of wrong expectations setting. Many are provided a rosy picture of data science but once the reality sets in, they are unfortunately not prepared for it.\n\nI have written detailed posts countering each of the above points. Link in comments.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientist \nhashtag\n#ml \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WNS1a6 \n Two interesting facts about P-values (you probably did not know).\n\n1) P-values are random variables.\n\n2) Under the Null Hypothesis, the p-value follows the uniform distribution.\n\nLinks supporting the above in first comment.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#probability \nhashtag\n#hypothesistesting \nhashtag\n#pvalue",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3GqHe02 \n Statistics' Physics connection !!\n\nThere are many terms in statistics that one might find quite unintuitive. \n\nFor example:\n\n- Moments\n- Degrees of Freedom\n- Probability 'Density' Function\n- Probability 'Mass' Function\n\nIf you thought these words came from Physics, you would be correct.\n\nIn this post, I will try to explain some of the etymology. \n\nMoments \n\nMoments are the expected value of a Random variable. Moments define the characteristics and shape of a probability distribution.\n\nTypically Moments about origin are called 'Raw Moments' and those around the mean are called 'Central Moments'.\n\nIn fact, Mean is the first raw moment.\n\nBut Why the name moment ?\n\nIt turns out that statisticians named it so as an allusion to 'Moment of inertia' . \n\nA more detailed explanation is provided in the excellent blog titled 'Understanding Moments' by Gregory Gundersen. Link in comment. \n\nProbability Density Function & Probability Mass Function\n\nIn my last post, I had reference Aerin kim's excellent article 'PDF is not a probability' (link provided again here in comments). In it she gives this beautiful explanation.\n\n\"This is analogous to mass density in physics  integrating the density to get the mass. If you think of a mass as a probability, we are integrating a probability density to get a probability (mass)\"\n\n Degrees of Freedom:\n\nIn physics, the degrees of freedom (DOF) of a mechanical system is the number of independent parameters that define its configuration or state.\n\nThe word 'number of independent parameters' is key here.\n\nHow is this concept used in statistics' degree of freedom ?\n\nWell, this is how I usually explain DOF to a layman.\n\nImagine being asked to choose 5 numbers that sum to 100. For simplicity sake you say 20, 20, 20, 20. Before you utter the 5th number, you are asked to stop and we proclaim that we know your 5th number is 20 as well !!.\n\nThis is because the first 4 numbers chosen by you summed up to 80 and the condition was to chose 5 numbers summing to 100.\n\nSo 100-80 = 20. The fifth number is 20.\n\nThe fifth number in a way chose itself due to the condition specified.\n\nYou see, you had 4 degrees of freedom alone. All one needed was the first four numbers.\n\nThis also intuitively explains the n-1. Here n being 5 and n-1 =4.\n\nWe could say we lost 1 DOF to the condition of \" 5 numbers should sum to 100\".\n\nIn other words we had only 4 degrees of freedom or in physics parlance 4 independent parameters.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3GqHe02 \n So 100-80 = 20. The fifth number is 20.\n\nThe fifth number in a way chose itself due to the condition specified.\n\nYou see, you had 4 degrees of freedom alone. All one needed was the first four numbers.\n\nThis also intuitively explains the n-1. Here n being 5 and n-1 =4.\n\nWe could say we lost 1 DOF to the condition of \" 5 numbers should sum to 100\".\n\nIn other words we had only 4 degrees of freedom or in physics parlance 4 independent parameters. \n\nIn Summary: It is fascinating to note how cross pollination of ideas from across the fields has greatly enriched the field of statistics.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#physics \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3vIfK05 \n First principles thinking in Data Science.\n\nFirst principles thinking is defined as \"boiling problems down to their most fundamental truths\".\n\nSo when it comes to Data Science, what are the first principles ?\n\nIn my opinion they are :\n\nMeasures of central tendency - Mean, Median, Mode.\nMeasures of dispersion - Variance, Standard Deviation, Interquartile Range.\n\nMost of the topics in Data Science somehow boil down to central tendency or dispersion . Let me explain through some examples.\n\n1. Linear regression :\nGenerally, One models the expected value (the mean) not the raw value of dependent variable.\nPls note one can model any quantile in linear regression.\n\n2. Probability distributions :\nThe famous normal distribution is characterized by location parameter (mean) and scale parameter (standard deviation).\nSimilarly other distributions too are characterized by location and scale parameter. \n\n3. Machine learning :\nModel Drift: When we say the model has drifted, it actually means that the existing model has drifted in terms of the location or scale parameter or both from the real model.\n\nAccuracy metrics: Accuracy metrics like F1 is nothing but harmonic mean.\n\n4. Outlier detection or Anomaly detection : We classify something as an outlier if some data point is 2SD or 3SD or even 6SD.\n\n5. Time series forecasting :\nWell one of the key concepts in Time series forecasting is stationarity. A stationary time series is the one whose properties such as mean, variance and autocorrelation structure stays constant over time. Stationarity is important because it is easier and more accurate to estimate parameters of a series whose properties do not change over time. If the mean and variance of the series keep on changing over time, the accuracy of the estimates will vary over time.\n\n6. Hypothesis testing :\nWe have hypothesis testing of mean and difference in means. For e.g. t-test and ANOVA.\n\n7. Information theory :\nMany algorithms like Decision trees, model comparison techniques like AIC use information theory at its core. Even probability distribution comparisons techniques KL Divergence uses Information theory concepts like Entropy, Information gain etc.\n\nWell Entropy again is the expected value (average) of the self-information of a variable\nor\nthe Entropy is the smallest possible average size of lossless encoding of the messages sent from the source to the destination.\n\nWhat other Data Science concepts boil down to measures of central tendency and dispersion?",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3vIfK05 \n Well Entropy again is the expected value (average) of the self-information of a variable\nor\nthe Entropy is the smallest possible average size of lossless encoding of the messages sent from the source to the destination.\n\nWhat other Data Science concepts boil down to measures of central tendency and dispersion?\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3G1TFy7 \n Accuracy and Precision are not the same.\n\nMany people conflate Accuracy with Precision.\n\nYears ago, here is how one of my Statistics professors lucidly explained the difference.\n\nImagine you are standing on a weighing scale.\n\nThe weighing scale is faulty and inflates the actual weight of a person by 1 kg (2.2 pounds).\n\nNow a person with actual weight of 75 kg (165 pounds) uses the weighing scale. The scale shows his weight as 76 kg (167 pounds).\n\nEach time the person uses the weighing scale, the scale shows the weight as 76 kg. Does the scale showing 76kg each time make it accurate ? NO. \n\nEach time, the weighing scale is 'Precise' but not 'Accurate'.\n\nIt would have been accurate had it shown the weight of the person as 75kg.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#analytics \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3C9ifMn \n Why are Stopwords called 'Stopwords' in Natural Language Processing (NLP)?\n\nA generic definition of stopwords is that they are words like 'a', 'an', 'the', 'of' etc. But very rarely does one find explanation on why these words are called 'Stopwords'.\n\nThe word 'Stopwords' actually comes from 'Stoplist'. The origin of 'stoplist' in turn come from information retrieval.\n\nIn an information retrieval system, we tend to build indexes so that a search query or word can be easily retrieved.\n\nThe problem with stopwords is that they occur more frequently and while building an index it can lead to excess memory utilization and also the retrieval speed of 'search queries or words' goes down. The goal is to not index these stopwords in an Information retrieval system.\n\nTo prevent this scenario, a stoplist is constructed. Whenever a new document is added, each word in it is checked against the stoplist (much like a dictionary lookup), and those that match are removed or 'stopped' from further processing.\n\nBecause these words are 'stopped' from taking further part in processing, they are termed as 'Stopwords'.\n\nhashtag\n#nlproc \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#naturallanguageprocessing",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3G9Epzs \n Programming vs Data Science\n\nOne of the reasons data science seems *easy to do* is because many algorithms can be fit in 23 lines of code. There is simply no intellectual pain.\n\nCompare this to programming. A person has to really think about the syntax, design pattern and logic. Also when things go astray in programming, there are multiple checkpoints in the form of errors like Runtime, Syntax error and compiler error.\n\nOne gets an immediate reality check on how good or bad a programmer he/she is. As a result, one does not go up and about calling themselves citizen software engineer.\n\nOn the flip side, When it comes to data science, there are no runtime or syntax error equivalent. There are no warning signs that says one cant apply a particular algorithm on the data.\n\nThere is no immediate reality check in data science.\n\nThis is one reason why people who advocate learning the fundamentals is not important go scot free. This is why fancy but harmful titles like Citizen Data Science arise.\n\nDoing data science is not easy. If one feels so, either he/she is a genius or perhaps the person is not doing it right.\nGiven the nature of the field, I tend to believe more in the latter.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#programming",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Ib0iRA \n The importance of knowing seminal work\n\nFor the last few weeks I have been interviewing data science aspirants. One thing that surprises me is that, how most of the candidates lack knowledge of seminal work. \n\n- Some had cited advanced NLP projects involving usage of transformer architectures. Yet they had no clue about word2vec !!\n\n- Some had cited topic modeling projects yet they had no idea what a Dirichlet distribution was or that LDA was invented by David Blei, Michael Jordan and the famous Andrew NG (whose courses they have cited on resume)\n\n- Some had cited XGBoost projects yet not many knew about Leo Breiman (inventor of CART and Random forest).\n\n- Some had cited entropy, Information loss, gini impurity in their decision tree projects and yet had not heard of Claude Shannon !\n\n- One had reference about Mixed effects model (my eyes lit up) and yet he didn't know about Nan Laird's seminal work.\n\n- Some had cited A/B testing yet didn't know about the work of Fischer, Neyman or Pearson with regards to Hypothesis testing.\n\n- Some had cited Time series forecasting projects yet they had not heard of Rob Hyndman's work.\n\n- Some had cited Deep Learning projects yet they were not aware of seminal works of Geoffrey Hinton or Yann Le Cunn.\n\nNow why does knowing Seminal work matter ? \n\nWell, we data scientists are not just 'button pushers'. We are thinkers and innovators. Knowing seminal work will help us know things from first principles.\n\nKnowing things from first principles will help us apply these techniques correctly to problems.\n\nSo here is a list of seminal work that I have compiled (not exhaustive by any means), but it could help broaden your knowledge. \n\nIt is always better to know things in depth about a field that you are entering into.\n\nHappy Monday.\n\nNote: Pls feel free to suggest other seminal works (books or research papers) in comments.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#nlproc \nhashtag\n#deeplearning \nhashtag\n#machinelearning \nhashtag\n#forecasting \nhashtag\n#econometrics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WTJdzI \n Linear Regression is Just Projection. Always has been. \n\nhashtag\n#linearregression \nhashtag\n#geometry \nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#mathematics \nhashtag\n#machinelearning \nhashtag\n#linearalgebra",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WvsWAV \n Hierarchical Models a.k.a Mixed Effects Models are pretty useful technique for any Data Scientist / Statistician to know. \n\nIt finds utility in many domains like Marketing (Marketing Mix Modeling), Social Sciences, HR (People analytics), Pharma etc.\n\nThe attached video is one of the lucid explanation of Hierarchical Models that I have come across.\n\nWant to learn more about Hierarchical Models? Links of resources in comments.\n\nPls feel free to suggest other good resources in comments.\n\nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#datascience \nhashtag\n#marketingmixmodeling \nhashtag\n#machinelearning \nhashtag\n#mixedeffectsmodels \nhashtag\n#linearregression \nhashtag\n#econometrics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3WPvgm3 \n 1) The normal distribution is not a normal empirical phenomenon !!\n\n2) Averages do not measure central tendency. \n\n3) There is no Regression to the mean.\n\n4) Spread of a distribution is not necessarily defined to be around an average.\n\n5) A test of statistical significance is not a test of scientific importance.\n\n6) Partial correlation does not partial out anything !!\n\n7) The independent variables of a regression are usually not statistically independent of the variable to be predicted by the regression. \n\n8) Independent contributions to a multiple regression are usually statistically dependent.\n\n9) An expected value is generally not to be expected.\n\n10) Stepwise regression, as currently practiced, is neither inference wise not theory wise.\n\nThere are a lot of misconceptions in Statistics. Surprising to see that what were misconceptions nearly 50 yrs ago have either remained the same now or got even more exacerbated !!\n\nThe above negative facts are taken from the famous paper 'What is Not What in Statistics' by Louis Guttman. \n\nAs they say negative facts tend to stick to our mind better\n, hopefully there will be less misconceptions going forward.\n\nThe link to the complete PDF 'What is Not What in Statistics' is in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientist \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3IckDWE \n Statistics : Less is more\nDeep Learning: More is less.\n\nThe above has been the philosophy of Statistics and Deep Learning respectively. It is why we have Inference in Statistics. The art of inferring things from samples and not pursuing to know what the population itself is.\n\nWhere as the belief in Deep Learning is that, more layers = better model, more data = better model. A pursuit to know it all.\n\nIn a way, Statistics is antithesis of Deep Learning.\n\nHowever, one interesting thing that I have noticed of late is that, Statisticians are gravitating towards Deep Learning while Deep Learning experts are gravitating towards core statistical philosophies !!\n\nPopular classical books like ISLR now include topics of Deep Learning. Where as Deep Learning pioneers like Andrew NG are advocating for a more 'Data centric' approach. An approach that focuses on smaller but good data. This has been the philosophy in statistics for years.\n\nIt is as though Deep Learning experts have hit a wall and are now reverting to the good ol' statistical beliefs, but the poor statisticians seems to have not got the memo .\n\nNote : The goal of the post is not to disparage the domain of Deep Learning. I have hugely benefited from many of the big strides made in DL (for example :Transformer based architectures in NLP). \n\nThe post is just my observation from the vantage point of having had an exposure to both statistical and Deep Learning methods. \n\nNot sure others would agree with me. Anyhow, let me know your thoughts in comments.\n\nhashtag\n#statistics \nhashtag\n#deeplearning \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#artificialintelligence \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3YRqBlH \n Everything you wanted to know about Lasso & Ridge Regression \n\nAt Aryma Labs, we have a habit of doing a thorough study before we develop any solutions / products. \n\nOver the years we have been studying and researching LASSO and Ridge as part of developing proprietary Marketing Mix Modeling (MMM) solutions. In this pursuit, we collated a lot of good resources.\n\nWe thought of sharing these resources as it may be useful to others.\n\nThe below PDF is a collection of LASSO and Ridge resources. \n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientist \nhashtag\n#linearregression \nhashtag\n#marketingmixmodeling \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Q2hwSL \n Obituary of R ? (Not so fast)\n\nThe recent announcement that RStudio will now be rebranded as 'Posit' drew some expected reactions. Especially from data science influencers who say \"Don't learn R\", \"R is dead\" or \"why learn R when python is there\".\n\nI never really understood this tribalism and R bashing. \n\nI have used SAS, R and Python so far in my Data Science career. Yes, granted that Python is better for certain ML tasks and for Deep Learning and NLP.\n\nHowever, that does not mean R is useless. I come from a Statistics background and quite often I find that data scientists (without statistics background) don't even use the full arsenal at their disposal. I would not be wrong if I said that, many may not be even aware of the tools (statistical techniques) in the arsenal. \n\nLet me illustrate by an example:\n\nEvery now and then we come across posts /articles that Logistic Regression is not Regression and that it is a classification algorithm.\n\nIt dawned on me that the reason people think LR is not regression has something to do with the programming language they use !!\n\nJust take a look at the R syntax in the image below. The R syntax clearly tells the user that :\n- The model is Generalized linear model. \n- That we are using Binomial regression with a logit link. \n- The argument 'Family' provides the user a hint that there are more models.\n\nNow people who use this syntax will never say \"Logistic Regression is not Regression\". \n\nOn the other hand, let us take the python syntax (specifically sklearn, since many data scientists use that).\n \nAs you can see, the syntax does not tell you much. It is just instantiation of the class. Of course, one needs to take a peek inside this class. But many don't because they have been sold on 'Implement machine learning in 2 lines of code'.\n \nA person (without a statistical training), implementing logistic regression code in python will not even have a hint whether it is indeed regression! Again, they have been educated on 'Supervised' and 'Unsupervised' algorithm. To them logistic regression is a 'supervised classification algorithm' because they have heard and seen it being used only for *classification* purposes.\n\nI know some might quip and say \"Do we need GLMs?\" . Well yes, contrary to popular belief not all errors are normally distributed and Normal Distribution is not the most prevalent.\n\nGLM provides a myriad of options to model errors that are not normally distributed.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: http://bit.ly/3Q2hwSL \n I know some might quip and say \"Do we need GLMs?\" . Well yes, contrary to popular belief not all errors are normally distributed and Normal Distribution is not the most prevalent.\n\nGLM provides a myriad of options to model errors that are not normally distributed.\n\nData science community will be better served if we understood the real potential of the programming languages, especially statistical ones. \n\nPerhaps we should not be quick to write the obituary of R. We data scientists could leverage best of both worlds (R & Python).\n\nhashtag\n#datascience \nhashtag\n#rprogramming \nhashtag\n#python \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Gsnuth \n Unpopular opinion : The 'Hello world' of Machine Learning should not be Linear Regression.\n\nHere is why\n\nThe internet describes 'Hello World' as a sample program designed to familiarize users with a particular programming language.\n\nNow the question is , Does Linear Regression familiarize users with the ML landscape or are they left with misconceptions.\n\nLinear Regression is perhaps the most written topic in Data Science.\n\nYet it is still the most misunderstood.\n\nMany think Linear Regression is easy and that it is just drawing a line between data points. For a layman this level of nuance is ok but not the level an aspiring data scientist should have.\n\nEvery data science aspirant writes a blog on Linear Regression to showcase his/her learning and to announce their initiation into the data science world. \n\nBut the unfortunate truth is that many of these blogs have lot of errors. \n\nI must have interviewed 100+ candidates in past 2-3 years. The shocking truth is that hardly 5-10 people truly understood Linear Regression !!\n\nOthers had the typical misconceptions as follows:\n\nBelieving we model the raw values and not the expected value (conditional mean)\nThe Data should be normal\nThe Dependent variable and Independent variable should be normal\nNot having high R squared means model is bad\nThat we can draw only straight line through data points and not any curved ones. \n\nThe first link in comments contains some of my articles on Linear Regression wherein I try to dispel some of these misconceptions.\n\n(Image adapted from the original by @ ash_lmb, original link in comments)\n\nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Ibbiyb \n Outliers\nThere are two types of Data Scientist.\nType 1 : Immediately deletes an outlier upon encountering it.\nType 2 : Ponders deeply about the outlier and thinks it could really have some good insights.\n\nIn this post, I want to kindle some thinking on what really is an outlier and why we should be careful with them. \n\nFew years ago, I saw a documentary on YouTube wherein they covered a very peculiar phenomenon. That phenomenon was a species of insects called 'magicicada' coming out of the ground after 17 long years.\n\nIt really blew my mind !\n\nI want to use this phenomenon to better explain outliers.\n\nSo, imagine a scenario where a person since birth, right up till he/she is 17, has never seen these insects.\n\nBut then on the 17th year, he/she just happens to find millions and millions of them.\n\nNow to that person, this is totally out of the blue and a real outlier event.\n\nLets role the clock another 17 yrs. Now the same person encounters these insects in huge numbers. This is no longer an outlier to him/her and establishes that this phenomenon happens every 17 yrs.\n\nLastly, lets take another scenario. Just 5 yrs past the last sighting of these insects, there is again an explosion of these insects. The person who witnessed the first two scenarios, is in for a shock !! \n\nThe assumption was that this phenomenon happens once in 17 yrs. But this happened way to early. This incident too is an outlier.\n\nSo, what are the learning from the above examples ?\n\n Outliers are relative - A data point is an outlier with reference to past information, other data points or say summary statistic like mean.\n\n Outliers and temporal nature - Outliers are considered as such, sometimes based on time. In our example, the last scenario was an outlier because the expectation of the event was once in 17 years, but the phenomenon happened just after 5 yrs from the last event.\n\n Outliers sometimes have more signal than all the other data points.\n\n Do not throw away outliers blindly - Just because a data point is an outlier does not mean we delete it. As mentioned in previous point, outliers sometimes have more signal. Careful judgement needs to be made when dealing with outliers. I have been told that there are industries (e.g. Biopharma) where it is pretty normal to see data points as far as 5 SD or even 10 SD and that they are not considered an outlier.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Ibbiyb \n Outliers sometimes have more signal than all the other data points.\n\n Do not throw away outliers blindly - Just because a data point is an outlier does not mean we delete it. As mentioned in previous point, outliers sometimes have more signal. Careful judgement needs to be made when dealing with outliers. I have been told that there are industries (e.g. Biopharma) where it is pretty normal to see data points as far as 5 SD or even 10 SD and that they are not considered an outlier.\n\n Domain knowledge is a double edged sword - From my experience I can say that domain experience can inform you 90-95% correctly whether something is an outlier. But it is the remaining 5- 10% of the cases where things get really tricky. Your domain experience might tell you to ignore outliers but then this decision may come back to bite you.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientist \nhashtag\n#outliers",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WQpfFL \n Model Selection > Feature Engineering\n\nI came across Adam Sroka's post yesterday where he claims 'Feature Engineering > Model Selection'. \n\nI disagree. \n\nHere is why.\n\nFirst we have to understand that any model is an abstraction of reality. There is an inherent step by step process to Data Science. \n\nThe first step is to understand the Data generating process (DGP) and think of a model that captures this DGP well. Only post this, does feature engineering even comes to play. \n\nIf you get your model selection wrong, no matter what great feature engineering you do, you will not have a good abstraction of the reality that you are trying to model. \n\nIn a way good Model selection dictates how good or bad your features are. Not the other way round. A feature is considered good if it better predicts the outcome variable or explains well the change in that outcome variable. \n\nNow what determines that ? your model function.\n\nFor example, some processes may have non normal error. In such case, your simple OLS regression will fail to capture the DGP properly. You can throw all the features that you think are good, but you will have a bad model fit. You will need GLMs to better capture the DGP.\n\nYour model is like your car engine. if you have a bad engine, no matter what top quality gas you put or no matter what top quality material clutch/gear box/brake pedals you put in your car; your car will just stutter. And in a way you would have failed in the primary mission - traveling from point A to B.\n\nSo it is first important to have a good engine. \n\nUnderstanding the concept of 'Model Misspecification' is pertinent. Sadly not many people understand it. \n\nPeople are fixated with symptoms rather than the root problem. \n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#featureengineering \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3YUHzQ2 \n Probability Distributions - The secret to cracking Data Science Problems !!\n\nHow do you know which Machine learning algorithm to apply ?\nHow do you know which statistical test to apply ?\nor\nHow do you know when not to apply a Machine Learning algorithm?\n\nWell, the clue is in Probability Distributions. \n\nIn any Data Science project, it is first important to know the Data Generating process (DGP) and what distribution it follows.\n\nKnowing this helps in deciding which ML algorithm to apply or not to apply (for e.g. in fat tailed processes). \n\nKnowledge of Probability distributions and the relationships between them also helps immensely when applying statistical tests. It also in a way helps to understand processes like Law of large numbers, CLT intuitively. \n\nHere in this short video, I try to demonstrate some of the popular probability distribution relationships. \n\nThe video was made using StatDist. Link to the website in comments. Also if you want to learn more about the probability distributions and the relationships, the Wikipedia link is in comments.\n\n*Correction : The beta distribution is the uniform distribution when alpha and beta parameters = 1. In the video, the words 'small and equal' are imprecise. \n\nhashtag\n#datascience \nhashtag\n#probability \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientist",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vvD6Wh \n Why use Logarithms ?\n\nSimply put, there are two reasons: \n\n1) Helps in representing large numbers\n2) We humans prefer addition to multiplication. \n\nThere is something about multiplication and multiplicative process that we simply find hard to wrap our minds around.\n\nYet in data science we sometimes overtly use logarithms. Before applying Log transformation we need to think if the underlying process is multiplicative in nature.\n\nOther 'wrong' reasons to apply log transformation include:\n\n1) To make data 'Normal'\n2) To stabilize variance and cover up outliers\n3) Just for aesthetic reasons - to spread out data points \n\nI had written a post elaborating on the above points (Link in comments).\n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WONFzB \n Why 'Estimation' and 'Prediction' are not the same.\n\nMany of us use the words Estimation and Prediction synonymously.\n\nHowever, if you are a hands-on statistician or Data Scientist, it is good to know the distinction between the two in a statistical sense.\n\nFirstly, let us unpack few terms: Statistic and Parameter\n\n Statistic describes the sample whereas a Parameter describes about the population.\n\n An estimator is a statistic that helps us infer the value of an unknown parameter.\n\nFor example:\n\nSample mean is an estimator of the Population mean (a parameter).\n\n Therefore, an estimate is a guess about the true state of nature (the parameter).\n\nIn my post \"Most Important Assumption Checks of Linear Regression\", I had covered Andrew Gelman's concept of 'Representativeness'.\n\nJust to recap the concept: \"A regression model is fit to data and is used to make inferences about a larger population, hence the implicit assumption in interpreting regression coefficients is that the sample is representative of the population.\"\n\nSo, you see whenever we write the Regression equation, we ideally need to use  instead of Y. Because we are actually estimating the true value of Y through .\n\nThis is illustrated in the image below. Please note, I have put a hat on top of Error (which is to be read as residual) in first equation because in a way residuals are estimates of error.\n\nSo, now coming to what is predictor and prediction.\n\n Predictor uses the data to guess at some random value that is not part of the dataset.\n\n A prediction in an essence is a guess about another random value.\n\nSo lets say we take the same regression model as an example. The regression model has dependent variable (Y) - weight of people and X (independent variable) - height\n\nNow we have the data as follows:\n\nY (in pounds): 110, 130, 145, 160, 175\nX (in inches): 50, 60, 70, 80 , 90\n\nNow if were to use our regression model to predict the weight of a person say at 100 inches height, then it would be a prediction.\n\nYou see the height of 100 inches is not part of the dataset. However we did use the data at hand.\n\nP.S: I would urge the readers to also read the excellent stack exchange answer on 'Difference between estimation and prediction'. (link in first comment)\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#econometrics \nhashtag\n#datascientists \nhashtag\n#linearregression \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3GoEfFE \n The German Tank Problem\n\nWhen we talk about application of statistics during world war 2, somehow the image of the airplane with red dots (survivorship bias) comes to mind. \n\nDon't worry I am not going delve on that again \n\nThere are however other lesser known statistical applications during world war 2 which had a huge impact in the outcome of the war. \n\nOne such application is in the 'German Tank problem'.\n\nThe 'German Tank Problem' is basically about statistical theory of estimation. Estimating population based on sample is nothin new, but the innovative application in the German tank problem is something to really learn about.\n\nThe problem is about estimating the number of tanks produced in a month by the German army just based on the serial numbers of the the captured tanks. The assumption of course here was that the tanks were numbered sequentially as they were manufactured. Also the sampling is done without replacement. \n\nThis humble technique of estimation even beat the Allies Intelligence agencies' prediction. \n\nWhile the Intelligence agencies' numbers were way off (around 1000-1500), the statistical technique had estimated the n.o of tanks to be 246 a month. This was later proven to be correct as the post war actual number showed it to be 245 tanks a month !!\n\nStatistics is an art of 'Doing More With Less'. \n\nP.S : Check out the link in comments for detailed statistical/mathematical details.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3YRruux \n Things that can get you rejected in a Data Science Interview *\n\n1) Talking only about ML metrics (Accuracy, F1, precision, recall) of your project rather than conveying what business impact your ML solution had.\n\n2) Saying \"Logistic Regression is not Regression but a classification algorithm\"\n\n3) Saying \"We accept the Null hypothesis\"\n\n4) Saying \"Stepwise regression is a type of regression\"\n\n5) Buffing up your resume with right words (e.g. NLP, CNN, RNN, SVM, Linear Regression, XGBoost, Spark, AWS, Azure, Docker, SQL) while having little or no experience in any of them. This might help you get through ATS but once a knowledgeable interviewer interviews you, you could be in trouble !\n\n6) Saying \"In linear regression the dependent and independent variables needs to follow normal distribution, if not they need to be log transformed\"\n\n7) Saying \"your modeling strategy is to try all the models on the data and choose one based on accuracy metric via some low code library\"\n\n8) Saying \"Central limit theorem kicks in at n=30\"\n\n9) Saying \"Confidence Interval is the probability that the parameter of interest lying between the interval is 90 % or 95%\"\n\n10) Saying \"You predicted the stock market successfully with xyz algorithm\"\n\n11) Saying \"your strategy to deal with outliers is to throw them out just to fit the curve more smoothly\"\n\n12) Saying \"your strategy for imputation of missing values is to fill it by average(mean) values\"\n\n13) Saying \"p-value is the chance of obtaining a result by sheer chance\"\n\n14) Saying \"Model Validation will trump feature selection and model specification\" (Though some kagglers propagate this myth. cross validation is a diagnostic, not a overfitting prevention technique.)\n\nAlso, the natural question you might have is, \"what is the correct answer to all of the above questions?\"\n\nPls check the comments section for the right answers to some of these questions.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientist \nhashtag\n#datasciencecareers \nhashtag\n#machinelearning \nhashtag\n#interviewquestions",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WPwTQH \n The Flaw of Averages\n\nIn late 1940s, US Air Force noticed unusually large number of air-crafts crashing. Even the best of the best pilots crashed !!\n\nThe investigation revealed that the cockpit was designed keeping in view the dimensions of an average man in 1926 !\n\nLt. Gilbert Daniels who was heading the investigation found that not one pilot out of 4,063 pilots fell within the average range of dimensions.\n\nHe soon suggested to the US air force to discard the old standard and instead design the cockpit based on the 'individual fit' .\n\nThe air force demanded that the cockpit be designed to fit a pilot whose measurements fell within 5% to 95% range on each dimension.\n\nPost these changes, the crashed vastly reduced and the performance of pilots soared.\n\nImportant lessons:\n\n Never design based on 'Average'.\n\n Sometimes it is better to have a range than an point estimate.\n\n Never assume the data distribution to remain stationary.\n\n Just being data driven won't guarantee success. One needs to apply the right data metric.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#insightsanddata",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FZ5q8B \n Anybody can fit a model\n\nbut ..\n\n Not everybody knows when not to fit a model.\n\n Not everybody knows how to diagnose when things go wrong with the model.\n\n Not everybody knows how to interpret the model in the context of business outcomes.\n\n The latter three points is what separates a good data scientist from an average one.\n\n The latter three points is why Data Scientists get paid good.\n\n The latter three points is also why no AutoML will take over Data Scientist jobs. \n\nhashtag\n#datascientist \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3i2Q3Uz \n Clarifying misconceptions around Frequentism. \n\nLast week, I had written a post on why people adopt Bayesian philosophy. As expected it drew some interesting comments for and against Frequentist methods.\n\nFrom those comments, I felt vindicated that people do misunderstand Frequentist principles. \n\nSo let me clarify some of the misconceptions around Frequentism.\n\n1) Frequentist inference is never about whether a particular hypothesis is true.\n\n2) Frequentists never put out a probability value of a hypothesis being true. \n\n3) Rather, Frequentists state the probability of observing the data (certain result) under the assumption that your hypothesis is true. This is what p-value captures not the probability of hypothesis being true.\n\nOther Criticisms: \n\nThere are also other criticism of Frequentism (but actually are not criticism at all).\n\nIn the book 'Probabilistic Machine Learning', Kevin Murphy remarks the following:\n\"because this method (Frequentist) only worries about trying to reject the null, it can never gather evidence in favor of the null, no matter how large the sample size.\"\n\nThis is not a 'bug' of Frequentist methods but a 'feature'. Frequentist methods are designed based on Popperian falsification principle. The goal is to falsify not to 'gather evidence'. This is something detractors of Frequentist methods fail to understand. The falsification principle is why Frequentists don't say \"We accept the Null Hypothesis\".\nCheck the link in comments for elaboration. \n\nSecond, the detractors of Frequentism say Frequentist methods will fail if the number of trial = 1. \n\nAgain this is a poor understanding of Frequentism. The very fact that Frequentism derives its name is, because it is about frequency of trails or long runs. Trail n=1, is hardly a frequency !! Frequentist method is all about long run probabilities. \n\nThe frequentists in a way draw confidence in 'how many times a event results in x and how many time it does not'. With your n=1, the confidence would be very flimsy. Hence a Frequentist would advocate for more trails. Large n's. Also with large n's, Frequentists believe that the estimate will be closer to the true value. \n\nNote: Frequentism might have its flaws but the above are not some of them. I am not advocating for Frequentist methods over Bayesian ones but rather trying to provide more information. Even if you want to shun Frequentist methods, pls do so for the right reasons :)",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3i2Q3Uz \n Note: Frequentism might have its flaws but the above are not some of them. I am not advocating for Frequentist methods over Bayesian ones but rather trying to provide more information. Even if you want to shun Frequentist methods, pls do so for the right reasons :) \n\nhashtag\n#bayesian \nhashtag\n#bayesianstatistics \nhashtag\n#frequentist \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#probability",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WU6dya \n The levels of Linear Regression understanding.\n\nFrom time to time, I keep coming across a lot of misinformation on Linear Regression.\n\nI hence created this meme to allay most of these misconceptions. \n\nNote: Because a meme can only convey so much, I would request the readers to go through the references and resources in comments for more thorough understanding.\n\nhashtag\n#linearregression \nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3PZcKFI \n Entropy and Getting Hired.\n\nIn information theory, there is a concept called Entropy. If you want a good refresher, I would highly recommend the article by Naoki (Link in comments).\n\nBasically, Low Entropy means receiving very predictable information.\nHigh entropy means receiving very unpredictable information (some also define it as an element of surprise).\n\nI have been interviewing data scientists for past 3-4 yrs and I must say most data science aspirants' resumes have low entropy.\n\nEveryone has the same projects :\n\n Titanic survivors prediction\n Heart disease prediction \n Home loan prediction\n Credit default prediction\n Same Kaggle exercises\n Some object detection exercise\n Predicting stock market via forecasting (LSTM)\n\nIf you want to improve your odds of getting hired for Internship or full time positions, I would highly recommend increasing the entropy of your resume.\n\nI know your natural question would be, \"How do I do that?\"\n\nWell for starters, don't do what everyone does. \n\nInstead of putting the run-of-the-mill examples cited above:\n\n Perhaps do your own pet project\n Collect your own data (need not be huge), \n Study the data thoroughly (talk about the data generating distribution)\n Present the insights (talk about the outliers and if they really contain some signal)\n Apply the appropriate statistical technique or ML algorithm.\n\nFinally,\n Don't just talk abt the accuracy metrics . \n Talk about how your solution helped you solve the problem and the tangible benefits you gained. \n\nAnd just like that you have increased the entropy of your resume.\n\nYour resume now has an \"element of surprise\" for the recruiter and not to mention you also have improved your odds of getting hired !!\n\nTalking about getting hired, we are hiring interns !! (Pls check out the link in comments).\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#internships \nhashtag\n#resumetips \nhashtag\n#machinelearning \nhashtag\n#datasciencejobs \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3hY0wAz \n The 'Normal Distribution' vs 'The Standard Normal Distribution'\n\nI have noticed that many don't realize the difference between 'Normal Distribution' and the 'Standard Normal Distribution'. \nOr \nthat they think both are the same !!\n\nSo here is quick explainer on why they are different.\n\nThe Standard Normal Distribution has no free parameters i.e. it has no unspecified parameters. The mean = 0 and SD =1.\n\nHowever, in the case of Normal Distribution, there are free parameters. The parameters are unspecified. The mean and SD could take values other than 0 and 1 respectively.\n\nThe Standard Normal Distribution is a special case of Normal Distribution. In the image, the red curve is the Standard Normal Distribution.\n\nWhy is it important to know this ?\n\nWell, I have noticed that many data scientists expect their data to be normally distributed (though this is not a requirement in Linear Regression. The normality assumption is with respect to errors. But even then it can be relaxed. Check the comments for further elaboration).\n\nSecondly when they expect the data to be normally distributed, they expect it to be 'standard Normally' distributed. More like expecting perfection. \n\nAs a result, people think anything that is not 'Standard Normally' distributed is not good enough or that something is wrong. This also leads to adoption of unnecessary transformation on the data.\n\nI believe George Box's following comment perfectly illustrates how we must think about Normal Distribution assumptions. \n\n\"The statistician knowsthat in nature there never was a normal distribution, there never was a straight line, yet with normal and linear assumptions, known to be false, he can often derive results which match, to a useful approximation, those found in the real world.\" - George Box\n\nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#data \nhashtag\n#datascience \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Wxfflb \n Statistics : Machine Learning :: Physics : Engineering\n\nFrom time to time, I come across posts and articles that underplays the importance of statistics in machine learning.\n\nSome machine learning engineers believe that they can 'tweak and iterate' to arrive at a solution.\n\nSometimes this 'tweak and iterate' or lets say 'trial and error' process may give solutions. \n\nHowever this does not mean this is a fool proof method and that one does not need statistics.\n\nGood engineers realize that their structures and machines operate under some laws of physics. \n\nSimilarly good data scientists know that the ML algorithms are governed by statistical principles. I even wrote a post on this (check the link in comments). \n\nAt the same time there are some data scientists who believe \"Knowledge of statistics is very rarely used in most DS roles'. Don't believe that such statement could be made? (check the link in comments )\n\nSo you see, some are totally disillusioned and propagate these misconceptions.\n\nJust because one is not aware of statistical principles, it does not mean that they don't govern the functioning of ML algorithms.\n\nAt the end of the day, real expertise is realized when things break down.\n\nAnybody can fit a model through 'tweak and iterate' process. \n\nHowever only a person with good statistical knowledge can tell you:\n\n Why and how your ML algorithm broke down.\n Why your coefficients are inflated.\n Why your model is mis specified.\n Why your model is poorly calibrated.\n Why your predictions are going wrong. \n\nPeople think most of ML is just Y- hat. However we humans are curious by nature. All Y-hat problems eventually do become -hat.\n\nTo summarize : Statistics is to Machine Learning what Physics is to Engineering.\n\nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3G5JTuG \n Why is MLOps so tough ?\n\nPutting ML models in production is tough. Monitoring and Maintaining it in production is even tougher.\n\nSome organizations think that their MLOps engineers can easily deploy these ML algorithms. \n\nWhat they and MLOps engineers don't get is, there is a difference between putting a set of rules (as in normal software programming) in production vs putting a mathematical function in production.\n\nI use the following analogy to explain why putting ML algorithms in production is difficult.\n\nPutting a mathematical function in production is akin to putting a jelly on a conveyor belt with heavy shaking. The mathematical function much like a jelly expands, squishes, moves around and sometimes totally collapses. \n\nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#statistics \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WSYyQK \n Things that surprise New Data Scientists when they first step into the corporate world.\n\n1) There is no perfect dataset. \nIn academia, one might have seen clean and perfect dataset. However, in the Industry perfect clean data is a more of a myth. One has to make do with what you have.\n\n2) No threshold is sacrosanct.\np<0.05 : 0.05 is not sacrosanct. Industries also use 0.01 and even 0.10.\nSimilarly in Logistic Regression, people rarely use the threshold of 0.5. If in Industry, people are sticking to default 0.5 cut off they may be doing it wrong.\n\n3) A highly accurate model might not be useful. A useful model might not be highly accurate.\n\n4) There is no perfect Normal distribution. Stop expecting to see perfect normal distribution, be it the errors or the data generating process. Real life processes rarely follow perfect Normal distribution. \n\n5) Practical significance > Statistical Significance\nSometimes in the industry, practical significance overrides statistical significance. Models that make sense 'business wise' are chosen over statistical significant models (which lack business sense).\n\n6) Data Science project cycle could be long\nIf you think much like hackathons or Kaggle competitions, projects can be done and dusted in 1-2 days, then you are in for a shock. In corporate world, data science projects often could take long from start to finish. Multiple reasons could hold up the project - Waiting for data, getting budget approval, trying to get a buy-in for the project, Decision paralysis etc.\n\nWhat else you would add ? \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Vw9DGv \n Why Statisticians make for good Data Scientists.\n\nCouple of days ago, I came across an excellent post from Christopher Molnar (link in comments) on 'Adopting a Statistician's mindset'.\nI concur with Christopher.\n\nStatisticians make for good Data Scientists because :\n\n1) They think in terms of data generating process (DGP).\n\nNow you might wonder why is this important. Knowing or thinking in terms of DGP in turn means thinking deeply about one's specific domain. In short this translates to domain expertise.\n\n2) They think in terms of probability distributions.\n\nFollowing from the previous point, it is also important to think in terms of probability distributions. Often whether a process is fat tailed or long tails can inform the data scientist whether any ML algorithm is applicable or not. Knowing when not to apply an ML algorithm is equally an super power as compared to knowing to apply ML algorithms.\n\n3) They have know-how of Design of Experiments.\n\nSome organizations think the job of DS starts only at the modeling phase. Needless to say, a statistician brings the knowledge of how to collect the data, what data to collect and at what granularity.\n\n4) Their approach to data science is robust and scientific.\n\nSome think hypothesis testing is a technique from the bygone era. However, hypothesis testing (NHST to be specific) trains a statistician to be more scientific.\nAsk how? \n\nWell much of science is about falsification. NHST at its core is based on principle of falsification. That is why we don't say \"we accept the Null hypothesis\".\n\nEven Feynman in his famous lecture said \"We can never be right, we can only prove we're wrong\". (check the link in comments for the Feynman lecture and my explanation on NHST) \n\n5) They have more tools in their arsenal. \n\nStatisticians have know-how of GLM, GAM, GLS etc. apart from your usual XGboost and Random Forests. Since most data science projects have tabular data, having these additional tools really helps. Moreover, not all tasks are Y hat tasks, you need B hat too.\n\n6) They also think about causality (Econometrics and Bayesian methods).\n\nCausality is not an easy problem. However there are major strides made in the field of Econometrics and Bayesian statistics to answer this problem. Analysis like A/B testing begs the question of causality. Statisticians are hence better equipped to deal with these.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Vw9DGv \n 6) They also think about causality (Econometrics and Bayesian methods).\n\nCausality is not an easy problem. However there are major strides made in the field of Econometrics and Bayesian statistics to answer this problem. Analysis like A/B testing begs the question of causality. Statisticians are hence better equipped to deal with these.\n\n\nNote: I don't want to sound Gate keeper-ish. When I say statistician I don't intend to mean that you ought to have a degree in statistics (though it is preferred).\n\nI know many friends and connections who don't have a conventional degree in statistics, yet they give statisticians a run for their money !! At the end of the day it is all about having good statistical knowledge and training. \n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3CbMaDD \n Standardization & Normalization - A Misnomer ?\n\nA week ago, I wrote a post on how 'Standard Normal Distribution' is different from 'Normal Distribution' (ICYMI, link is in comments).\n\nIn my opinion, the naming of Standardization & Normalization is swapped and what people mean when they use this terminology is perhaps not correct.\n\nFor me this terminology is counter-intuitive. Here is why:\n\nA. Normalization :\nIn classical statistical sense, Normalization means (x- mu)/sd. When statisticians says normalize they mean to say a distribution having mean =0 and variance =1. This is also known as standard normal distribution. Any normal random variable X can be transformed into a standard score or z score via the equation (x- mu)/sd. If one thinks Etymologically , it makes sense to call the above process normalization since normal here refers to the standard normal.\n\nB. Min Max scaling (informally referred as normalization but I would call it standardizing)\n\nX= X-X(min) / X(max) - X(min)\nThe above formula helps in scaling the values in the range of [0,1] and in a true sense standardizes the values.\n\nCome to think of it, how is scaling the values between [0,1] normal or Normalization? It simply does not stick or make sense. I would hence rather call this process standardizing as things are standardized in the value range [0,1]\n\nSometimes, things gets misinterpreted/ ill defined . I think this is a classic case where a statistical concept got ill defined / misinterpreted.\n\nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#data \nhashtag\n#datascience \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WxfRHv \n Strange Nomenclatures in Statistics\n\nSome of the nomenclatures in statistics can be quite counterintuitive. \n\nTake for example 'Marginal Probability'. In pure English, the term 'Marginal' would mean something of lesser effect or of minor importance. \n\nSo why is 'Marginal Probability' named so?\n\nDoes it mean that Marginal Probability is something less significant?\n\nActually the name 'Marginal' in Marginal Probability does not mean 'lesser effect' or 'minor importance'. It has more to do with the other meaning of Marginal - 'on the sidelines'.\n\nIn the pre computer era, people use to write down the probabilities in a table (rows and columns). As shown in the image, the marginal probabilities of the outcomes were literally written in the 'Margins' (sidelines). Hence the name 'Marginal Probability'. Again to reiterate, 'Marginal Probability' does not mean something of minor importance.\n\n============================================\n\nRule of Thumb n =30\n\nAnother strange case in statistics is that of the rule of thumb n = 30. \n\nThe rule of thumb that if n>30 use z test and if n< 30 use t test is not entirely correct. \n\nThe n=30 heuristic was not devised out of purely mathematical or statistical reasons. Back in the day (when computers were in its infancy), people used tables much like a logarithm table to see the values of any distribution for any combination of DF and significance level.\n\nAnd because such a table had to fit many distributions (t, normal, chi square etc.), the t distribution page was restricted to only 30 entries. Also at n=30, normal distribution and t distribution were deemed 'approximately' same.\n\nSo you see in statistics, the nomenclatures and rule of thumb can often be confusing. It is better to know the real etymology. I hope through this post, at least two of such confusing nomenclatures or rule of thumb could be clarified.\n\nhashtag\n#statisticsandprobability \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3i6XB8G \n Degrees of Freedom and Sudoku\n\nLot of aspiring Data Scientists take courses on statistics and get befuddled with the concept of Degrees of Freedom. Some memorize it by rote as n-1'.\n\nBut there is a intuitive reason why it is n-1.\n\nLets consider the following example:\n\nImagine being asked to choose 5 numbers that sum to 100. \n\nFor simplicity sake you say 20, 20, 20, 20. Before you utter the 5th number, I tell you that your 5th number is 20 as well. This is because the first 4 numbers chosen by you summed up to 80 and the condition was to chose 5 numbers which sums to 100.\nSo 100 -80 = 20. The fifth number is 20.\n\nThe fifth number in a sense chose itself due to the condition specified.\n\nYou see you had 4 degrees of freedom alone. All one needed was the first four numbers.\n\nThis also intuitively explains the n-1. Here n being 5 and n-1 =4.\n\nYou lost one degree of freedom to the condition of 5 numbers should sum to 100\nor\nIn other words you had 4 degrees of freedom.\n\n===========================================\n\nThe Sudoku Connection\n\nSudoku is a very familiar game to most of us. I believe there is some connection between sudoku and degrees of freedom.\n\nI am not sure whether the online version of the game take into account degrees of freedom to rate the game as Easy, Medium and Expert. But perhaps the game could be rated this way !!\n\nTake for instance the example in the image.\n\nIn each of the levels we notice something interesting. The number of filled in cells keeps getting smaller as the level goes from Easy to Evil !!\n\nBasically as you have more cells to fill in (blank cells) the level keeps getting harder. The more degrees of freedom you have, the tougher it is solve the puzzle !! Having lot of freedom is not ideal in this case at least.\n\nSo, More Degrees of Freedom = Tougher Sudoku !!\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WPr3id \n Sometimes you gotta think like a crook and know the ways of a crook to catch the crook !! \n\n- P hacking\n- Data Dredging\n- Chart Axes manipulation\n- Sample Size manipulation\n- Reporting only Accuracy metrics in the case of imbalanced class.\n\nThe above are just few examples of statistical hacks to manipulate the results.\n\nI would highly recommend the books \"Statistics Done Wrong\" & \"How to lie with statistics\" to any aspiring data scientist.\n\nThe goal of these suggestions is not to make the aspirants know the 'cheat codes' so that they can manipulate the results. The real goal is to make the data aspirants wary of all these statistical tricks.\n\nAny Data Science aspirant should know about these hacks becuase:\n\n As a data scientist you will be exposed to many research papers. One should always scrutinize the statistical methodology in the paper before accepting the results of the paper as gospel.\n\n You will be exposed to many reports of charts and graphs on a day to day basis. It is imperative that you check the axes so as not to infer wrongly.\n\nSometimes statistics gets a bad rap because of all these malpractices. Some even quote the infamous saying \"Lies, Damned Lies and Statistics\".\n\nBut as Frederick Mosteller once said \"It is easy to lie with statistics; it is easier to lie without them\". People use statistics to lie because it gives them credibility.\n\nLink to the above books in the comments.\n\nPls feel free to share in comments other statistical hacks that you would want data science aspirants to know.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientist \nhashtag\n#analytics \nhashtag\n#dataliteracy",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3GmS867 \n Data Science Mistakes - Halloween Special \n\nBelow are some of the mistakes that for sure will send shivers down the spine. \n\n Reporting Accuracy on an imbalanced dataset\n\n Accepting the Null Hypothesis\n\n Believing p values is about the probability of Null hypothesis being true.\n\n Believing Confidence Interval is the probability that the parameter of interest lying between the interval is 90 % or 95%'\n\n Deleting the outliers without questioning\n\n Letting test set samples into training set (Data Leak)\n\n Performing Stepwise Regression\n\n Believing Normal Distribution is the most prevalent in nature. Such a belief is actually para-normal .\n\n Believing Kaggle competitions and real life data science projects are same.\n\n Believing one algorithm will be the best for all scenarios (XGBoost is all we need?) - No free lunch theorem.\n\n Believing Logistic Regression is not Regression. \n\n Correlation is not causation. (Correlation is correlation).\n\n Manipulating your Y axis to make the chart look bigger or smaller.\n\n Believing we have achieved AGI \n\nAnd finally. This last one is for MLops folks. I too have done it in the past and nearly got fired for it. Don't ask me about the AWS bill. \n\n Forgetting to turn off the instance.\n\nPls feel free to add more data science mistakes in the comments. \n\nHappy Halloween.\n\nhashtag\n#datascience \nhashtag\n#halloween \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FV376c \n Is Inferential statistics still relevant in the era of 'Big Data'?\n\nWe are in the era of massive storage and faster compute (aka Big Data Era). In this day and age, some might feel why not collect all the data (population), why do we have to take samples and make inferences about the population?\n\nLet me provide just two points as to why Inferential statistics will never be obsolete.\n\n1) Cost factor\n\nSure, we may have massive storage and compute capabilites now but there is still an enormous cost to collecting this data, storing it, retreiving it, cleaning and standardizing it. Collecting samples and making inferences about the larger population is still a cost effective way.\n\n2) Human Factor\n\nWe can't test a medicine on the whole population to know its efficacy. That would be catastrophically dangerous !! Again thank God we have inferential statistics.\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#inferentialstatistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3VxWe0o \n Why your Confidence Interval will be always narrower than Prediction Interval.\n\nHave you observed that in your Linear Regression, the confidence Interval are always narrower than the prediction Interval.\n\nNow why is that ?\n\nBefore we answer that, let me quickly provide a correct definition of Confidence Interval.\n\nConfidence Interval : If one ran the same statistical test taking different samples and constructed a confidence interval each time, then in 95% of the cases, the confidence interval so constructed for that sample will contain the true parameter.\n\nOne may come across wrong explanations like \"There is 95% probability that the mean weight is between 50 kg and 70 kg\" or \"The probability that mean will *range* between 50kg and 70kg is 95%\".\n\nHowever, pls keep in mind that from a Frequentist perspective, these definitions does not make any sense.\n\nThe parameter of interest either lies within the interval or does not. There is no probability or chance (represented as %) associated with it.\n\nNow lets get back to why CI is always narrower than PI.\n\nYou see in Linear Regression, we don't model the raw response, rather we model the conditional mean (expected value) -> E(Y|X).\n\nThis conditional mean is the parameter which we try to estimate.\n\nThrough CI, we are trying to answer the question - \"If we were to do this Linear Regression again and again (in a way redrawing samples from the population), then does the CI constructed each time contain the true parameter or not\"? If in 95% of the cases, we do end up having the true parameter, then we are set to have 95% CI.\n\nNow you see, the only error that is accounted for while calculating CI is the inherent sampling error.\n\nWhere as in the case of prediction Interval, we are concerned about Y-hat. The Y-hat (or predicted value) accounts for the sampling error as well as the variance around that prediction. \n\nIt is for this reason the Confidence Interval will always be narrower than Prediction Interval.\n\nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3G4rGhm \n In Data Science don't just be a Steve Wozniak, also be a Steve Jobs\n\nIn my opinion, there are two types of Data Scientist.\n\n1) Type Steve Wozniak: \n\n One who knows the inner workings of the ML algorithms, knows deeply the math behind these algorithms.\n\n One who Knows when to use which algorithm. \n\n One who Knows how to innovatively apply an algorithm or combinations of algorithms to the problem. \n\n2) Type Steve Jobs: \n\n One who is very good story telling and persuasion.\n\n Knows how to communicate the benefits of ML solutions without boring the people with math and statistics. \n\n Knows how to create an intrigue about the solution. \n\n Most importantly, is able to envision a unique use case of the ML technique that benefits the business profitably. \n\nYou see there are two types of geniuses here.\n\nOne who innovates at the algorithm / statistical technique level.\nand \nOne who innovates about the possible application of ML technique and is masterful at communicating the benefits of the solution.\n\nMany data scientists focus on becoming the former. While there is nothing wrong in it, IMHO that alone does not make a Data Scientist 'holistic'. \n\nWhile I don't believe in the notion of 'Full Stack Data Scientist' (see the link in comments on why). I do believe in a different version of 'Full Stack Data Scientist'.\n\nFor me a full stack data scientist is person who is a combination of Type Steve Wozniak + Type Steve Jobs.\n\nWhy is being a Type Steve Jobs Data Scientist important ?\n\nIn commercial Data Science, driving usage is very important. That is what pays the bill for both type of data scientists. While it is good to innovate at the algorithm level, someone has to effectively translate the benefit of it and find a possible use cases. This is where a Steve Jobs type data scientist shines. \n\nI guess to most of us Data Scientists, being type Steve wozniak is easy. It is perhaps more natural to us. But becoming a type Steve Jobs DS is a lot of work.\n\nThrough hardknocks I have realized that in commercial data science, one has to become a Type Steve Jobs too.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientist \nhashtag\n#softskills",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vm1Gcm \n Why the Confidence Interval has an 'Hourglass' Shape ?\n\nIn my previous posts on Confidence Interval and Prediction Interval, I had covered the reason on why confidence interval is narrower than prediction interval. (ICYMI link in comments).\n\nThe next question some data science aspirants ask is that, why the shape of the CI is like that of an hourglass or sand clock ?\n\nWell, the answer again boils down to what Regression does and what Confidence Interval calculates. \n\nIn Linear Regression, we we don't model the raw response, rather we model the conditional mean (expected value) -> E(Y|X).\n\nIn Linear Regression, the line or plane passes through (x,y)\n\nSecondly, in confidence Interval we try to see whether the conditional mean (the parameter) is present in the interval or not. \n\nWe basically answer the question of \"If we were to do this Linear Regression again and again (in a way redrawing samples from the population), then does the CI constructed each time contain the true parameter or not\"? If in 95% of the cases, we do end up having the true parameter, then we are set to have 95% CI.\"\n\nNow that we have the two pieces of information that a regression line or plane passes through (x,y)\nand \nthat CI is an interval to see the presence of the parameter.\n\nIt becomes clear to us that the point (x,y) kind of acts like a fulcrum. \n\nTalking about fulcrum, one might find it relatable that 'moments' in statistics has a physics origin !! (check the link to my post \"statistics' physics connection\" in comments). It must be noted that 'Mean' is the first raw moment.\n\nNow with the analogy that (x,y) acting like a fulcrum, you can imagine this regression line to be like a see-saw. If the fulcrum is at the centre, one can easily see that the movement at the centre is limited and while at the ends the movement is more pronounced. \n\nNow with all these reasons, I hope one can see why the confidence interval has an hour glass shape.\n\nP.S: Please also do check out the stack exchange answer (link in comments).\n\nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3GsoBsK \n Allaying two popular myths about R Squared value\n\n1) R Squared value is not a measure of predictive power\n\nThough some have this misconception, R squared value is more about retrodiction than prediction.\n\n2) There is no ideal R squared value\n\nAnother misconception of R squared value is that, higher the value the better. However this is not true. It depends on the data and the domain.\nFor e.g. for a psychologist R squared value of 0.4 would be fabulous. However the same might be too less for a marketer.\n\nP.S: I would urge the readers to read the Lecture notes by Dr. Cosma Shalizi. The notes expands on some of the popular myths about R squared value. Link in comments.\n\nhashtag\n#linearregression \nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WuUlD6 \n Why so many different names for Dependent and Independent Variable?\n\nAn interesting pattern I have observed is that, people without statistical or related background predominantly use the term 'Target' and 'Feature'. \n\nPeople with statistical and related background used the following for Dependent and Independent variable:\n\n Dependent variable : Regressand, Predicted, Endogenous, Target, Response, Explained Variable\n\n Independent variable: Regressor, Predictor, Exogenous, Feature, Covariate, Explanatory Variable\n\nNow I don't want to be pedantic about this and I am not advocating that there is only one ideal term to use.\n\nI am just sharing this knowledge only on a 'good to know' basis and perhaps to allay some misconceptions. \n\nAlso I believe knowing the etymology and background behind this naming convention will perhaps help you appreciate the subject better.\n\nI will talk about four terms in this post (any more will perhaps warrant an article).\n\n Independent variable and Dependent variable: \n\nMisconception: They are named so because the dependent variable is 'dependent' on a set of variables - 'the independent variables'. And the independent variables are 'independent' of each other and the dependent variable.\n\nExplanation: The names 'independent' and 'dependent' variable came about in context of controlled experimental studies. If one read RA Fischer's work (Arrangement of Field Experiments or Statistical methods for research workers), we can see that there were studies on crop growth pattern vis a vis the treatment of fertilizers.\n\nIn these cases, the choice of fertilizers or say type of soil were chosen independently. This is how the name 'independent' came about.\n\nMany say the Independent and Dependent variable is poor terminology. The relationship is never unidirectional. In a statistical sense, the independent variables are not 'independent' of each other or the dependent variable. Also, one should not make the mistake of assuming causality because of the terms IV and DV.\n\nIn the context of linear regression, perhaps it best to think of it as Expected value of Y (mean) being conditioned on X.\n\nThe formulaic structure is given as\nE(Y|X)=0+1X.\n\n Endogenous and Exogeneous variables: \n\nThe term 'Endogenous and Exogeneous' is predominantly used in the context of Structural Equation Modeling (SEM).",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WuUlD6 \n In the context of linear regression, perhaps it best to think of it as Expected value of Y (mean) being conditioned on X.\n\nThe formulaic structure is given as\nE(Y|X)=0+1X.\n\n Endogenous and Exogeneous variables: \n\nThe term 'Endogenous and Exogeneous' is predominantly used in the context of Structural Equation Modeling (SEM).\n\nIt must be kept in mind that, these terms are rarely used in context of linear regression. The reason being, exogenous by definition means variables that are not affected by any other variable. However they do affect the Endogenous variable. Also, Endogenous variable in no way affect exogenous variables.\n\nHope this was useful.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#econometrics \nhashtag\n#linearregression \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3FZM0QN \n Majority of the data science projects involve tabular data. \nOne of the elementary mistake data science aspirants make with regard to tabular data is that, they think NA = 0 and impute NA (missing) as 0.\n\nThis is wrong.\n\nThere is a difference between missing data and a value of 0. Data could be missing for variety of reasons.\n\nThe following are types of missing data\n\n Missing completely at random (MCAR): Here, data are missing completely at random and the 'missingness' is not related to any variable measured in the study.\n\nExample: The respondents might forget to answer the question, or data gets lost during study.\n\n Missing at random (MAR): Here, data are not missing at random. The probability that the value is missing depends on the values of the variables measured.\n\nExample: In a survey, females are less likely to provide some personal information such as income or weight. So, the missingness of the data depends on the variables measured in the study i.e. Gender.\n\n Missing not at random (MNAR): Here, the data are missing because of the variable itself.\n\nExample: Respondents with very high income and very low income are less likely to provide that information. This type of missing data is also called 'non-ignorable non-response'. This is the most difficult type of data to handle.\n\nSo remember NA  0.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3CceoOK \n Are least squares and Linear Regression same?\n\nChatGPT says \"yes, they are same\" . \n\nSorry , this is wrong. Least squares is a method to estimate the parameters. Linear Regression is a model. Least squares is one method used to estimate the parameters in Linear Regression. There are other methods as well like Maximum likelihood.\n\nPlease don't use ChatGPT to learn core concepts of statistics / Data Science just yet.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#linearregression \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3WVy29D \n Difference between Principal Component Analysis (PCA) and Factor Analysis\n\nPCA creates new set of variables from the old variables. The new variable /component is a linear combination of old variables with weights assigned to each of the old variable. Here the goal is to reduce the no. of variables to smaller components for ease of model building.\n\nFactor Analysis on the other hand, assumes that the observed variables have a similar pattern of response and can be attributed to one latent factor. The latent factors formed, explain the correlations between the variables. The goal is to test a model of latent factors 'causing' observed variables.\n\nP.S: Don't forget to check out the excellent stack exchange answer on PCA, Eigen vectors and eigen values. (Link in comments).\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#mathematics \nhashtag\n#linearalgebra",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3YVPooq \n Assumption Selection & Feature Augmentation\n\n Assumption Selection\n\nEveryone talks about Feature Engineering / Feature Selection. However not many pay attention to 'Assumption Selection'. \n\nEvery model has some assumption. It is the skill of a Data Scientist to decide, which ones to strictly enforce and which ones to relax.\n\nFor example, here is the order of importance of Assumption in Linear Regression:\n\n1) Validity\n2) Representativeness\n3) Linearity\n4) Independence of errors\n5) Homoscedasticity\n6) Normality of errors\n\nFor more detailed explanation of the above pls refer to the link in comments.\n\n Feature Augmentation\n\nMany are aware of the concept 'Feature Selection' but a relatively less known concept is 'Feature Augmentation'.\n\nI believe one of the underrated skill in Data Science is Feature Augmentation.\n\nNot many know how to create new features from existing ones, to do that domain experience is key.\n\nSometimes feature augmentation can be the difference between a robust model and a mediocre one (both from a machine learning perspective and business wise).\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#featureengineering \nhashtag\n#linearregression",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3jAkiTd \n Is correlation not causation ?\n\n\"Correlation is not causation\". \nYou might have come across this phrase many times. However, overgeneralization of this phrase has lead to more harm than good.\n\nLet me explain.\n\nMany use this phrase to dismiss arguments without thinking about them thoroughly.\n\nSuch callous usage of the phrase leads to outright rejection of any further discussion into causality. \n\nNobody is saying \"correlation is causation\" (at least not right off the bat). But a strong correlation does lead to the question of causality.\n\nCorrelation at the least expresses a hypothesis that the two variables or phenomenon might be related. It also leads us to explore more robust methodology like Hill's criteria to establish causality .\n\nOne of my friend suggested to me that one should use \"X has a strong normalized covariance with Y\" instead of \"X is strongly correlated with y\". This prevents people from retorting \"correlation is not causation\".\n\nI think the best summary to all this is by Randall Munroe XKCD cartoon. \"Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing 'look over there'\"\n\nhashtag\n#statistics \nhashtag\n#correlation \nhashtag\n#causalinference \nhashtag\n#datascience \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3Z0YgsX \n Does PCA really solve multicollinearity?\n\nThe answer is both yes and no.\n\nPCA for dimension reduction is fine. But if your reasons for using PCA is to eliminate multicollinearity among variables, then you might be going about it wrong.\n\nIn linear regression setting, multicollinearity is a problem because you are unable to ascertain and disentangle the effect of independent variables on the dependent variable.\n\nIn domains like Marketing Mix Modeling (MMM) where knowing the attribution of each independent variable is crucial, multicollinearity is a huge problem. The attribution tends to get all messed up.\n\nNow you could drop redundant variables but you run the risk of having OVB (Omitted variable bias).\n\nSo some data scientists resort to PCA to eliminate multicollinearity. But lets remember that PCA creates new set of variables from the old variables. The new variable /component is a linear combination of old variables with weights assigned to each of the old variable.\n\nWhen these 'new variables' are created, they may seem to be devoid of multicollinearity (as they are orthogonal). However, they still haven't solved the problem that you were looking to solve i.e. ascertain and disentangle the effects of each independent variables. \n\nWhile PCA's new variables might have reduced overall multicollinearity, it has now created a hotch-potch set of variable/s. One really can't draw any meaningful interpretation out of it.\n\nSo bottom line - If you have Y-hat type problems, PCA may be alright to use. However if you have  -hat problems, PCA might do more harm than good.\n\nP.S : Check the link in comments for Y-hat and  -hat problem explanation.\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#mathematics \nhashtag\n#linearalgebra",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3jwJV7n \n Why saying \"We accept the Null Hypothesis\" is wrong. - An Intuitive explanation.\n\nWe often come across YouTube videos , posts, blogs and private courses wherein they say \"We accept the Null Hypothesis\" instead of saying We fail to reject the Null hypothesis.\n\nIf you correct them, they would say what s the big difference? The opposite of Rejecting the Null is Accepting isnt it ?\".\n\nWell, it is not so simple as it is construed. We need to rise above antonyms and understand one crucial concept. \n\nThat crucial concept is Popperian falsification.\n\nThis concept or philosophy also holds key to why we use the language Fail to reject the Null.\n\nBasically, the Popperian falsification implies that Science is never settled. It keeps changing or evolving. Theories held sacrosanct today could be refuted tomorrow.\n\nSo under this principle, scientists never proclaim X theory is true. Instead what they try to do is, they try to prove that the theory X is wrong. This is called the principle of falsification.\n\nNow having tried your best and you still could not prove the theory X is wrong, what would you say ? You would say I failed to prove theory X is wrong. Ah.. now can you see the parallels between I failed to prove theory X is wrong and We fail to reject the Null .\n\nNow lets come to why you cant say we accept the Null hypothesis.\n\nWe could not prove theory X is wrong. But does that really mean theory X is correct ? No, somebody more smarter in the future could prove theory x is wrong. There always exists that possibility. Remember above that we said science is never settled.\n\nA more classic example is that of the Black Swan.\nSuppose a theory proposes that all swans are white. The obvious way to prove the theory is to check that every swan really is white  but theres a problem. No matter how many white swans you find, you can never be sure there isnt a black swan lurking somewhere. So, you can never prove the theory is true. In contrast, finding one solitary black swan guarantees that the theory is false.\n\nNote : The post is merely to drive home the point how the language we fail to reject\" came about. It is not a post favoring inductive reasoning over deductive reasoning or vice versa. Neither it is an effort to prove or disprove Karl Popper's falsification principle.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#analytics \nhashtag\n#datascientist \nhashtag\n#hypothesistesting \nhashtag\n#abtesting",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3RY36FV \n Why Data Scientists love Notebooks while Software engineers hate it.\n\nMany software engineers / MLops engineer scorn at data scientists for using notebooks. I am a statistician but funnily I started by career as a software engineer.\n\nSo I can say that I know the psyche of both data scientists and software engineers.\n\nLet me first come to the defense of the data scientists. \n\nThere is a reason why data scientists love notebooks. A Data Science project by nature is filled with many pit stops and mirrors the scientific process outlined below:\n\nObserve\nExperiment\nRecord findings\nTweak Experiments\nRecord findings\nReport and publish\n\n\nA typical data science project has the following steps:\n\n1. Import the data\n2. Plot the data\n3. check how the data looks like, check for missingness in the data.\n4. Fit a model to the data.\n5. stop and see the relevant goodness of fit metrics or accuracy metrics.\n6. If the metrics are not as desired, then tweak the hyper parameters. \n7. See the model metrics again. \n\nRepeat steps 4-7 till model is finalized.\n\nYou see, in order to incorporate all these pit stops, the notebooks are designed with cells. Each step requiring careful deliberation or pausing.\n\nNow lets come to why Software engineers hate notebooks. \n\nPredominantly software engineers are not used to such pit stops. The code is written incorporating rule based logic and then functionalized.\n\nThe functions are then called and as they say \"the code goes brrr\" giving one the desired final output.\n\nSoftware engineers find the habit of stopping and running each cell exactly in certain order strange. They also feel that notebooks have many hidden states and are hard to debug.\n\nI see merit in the software engineers arguments as well. \n\nPersonally I would prefer using notebooks for experimentation and explorative purposes. But when it comes to productionizing the ML algorithms I would rather prefer .py scripts with good desing pattern.\n\nP.S check out the excellent critique of notebooks by Joel Grus in comments. \n\nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#notebooks \nhashtag\n#datascientist \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3voUNKk \n LASSO stands for east bsolute hrinkage and election perator.\n\nTwo of the primary motivation behind LASSO was :\n1) Obtain a parsimonious model\n2) To find a variable selection method\n\nWant to know more about LASSO and Ridge?\n\nThe below PDF is a collection of LASSO and Ridge resources. \n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientist \nhashtag\n#linearregression \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41A18yR \n Could not find the main content of the post.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41BreBk \n AUROC is all about Ranking ?\n\nOne of the lesser known understanding about AUROC (Area under the curve) is that, it is about Ranking !!\n\nThe AUC (Area under the curve) of ROC (Receiver operating characteristic) or simply AUROC is one of the most often used metric to gauge the performance of a classifier.\n\nOne of the lesser known interpretations is that AUROC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than randomly chosen negative one.\n\nSo, technically AUROC is the probability of correctly ranking a (positive, negative) pair.\n\n============================================\n\nInteresting connection of AUROC with Mann Whitney U test statistic\n\nThere is also an interesting connection between AUROC and Mann Whitney U test statistic.\n\nThe Mann Whitney U test is a non parametric test. (Note : non parametric does not mean 'NO parameters', it in fact means many parameters and that there are no assumption made about the probability distribution).\n\nThe Mann Whitney U test's null hypothesis is that it is equally likely that a randomly selected value from one population will be less than or greater than a randomly selected value from a second population.\n\nQuite clearly there are lot of similarities between AUROC and Mann Whitney U test statistic.\n\nThe relationship is given be the formula :\n\nAUC = U / n1 n2 \n\nwhere U stands for Mann Whitney U test statistic, AUC is the area under the curve, n1 and n2 are positive-negative samples.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientist",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vgfusd \n Myth - Statistical Inference belongs to 20th Century .\nReality - Statistical Inference is useful even in the computer age.\n\nWe are in the era of massive storage and faster compute (aka Big Data Era). In this day and age, some might feel why not collect all the data (population), why do we have to take samples and make inferences about the population?\n\nLet me provide just two points as to why Inferential statistics will never be obsolete.\n\n1) Cost factor\n\nSure, we may have massive storage and compute capabilities now but there is still an enormous cost to collecting this data, storing it, retrieving it, cleaning and standardizing it. Collecting samples and making inferences about the larger population is still a cost effective way.\n\n2) Human Factor\n\nWe can't test a medicine on the whole population to know its efficacy. That would be catastrophically dangerous !! Again thank God we have inferential statistics.\n\nThis book by Bradley Efron and Trevor Hastie is a great read for anybody wanting to develop a good understanding of statistical inference.\n\nLink to the book in comments.\n\n#statistics #machinelearning #datascience #datascientists #inferentialstatistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48hksmX \n Data Science projects that are best avoided in your resume.\n\nThe mention of the below projects in resumes rather work negatively and gives an impression that one does not know the subject !!\n\nLet me provide reasons for these one by one.\n\n1) Predicting Heart Disease using ML\n\nReason to avoid: \n\nMany blindly apply ML algorithms on this data set.\n\nThere is a fundamental problem in the dataset. \nIf you notice, there is only one data point under each feature for a patient. Features like BP, cholesterol, heart beat are not static. They range. \n\nBlood pressure of a person varies hour to hour and on a daily basis, so does heart beat. So when it comes to prediction problem there is no telling weather 135 mm hg blood pressure was one of the factors to cause the heart disease or was it 140, all while the data set might be reporting 130 mm hg. \n\nIdeally, multiple measurements need to be had for each feature for a patient. And that multiple measurements leads to Panel data. \n\nFurther explanation of panel data, cross section data in comments. Also further reasons to avoid this dataset has been covered in one of my articles (link in comments).\n\n2) Predicting stock market prices using ML\n\nReason to avoid:\n\nIf data scientists knew how to predict stock market prices, they would not be working. They would be sipping their favorite beverage on their yacht.\n\nThings that are stochastic in nature can't be predicted accurately. Stock market is one of them.\n\n\n3) Titanic Dataset - predicting whether a passenger would survive or not.\n\nReason to avoid: \n\nIrrelevancy. How does it matter whether you are able to predict the survival of already dead people. I mean given a life / death scenario do you think passengers will wait for you to read out your predicted probabilities of survival. I have never been a fan of this dataset. At best this is a best example of where not to use ML.\n\n4) Predicting covid spread in a population through ML.\n\nAgain you can't predict spread of covid in population accurately through ML. Epidemiologists use SIR models and more. Perhaps it is best to learn those in detail.\n\nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#resumetips \nhashtag\n#datasciencejobs",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/4aOrIsn \n Last week I interviewed quite a few econometricians and statisticians for Causal Inference role. One applicant asked me, what are the key concepts they must know for this role.\n\nIn my opinion, the following concepts are a must know:\n\n Probability theory\n Multi Linear Regression\n Concept of confounding\n Instrumental variables\n 2SLS\n DID (Difference in difference models)\n Counterfactuals\n Mediation\n DAG\n Propensity scores\n Design of Experiments\n\nIf you can learn the above concepts (even say 60-70% of it), you will be a much sort after talent.\n\nSo here is a cheat code to crack my interviews . I have compiled a curated list of resources on causal inference (link in comments).\n\nAnd if you already have a reasonable knowledge of the above, you can apply to us at careers@arymalabs.com. \n\nWe are hiring for both intern roles as well as FTE (check the link for JD). In subject line, mention your name_CI_FTE or Internship (depending on what you are applying for).\n\nhashtag\n#causalinference \nhashtag\n#datascience \nhashtag\n#econometrics \nhashtag\n#statistics \nhashtag\n#marketingmixmodeling \nhashtag\n#marketingeffectiveness \nhashtag\n#hiring \nhashtag\n#internship",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48iKZR2 \n Which Error is worse - Type 1 Error or Type 2 Error ?\n\nBe it hypothesis testing or machine learning classification tasks, one encounters Type 1 Error (False Positive) and Type 2 Error (False Negative).\n\nA quick way to remember Type 1 and Type 2 error is through the 'Boy who cried wolf story'.\n\nBoy cried wolf (when actually there was no wolf). But the villagers believed him - Type 1 Error (False Positive)\n\nBoy cried wolf (when actually there was a wolf). Now the villagers didn't believe him - Type 2 Error (False Negative)\n\nNow the question is, which one is worse.\n\nWell the answer is - it depends.\n\nIn the field of marketing, Type 1 error might not be life threatening. However, Type 1 error in medicine could be life threatening (imagine a patient with no cancer being subjected to heavy chemotherapy or invasive surgery !!).\n\nSimilarly Type 2 Error too could be dangerous. Imagine a cancer patient being diagnosed as not having cancer !!\n\nI particularly liked the approach suggested in one Reddit answer (see image). Always put a number to gauge the consequence of being wrong.\n\nComing to whether Type 1 error is more important than Type 2, Daniel Lakens makes some excellent points in this blog - \"Why Type 1 errors are more important than Type 2 errors (if you care about evidence)\". Link in comments.\n\nPlease feel free to add your comments on which error is worse to have - Type 1 or Type 2 error.\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#hypothesistesting",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48ywRCy \n When it comes to coding, we can always look up stack overflow for some ready made code snippets to our problem. However, when it comes to Data science, there is no ready made answers. One reason is that the data, domain and the context varies. There is no 'one size fits all' solutions.\n\nThe closest we can come to stack overflow like solution in data science is stackexchange and cross validated. Many intuitive answers are provided to statistical and data science questions.\n\nOver the years, I have personally learnt a lot from stackexchange and cross validated answers.\n\nThe PDF below comprises of list of people to follow on stackexchange and cross validated. Just go to the respective profiles and see their top questions and answers.\n\nHappy Learning.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3TEzYVF \n Rules of Thumb in Statistics\n\nWe come across 'Rules of Thumb' in:\n\n Experimental designs\n Statistical tests to apply\n Thresholds/cutoff to set in ML algorithms\n\nRules of thumb arise out of practical experience rather than proven theory. The problem is that, it may not be generalizable everywhere or in every situation.\n\nHere are some Interesting examples.\n\n N>30 \nThe two common issues with N>30 is that a) It is believed that at N=30, CLT kicks in. b) It is believed that one could should use t test when N<30 and when N>30, Z test.\n\nReason: Back in the day (when computers were in its infancy), people used tables much like a logarithm table to see the values of any distribution for any combination of DF and significance level.\n\nAnd because such a table had to fit many distributions (t, normal, chi square etc.), the t distribution page was restricted to only 30 entries. Also at n=30, normal distribution and t distribution were deemed 'approximately' same.\n\n P < 0.05 \nMany believe that significance level of 0.05 is sacrosanct. \nBut how did this p<0.05 come about. It is from misinterpreting RA Fisher's words.\nRA Fisher's these words try to dispel the myth:\n\"If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance\"\n\n 1 in 10 rule \nThis particular rule applies in Logistic regression.\nIt basically says that you could include 1 predictor per 10 cases. Many statisticians have advocated against this citing reasons of 'accuracy of logit coefficients' and 'separation'. (Link of paper in comments.)\n\n 10 % Sample Size \nThe rule states that the sample size should not be more than 10% of the population. This rule is generally cited when dealing with case of CLT where sampling is without replacement.\n\nSimilarly, this rule is also cited with independence criteria of Bernoulli trials. Ideally, Bernoulli trials must be independent but this condition is relaxed if sample size is 10 % of population or lesser.\nThis rule of thumb is not much wrong but just that it is still arbitrary.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3TEzYVF \n 10 % Sample Size \nThe rule states that the sample size should not be more than 10% of the population. This rule is generally cited when dealing with case of CLT where sampling is without replacement.\n\nSimilarly, this rule is also cited with independence criteria of Bernoulli trials. Ideally, Bernoulli trials must be independent but this condition is relaxed if sample size is 10 % of population or lesser.\nThis rule of thumb is not much wrong but just that it is still arbitrary.\n\n Chi square test 20 % of the cell rule.\nFirstly, many think the rule of if 20% of the cells are less than 5, one can't perform chi square' applies to observed value. In fact it actually refers to expected value.\n\nSecondly, this rule of thumb is considered too stringent by some statisticians and have advocated alternatives like Fisher's exact test.\n\n: It is always better to check how these rules came about. If it is due to reasons like Lack of technology & misunderstanding of concepts, then one must be careful in using them.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48xYyvy \n Why Statisticians make for good Data Scientists.\n\n1) They think in terms of data generating process (DGP).\n\nNow you might wonder why is this important. Knowing or thinking in terms of DGP in turn means thinking deeply about one's specific domain. In short this translates to domain expertise.\n\n2) They think in terms of probability distributions.\n\nFollowing from the previous point, it is also important to think in terms of probability distributions. Often whether a process is fat tailed or long tails can inform the data scientist whether any ML algorithm is applicable or not. Knowing when not to apply an ML algorithm is equally an super power as compared to knowing to apply ML algorithms.\n\n3) They have know-how of Design of Experiments.\n\nSome organizations think the job of DS starts only at the modeling phase. Needless to say, a statistician brings the knowledge of how to collect the data, what data to collect and at what granularity.\n\n4) Their approach to data science is robust and scientific.\n\nSome think hypothesis testing is a technique from the bygone era. However, hypothesis testing (NHST to be specific) trains a statistician to be more scientific.\nAsk how?\n\nWell much of science is about falsification. NHST at its core is based on principle of falsification. That is why we don't say \"we accept the Null hypothesis\".\n\nEven Feynman in his famous lecture said \"We can never be right, we can only prove we're wrong\". (check the link in comments for the Feynman lecture and my explanation on NHST)\n\n5) They have more tools in their arsenal.\n\nStatisticians have know-how of GLM, GAM, GLS etc. apart from your usual XGboost and Random Forests. Since most data science projects have tabular data, having these additional tools really helps. Moreover, not all tasks are Y hat tasks, you need B hat too.\n\n6) They also think about causality (Econometrics and Bayesian methods).\n\nCausality is not an easy problem. However there are major strides made in the field of Econometrics and Bayesian statistics to answer this problem. Analysis like A/B testing begs the question of causality. Statisticians are hence better equipped to deal with these.\n\n\nNote: I don't want to sound Gate keeper-ish. When I say statistician I don't intend to mean that you ought to have a degree in statistics (though it is preferred).",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48xYyvy \n Causality is not an easy problem. However there are major strides made in the field of Econometrics and Bayesian statistics to answer this problem. Analysis like A/B testing begs the question of causality. Statisticians are hence better equipped to deal with these.\n\n\nNote: I don't want to sound Gate keeper-ish. When I say statistician I don't intend to mean that you ought to have a degree in statistics (though it is preferred).\n\nI know many friends and connections who don't have a conventional degree in statistics, yet they give statisticians a run for their money !! At the end of the day it is all about having good statistical knowledge and training.\n\nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#econometrics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48cF8MK \n The levels of Linear Regression understanding.\n\nFrom time to time, I keep coming across a lot of misinformation on Linear Regression.\n\nI hence created this meme to allay most of these misconceptions.\n\nNote: Because a meme can only convey so much, I would request the readers to go through the references and resources in comments for more thorough understanding.\n\nhashtag\n#linearregression \nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vdg1uX \n Multivariate  Multivariable\n\nI often come across people using multivariate and multivariable synonymously. \n\nDear Statisticians/ Data scientists, \n\nThe two are not the same. When you use the term interchangeably, it causes lot of confusion.\n\nHere is a quick explainer:\n\nMultivariate : It means many dependent variables.\n\nMultivariable: It means many independent variables but only one dependent variable.\n\nHaving correct knowledge of what you are modeling is very important and this further informs you of how to model accurately.\n\nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#machinelearning \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/4aDIEkY \n What is the meaning of p value and t value in statistical tests?\n\nFollowing my previous post 'Why you should be careful of Automated Feature Selection - Example (Chi Square Test of Independence)', some of you had evinced interest in knowing the topic more intricately. \n\nWhile some had also asked me 'Does knowing the intuition behind p value, t value and design of experiments really help in the era of ML or chatGPT?'\n\nOn the later question, my response is very simple. Yes, it does help immensely. For all the sophistication that is chatGPT, it is still found wanting in many of the core statistical functionalities. It may prompt chunks of code correctly but it still falters in explanation of statistical concepts (ICYMI, link to the post on chatGPT's performance on certain statistical questions in comments)\n\nKnowing the intricacies of design of experiments, p values and t values comes in very handy in A/B testing, Marketing Mix Modeling, gauging marketing campaign efficacy etc.\n\nI would highly recommend reading the answers (especially whuber's) to the stackexchange question 'What is the meaning of p value and t value in statistical tests?'.\n\nLink to the stackexchange question in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#abtesting \nhashtag\n#marketingmixmodeling \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3RxqHMi \n Why Data Science aspirants are not finding Data Science Jobs.\n\nOver the last 5-6 yrs, I have interviewed many data science aspirants. \n\nOne trend that I have noticed in all these years is that the resumes of data science aspirants are increasingly filled with NLP and CV projects while there is no mention of any statistical or classic machine learning words !!\n\nI know, you might be wondering how this has got anything to do with data science aspirants not finding data science jobs. \n\nLet me point out two reasons.\n\n1. The adoption of Data Science / AI in various domains is still at a nascent stage (especially NLP and CV).\n\n2. Most companies have tabular data.\n\nNow the above two points are crucial. Data Science aspirants have been told that NLP and CV are the future. Sure, I don't deny that. I am long on NLP too (but not on CV).\n\nBut you see the adoption of NLP and CV is still very nascent. Also the nature of NLP and CV is such that, once the science is formalized and established, then it just becomes a scaling game.\n\nTake example of chatgpt, the science/mathematics behind chatgpt is already cracked. Now the game is that of scaling (65 bllion parameters or 100 billion parameters etc). Such being the case, not many data scientists would be required.\n\nSo my suggestion to data science aspirants would be to pick up data science skill sets like good know-how of Linear Models, GLMs, GAMs, Design of experiments and classic ML algorithms. Because these can be applied to tabular data. \n\nMany companies have tabular data but don't know what to do with it. We at Aryma Labs consult for various companies and have felt this need up close.\n\nWe at Aryma Labs are also hiring aggressively. If you are a statistics (or related field) graduate with good knowledge of Linear Models, GLMs, GAMs, Design of experiments and classic ML algorithms, you could apply to us at careers@arymalabs.com. Detailed JD in comments.\n\nhashtag\n#datascience \nhashtag\n#hiring \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#ai",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3TFmDfv \n The relationship between Curse of Dimensionality and Degrees of Freedom.\n\nStatisticians/Data scientists often remark \"Don't add more variables, you won't have enough degrees of freedom\"\n\nWhat does that mean ?\n\nLets take an example of Multi linear regression\n\nWhen building a multi linear regression model, adding more independent variables into the model reduces the degrees of freedom.\n\nThis is also related to the \"curse of dimensionality\". Adding more variables is equal to adding more dimensions.\n\nYour next question might be - Why is it a curse? \n\nWell, the problem is, while we add more dimensions, the data we have at hand remains the same. So the data quickly becomes sparse at higher dimensions.\n\nSee in the image below, how the same number of data points in 2D looks dense but in 3D it starts to look very sparse. One can imagine the sparsity in higher dimensions !!\n\nWe need more data to fill up the space, but unfortunately the data at hand is limited.\n\nAdding more variables causes two more things:\n\n1. Loss of degrees of freedom.\n2. Highly inflated R squared value.\n\nSo now you know why statisticians are apprehensive about adding more features into the model .\n\nP.S: An intuitive explanation of Degrees of Freedom is provided in comments.\n\nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#machinelearning \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3RYtfVl \n Linear Regression is a classic example of 'So-much-has-been-written-yet-so-little-has-been-understood'.  \n\nThe first link in comments contains some of my articles on Linear Regression wherein I try to dispel some of these misconceptions.\n\n(Image adapted from the original by @ ash_lmb, original link in comments)\n\nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists \nhashtag\n#artificialintelligence",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3TB2aZv \n Sticking to Defaults - The reason your Data Science project could fail\n\nLack of not knowing how the ML algorithms works under the hood leads to acceptance of 'default'.\n\nHave seen many Data Scientists stick to threshold of 0.5 in Logistic Regression because they believe it is the 'convention' one must stick to.\n\nSame applies to p-value of 0.05 and other such 'cut-offs'. \n\nThe many low code data science library exacerbate the problem as they obscure the various knobs and levers required to correctly specify the models. \n\nThe below image highlighting the fact that Sklearn had used L2 penalization with lambda =1 as default is a classic example on why you need to always peek under the hood.\n\nSo always remember to check if your off-the-shelf ML library has defaults and if they do, what are they.\n\nSticking to 'default' could prevent one from solving the business problem effectively.\n\nP.S: You can read more about the L2 penalization saga through the link in comments.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48xkbMl \n The secret to cracking Data Science Problems.\n\nWe consult for various companies across the globe. Some of the curious questions we get from clients are:\n\n- How do you know which Machine learning algorithm to apply ?\n- How do you know which statistical test to apply ?\nor\n- How do you know when NOT to apply a Machine Learning algorithm?\n\nOur answer - It is all about knowing the Probability Distributions.\n\nIn any Data Science project, it is first important to know the Data Generating process (DGP) and what distribution it follows.\n\nKnowing this helps in deciding which ML algorithm to apply or not to apply (for e.g. in fat tailed processes).\n\nKnowledge of Probability distributions and the relationships between them also helps immensely when applying statistical tests. It also in a way helps to understand processes like Law of large numbers, CLT intuitively.\n\nHere in this short video, I try to demonstrate some of the popular probability distribution relationships.\n\nThe video was made using StatDist. Link to the website in comments. Also if you want to learn more about the probability distributions and the relationships, the Wikipedia link is in comments.\n\n*Correction : The beta distribution is the uniform distribution when alpha and beta parameters = 1. The words 'small and equal' are imprecise.\n\nhashtag\n#datascience \nhashtag\n#probability \nhashtag\n#machinelearning \nhashtag\n#statistics \nhashtag\n#datascientist",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3RBj7QP \n Why does  show up in Normal Distribution ?\n\nWhen I first started learning statistics some 15 years ago, I used to get befuddled seeing the e and  in so many equations. Truth be told, like most, I first learnt the PDF of normal distribution by rote.\n\nI didn't have much of a clue why the e and  showed up so often in statistics. I used to ponder - \"Does it have something to do with area of a circle?\". Quite frankly very few were able to give a convincing or first principle answer to the question. \n\nI guess most people switch off from statistics because things like these are not explained from first principles.\n\nHere is a great explanation by Grant Sanderson (3blue1brown) on why  shows up in the PDF of Normal Distribution (link to the video in comments).\n\nI am sure these intuitive first principles explanations will make you like mathematics and statistics again.\n\nLink to other explanations that I found useful years ago are in comments too.\n\nhashtag\n#statistics \nhashtag\n#mathematics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3tyoR5S \n Why was ANOVA invented ?\n\nI have interviewed many statisticians over the years. One misconception many still have is that, \"If we have more than 2 groups, then we can't perform t-test and that is why we perform ANOVA\".\n\nIt is not that we can't perform t-test with more than 2 groups. We certainly can. \n\nThe real reason why ANOVA came to be is because, doing a string of t-tests would lead to two things:\n\n1) Multiple comparison issue (multiplicity).\n2) Error inflation.\n\nRemember that, each time one would conduct a t-test, there exists a possibility of making a type 1 error. As you conduct many such individual t-tests, you are simply compounding the type 1 error rate.\n\nYou could mitigate compounding these type 1 errors by running a single test - ANOVA.\n\nOne could then use Dunnett's test as a post hoc test. (check the link in comments to learn more).\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3TyfttE \n How useful is F test in Linear Regression?\n\nAfter my last post on \"Why was ANOVA invented?\", some of you asked me about F test in linear regression and its utility.\n\nQuite frankly, F test (for overall significance) in Linear Regression is not that useful. Before explaining why, let me give you a background of what F test does in the context of Linear Regression.\n\nThe F test in Linear Regression essentially tests for 'goodness of fit'.\n\nIt has the following hypothesis:\n\nNull: The intercept only model (no independent variables) fits the data as good as your full model( all your independent variables included).\n\nAlternate : The intercept only model fits the data poorer than your your full model.\n\nNow you run the test and get a very low p value. \n\nShould you celebrate that your full model has provided a better fit?\n\nNo, not so fast. \n\nThe F test in Linear Regression talks more about the combined effect of all the independent variables rather than effect of individual variables.\n\nTechnically, it is telling that the coefficients of all the IVs are not zero. But what if you decide to run individual t tests on them. You would sometimes be surprised to see that while the F test provides you a significant result, the individual t tests could showcase that the variables are not significant. In a way this is Simpson's paradox (If you want to learn more about it, link to Ridhima's post is in comments).\n\nSo you see if your goal is attribution, much like in Marketing Mix Modeling, F test at this stage is not telling you much. It simply kinda says \"you are directionally correct\". It does not provide the specifics that you desired.\n\nF test is useful but only in the right context. \n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41BltUm \n In my last post (link in comments), I wrote about the utility of F test in Linear Regression.\n\nJust to recap, the F test in Linear Regression essentially tests for 'goodness of fit'.\n\nIt has the following hypothesis:\n\nNull: The intercept only model (no independent variables) fits the data as good as your full model( all your independent variables included).\n\nAlternate : The intercept only model fits the data poorer than your your full model.\n\nI would like data science aspirants to view the above hypothesis from a new lens. That lens is - Mean and Conditional Mean.\n\nI see that many data science aspirants still are unaware of the fact that in linear regression, we predict the conditional mean and not raw values of dependent variable !!\n\nNow lets come back to the above hypothesis. In the intercept only model, we are merely saying that the best prediction of the dependent variable is its mean. Now if you put your thinking cap on, you will find that it would be very inaccurate. Imagine you have a group of 10 people and you predict each person's weight as the average of all the 10. You are bound to more wrong than right.\n\nNow imagine if you also had the information of the height of the individuals. Now your prediction of weight is conditioned upon the height. You are likely going to be more accurate than merely stating the mean of the dependent variable. In linear regression involving multiple IVs, your response is conditioned on these IVs.\n\nSo bottom-line, if your intercept only model is as good as your full model, then you need to question the variables you have put in the model.\n\nhashtag\n#statistics \nhashtag\n#linearregression \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48rKUKV \n The key to mastering Statistics/Data Science.\n\nData science aspirants often ask me \"Can you please suggest topics to get a good grasp on statistics?\"\n\nMy suggestion  \n\nThink in terms of :\nMeasures of central tendency - Mean, Median, Mode.\nMeasures of dispersion - Variance, Standard Deviation, Interquartile Range.\n\nBut why?\n\nMost of the topics in Statistics/Data Science somehow boil down to central tendency or dispersion. Let me explain through some examples.\n\n1. Linear regression :\nGenerally, One models the expected value (the conditional mean) not the raw value of dependent variable.\nPls note one can model any quantile in linear regression.\n\n2. Probability distributions :\nThe famous normal distribution is characterized by location parameter (mean) and scale parameter (standard deviation).\nSimilarly other distributions too are characterized by location and scale parameter.\n\n3. Machine learning :\nModel Drift: When we say the model has drifted, it actually means that the existing model has drifted in terms of the location or scale parameter or both from the real model.\n\nAccuracy metrics: Accuracy metrics like F1 is nothing but harmonic mean.\n\n4. Outlier detection or Anomaly detection : We classify something as an outlier if some data point is 2SD or 3SD or even 6SD.\n\n5. Time series forecasting :\nWell one of the key concepts in Time series forecasting is stationarity. A stationary time series is the one whose properties such as mean, variance and autocorrelation structure stays constant over time. Stationarity is important because it is easier and more accurate to estimate parameters of a series whose properties do not change over time. If the mean and variance of the series keep on changing over time, the accuracy of the estimates will vary over time.\n\n6. Hypothesis testing :\nWe have hypothesis testing of mean and difference in means. For e.g. t-test and ANOVA.\n\n7. Information theory :\nMany algorithms like Decision trees, model comparison techniques like AIC use information theory at its core. Even probability distribution comparisons techniques KL Divergence uses Information theory concepts like Entropy, Information gain etc.\n\nWell Entropy again is the expected value (average) of the self-information of a variable\nor\nthe Entropy is the smallest possible average size of lossless encoding of the messages sent from the source to the destination.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48rKUKV \n 7. Information theory :\nMany algorithms like Decision trees, model comparison techniques like AIC use information theory at its core. Even probability distribution comparisons techniques KL Divergence uses Information theory concepts like Entropy, Information gain etc.\n\nWell Entropy again is the expected value (average) of the self-information of a variable\nor\nthe Entropy is the smallest possible average size of lossless encoding of the messages sent from the source to the destination.\n\nP.S: The video below is a great illustration why Standard Deviation is called the \"Measure of Dispersion\". Credits - Twitter (@ TimBrzezinski)\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41GB7hs \n Red pill moments in Data Science.\n\n Normal Distribution is not the most prevalent distribution.\n\n Normality testing can be reasonably ignored in Linear Regression. In fact it is one of the least important assumption. \n\n There is a difference between Normal Distribution and Standard Normal Distribution.\n\n Log transformation will make your data near \"Normal\" only if your original data distribution was Log Normal.\n\n In Linear Regression, you model the conditional mean (expected value) not the raw values of DV.\n\n F test in Linear Regression is not that useful. \n\n R Squared value is not a measure of predictive power\n\n There is no ideal R squared value.\n\n The standard error is the estimate of the standard deviation.\n\n Confidence Interval is not the probability that the parameter of interest lying between the interval is 90 % or 95%\"\n\n Confidence Interval will be always narrower than Prediction Interval.\n\n Logistic Regression outputs probabilities not just 0 and 1. \n\n P values are random variables.\n\n P value is uniformly distributed under the null hypothesis.\n\n You can never accept the Null hypothesis.\n\n Central limit theorem does not kick in at n=30.\n\n Estimation is not same as prediction.\n\n Accuracy and precision are not the same.\n\n Model Selection > Feature Engineering.\n\n Machine Learning  Software Engineering.\n\nLink for explanation to all the above points can be found in comments.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3GZrwsA \n The need for Statistical Assumptions.\n\nWhen it comes to statistical tests or ML algorithms, we make many assumptions.\n\nFor e.g. in Linear regression we make assumptions like:\n\n Errors need to be normally distributed\n\n Independence of errors\n\n Linearity\n\n Homoscedasticity\n\nBut why do we make these assumptions ? What purpose do they serve ?\n\nOne might be right in thinking that it is for mathematical / statistical convenience.\n\nThe deeper answer is that:\n\n We make assumptions because in a way it means that we have less parameters to estimate.\n\n The more assumptions we make, the lesser parameters needs to be estimated.\n\n The lesser the parameters, more parsimonious the model. \n\nImportant caveat : Assumptions may not always be helpful.\nFor e.g. in Student t -test we do assume equal variance. But the Welch's test (which makes no assumption of equal variance) is generally advocated by statisticians over student t test because of better statistical power.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3TEaJ5O \n How do you know you are applying the right ML algorithm or statistical test?\n\nAs we consult for various firms across the globe, we often get asked by clients \"How do you know this is the right ML algorithm?\"\n\nDescriptive statistics and good data visualizations are your two best friends when you are trying to prove or disprove application of a particular ML algorithm. Especially to the non data science background stakeholders/management.\n\nGood descriptive statistics and visualizations can clearly bring out the anomalies or incompleteness in the data or show how the data generating process looks like (PDF).\n\nThis in turn can give you strong talking points on what algorithm can be applied or perhaps that no algorithm can be applied !\n\nThe adage of 'seeing is believing' holds true in these contexts.\n\nP.S: The image attached is that of Anscombe's quartet. The link to details is in comments. It is always helpful to visualize your data along with reading descriptive statistics. \n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/47f2Oit \n Bootstrapping \n\nIn statistics, especially inferential statistics, the corner stone paradigm is that of sample-population.\n\nWe most often don't have the population details. We hence try to infer things about the population through the sample.\n\nFor e.g. Population parameter is estimated through sample statistic.\n\nHowever, when we talk about bootstrapping, the technique may be quite unintuitive for uninitiated.\n\nBootstrapping simply put is a method of repeated sampling (with replacement) of a sample. Of course the sample chosen is assumed to be a good representation of the population.\n\nNow the question one might have is, what do we gain by bootstrapping?\n\n Bootstrapping should be seen as a method to learn about the sampling distribution rather than a method to estimate the population parameter.\n\nIf we take simple linear regression as example, the model is fit to data and is used to make inferences about a larger population, hence the implicit assumption in interpreting regression coefficients is that the sample is representative of the population.\n\nAs stated above, we may not have access to the population quite often. All we have is the sample. \n\nThe question then arises about the quality of the sample and its estimate. How can we be sure of the coefficients in linear regression? Bootstrapping can provide information about the variability of the coefficients. \n\nBootstrapping can hence be used to construct confidence intervals. One communality between bootstrapping and confidence intervals is 'repeated sampling'. Many get the explanation of the confidence interval wrong because they forget the 'repeated sampling' that is inherent in the process of constructing CI. \n\nSimple put, confidence interval is as follows : If one ran the same statistical test taking different samples and constructed a confidence interval each time, then in 95% of the cases, the confidence interval so constructed for that sample will contain the true parameter.\n\nThere is huge body of work wrt to bootstrapping. One excellent resource is the \"An introduction to the bootstrap\" by Bradley Efron and Rob Tibshirani.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#analytics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48wHiGZ \n Gentle Reminder : A high t statistic does not indicate strong relationship of IVs with the DV.\n\nJust the other day, I saw a post (again) stating that a t statistic indicates strength of relationship of IVs to DVs. \n\nSo here is a gentle reminder (sharing it again).\n\nThe high t statistic here does not indicate a strong relationship with the dependent variable. \n\nWe must first understand what is the null hypothesis in the current context. The null hypothesis is slope =0 or in other words the beta = 0.\n\nThe alternate hypothesis being, slope 0.\n\nSo even if we reject the null, we still don't get 'quantification of the effect size'. All we know now is that 'there is non zero effect' but don't know exactly how much.\n\nSo, pls stop inferring that \"A high T statistic value indicates that the variable has a strong relationship with the dependent variable\".\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3RXvcBc \n Myth Busting Time \n\nNon parametric does not mean NO parameters rather it means many parameters. \n\nMany data scientists think that non parametric means no parameters or parameter free. \n\nWhen we talk about non parameter or non parametric tests, it is just that we are not able to estimate them or simple choose not to. \n\nEstimating parameters makes life easy as you get a lot of information about the distribution (as in shape, size, fat/long tailedness). However, estimating them could get tedious. \n\nhashtag\n#statistics \nhashtag\n#datascientists \nhashtag\n#datascience \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48cApuK \n Seasoned data scientists and data science aspirants alike lament the fact that they are not able to find high quality blogs and resources on the internet.\n\nYes, some publications on Medium were good once upon a time. But over the years, most of the blogs there have started to have very low signal to noise ratio. \n\nWithout adequate training in statistics, most blogs are full of errors. Unfortunately, the advice of 'share your ideas and learning' has been taken wrongly by many data science aspirants. Most write to gain likes and to become data science influencers.\n\nI am all for democratizing data science but please remember, when you write, you owe some responsibility to the readers. Some of the readers could be data science job aspirants or exam takers.\n\nA blog with incorrect information can prove disastrous for job aspirants and exam takers.\n\nFrom time to time, I try to share some good resources for data scientists and aspirants.\n\nOne resource that I have found useful and with a better signal to noise ratio is the R bloggers website.\n\nR bloggers have blogs on some intricate topics in statistics. \n\nSince R is predominantly used by people with a background in statistics or related fields (either by vocation or academic), R bloggers has a inbuilt quality check.  \n\nMost blogs contain R code snippets for you to try and replicate.\n\nSo even if you are data scientist with no understanding of R, I would still recommend R bloggers for the sheer depth of content and statistical rigor.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3twg2cQ \n Most Data Science projects start with a high probability of failure.  \n\nThe reason - Data Scientists or Statisticians are roped in too late in the business life cycle.\n\nIn our consulting engagements, we suggest the clients to onboard data scientists from Day 1 (i.e. right at the business use case definition stage and Data collection stage).\n\nQuite often we find that organizations have disparate data and ask data scientists \"Can you help us solve the business problem through these data?\"\n\nIMHO, at this stage half the battle is already lost. \n\nTypically, a Data Science project lifecycle can be broken down into 7 stages (as depicted in the image).\n\nI believe a Data Scientist should be hired right at the 'Business use case definition stage' or no later than 'Data Collection stage'. \n\nWhy?\n\nData Collection Stage- Many companies lament that 80% of their time goes in data cleaning. It is not just data cleaning but weeding out poorly collected data. Not to mention the data collected in disparate DBs and the cost of maintaining these DBs. \n\nData Scientist hired at this stage would be able to tell\n1) What data to collect\n2) How to collect \n3) From where to collect\n\nThus the '80% of time' can be mitigated to a large extent.\n\nLets come to the Business use case stage.\n\nBusiness Use Case Stage - Hiring at the use case stage will allow the Data Scientist to give feedback on the feasibility of applying Data Science in the project. The very first question we try to answer in our consulting engagement is \"Does your business problem need a ML/Stat solution?\".\n\nMany companies lament the sunken cost when they come to realize that perhaps a not so ideal data science method has been applied or a data science method is applied where simple baseline would have sufficed !!\n\nAs RA Fischer said, \"To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.\"\n\nSo businesses need to make sure that they don't engage the data scientists too late.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#ai \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vdiRjB \n Data Science Consulting - Success Mantra\n\nThey say most startups shut down within 1 year of starting. We at Aryma Labs have completed 4 yrs in Data Science business.\n\nWhat is our Success Mantra?\n \nWell we ask ourselves 5 key questions before we take up any project.\n\n1) Do you understand the business?\n\nThis is the most fundamental question we ask of ourselves before any project. There is no point taking up a project in a field where you have no clue how things operate.\n\n2) Does the problem warrant a ML solution?\n\nQuite often businesses want to adopt Deep Learning, NLP and now chatGPT just because it is the latest cool thing. However, the reality could be that their problem may not be suitable for ML. Chances are that a simple baseline solution could do the job. \n\nSome might also say this is not a wise move business wise but trust me, we have had clients give us more business just because we are honest with them.\n\nWe at Aryma Labs believe that the sign of a good data scientist is not only knowing when to apply a particular algorithm but also in knowing when NOT to apply any algorithm.\n\n3) Can you correctly translate the business problem into a ML problem?\n\nOver the years I have noticed that there are variety of skill sets that are required to make a data science project successful. One key skill that is paid very less attention to is 'translating the business problem into a ML problem'. This is both a soft skill as well as hard skill. \n\nFor example: \nYou have a churn rate problem, what distribution does the churn rate follow? Can you model it? \nYou have a marketing mix modeling (MMM) problem. What causal technique would you layer on top of your regression?\n\nThis translation is not easy. As with any translation, knowledge of 2 languages is required. Similarly translating business problem to a ML problem requires understanding of both business and Data Science.\n\n4) Can you correctly choose and build the ML solution for the problem?\n\nEven if your answer was 'yes' to all the above questions, the next bottleneck is the question 'can you correctly choose and build the model'.\n\nHow to specify a model correctly is rarely taught. It comes with practice and a with a lot of hard knocks. Most people overtly focus on getting the data engineering pipeline right. It is important no doubt. But the working model is the nucleus of the project. You don't get the nucleus right, none of the data plumbing is going to matter.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vdiRjB \n Even if your answer was 'yes' to all the above questions, the next bottleneck is the question 'can you correctly choose and build the model'.\n\nHow to specify a model correctly is rarely taught. It comes with practice and a with a lot of hard knocks. Most people overtly focus on getting the data engineering pipeline right. It is important no doubt. But the working model is the nucleus of the project. You don't get the nucleus right, none of the data plumbing is going to matter.\n\n5) Can you correctly interpret the results in terms of what it means to the business?\n\nI have seen teams do all the hard work but only to get stuck on 'how to interpret the results and its impact on the business. \n\nMost businesses often don't care what algorithm you use as long as you can tell them accurately what to do with the results and how it will impact their business. This final leg of the project is a make or break scenario. Getting this right means renewed contract and continued business.\n\nhashtag\n#datascience \nhashtag\n#ai \nhashtag\n#statistics \nhashtag\n#consulting",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/47dTb3x \n \"I will learn the math and stats behind the algorithm post implementing the algorithm\" is the chief reason for Data Science's technical debt.\n\nThere is one other major reason for technical debt in data science - Lack of Data Discipline.\n\nWe have consulted for various companies over the years. One thing that sets apart a company with good data science process from a company lacking it is - Data Discipline.\n\nData Discipline is manifested in different ways. \n\nStarting from:\n\n How you collect data.\n How you store data.\n How you make the data available to your data scientists.\n How data scientists access the data.\n Not to mention and our topic of discussion today 'How data scientists label or don't label their notebooks and scripts'.\n\nI am not against data scientists using notebooks. In fact I understand why data scientists prefer it and why software engineers hate it (see link in comments).\n\nHowever, I would implore data scientists to please label their notebooks. It creates better clarity firstly for yourself and then for your peers who work with you.\n\nI also notice the habit of writing and rewriting disparate pieces of code in the same notebook again and again. This is prevalent among R users too.\n\nI would suggest such data scientists to pick up few design pattern lessons to maintain their code properly.\n\nMost often I have noticed that companies that have data scientists not labeling their scripts / notebooks also struggle with ML or statistical techniques applications. Correlation is not causation you say.  Well in this case I beg to differ. \n\nI guess it all starts from having data discipline.\n\nhashtag\n#datascience \nhashtag\n#technicaldebt \nhashtag\n#python \nhashtag\n#rprogramming \nhashtag\n#datascientists \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3v7Njve \n Polynomial regression is a linear model\n\nDear Data Science Aspirants,\n\nRecently, I came across a post where the author states the following \"Are you encountering a data set where the relationship between the independent and dependent variables appears nonlinear?\"\n\nFirst let me state what is wrong with the statement.\n\n In regression, linearity or non linearity is not defined based on the raw relationship between response and predictors (see attached snippet from Woolridge's book). Rather linearity or non linearity is defined based on the function E[Y|X]. This function that is the conditional expectation is either linear in parameters or not.\n\n Second the OP's statement gives an impression that one can simply eye ball relationships between the response and predictors without fitting a model. \n\nNow why are these nuances important?\n\n In statistics, there is no poetic license. We can't twist words, lest it loses it original meaning. \n\n Going by OP's statements, data science aspirants would just begin to plot response and predictors and upon finding non linear relationship, they would start applying transformations to response or predictors or both. This is such a slippery slope. Most people don't think of regression in terms of conditional expectation E[Y|X]. \n\n Any transformation would affect this function E[Y|X] and make its interpretability complicated.\n\nSo to recap:\n\nWhen one says linear regression, it is important qualify it with the statement \"Linear in terms of parameter\". \n\nAlthough many blogs and articles may say that polynomial regression is for modeling non linear relationship, it would be wrong. Statisticians qualify a model as linear or non linear in terms of parameter. \n\nPolynomial regression hence is still a linear model. That is we don't see the parameters in the model raised to some power or other transformations.\n\nI am all for democratizing data science and encouraging people to share their knowledge. But please remember putting out content especially technically nuanced one is a big responsibility. Many people would use these content to prepare for exams or prepare for interviews. The last thing we would want is them to fail.\n\nLink to referred resources in comments.\n\nhashtag\n#statistics \nhashtag\n#econometrics \nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48Bylwd \n Why Linear Regression is not all about predictions.\n\nOften I come across posts and comments from people where they make claims like 'Linear regression is all about predictions'.\n\nWell they are wrong but I don't quite blame them. Thanks to the machine learning take over of statistical nomenclatures, any prediction task is now labelled as 'Regression task' !!\n \nThis is of course two distinguish from the classification tasks. This peculiar nomenclature borne out of poor understanding is why Logistic regression is wrongly considered as a classification algorithm.\n\nBoth Adrian Olszewski and I have written extensively about why logistic regression is regression. And thanks largely to undeterred efforts of Adrian, even scikit learn changed its documentation to reflect the fact that\nlogistic regression is regression but used as a classification when an external cut off (threshold) like 0.5, 0.6 etc. is placed.\n\nYou can check out all the relevant links to the 'Logistic regression is regression' saga in comments.\n\nOk now lets come back to why Linear Regression is not all about predictions.\n\nSo here are some clues.\n\n The goodness of fit measures.\n\nLets take the example of R squared value. Well if you thought R squared value measures predictive power of liner regression, sorry you are wrong.\n\nR squared value is a goodness of fit measure. R squared value is more about retrodiction than prediction. I know you might be wondering 'what is retrodiction?'.\n\nIn short retrodiction is about making predictions about the past. \nBut does R squared value do that ?\n\nYes, R squared value being a goodness of fit measure, tests how well does the model describes the data that went into generating that fit in the first place.\n\nAlso the word 'goodness of fit' itself offers clue that we are testing how well the model fits the data. \n\nYou see the earliest tools to gauge Linear Regression's performance was goodness of fit and not some MAPE or actual vs predicted metrics.\n\n Estimation of Parameters.\n\nThe second biggest clue is that Linear regression has its origin in design of experiments. \nAlthough nobody thinks about Linear Regression this way anymore, it does not mean that it ceases to exist. \n\nLinear regression is still about sampling. That is drawing samples, and estimating the parameters through the sample statistic.\n\nIt is because of this sampling philosophy that you still see Confidence Interval in the output tables. Again a link allaying misconception about CI is in comments.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48Bylwd \n The second biggest clue is that Linear regression has its origin in design of experiments. \nAlthough nobody thinks about Linear Regression this way anymore, it does not mean that it ceases to exist. \n\nLinear regression is still about sampling. That is drawing samples, and estimating the parameters through the sample statistic.\n\nIt is because of this sampling philosophy that you still see Confidence Interval in the output tables. Again a link allaying misconception about CI is in comments.\n\nSo, overall you see the goal is to estimate the parameters not just prediction.\n\nPrediction is just a positive side effect of having fit the model correctly. And you have fit the model correctly because you have good estimates of the parameter.\n\nI hope now it is clear why Linear regression is not only about prediction.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48wLeHB \n Seeing Statistical Tests through the lens of Signal vs Noise.\n\nMost statistical tests are designed to discern signal from the noise.\n\nLet me take a classic example - ANOVA\n\nThe F test for one way ANOVA is given as follows:\n\nF= Variance between treatments / Variance within treatments.\n\nOne could look at the numerator as the signal and the denominator as the noise.\n\nBut why is the denominator 'variance within treatments' noise ?\n\nLets take a small example.\n\nImagine you have 4 schools A, B, C, D and the heights of 5th grade students in all the 4 schools.\n\nYou have a hypothesis that there is a difference in heights of 5th grade students across the schools.\n\nSo you start collecting data of 5th grade students from schools A, B, C and D.\n\nYou notice that there is slight variations in heights of 5th grade students within a school itself. Something like 3.7ft, 3.8ft..4.2ft.\n\nThis is the within-subjects variance.\n\nYou see when our goal is to discern whether there is a significant difference in the heights of 5th grade students across the schools, the within school variance becomes an impediment.\n\nSay your between subject variance was 5 and within subject variance was also 5. You get a ratio of 1. There is not a lot of signal coming out.\n\nBut imagine a between subject variance was 5 and within subject variance was 2. You get a ratio of 2.5. \n\nSo you can say a given school is 2.5x times different than other schools. \n\nBecause ANOVA is omnibus test you wont know where the difference between the groups are. You will have to conduct a post hoc test.\n\nSo overall, it is always about mitigating noise to enhance the signal.\n\nYou can find this signal vs noise duel in most of the statistical tests. Example goodness of fit measures like R squared value, Power of a test etc.\n\nIf you are interested, link to my other posts on ANOVA are in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#anova \nhashtag\n#linearregression",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48v9R7J \n Prediction  Estimation\n\nFew days back I had posted on the topic \"Why Linear Regression is not all about prediction\". ICYMI link is in comments.\n\nIn the post, I was surprised that even seasoned data scientists believed that Ordinary Least Squares (OLS) is a prediction technique rather than a technique to fit the data !!\n\nI know many of us use the words Estimation and Prediction synonymously.\n\nHowever, if you are a hands-on statistician or Data Scientist, it is good to know the distinction between the two in a statistical sense.\n\nFirstly, let us unpack few terms: Statistic and Parameter\n\n Statistic describes the sample whereas a Parameter describes about the population.\n\n An estimator is a statistic that helps us infer the value of an unknown parameter.\n\nFor example:\n\nSample mean is an estimator of the Population mean (a parameter).\n\n Therefore, an estimate is a guess about the true state of nature (the parameter).\n\nIn my post \"Most Important Assumption Checks of Linear Regression\", I had covered Andrew Gelman's concept of 'Representativeness'.\n\nJust to recap the concept: \"A regression model is fit to data and is used to make inferences about a larger population, hence the implicit assumption in interpreting regression coefficients is that the sample is representative of the population.\"\n\nSo, you see whenever we write the Regression equation, we ideally need to use  instead of Y. Because we are actually estimating the true value of Y through .\n\nThis is illustrated in the image below. Please note, I have put a hat on top of Error (which is to be read as residual) in first equation because in a way residuals are estimates of error.\n\nSo, now coming to what is predictor and prediction.\n\n Predictor uses the data to guess at some random value that is not part of the dataset.\n\n A prediction in an essence is a guess about another random value.\n\nSo lets say we take the same regression model as an example. The regression model has dependent variable (Y) - weight of people and X (independent variable) - height\n\nNow we have the data as follows:\n\nY (in pounds): 110, 130, 145, 160, 175\nX (in inches): 50, 60, 70, 80 , 90\n\nNow if were to use our regression model to predict the weight of a person say at 100 inches height, then it would be a prediction.\n\nYou see the height of 100 inches is not part of the dataset. However we did use the data at hand.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48v9R7J \n Now we have the data as follows:\n\nY (in pounds): 110, 130, 145, 160, 175\nX (in inches): 50, 60, 70, 80 , 90\n\nNow if were to use our regression model to predict the weight of a person say at 100 inches height, then it would be a prediction.\n\nYou see the height of 100 inches is not part of the dataset. However we did use the data at hand.\n\nP.S: I would urge the readers to also read the excellent stack exchange answer on 'Difference between estimation and prediction'. (link in comments).\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#econometrics \nhashtag\n#datascientists \nhashtag\n#linearregression \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48uM47O \n The magic numbers in Statistics\n\nOver the years I have noticed that the numbers (0.05, 30, 95%, 0.5) have been held very sacrosanct. \n\nIn this post, I want to dispel some myths around these numbers and show the readers that most of these numbers were borne out of convenience or arbitrariness. \n\n Why p  0.05 ?\n\nP  0.05 is treated as the only legit significance level by many. But why is it so.\n\nMany point to R A Fischer for 0.05. But did he really advocate for 0.05 to be the only significance level ?\n\nNo. \n\nHere is an excerpt of R A Fisher from his book \"SMRW(1925)*\"\n\n\"The value for which P=0.05, or 1 in 20, is 1.96 or nearly 2; it is convenient to take this point as a limit in judging whether a deviation ought to be considered significant or not.\nDeviations exceeding twice the standard deviation are thus formally regarded as significant. \nUsing this criterion we should be led to follow up a false indication only once in 22 trials, even if the statistics were the only guide available. \nSmall effects will still escape notice if the data are insufficiently numerous to bring them out, but no lowering of the standard of significance would meet this difficulty.\"\n \nIn the book \"The arrangement of field experiments (1926)\", he was even more clear that 0.05 need not be the only standard of significance.\n \n\"If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty (the 2 per cent point), or one in a hundred (the 1 per cent point). Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fail to reach this level. \nA scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance.\"\n \n N=30.\n \n  It is a myth that \"sample mean tends to be approximately normally distributed if the sample size is at least 30.\"\n\n The rule of thumb that if n>30 use z test and if n< 30 use t test is not entirely correct. (You can use t test for larger samples too.)\n\nWhy did n=30 come about ?\n\nThe n=30 heuristic was not devised out of purely mathematical or statistical reasons. Back in the day (when computers were in its infancy), people used tables much like a logarithm table to see the values of any distribution for any combination of DF and significance level.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48uM47O \n The rule of thumb that if n>30 use z test and if n< 30 use t test is not entirely correct. (You can use t test for larger samples too.)\n\nWhy did n=30 come about ?\n\nThe n=30 heuristic was not devised out of purely mathematical or statistical reasons. Back in the day (when computers were in its infancy), people used tables much like a logarithm table to see the values of any distribution for any combination of DF and significance level.\n\nAnd because such a table had to fit many distributions (t, normal, chi square etc.), the t distribution page was restricted to only 30 entries. Also at n=30, normal distribution and t distribution were deemed 'approximately' same.\n\n The 0.5 cut off in Logistic Regression.\n\nMany believe the default cut off value should only be 0.5. \n\nYou should ideally explore other cut offs as well, having studied your class imbalance and the level sensitivity and specificity you seek. \n\nSo folks, there is nothing magical about these numbers. Choose the number based on your context. \n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NINTGb \n P(A|B)  P(B|A).\n\nWant an example ?\n\nP(Dead | Hung)  P(Hung | Dead)  \n\nBasically, a probability that the man is dead because he was hung is very high. Whereas the probability that a man was hung because he is dead is not.\n\nFeel free to share your examples in comments.\n\nP.S: Example taken from Beaulieu-Prvost (link to the paper in comments)\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#probability \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3S0ZenS \n Interesting Fact:\n\nImagine you sculpted your distribution out of wood and tried to balance it on your finger. The balance point would be the mean regardless of the shape of the distribution !!\n\nNow why is that?\n\nThis is where the concept of Moments come into play. Many of you might be wondering, how did the physics concept of 'Moments' came into statistics.\n\nWell, statistics has a physics connection.  \n\nMoments are the expected value of a Random variable. Moments define the characteristics and shape of a probability distribution.\n\nTypically Moments about origin are called 'Raw Moments' and those around the mean are called 'Central Moments'.\n\nIn fact, Mean is the first raw moment.\n\nBut Why the name moment ?\n\nIt turns out that statisticians named it so as an allusion to 'Moment of inertia'.\n\nNow coming back to our original question of 'why the balance point of any distribution regardless of its shape is the mean'.\n\nThe first moment is the 'center of mass' of a probability distribution.\n\nIn physics, the center of mass is the point at which all the torques sume to zero. \n\nA torque is a vector quantity which means that it has both a magnitude (size) and a direction associated with it. \n\nIf the size and direction of the torques acting on an object are exactly balanced, then there is no net torque acting on the object and the object is said to be in equilibrium. \n\nBecause there is no net torque acting on an object in equilibrium, an object at rest will stay at rest, and an object in constant angular motion will stay in angular motion.\n\nNow you know why the balance point of any distribution is its mean.\n\nI would urge the readers to read the excellent article on Understanding moments by Gregory Gundersen. Link in comments.\nAlso a link to drive home the intuition on torque is also in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning \nhashtag\n#physics \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48uPbww \n PCA is Principal Component Analysis\n\nPCA is not exactly a dimension reduction technique. I know most of you would shocked but let me clarify.\n\nPCA is a dimension reduction technique as much as Logistic Regression is a classification algorithm. \n\nYou see in LR the algorithm becomes a classification only after we externally put a cut off of say 0.5, 0.6 etc. By default, LR is regression. It outputs continuous probabilities in the 0-1 range.\n\nSimilarly lets take PCA. \n\nPCA creates new set of variables from the old variables. \nThe new variable /component is a linear combination of old variables with weights assigned to each of the old variable. \n\nYou see if you had 10 variables to begin with, the PCA will construct 10 new variables (principal components). However the first 2 or 3 principal components may explain the maximum variance in the data. \nThis does not mean your other PCs have vanished. NO. It is just that we prioritize only the top PCs that explain the maximum variance in the data. \n\nSo technically PCA has not eliminated any variables as such. \n\nPCA is principal component analysis. It analyses the principal components of the data. Nothing more, nothing less. \n\nNow lets come to utility of PCA and some caveats in using it.\n\n1) PCA as feature selection\n\nOne of the caveats in using PCA for selection is that, one must first understand if the the important variables in your data do contain the most variation.\nImagine if you are predicting sales as a function of price, various marketing spends (say youtube, tv, tiktok, facebook ad, google ads). You know that historically youtube and tv has been your major drivers of sales. But if the spend pattern was constant like $1000 every month for TV and Youtube where as price and other marketing spends show variance, then chances are that the first PC will not contain TV and youtube.\n\nSecondly, PCA by nature considers linear relationship. It does not take into account higher order interaction effects of the variables.\n\n2) PCA as a cure to Multicollinearity\n\nIn linear regression setting, multicollinearity is a problem because you are unable to ascertain and disentangle the effect of independent variables on the dependent variable.\n\nSo some resort to PCA to eliminate multicollinearity.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48uPbww \n Secondly, PCA by nature considers linear relationship. It does not take into account higher order interaction effects of the variables.\n\n2) PCA as a cure to Multicollinearity\n\nIn linear regression setting, multicollinearity is a problem because you are unable to ascertain and disentangle the effect of independent variables on the dependent variable.\n\nSo some resort to PCA to eliminate multicollinearity.\n\nWhen these 'new variables' are created, they may seem to be devoid of multicollinearity (as they are orthogonal). However, they still haven't solved the problem that you were looking to solve i.e. ascertain and disentangle the effects of each independent variables.\n\nWhile PCA's new variables might have reduced overall multicollinearity, it has now created a hotch-potch set of variable/s. One really can't draw any meaningful interpretation out of it.\n\nAs they say \"the cure must not be worse than the disease\". Applying PCA to solve multicollinearity is like a cure worse than the disease when the motive is attribution of effect of independent variables.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#mathematics \nhashtag\n#linearalgebra",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48BC2SB \n The Big XY problem in Data Science\n\n\"Do you think the model accuracy metrics are good enough? What should we do to improve the accuracy metrics ?\".\n\nThe above question is very typical. You might have been asked this question at your work, Kaggle competitions or hackathons.\n\nThis is a typical XY problem. So let's unpack what is a XY problem first.\n\nXY problem is actually asking about your attempted solution rather than asking about your actual problem.\n\nSo basically you try to solve problem X, and you think solution Y would work, but instead of asking about X when you run into trouble, you ask about Y.\n\nSo in the above case, \"Y\" is asking about accuracy metrics and how to improve accuracy metrics.\n\nWhile in reality the real problem \"X\" was whether the chosen ML model was the right one or not.\n\nAccuracy metrics will only tell you how well your selected model is performing. It will not tell you whether the model you have chosen in the first place, is the right one or not.\n\nThis is why fixation with accuracy metrics often can be misleading.\n\nAlso, when you are choosing a ML model from a cohort of different models just based on accuracy metrics (like in case of low code ML libraries), you are committing a typical XY problem.\n\nThis XY problem has many such examples in Statistics and machine learning.\n\nIllustration 1:\n\n\"I applied Box-cox transformation on my Dependent variable because the Dependent variable was not normal\".\n\nSolution Y - Transform the variables through Box-cox transformation.\n\nProblem X - Actually None (only the errors and residuals are assumed to follow normal distribution. Not the raw data itself).\n\nRefer the link in comments for elaboration.\n\nIllustration 2:\n\nApplying K means\n\nSolution Y - \"I performed K means clusters and my clusters are not distinct enough\" (poor silhouette score or no discernable elbow in elbow plot)\n\nProblem X - K means will cluster almost anything, are we sure k means was the right choice !!\n\nRefer the link in comments for elaboration.\n\nIn Data Science, it is important to ask the right question first, before providing the 'right' solution.\n\n\"Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.\" - John Tukey\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#kaggle \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3RD91Pl \n Statistics - First understand and then predict.\nMachine Learning - First predict and then understand.\n\nIMHO, the above two statements succinctly captures the philosophical differences between statistics and machine learning.\n\nLet me provide some examples to substantiate my claim.\n\n Cross validation (CV) vs Train / Test Split\n\nStatisticians advocate for the usage of Cross Validation (CV). \nWhile the Machine learning folks advocate for Train-Test split.\n\nThis is because statisticians focus more on understanding the behavior of the model and its intricacies rather than merely focusing only on prediction.\n\nIf you take CV and introspect deeply, you will notice that CV is in a way a technique to assess retrodiction or in other words a goodness of fit analysis.\n\nYou see the focus of CV is inherently not temporal (a focus towards future time periods). \n\nIts focus is too ascertain model fit on various sub samples of data regardless of the temporal component.\nFurther, CV tests the model on the entire dataset although in subsequent sub samples or folds.\n\nTherefore one kind of gets an 'average' measure of model consistency via CV. Establishing this model consistency is crucial as it also can lend to predictive accuracy.\n\n Train / Test Split\n\nNow lets turn our attention to Train/ Test Split.\n\nTrain/ Test Split as used currently by many machine learning practitioners has a temporal component. The technique is inherently temporal in nature.\n\nThe test split being at t+1, t+2 (future time periods). Since the 'test set' succeeds 'train set' any introspection of model fit only comes after the prediction is made.\nYou see in Train-Test, not much can be known about model consistency merely based on one test set.\n\n The SHAP and LIME ML Explanations\n\nThe SHAP and LIME are excellent examples of \"First predict and the understand\". ML practitioners overtly focus on predictions. \n\nBut because we humans are curious creatures we always demand explanations. ML practitioners hence had to additionally adopt SHAP and LIME techniques. It is another matter that SHAP and LIME rarely works on real life business cases.  \n\nOverall model understanding can't be ignored or postponed. It is like karma, it always comes back.  \n\nP.S: Links to related posts are in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3GZUIiQ \n Few days ago I wrote a post on the distinct differences in the philosophies of Statistics and Machine Learning.\n\nICYMI (link is in comments). \nBut TL;DR version is  \n\n Statistics - First understand and then predict.\n Machine Learning - First predict and then understand.\n\nMachine learning overtly focuses on prediction but then later on realizes the need for understanding (inference). Hence external methods like SHAP and LIME are devised. But they end up like square pegs in a round hole, these 'explanatory' techniques hardly explain the model well in real life setting.\n\nI would like to share this excellent paper by Galit Shmuleli - 'To Explain or to predict'.\n\nIn a nutshell:\n\n Y-hat (prediction) -hat(inference).\n A model with good predictive accuracy need not be good at explaining its underlying mechanism.\n Y-hat + -hat = complete model.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#datascientists \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3tHtMRU \n How Marketing Mix Modeling (MMM) helped me learn Linear Regression from first Principles.\n\nSome of you have appreciated my posts on Linear Regression and other statistics topics.\n\nSome of you also often ask me resources to learn Linear Regression. I always provide a list of books or articles that I have personally read.\n\nBut when I look back, there is one thing that helped me learn the concept of Linear Regression from first principles. And it wasn't just the books or articles.\n\nIt was practice.\n\nMy understanding of Linear Regression really deepened because of Marketing Mix Modeling. \n\n8 years ago, Ridhima Kumar taught me the nuances of MMM. I will forever be thankful to her for that because it helped me not only see Linear Regression from a totally different perspective but also experience the various nuances in it.\n\nMMM is application of Linear Regression and its variants to real life problems. \n\nBecause we had to apply it to real life business problems, there was skin in the game to get the application right. Else we would not get paid.  \n\nOverall, building MMM models helped me experience the below concepts which were just theory before.\n\n1) Multicollinearity \n2) Endogeneity\n3) Autocorrelation\n4) Omitted Variable Bias\n5) Suppression Effect\n6) Regression Dilution\n7) N<P problem\n8) Negative R squared Value\n9) Inflated Zero problem\n10) Interaction effects and confounding.\n\nLink to mine and Ridhima's posts on Linear Regressions are in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#linearregression \nhashtag\n#marketingmixmodeling \nhashtag\n#marketingattribution",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41DYjNd \n Right Approach : Fit Model to Data\n Wrong Approach : Fit Data to Model\n\nThe above is not just a case of word shuffling but a serious problem.\n\nThere is a fundamental difference between the two approaches and the approach of \"Fitting Data to Model\" is not correct.\n\nHere is why.  \n\n Data Transformations\n\nWhen we try to fit data to model, we try to manipulate the data in a way to conform to some model assumptions.\n\nA classic example is, data scientist misunderstanding the assumptions of Linear Regression and thinking that the Dependent variable or Independent variables should follow Normal Distribution.\n\nThe next thing the data scientist does is, carry out transformations like log transformation or some sort of power transformations.\n\nAs a result not only do they deviate from the true data generating process but also end up with wrong interpretation of the model.\n\nOutlier removal\n\nAnother effort by data scientist trying to fit data to model is that they try to remove the outliers. \n\nThe main problem is that one tends to loose crucial information. And because one has let go of crucial piece of information, this leads to model misspecification and thereby dangerously wrong conclusions !!\n\nSo always 'fit model to data' and NOT 'data to model'.\n\nP.S: Link to post on perils of log transformation and what is an outlier is in comments.\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascientists",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/47bMOh3 \n There are two kinds of uncertainties Aleatoric and Epistemic. \n\nAleatoric uncertainty: It is the inherent uncertainty present in the data or the model. We generally can't do anything about Aleatoric uncertainty because well it is inherent. One could think of it as some kind of stochastic process.\n\nEpistemic uncertainty: Epistemic uncertainty is caused because of lack of knowledge. Good knowledge infused into the model generally reduces epistemic uncertainty. Examples include model assumptions or in case of Bayesian framework - The Priors.\n\nBut here is the catch.\n\nIn the Bayesian framework, a bad prior or uninformative prior increases the Epistemic uncertainty.\n\nAnd as a whole the uncertainty (Aleatoric + Epistemic) of the system just gets exacerbated !\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/4aDPwig \n Following my last post, some of you asked me what is my beef with Bayesian Framework.\n\nAnswer - Nothing.\n\nAs an eternal student of statistics and as a practitioner of it on real life business cases, I always want to explore the pros and cons of the methodology. \n\nThe various Frequentists hacks as been well documented (p-hacking, sample size manipulation, confidence interval misinterpretation etc.)\n\nSometimes criticism of Frequentist methods go overboard and people demand abolishing of p-value altogether (a case throwing the baby with bath water).\n\nComing to Bayesian methods, everyone seems to be so gung-ho about its beautiful philosophy that people forget to introspect the flaws.\n\nIt is like seeing a beautiful car and being so awestruck by its exterior design that you forget to question the engine performance.  \n\nI never got vitriolic response when I posted about p-hacking or other flaws in frequentist methods. But when I or anybody says something slightly critical of Bayesian methods, a riot breaks out. Almost a cultist fervor takes over and any reasonable effort to explore the cons gets lost in hand wavy explanations.\n\nAnyhow here is a paper by Bradley Efron on \"Why Everyone isn't a Bayesian\"\n\nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/4auN2Tz \n Bayesian MMM is not a silver bullet for MMM's Multicollinearity issue.\n\nI came across a post few days back which stated that Bayesian Methodologies are better at handling Multicollinearity in MMM.\n\nThis is simply not true.\n\nMulticollinearity is a information redundancy problem and Bayesian methodology can't magically solve it.\n\nRather the problem becomes worse in case of Bayesian MMM because your posterior distribution keeps getting wide as you have more multicollinearity.\n\nWhat this means in layman terms?\n\nLets take an analogy:\n\nImagine you are on a search party to track down your pet dog that got lost in woods.\n\nYou can hear your dog barking and you set up a search radius of say 1km. \n\nBut then you also hear another dog (similar to your pet dog) barking in the woods in another direction.\n\nNow you think of expanding your radius by another 1 km.\n\nYou see that your search of truth (or your pet dog) got much much difficult because there are same kind of barking noises (signals).\n\nThis is what happens in Bayesian MMM too. As you have multicollinearity, your posterior distribution keeps getting wide and you have no clue where the 'true' value lies.\n\nP.S: Image of Multicollinearity and its effect on Posterior distribution can be found in comments. The same is taken from Richard McElreath's book 'Statistical Rethinking'\n\nhashtag\n#marketingmixmodeling \nhashtag\n#marketingattribution \nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NHAM8j \n The Richard McElreath's Quartet\n\nA week ago, I talked about epistemic uncertainty in Bayesian framework as a result of uninformative priors. That post drew expected reactions and many of Bayesian loyalists provided only hand wavy refutations.\nICYMI the link to post is in comments.\n\nAnyhow, I stumbled upon an interesting tweet from Richard McElreath. I have named it 'Richard McElreath's Quartet' much like the Anscombe's quartet. \n\nWhat is this Richard McElreath's Quartet?\n\n\nRichard McElreath begins his post by saying \"Don't trust intuition, for even simple prior + likelihood scenarios defy it\".\n\nThis point is very fundamental and I will link it back to Bayesian MMM later in the post.\n\nBut to stay on point and to build up on what this quartet is all about see below:\n\nThe quartet example shows that the choice of prior and likelihood distribution matters a lot. More specifically the 'tailed-ness' of the distribution matters more.\n\nThe author goes on to give the following 4 options:\n\n1) Normal prior, Normal Likelihood (All if fine here)\n\n2) t-distribution prior, t-distribution likelihood (things start to go awry, we know have bimodal posterior)\n\n3) t-distribution prior, Normal likelihood (the posterior mimics the likelihood and in a way, prior had absolutely no sway).\n\n4) Normal Prior, t-distribution likelihood (The posterior mimics the prior and in a way likelihood had absolutely no sway).\n\nNow what does all this mean?\n\nI can speak about Bayesian MMM and perhaps one could generalize it to other situations where people assume Normal distribution is the most common distribution in nature (It is not).\n\nIn Bayesian MMM, most people assume and specify Normal distribution for media / marketing variables as prior. The problem is that not only is normal distribution uninformative in many cases, it also does not help much when your likelihood follows a distribution with fat tails (like t distribution), see case 4 above.\n\nMy point is not that Bayesian MMM or Bayesian Inference is useless. My point is rather that it requires a lot of hardwork and knowledge to get it right.\n\nAn averag MMM analyst may not even think about convolutions of probability distributions. But when you are in the bayesian world, one has to think of the convolutions. \n\nIn a nutshell, Bayesian inference or Bayesian MMM is like a sledge on a ice with no guard rails. So many possibilities to veer off !!",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NHAM8j \n My point is not that Bayesian MMM or Bayesian Inference is useless. My point is rather that it requires a lot of hardwork and knowledge to get it right.\n\nAn averag MMM analyst may not even think about convolutions of probability distributions. But when you are in the bayesian world, one has to think of the convolutions. \n\nIn a nutshell, Bayesian inference or Bayesian MMM is like a sledge on a ice with no guard rails. So many possibilities to veer off !!\n\nTo summarize in the Bayesian MMM context \"Don't trust intuition, for even simple prior + likelihood scenarios defy it\".\n\nP.S talking about fat tail property of t-distribution. It helps in t-SNE (check my post on the same in comments if interested). Link to Richard McElreath's tweet also in comments.\n\nhashtag\n#marketingmixmodeling \nhashtag\n#marketingattribution \nhashtag\n#datascience \nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#Bayesian",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41AVJHC \n Why we report both confidence Interval and Prediction Interval in our MMM models.\n\nMMM is a type of linear regression but with lot more bells and whistles (check the link in comments for a primer on MMM).\n\nIf you must have noticed in Linear Regression, the confidence interval are always narrower than the prediction interval.\n\nYou see in Linear Regression, we don't model the raw response, rather we model the conditional mean (expected value) -> E(Y|X).\n\nThis conditional mean is the parameter which we try to estimate.\n\nThrough CI, we are trying to answer the question - \"If we were to do this Linear Regression again and again (in a way redrawing samples from the population), \nthen does the CI constructed each time contain the true parameter or not\"? If in 95% of the cases, we do end up having the true parameter, then we are set to have 95% CI.\n\nNow you see, the only error that is accounted for while calculating CI is the inherent sampling error.\n\nWhere as in the case of prediction Interval, the sampling error as well as the variance around that prediction is accounted for. \nThis is why prediction intervals are always wider than CI.\n\nNow in the context of MMM, we report both the CI and PI because CI and PI gives a true picture of the MMM model.\n\nIn a way the PI is the delta of error that is only because of 'variance around the expected mean'. This in a way could further point to issues with multicollinearity. \n\nAgain multicollinearity is a huge issue in MMM. But there are methods to address them such as regularization or residualization. \n\nSo overall, reporting PI and CI leads you to better diagnose and specify a MMM model.\n\nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#marketingmixmodeling \nhashtag\n#econometrics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NEpnWM \n Why do we need assumptions in Statistics? \n\nWhen it comes to statistical tests or ML algorithms, we make many assumptions.\n\nFor e.g. in Linear regression we make assumptions like:\n\n\"Errors need to be normally distributed\"\n\"Independence of errors\"\n\"Linearity\"\n\"Homoscedasticity\"\n\nWhy do we make these assumptions ? What purpose do they serve ?\n\nOne might be right in thinking that it is for mathematical / statistical convenience.\n\nBut the deeper answer is that:\n\nWe make assumptions because in a way it means that we have less parameters to estimate.\n\nThe more assumptions we make, the lesser parameters needs to be estimated.\n\nImportant caveat : Assumptions may not always be helpful.\nFor e.g. in Student t -test we do assume equal variance. But the Welch's test (which makes no assumption of equal variance) is generally advocated by statisticians over student t test because of better statistical power.\n\nhashtag\n#statistics \nhashtag\n#machinelearning \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NMhP4j \n What the word 'confidence' in 'Confidence Interval' Signifies.\n\nA lot of people switch to Bayesian methods not because it is better than Frequentist ones, but mainly because they find it hard to wrap their heads around Frequentist concepts.\n\nOne such concept is Confidence Interval.\n\nOne of the common misconception people have wrt to CI is that \"it is the range in which the probability of finding the parameter of interest is 90% or 95%\". \n\nMany conflate this to 'confidence' of finding the parameter with a certain probability.\n\nBut this is wrong.\n\nLets first look at the correct definition of CI.\n\nExample : 95% CI. \n\nIf one ran the same statistical test taking different samples and constructed a confidence interval each time, then in 95% of the cases, the confidence interval so constructed for that sample will contain the true parameter.\n\nNow that we have got the correct definetion, lets answer the question of 'Confidence' in confidence Interval.\n\nTo understand what the word 'confidence' signifies, we need to first consider another word - 'Coverage'.\n\nConfidence Interval is all about coverage. \n\nThat is, if one ran the same statistical test 90 times or 95 times by taking different samples and constructed a confidence interval each time, would they find the parameter of interest in those intervals each time. \n\nTo further drive home the point, consider an analogy.\n\nYou have a goalkeeper who is static in the ground. Your objective is to erect two poles, each on either side of the goalkeeper (goal post) so as to encompass the goalkeeper.\n\nEven though the goalkeeper is static you find it difficult to place the poles because of fog and your own strange inconsistency in placing the poles around the keeper.\n\n- Sometime you place the poles around the goalkeeper but the poles are too distant from the goalkeeper on either side.\n- Sometimes you place it too close to the goalkeeper.\n- Sometimes you totally miss the goalkeeper.\n\nIn the above example, the goalkeeper is the parameter of interest. If you were to successfully place the goal posts around the goalkeeper 95% of the time, you are said to have 95% CI.\n\nIn a way, the confidence is with respect to your skill of placing the goal posts around the goalkeeper each time. \n\nNow you can imagine these goal posts constrict or expand around the goal keeper. This in a way depicts the confidence interval range.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NMhP4j \n In the above example, the goalkeeper is the parameter of interest. If you were to successfully place the goal posts around the goalkeeper 95% of the time, you are said to have 95% CI.\n\nIn a way, the confidence is with respect to your skill of placing the goal posts around the goalkeeper each time. \n\nNow you can imagine these goal posts constrict or expand around the goal keeper. This in a way depicts the confidence interval range.\n\nAt Aryma Labs, We use Confidence Intervals extensively in Marketing Mix Modeling (MMM), Causal Inference (DiD, RDD etc.) and Experimentations (A/B testing, Incrementality testing etc.).\n\nIn each of these cases, the 'confidence' connotes to the process of generating these confidence intervals. In an essence it is all about the design of experiment.\n\nBottomline: The confidence interval is about coverage. The confidence is more to do with the CI generating process 'the experiments' rather than the interval itself.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#marketingmixmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3TFqjhr \n In MMM, You don't need to train/test split your data.\n\nOkay, some of you might be shocked since I am going against the conventional wisdom prevalent in ML circles.\n\nBut let me elaborate.\n\nIdeally if your goal is inference, you don't need to train/test split your data. In case of prediction, train/test split is justified as the model making such predictions is often black box-ish. \n\nWhen the goal is inference, just as it is in case of Marketing Mix Modeling (MMM), train/test split means that your are 'wasting data'.\n\nThe 25% or 30% data that could have been utilized for better specification of the model and thereby better understanding of data generating process is unnecessarily wasted to check the predictive power of the model.\n\nMMM being a variant of Linear Regression is more about inference rather than prediction (check the link in comments for my previous posts on the same topic). \n\nThe fact that MMM could be used for prediction is just a positive side effect of having specified the model properly. The prediction is just a special case of retrodiction.\n\n How do we validate MMM model performance then?\n\nI know that one of the ways to *prove* that MMM works much like any other ML model is to show the performance on unseen data aka 'test data'.\n\nBut to check the MMM's performance, one could utilize Cross validation.\n\nIn an essence Cross Validation tests the model on the entire dataset although in subsequent sub samples or folds.\n\nOne kind of gets an 'average' measure of model consistency via CV.\n\nWhere as in Train-Test, not much can be known about model consistency merely based on one test set.\n\nThe only caveat wrt to CV in case of MMM is that one must be circumspect of the temporal effects. MMM has a temporal component and hence one should not shuffle the data haphazardly and perform CV. \n\nThis is akin to why one does not do shuffled CV in case of time series forecasting.\n \nThe right way to perform CV would be sequential, i.e. respecting the temporal aspect.\n\nP.S The real test of any MMM is to implement it on the ground  \n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#marketingmixmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3RBoNu7 \n Explaining the 'Hourglass' shape of Confidence Interval.\n\nCouple of weeks back I wrote a post on \"Why we report both confidence Interval and Prediction Interval in our MMM models.\" ICYMI links are in comments.\n\nIf one were to notice the shape of the confidence Interval, one would notice that it is in the shape of 'hourglass' or 'sand clock'.\n\nNow why is that?\n\nWell, the answer again boils down to what Regression does and what Confidence Interval calculates.\n\nIn Linear Regression, we we don't model the raw response, rather we model the conditional mean (expected value) -> E(Y|X).\n\nIn Linear Regression, the line or plane passes through (x,y)\n\nSecondly, in confidence Interval we try to see whether the conditional mean (the parameter) is present in the interval or not.\n\nWe basically answer the question of \"If we were to do this Linear Regression again and again (in a way redrawing samples from the population), then does the CI constructed each time contain the true parameter or not\"? If in 95% of the cases, we do end up having the true parameter, then we are set to have 95% CI.\"\n\nNow that we have the two pieces of information that a regression line or plane passes through (x, y)\nand that CI is an interval to see the presence of the parameter.\n\nIt becomes clear to us that the point (x, y) kind of acts like a fulcrum.\n\nTalking about fulcrum, one might find it relatable that 'moments' in statistics has a physics origin !! (check the link to my post in comments). It must be noted that 'Mean' is the first raw moment.\n\nNow with the analogy that (x, y) acting like a fulcrum, you can imagine this regression line to be like a see-saw. If the fulcrum is at the Centre, one can easily see that the movement at the Centre is limited and while at the ends the movement is more pronounced.\n\nNow with all these reasons, I hope one can see why the confidence interval has an hour glass shape.\n\nP.S: Please also do check out the stack exchange answer (link in comments).\n\nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#marketingmixmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NJjZ4R \n Rows > Columns\n\nIf your job involves dealing with tabular data, then it is always prudent to ask for more data (rows) rather than ask for more features (columns).\n\nThere is no point in having more columns which are shallow (i.e. missing data).\n\nWe at Aryma Labs deal with tabular data a lot, especially in the form Marketing Mix Model (MMM) data.\n\nWhenever client provides us data and asks the question \"Are the data enough or should we get more features?\"\n\nWe always ask them if they can provide more rows than columns.\n\nStatistically speaking too, having more rows is better because the chances of you estimating the coefficients with less bias is more. \n\nLess number of rows per columns always pose the risk of higher bias and there by leading to model misspecification. \n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#marketingmixmodeling \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3H3chhW \n Mean is not a prediction model.\n\nYesterday I came across a post in which the OP had stated that \"The mean is the best prediction model\" and to substantiate it the following points were cited.\n\n Needs no features\n Easy to compute\n No overfitting\n Interpretable\n Optimizes L2\n Analytical solution\n\nWhile I believe the OP made the post in Jest, I believe some clarification and corrections are required so that people don't take it literally.\n\nSo lets come to the fundamental question: \"Is mean first of all a model?\"\n\nWell loosely speaking yes. But strictly speaking no. I will link the resources in comments on what exactly is a model. \n\nMean is in a way a model because like any model it makes assumption (see mins 24 -26 in Kristin Lennox's famous youtube talk: link in comments). That assumption being that your data generating process was unimodal. Mean calculation becomes tricky if one had multimodal distribution.\n\nWell then, lets come to the meat of the topic. Is Mean a prediction model?\nNo.\n\nA prediction in an essence is a guess about another random value. Also prediction as agreed and defined by statisticians, is actually an exercise to arrive at a future value.\n\nSo prediction definitely has a temporal component. \n\nSure one could predict values within a range. But that then is called retrodiction. Mean just retrodicts. \n\nFor e.g. If I have numbers like 2,4,10,15, 20 the mean would be 10.2 \n\nThis mean value will always fall within the range of 2 and 20. This mean value will never go out of bounds say greater than 20. \n\nIn fact as many had noted in the OP's post, even decision trees retrodicts.\n\nDecision trees can't extrapolate. In fact as I had written earlier (Link in comments) Linear regression too is not merely about prediction it is also about retrodiction.\n\n Conflation with conditional mean\n\nMany in the OP's post also mentioned that in \"Linear regression we model the mean\". Well it is not just the mean. It is the conditional mean E(Y|Xi].\n\n\nComing to other points:\n\nOverfitting \nWell we are not technically overfitting because mean just does not predict.\n\nNeeds no features\nWell you do need the numbers.  \n\nEasy to compute \nyes, since we invented calculators and computers. This take would not have been agreeable to folks from 1800's and beyond.\n\nInterpretable\nYes, But it depends.\n\nOptimizes L2. \nNot really. Look at the formula and you will see it is not really optimizing in the strict manner of 'optimizing'.\n\nAnalytical Solution\nYes. \n\nSo, What is a Mean?",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3H3chhW \n Overfitting \nWell we are not technically overfitting because mean just does not predict.\n\nNeeds no features\nWell you do need the numbers.  \n\nEasy to compute \nyes, since we invented calculators and computers. This take would not have been agreeable to folks from 1800's and beyond.\n\nInterpretable\nYes, But it depends.\n\nOptimizes L2. \nNot really. Look at the formula and you will see it is not really optimizing in the strict manner of 'optimizing'.\n\nAnalytical Solution\nYes. \n\nSo, What is a Mean?\n\nMean can be considered as an estimator and also perhaps as a data reduction method. But mean certainly is not a prediction model.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48cHCee \n Dynamic Time Warping and its Innovative Use Cases in Marketing Science\n\nIf you thought I would be talking about time travel, sorry to disappoint you. \n\nBut one can't be blamed for thinking that DTW may be something about time travel because of its very name. Anyhow the method is no less intriguing.\n\nDynamic Time Warping (DTW) is a time series analysis technique used for measuring similarity between two temporal sequence. These sequences or time series could be of different length as well.\n\nIt calculates the optimal match between two series using a set of rules. Suppose there are two series  A and B. The DTW algorithm minimizes the distance between them by creating a warping path W (now you know where the word warping come from in DTW).\n\n Dynamic Time Warping vs Euclidean Distance\n\nEuclidean distance is useful when both the time series are in sync and will compare the value of series 1 at time t with the value of series 2 at time t. Basically, it does one-to-one match and does not take into account time shifts.\n\nDTW on the other hand applies one-to-many or many-to-one matches in such a way that the overall distance between the two series is minimized. It allows two similar shaped series of different lengths to match.\n\n How we innovatively used DTW in Marketing Science.\n\nWe used DTW to understand which Candidate Markets were similar in behavior to the Ideal Market'.\n\nThe result:\n\n $2.1 Mn Incremental Revenue\n $250k Saved\n Expansion in new markets and boost in lead generation\n\nDetailed case study link in comments.\n\nWe are also leveraging DTW for MMM in an innovative way. More on this later. Stay tuned.  \n\nhashtag\n#marketingmixmodeling \nhashtag\n#statistics \nhashtag\n#timeseriesanalysis \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/4ayvEgx \n Multicollinearity and Statistical Power\n\nIf you are familiar with Marketing Mix Modeling (MMM) or just multi linear regression in general, you must have noticed the following effects at some point in time:\n\n1) Signs of variables changing\n2) Wide Confidence Intervals\n3) Large Standard Errors\n4) Inflated R Squared value\n5) Overall bad model fit\n\nThese are tell tale signs of multicollinearity. \n\nIn the marketing mix modeling space, attribution is everything. Failure to do so is a huge downer.\n\nBoth Ridhima and I have talked about this issue in many of our posts (links to some in comments).\n\nIn this post however, I want to talk in depth about the statistical power. \n\nStatistical power can be Eli5'ed as below (reference reddit):\n\n\"Statistical power could be considered analogous to the 'magnifying power' of a magnifying glass.\n\nSuppose you have two objects positioned so close to another that you can't tell by the naked eye whether they are physically connected or not. You know that the objects are either connected or not, but you need a magnifying glass in order to visualize the space between them (if it exists).\n\n(Suppose we're talking VERY tiny distances here)\n\nIn this analogy, the distance between the objects is analogous to an effect size, and you are attempting to show that the distance is >0.\n\nIf the distance is relatively large, then a weak magnifying glass would be sufficient to show that there is a difference between the two objects. However, if the distance is very small, you might need a very powerful magnifying glass before you could see any difference.\n\nSimilarly the more \"powerful\" a statistical test is, the smaller the difference between two quantities it can resolve (for some allowable degree of uncertainty).\"\n\nSo taking the above analogy, Multicollinearity makes the two objects very hard to discern. Therefore it becomes really difficult to know the effect size of each variable. Thus the overall statistical power is lowered.\n\nMulticollinearity is a signal redundancy problem rather than a signal deficit problem. \n\nWe at Aryma Labs have found good solutions to detect as well as mitigate the issue of Multicollinearity in our MMM models. Some of the solution links in comments.\n\nhashtag\n#marketingmixmodeling \nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#linearregression",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41JRnyc \n Selecting MMM models via AIC? Some key pointers  \n\nThe Akaike information criterion (AIC) is given by:\n\nAIC = 2k -2ln(L)\n\nwhere\nk is the number of parameters\nL is the likelihood\n\nThe underlying principle behind usage of AIC is the 'Information Theory'.\n\nTalking about information theory, we have been researching and implementing these concepts innovatively in MMM. (Check the link to our whitepapers in comments).\n\nComing back to AIC, In the above equation we have the likelihood. We try to maximize the likelihood.\n\nIt turns out that, maximizing the likelihood is equivalent of minimizing the KL Divergence.\n\n But what is KL Divergence?\n\nFrom an information theory point of view, KL divergence tells us how much information we lost due to our approximating of a probability distribution with respect to the true probability distribution.\n\n Why we choose models with lowest AIC\n\nWhen comparing models, we choose the models with lowest AIC because in turn it means that the KL divergence also would be minimum. Low AIC score means little information loss.\n\nNow you know how KL divergence an AIC are related and why we choose models with low AIC score.\n\n Breaking myths about AIC in context of MMM\n\nOne of the misconceptions about AIC is that the AIC helps in choosing the best model out of a given set of models.\n\nHowever, the key word here is 'Relative'. AIC helps in choosing the 'best model' relative to other models.\n\nFor example, if you had 5 MMM models (fitted for same response variable) and all 5 are overfitted badly, then AIC will choose the least overfitted model among all models.\n\nNote: AIC will not caution that all your MMM models are poorly fitted. Much like SHAP values (but more on this in future posts). In a way AIC is like a supremum of a set.\n\nP.S: Link to a comprehensive paper by David Anderson and Kenneth Burnham on Myths about AIC is in comments.\n\nhashtag\n#datascience \nhashtag\n#marketingmixmodeling \nhashtag\n#statistics \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vfCojk \n How we build robust MMM models with help of Bootstrapping.\n\nIn statistics, especially inferential statistics, the corner stone paradigm is that of sample-population.\n\nWe most often don't have the population details. We hence try to infer things about the population through the sample.\n\nFor e.g. Population parameter is estimated through sample statistic.\n\n What is Bootstrapping?\n\nBootstrapping simply put is a method of repeated sampling (with replacement) of a sample. The sample chosen is assumed to be a good representation of the population.\n\n What is the utility of Bootstrapping?\n\nBootstrapping should be seen as a method to learn about the sampling distribution rather than a method to estimate the population parameter.\n\nIf we take simple linear regression as example, the model is fit to data and is used to make inferences about a larger population, hence the implicit assumption in interpreting regression coefficients is that the sample is representative of the population.\n\nThe question then arises about the quality of the sample and its estimate. \n\nHow can we be sure of the coefficients in linear regression? Bootstrapping can provide information about the variability of the coefficients. \n\nBootstrapping can hence be used to construct confidence intervals. \n\n Communality between Bootstrapping and Confidence Intervals\n\nOne communality between bootstrapping and confidence intervals is 'repeated sampling'. \n\nMany get the explanation of the confidence interval wrong because they forget the 'repeated sampling' that is inherent in the process of constructing CI.\n\n How we use Bootstrapping in MMM.\n\nMMM is built using various forms of Multi linear regression. \n\nThe core emphasis is on attribution. This attribution is made possible because of the coefficients. Therefore it becomes imperative that we are really sure about coefficients, because many of the downstream applications like saturation curves, budget optimization etc. are artifacts of the MMM model.\n\nBootstrapping thus helps us by providing information about coefficient variability.\n\nOverall, bootstrapping helps us build robust MMM models. And robust MMM models are the models which clients can trust.\n\nP.S: There is huge body of work wrt to bootstrapping. One excellent resource is the \"An introduction to the bootstrap\" by Bradley Efron and Rob Tibshirani.\n\nVideo Courtesy: Bootanim (University of Auckland)\n\nhashtag\n#statistics \nhashtag\n#marketingmixmodeling \nhashtag\n#datascience \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3GYN7kE \n Decile Analysis - An interesting NON CLASSIFICATION use of Logistic Regression.\n\nIf you asked the current crop of data scientists, I bet at least 70-80 % would tell you that \"Logistic Regression is a classification algorithm and it is not regression\".\n\nIf you tried to correct them they would think it is you who don't know statistics.  \n\nAnyhow, in our marketing science world, we time to time get ample opportunities to use Logistic Regression for what it was originally designed for - Regression.\n\nSo here is a use case which we developed for one of our client and we still employ this technique for our other clients.\n\n Scenario:\n\nYou have data of 10,000 customers with a history of both purchase and non-purchase of your product. \n\nThe management wants you to roll out a campaign to increase the adoption of the product.\n\nThe management wants you to identify only the highly probable set of customers who will buy the product. This is mandated to save time and cost.\n\nWhat do you do?\n\n Enter Decile Analysis.\n\n Decile Analysis is used to categorize dataset from highest to lowest values or vice versa. (Based on predicted probabilities).\n\n As obvious from the name, the analysis involves dividing the dataset into ten equal groups. Each group should have the same no. of observations/customers.\n\n It ranks customers in the order from most likely to respond to least likely to respond.\n\nWe implemented Decile Analysis for this client and helped them save costs and improve MROI at the same time.\n\nLink to the case study in comments. Also other Interesting articles about Logistic Regression too are in comments.\n\nhashtag\n#statistics \nhashtag\n#datascience \nhashtag\n#machinelearning",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48dKW8L \n Why Heteroscedasticity matters in Marketing Mix Modeling (MMM).\n\nMMM is something that always leads you to reminisce about the learning one had or hadn't during their #statistics course.\n\n15 years ago, when I was a student, I attended one statistics seminar. In it the professor told \"Always be testing your assumptions\".\n\nThat statement rings true even now, especially in MMM.\n\nIn MMM, inference is the name of the game. \n\nIn inference, one deeply cares about the preciseness and un-biasedness of the regression coefficients. \n\nSo the question arises, what can throw a spanner at your inference?\n\nHeteroscedasticity is one of the culprits.\n\n But what exactly is Heteroscedasticity? Can you simply eye ball it from your data?\n\nOne of the assumptions in #linearregression is Homoscedasticity which means that once you have fitted the model, the residuals don't have any pattern.\n\nResiduals offer telltale signs of how consistent your model is. Ideally you would want some kind of stability in your model.\n\nIn case of Heteroscedasticity, the residuals by and large have some pattern. Lets take an example of one of the popular pattern (funnel shape).\n\nWhen you have fan/funnel shape of the residuals, it means the model is getting worse over time since it indicates inflation of error.\n\n Can you eye ball Heteroscedasticity just by looking at data?\nNo. You can eye ball the residuals and infer heteroscedasticity but not just the raw data.\n\nHeteroskedasticity is an artifact of the model. One would have to build the model first. So pls don't make the mistake of just looking at raw data and say \"my sales is showing Heteroscedasticity\"\n\n Why Heteroscedasticity happens?\n\nHeteroscedasticity often happens because of outliers or huge disparity in the range of your independent variables. For e.g. a company spends in the range of 10-15k USD every month on youtube ads. But in few instances, say during BFCM the company decided to really ramp up their spends. Lets say this in the range of 85k-100k. Data like these would lead Heteroscedasticity in the model.\n\nHow does Heteroscedasticity affect MMM?\n\nAs mentioned earlier, the name of the game in MMM is inference. You want your estimates to be precise and unbiased. However Heteroscedasticity makes your estimates less precise even though it may not bias them.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48dKW8L \n How does Heteroscedasticity affect MMM?\n\nAs mentioned earlier, the name of the game in MMM is inference. You want your estimates to be precise and unbiased. However Heteroscedasticity makes your estimates less precise even though it may not bias them. \n\nHeteroscedasticity reduces the trust factor in your MMM. Given that companies make million dollar decisions to spend on certain marketing variables, it becomes imperative that the MMM model you build is trust worthy and accurate.\n\nStatistically, heteroscedasticity would make you infer 'effect' even when there is none or relatively small effect because of very low p-values.\n\nWhat is the solution?\nRobust regression could be one of the solution. \n\nTo summarize: Statistical rigor is every important. Especially when million dollar decisions are being made based on the model you develop. And as they \"Always be testing your assumptions\".\n\n#marketingmixmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48eAgqu \n The utility of F test in Marketing Mix Modeling (MMM)\n\nAs most of you know, one of the interesting application of linear regression is Marketing Mix Modeling (MMM).\n\nEven though additional bells and whistles are added in MMM over and above what a traditional linear regression entails, the core of MMM is still linear regression (or its many variants).\n\nGiven this background, it becomes imperative that you diagnose your MMM model properly.\n\nMany misread the diagnostic plots and metrics in Linear Regression. And as a result this extends to wrong interpretation of the MMM model too.\n\nSo lets delve into what people get wrong about the F test.\n\nBut before that, let me give you a background of what F test does in the context of Linear Regression.\n\nThe F test in Linear Regression essentially tests for 'goodness of fit'.\n\nIt has the following hypothesis:\n\n Null: The intercept only model (no independent variables) fits the data as good as your full model( all your independent variables included).\n\n Alternate : The intercept only model fits the data poorer than your your full model.\n\nNow you run the test and get a very low p value. \n\nShould you celebrate that your full model has provided a better fit?\n\nNo, not so fast. \n\nThe F test in Linear Regression talks more about the combined effect of all the independent variables rather than effect of individual variables.\n\nTechnically, it is telling that the coefficients of all the Independent variables are not zero. But what if you decide to run individual t tests on them? \n\nYou would sometimes be surprised to see that while the F test provides you a significant result, the individual t tests could showcase that the variables are not significant. In a way this is Simpson's paradox (If you want to learn more about it, See link in comments).\n\n How does all this affect MMM?\n\nThrough MMM, our ultimate goal is attribution and more specifically we are looking at attribution of each individual marketing variables towards the KPI (e.g. sales, brand equity, CAC etc.)\n \nAs mentioned above, The F test in Linear Regression talks more about the combined effect of all the independent variables rather than effect of individual variables.\n\nF test at this stage is not telling you much. It simply kinda says \"you are directionally correct\". It does not provide the specifics that you desire.\n\nTherefore one needs to go further and explore post hoc test as well.\n\nhashtag\n#marketingmixmodeling \nhashtag\n#linearregression \nhashtag\n#statistics",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48wNxKP \n Validating MMM Models the right way\n\nI often come across posts where people overtly prescribe out of sample error and incrementality testing as the only way to validate the MMM model.\n\nThey often do so by belittling the importance of goodness of fit.\n\nI am sorry to say, but people who say \"R squared value and statistical significance measures (e.g. p-values) don't help in validating the MMM model\" simply don't understand statistics and as an extension don't understand MMM either.\n\nIf your vendor is trying to water down statistical significance measures, then you should be really wary of them.\n\nMMM models do need to be validated on out of sample data and incrementality testing (with causal framework). \n\nBut that is Path B. One can't reach path B without traversing Path A. That Path A is 'Goodness of fit'.\n\nAs a MMM modeler, your first job is to make sure you have a good model fit. \n\nTalking about model fit, lot of people blindly discard R squared value much like people callously say \"correlation is not causation\".\n\nGranted R squared value is prone to inflation as you add more variables (anyhow to mitigate this you also have adj R squared value). \n\nBut at its core, it answers one key question that should be the holy grail for MMM modelers.\n\nThat question is \"how much of the variation in KPI is accounted for by the independent variables in the model\".\n\nIn MMM, the goal is attribution. But before you get to attribution, you need to make sure that you have all the right candidates that played a crucial role in impacting the KPI.\n\nR squared value provides that clue.\n\nFurther one can always strike a balance by relying on information theoretic approaches like AIC that gives you information on parsimony of the model.\n\nSo together with R squared value, AIC and other significance measures one can gauge the goodness of fit of the model.\n\nAnother excellent way to measure goodness of fit is through Cross validation. The beauty of CV is that you won't be wasting precious data by doing train-test split.\n\nIn case you are wondering why I am saying 'wasting data', check my post 'Dont train-test split your data in MMM'. Link in comments.\n\nIn summary - One can't magically start validating the model through out of sample and incrementality. You need to first get right the Goodness of fit. \n\nGoodness of fit provides you crucial information on Multicollinearity and Endogeneity. Both are kryptonite for MMM.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48wNxKP \n In case you are wondering why I am saying 'wasting data', check my post 'Dont train-test split your data in MMM'. Link in comments.\n\nIn summary - One can't magically start validating the model through out of sample and incrementality. You need to first get right the Goodness of fit. \n\nGoodness of fit provides you crucial information on Multicollinearity and Endogeneity. Both are kryptonite for MMM. \n\nPlease remember - The path to validating the MMM model starts with Goodness of Fit.\n\nhashtag\n#marketingmixmodeling \nhashtag\n#statistics \nhashtag\n#linearregression",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/47eF7GT \n Calibration vs Validation\n\nA lot of people use calibration and Validation interchangeably. The two are not the same.\n\n Calibration\n\nIn a regression setting, calibration of a model is about understanding the model fit.\n\nGoodness of fit measures like R squared values, P value, Standard Error, Cross validation and within sample MAPE/MAE/RMSE inform you how well you have fit the model.\n\nBased on these metrics, one could tune and calibrate the model better.\n\n Validation\n\nValidation of a model is in a way a test of your model fit. One can normally validate their model through the following ways:\n\n- Incrementality testing\n- Geo-lift\n- Causal experiments\n- Out of sample MAPE/MAE/RMSE\n\nWithout a strong focus on Calibration, Validation would be a futile exercise. \n\nICYMI, I wrote about the importance of goodness of fit in Marketing Mix Modeling (MMM) yesterday (link in comments).\n\nOne should not compromise on calibration.\n\nThe path to validating any MMM model (or any regression model) starts with Goodness of Fit.\n\nhashtag\n#linearregression \nhashtag\n#statistics \nhashtag\n#marketingmixmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41BdV3Q \n In any data science project, the biggest hurdle is translating the business problem into a statistics/ML problem.\n\nLot of things gets lost in this translation which eventually leads to inaccurate models and unhappy customers.\n\nIn MMM, especially Bayesian MMM, this 'lost in translation' problem is more pronounced.\n\nThe client is sold the magic that through Bayesian MMM they can encode their prior beliefs into the model.\n\nBut in my experience, to encode prior beliefs one needs really good grasp on probability distributions.\n\nEven trained statisticians find it challenging to convert \"Hey I feel the marketing channel xyz's ROI should be around 2.3\" into a particular probability distribution.\n\nBecause many don't know what the ideal probability distribution should be, they resort to defaults (e.g. normal distribution or half normal).\n\nGiven this uphill task, it is unfair to expect clients to tell the agency what the prior probability distribution should be. \n\nThey will not have much of a clue and any client would get chided with the agency because rather than solving the problem themselves, they are asking the client to do the heavy lifting.\n\n The prior matters less or does it?\n\nGranted your prior distribution matters less in Bayesian framework, as you get more and more data. \n\nBut the problem with MMM is, it is a small data problem. You don't get more data (or lets say enough data to make your prior choice irrelevant).\n\n What prior to set?\n\nThe second problem is 'what should be the prior when you have no clue?'\n\nAs MMM is being adopted by new domains who had no prior experience with MMM, the challenge of what prior to set arises. \n\nTill date, I have not got a convincing reply from a Bayesian MMM practitioner on what the prior distribution should be in this case.\n\nFor all of the above reasons (and more - see link in comments), I generally find frequentist MMM to be a better fit.\n\nhashtag\n#marketingmixmodeling \nhashtag\n#statistics \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3twH6Zm \n How a statistical technique that helped solve German Tank problem during WW2 is helping us get accurate attribution in Marketing Mix Modeling (MMM).\n\nWhen we talk about application of statistics during world war 2, somehow the image of the airplane with red dots (survivorship bias) comes to mind.\n\nDon't worry I am not going delve on that again \n\nThere are however other lesser known statistical applications during world war 2 which had a huge impact in the outcome of the war.\n\nOne such application is in the 'German Tank problem'.\n\nThe 'German Tank Problem' is basically about statistical theory of estimation. Estimating population based on sample is nothin new, but the innovative application in the German tank problem is something to really learn about.\n\nThe problem is about estimating the number of tanks produced in a month by the German army just based on the serial numbers of the the captured tanks. The assumption of course here was that the tanks were numbered sequentially as they were manufactured. Also the sampling is done without replacement.\n\nAt the core of it, the principle behind this technique is 'Minimum-variance unbiased estimator (MVUE)'\n\nThis humble technique of estimation even beat the Allies Intelligence agencies' prediction.\n\nWhile the Intelligence agencies' numbers were way off (around 1000-1500), the statistical technique had estimated the n.o of tanks to be 246 a month. This was later proven to be correct as the post war actual number showed it to be 245 tanks a month !!\n\nThe relevance of MVUE in MMM\n\nMMM is a problem of attribution. Statistically, correct attribution can only be had if your model is well specified. Because your MMM model is an abstraction of the marketing reality, the coefficients (or effect sizes) of marketing channels are only estimates.\n\nYou would want these estimates to reflect the true marketing reality. To do that your estimates needs to be unbiased and have minimum variance. You don't want your estimates to be fickle. In general, one has to strive to find estimators that are MVUE.\n\nAt Aryma Labs, we strive to build MMM models whose estimators are MVUE. This in turn leads to robust and trustworthy models based on which clients take marketing decisions boldly.\n\nP.S: References and useful links in comments.\n\nhashtag\n#statistics \nhashtag\n#marketingmixmodeling \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/41Cr5h3 \n Why customers should not expect their Marketing Mix Models (MMM) to have very high R squared value.\n\nSomehow over the years, two myth has been propagated :\n\n High R squared value = good \n R square is a sign of predictive power of a model\n\nI guess we statisticians are partially to blame for propagating these myths.\n\nLet's break these myths:\n\n1) R Squared value is not a measure of predictive power\n\nThough some have this misconception, R squared value is more about retrodiction than prediction. R square value is more about 'goodness of fit'.\n\n2) There is no ideal R squared value\n\nBecause of the myth that R square is a measure of predictive power, \nmany clients develop the expectation that they need to have a 'high R squared value' else the model is not useful. \n\nI have seen cases where a vendor developed and presented client a MMM model with R squared of 0.9 (a highly overfit model) just because of the fear that the model with relatively lower R squared would not be accepted !!\n\nA high R squared value does not necessarily mean it is good. It depends on the data and the domain.\n\nSadly, the high R squared value fixation becomes a manifestation of Goodhart's law. \"When a measure becomes a target, it ceases to be a good measure.\"\n\nIn the pursuit of high R squared values, one might end up with overfitted and poorly specified MMM models.\n\nIt will be prudent to not over fixate on R square values.\n\nP.S: I would urge the readers to read the Lecture notes by Dr. Cosma Shalizi. The notes expands on some of the popular myths about R squared value. Link in comments.\n\nhashtag\n#linearregression \nhashtag\n#marketingmixmodeling \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NGY5yV \n Marketers are sometimes given bad advice that they should not go for advanced methods for marketing impact measurements. \n\nInstead they are suggested to adopt simple analysis like correlation.\n\nIll advised suggestions like \"All you need is only correlation\" will do more harm than good.\n\nSo I will detail out the advice that I came across in one such post and explain why it is wrong to simply rely only on correlation.\n\nThe advice goes something like this:\n\nCalculate correlation coefficient between CAC and advertising spends.\n\nIf you observe a correlation coefficient closer to 1, then your ad efforts were a waste.\n\nNow why is the above advice flawed?\n\nFor this we will have to delve a little bit into \nhashtag\n#statistics .\n\nCorrelation has limitations:\n\n1. Correlation assumes linear relationship.\n\nThe first question to ask even before calculating Correlation coefficient is that: is the relationship between my variables linear?\nDoes CAC and ad spends have a linear relationship? In most cases you will find that this is not the case.\nSo application correlation will simply give you erroneous results.\n\n2. Correlation is prone to outliers.\n\nNow what is an outlier is subjective. But lets assume a brand is spending on $10k every month on ad spends. Now if this brand suddenly spent $50k in 1 month, then that would be considered an outlier. Does such a scenario happen? yes, such pattern is observable during BFCM.\n\nNow imagine you have this huge bump in ad spends for 1 month. Then you calculate correlation between CAC and ad spends. \nThe corr coefficient will be either severely affected. Now if this outlier inflated your correlation coefficient, you would assume that your ad spends are not working !! You see, just one data point can really make you take bad decisions.\n\n3. Correlation != causation\n\nIn the above advice, one major drawback is that we are assuming the relationship to be linear and also we believe there is a causal relationship between CAC and ad spends.\n\nCorrelation is an insufficient condition for causality. \n\nCAC (in most cases) can get impacted by variables other than ad spends. But our correlation coefficient only accounts for ad spends.\n\nIf you are making million dollar marketing decisions, then correlation coefficient simply won't cut it. \n\nSo now the next question is where do we go from here?\n\nWe need a solution to measure marketing impact that can also factor in non linear relationships, is robust to outliers and can also factor in causality.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3NGY5yV \n CAC (in most cases) can get impacted by variables other than ad spends. But our correlation coefficient only accounts for ad spends.\n\nIf you are making million dollar marketing decisions, then correlation coefficient simply won't cut it. \n\nSo now the next question is where do we go from here?\n\nWe need a solution to measure marketing impact that can also factor in non linear relationships, is robust to outliers and can also factor in causality.\n\nThe answer - \nhashtag\n#marketingmixmodeling. Plain vanilla regression will have most of the drawbacks as that of correlation. \n\nBut with a lot more bells and whistles (adstocks, robust regression,\ncontrolling for other variables, incrementality and causal experiments), MMM becomes a potent and efficient solution.\n\nSo is MMM a simple method? No. But it is a solution that most marketers need.\n\nAs always, pursuit of simple solutions is great but pls see to it that simple solution adequately solves the problem.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3tz7AcJ \n I find this quote by John Tukey very relevant and intriguing at the same time. When I saw this quote from the lens of building Marketing mix models (MMM), I gained a new perspective.\n\nThe noise cancellation\n\nAt Aryma Labs, we have built and delivered over 350+ MMM models.\n\nThe real marketing environment is full of noise. At the outset, one is not sure which variable moved the needle of your KPI and by how much.\n\nOur first approach when building an MMM model is to control for (include) relevant variables that could have impacted the KPI.\n\nBy controlling for relevant variables in the model, we inherently reduce the noise.\n\nHowever, omitting certain variables could lead to Omitted Variable Bias and endogeneity.\n\nFurther Multicollinearity (even though it is a signal redundancy problem), too much of signal redundancy at some level also becomes noise.\n\nSignal magnification\n\nMMM's main focus is attribution, and we need to magnify the signal to clearly attribute the change in KPI to the respective independent variables.\nBut before we get to this part, we need to cancel out the noise.\n\nIn a way cancelling out the noise indirectly leads to signal amplification.\n\nIn summary: Be it MMM or any modeling, it is better to choose coefficients to cancel out the noise first. \n\nhashtag\n#statistics \nhashtag\n#marketingmixmodeling \nhashtag\n#datascience \nhashtag\n#linearregression",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3tz7Fx3 \n Are you confused about the meaning of 'statistic' in statistics? \n\nYou're not alone. Many blogs and posts on the internet use the term loosely.\n \nIn statistics, a statistic is defined at a sample level, whereas a parameter is defined at a population level. To estimate the parameter, we use a statistic. \n\nOne of the mental models that I use to get these concepts right is to think of these concepts in the form of Matryoshka dolls (a.k.a Russian nested dolls).\n\nPlease note that barring the first doll (Population and sample), I don't intend to showcase them in a way that one is smaller than the other.\n\nRather I would want the readers to see the emphasis on identical nature of the dolls. \n\nWhy focus on that ?\n\nWell lets start with the first doll (Population-Sample). To me this image clearly conveys the point that the sample is representative of the population.\n\nThis is very important because only when your sample is representative of the population, you can make inferences that generalizes at the population level.\n\nOnce your sample is representative of the population, you can then try to estimate your population parameter through the sample statistic.\n\nFor example you can estimate your population mean through sample mean.\n\nFurther this Matryoshka doll mental model also would allow you to see the statistical model and ML models from a new perspective.\n\nFor example, these dolls help me think that the model (small doll) is an abstraction of reality (the bigger doll). \n\nIf your model is well specified, it will look more like the reality you are trying to model. \n\nThis could be Marketing Mix model trying to capture the marketing reality or any model trying to abstract the reality.\n\nhashtag\n#statistics \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vb6OTD \n Poisson Distribution is not about modeling *rare* events.\n\nAlong with 'Normal distribution is everywhere', the myth that Poisson distribution should only be used to model rare events is prevalent. \n\nIs this correct ?\nNope .\n\nMany data science aspirants succumb to this myth and refrain from applying poisson distribution. They somehow think it models 1 in a million event or something. \n\nAs some of you might know, poisson distribution is a single parameter (the lambda) distribution. \n\nThe lambda is the expected value (mean). And it is assumed to be constant. \n\nNow here is where the myth is busted. If you had a constant rate (lambda), is your process really rare ? \n\nOr perhaps we should really qualify our understanding of what is *rare*. \n\nPoisson distribution is used to model occurrence of events in a given span of time. It is related to binomial distribution and can be approximated by Normal distribution when n-> . \n\nIf you want a better intuition into poisson distribution, I would highly recommend reading this article (Link in comments) by Aerin Kim.\n\nhashtag\n#statistics \nhashtag\n#datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48cOrfE \n Why confidence intervals in a way are barometer of your Marketing Mix Models (MMM)?\n\nA lot of people blame Confidence Intervals and its 'unintuitive' nature for switching to Bayesian side of things.\n\nBut if you are in Marketing Mix modeling domain, frequentist concept of confidence interval makes more sense.\n\nBefore I elaborate, let me provide a quick recap of what exactly is Confidence Interval.\n\n What is confidence Interval?\n\nLets say we are talking about the popular 95% CI.\n\nThe definition of it would be - If one ran the same statistical test taking different samples and constructed a confidence interval each time, \nthen in 95% of the cases, the confidence interval so constructed for that sample will contain the true parameter.\n\nNow that we have got the correct definition, lets answer the question of 'Confidence' in confidence Interval.\n\nTo understand what the word 'confidence' signifies, we need to first consider another word - 'Coverage'.\n\n Confidence Interval is all about coverage. \n\nThat is, if one ran the same statistical test 90 times or 95 times by taking different samples and constructed a confidence interval each time, \nwould they find the parameter of interest in those intervals each time. \n\n What Confidence Interval tells you about MMM that Credible Interval does not?\n\nI like to think of MMM as an apparatus that is trying to capture the true Marketing ROI. The data that goes into modeling is a sample from the marketing reality (population).\n\nThis data contains the true marketing ROI and your MMM is the experiment that either captures the true marketing ROI or not. \n\nNow as a client, you would want to know how well this apparatus (MMM) is constructed and does it capture the true marketing ROI most of the time? \n\nConfidence Intervals thus in a way informs you about the construct of the MMM model and its reliability.\n\n Now what about credible intervals?\n\nWell credible interval tells you the probability with which the parameter of interest lies in the particular interval. Now this does not tell us anything about the build quality of the MMM process or design of experiment in general. \n\n In summary: Confidence Intervals are a barometer of your MMM process itself. So, if you are in the MMM domain, do not shun Confidence Intervals. Instead, embrace them as a critical tool for evaluating your model's reliability. \n\nhashtag\n#statistics \nhashtag\n#marketingmixmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/48vf6Er \n Looking to get started on MMM? Know your type of Data.\n\nOne key advice I give to statisticians/data scientists looking to get started on Marketing Mix Modeling (MMM) is - Know your type of Data.\n\nA big chunk of the projects in the industry involves dealing with tabular data. \n\nThis is particularly true of Marketing Measurement and Attribution Industry.\n\nHowever, not many have the knowledge of Time Series Data, Cross Sectional Data and Panel Data.\n\nAll these types of data have a temporal component. \n\nIt is thus very important to know the subtleties because the type of regression that you would apply will vary slightly depending on the data and business objective at hand.\n\nSo here is a quick explainer:\n\n1.Time series\nWe are collecting data about Brand 'XY1' sales over the years\n\nYear  sales\n2017 50,000\n2018 54,000\n2019 65,000\n2020 45,000\n\n Key Difference: Notice above that the data is collected for the same entity 'Brand XY1' but at different intervals of time.\n\n2. Cross section data\nNow we are collecting not only Brand XY1 data but also its rivals. However we are just interested in the year 2020.\n\nBrand  Year   Sales\nXY1   2020  45,000\nEL3    2020  23,000\nFT3    2020  56,000\nTY4    2020  24,000\n\n Key Difference: Notice, we are interested in data at a particular point in time (i.e. yr 2020). This interest in particular point in time distinguishes Cross section from Time series.\n\n3. Panel Data:\nPanel data is what you get if you mix both the Time series and Cross Sectional data. Lets say that now you want to track brand XY1 and its rivals sales over the years. This results in data like below:\n\nBrand Year Sales\nXY1 2017 50,000\nXY1 2018 54,000\nEL3 2017 12,000\nEL3 2018 23,000\nFT3 2017 32,000\nFT3 2018 35,000\n\nIn summary: In MMM, attribution is the name of the game in MMM. It is hence very important to know to which time period you are attributing. Developing a good understanding of the data is one of the key steps in specifying a good MMM model.\n\nI would highly recommend Woolridge's econometrics book and Rob Hyndman's Forecasting book to anybody looking to get started on MMM. Link to the books in comments.\n\nhashtag\n#statistics \nhashtag\n#timeseriesforecasting \nhashtag\n#marketingmixmodeling",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vgKNDb \n Don't Stepwise Regression your MMM model.\n\nSo recently a client hired us to build MMM models for them after failed attempts to in-house the MMM capability. \n\nEarlier their in-house Machine Learning engineers (with no statistics background) had built their MMM models thinking that it is just 'linear regression'.\n\nWe sat down with the MLEs and wanted to know how they went about building the models. \n\nThe MLEs said they chose variables based on 'Trial and Error'. Which then turned out to be backward stepwise regression. If you want a refresher on what is stepwise regression (links are in comments).\n\nFurther we noticed that they had not applied proper adstock on any of\ntheir media variables (but this could be a topic for another post).\n\nBut coming back to the topic, stepwise regression is a recipe for disaster if your goal is attribution and inference.\n\nMMM is all about attribution.\n\nI will first highlight the problems stepwise regression cause to your model. The below are taken from Frank Harrell's book Regression Modeling strategies.\n\n1. It yields R-squared values that are badly biased to be high.\n\n2. The F and chi-squared tests quoted next to each variable on the printout do not have the claimed distribution.\n\n3. The method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\n\n4. It yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\n\n5. It gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani [1996]).\n\n6. It has severe problems in the presence of collinearity.\n\n7. It is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\n\n8. Increasing the sample size does not help very much; see Derksen and Keselman (1992).\n\nFinally this one is my favourite \n\n9. It allows us to not think about the problem.\n\nIf you notice above, one of the main problem caused by stepwise regression is Bias. Bias is kryptonite of attribution. If your estimates are biased, you will no longer have a accurate picture of your marketing ROI.\n\nOur humble suggestions to companies looking to in-house MMM is this:\n\n Hire trained statisticians with experience in MMM and good business understanding.\n\n MMM is not just Linear Regression\n\n MMM can't be built without proper statistical knowledge.",
    "metadata": {
      "source": "LinkedIn"
    }
  },
  {
    "content": "LinkedIn Link: https://bit.ly/3vgKNDb \n If you notice above, one of the main problem caused by stepwise regression is Bias. Bias is kryptonite of attribution. If your estimates are biased, you will no longer have a accurate picture of your marketing ROI.\n\nOur humble suggestions to companies looking to in-house MMM is this:\n\n Hire trained statisticians with experience in MMM and good business understanding.\n\n MMM is not just Linear Regression\n\n MMM can't be built without proper statistical knowledge.\n\n MMM is not just 'click-one-button-get-all-results' exercise. Anybody selling you such tool is fooling you.\n\nIf you have trouble in-housing MMM, just hire experienced hands like Aryma Labs. We know our MMMs. \n\nLink to resources on stepwise regression and alternatives for it are in comments.\n\n#marketingmixmodeling #statistics #datascience",
    "metadata": {
      "source": "LinkedIn"
    }
  }
]